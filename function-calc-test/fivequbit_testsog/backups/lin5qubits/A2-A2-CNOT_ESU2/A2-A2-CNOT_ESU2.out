/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Sun Mar 24 17:52:01 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 17:53:12 2024]  Iteration number: 0 with current cost as 0.5266736777098473 and parameters 
[-1.50725454  2.23743464 -2.1242794  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.1855201  -1.06648308]. 
Working on 0.4 fold... 
[Sun Mar 24 17:55:40 2024]  Iteration number: 0 with current cost as 0.21738920405150636 and parameters 
[ 1.20231152  2.23743447 -2.12427964 -0.11653103  0.55388691 -2.77010914
  3.06858465  2.18960145  1.18551998 -1.06648342]. 
Working on 0.6 fold... 
[Sun Mar 24 17:58:29 2024]  Iteration number: 0 with current cost as 0.21359071537449859 and parameters 
[ 1.24075897  2.23743464 -2.12427943 -0.11653082  0.55388708 -2.77010918
  3.06858477  2.18960145  1.1855202  -1.0664833 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:01:12 2024]  Iteration number: 0 with current cost as 0.21892701612491255 and parameters 
[ 1.17066511  2.23743443 -2.12427943 -0.11653123  0.55388708 -2.77010918
  3.06858477  2.18960145  1.18552019 -1.06648329]. 
Working on 1.0 fold... 
[Sun Mar 24 18:04:20 2024]  Iteration number: 0 with current cost as 0.2150946344684368 and parameters 
[ 1.34838651  2.23743484 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18552019 -1.0664835 ]. 
Training complete taking 780.2293274402618 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 18:06:10 2024]  Iteration number: 0 with current cost as 0.5266736777098473 and parameters 
[-1.50725454  2.23743464 -2.1242794  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.1855201  -1.06648308]. 
Working on 0.4 fold... 
[Sun Mar 24 18:08:39 2024]  Iteration number: 0 with current cost as 0.21738920405150636 and parameters 
[ 1.20231152  2.23743447 -2.12427964 -0.11653103  0.55388691 -2.77010914
  3.06858465  2.18960145  1.18551998 -1.06648342]. 
Working on 0.6 fold... 
[Sun Mar 24 18:11:22 2024]  Iteration number: 0 with current cost as 0.21359071537449859 and parameters 
[ 1.24075897  2.23743464 -2.12427943 -0.11653082  0.55388708 -2.77010918
  3.06858477  2.18960145  1.1855202  -1.0664833 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:14:03 2024]  Iteration number: 0 with current cost as 0.21892701612491255 and parameters 
[ 1.17066511  2.23743443 -2.12427943 -0.11653123  0.55388708 -2.77010918
  3.06858477  2.18960145  1.18552019 -1.06648329]. 
Working on 1.0 fold... 
[Sun Mar 24 18:17:01 2024]  Iteration number: 0 with current cost as 0.2150946344684368 and parameters 
[ 1.34838651  2.23743484 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18552019 -1.0664835 ]. 
Training complete taking 759.3571450710297 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 18:18:48 2024]  Iteration number: 0 with current cost as 0.5266736777098473 and parameters 
[-1.50725454  2.23743464 -2.1242794  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.1855201  -1.06648308]. 
Working on 0.4 fold... 
[Sun Mar 24 18:21:23 2024]  Iteration number: 0 with current cost as 0.21738920405150636 and parameters 
[ 1.20231152  2.23743447 -2.12427964 -0.11653103  0.55388691 -2.77010914
  3.06858465  2.18960145  1.18551998 -1.06648342]. 
Working on 0.6 fold... 
[Sun Mar 24 18:24:13 2024]  Iteration number: 0 with current cost as 0.21359071537449859 and parameters 
[ 1.24075897  2.23743464 -2.12427943 -0.11653082  0.55388708 -2.77010918
  3.06858477  2.18960145  1.1855202  -1.0664833 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:26:50 2024]  Iteration number: 0 with current cost as 0.21892701612491255 and parameters 
[ 1.17066511  2.23743443 -2.12427943 -0.11653123  0.55388708 -2.77010918
  3.06858477  2.18960145  1.18552019 -1.06648329]. 
Working on 1.0 fold... 
[Sun Mar 24 18:29:47 2024]  Iteration number: 0 with current cost as 0.2150946344684368 and parameters 
[ 1.34838651  2.23743484 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18552019 -1.0664835 ]. 
Training complete taking 765.7391102313995 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 18:31:33 2024]  Iteration number: 0 with current cost as 0.5266736777098473 and parameters 
[-1.50725454  2.23743464 -2.1242794  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.1855201  -1.06648308]. 
Working on 0.4 fold... 
[Sun Mar 24 18:34:04 2024]  Iteration number: 0 with current cost as 0.21738920405150636 and parameters 
[ 1.20231152  2.23743447 -2.12427964 -0.11653103  0.55388691 -2.77010914
  3.06858465  2.18960145  1.18551998 -1.06648342]. 
Working on 0.6 fold... 
[Sun Mar 24 18:36:38 2024]  Iteration number: 0 with current cost as 0.21359071537449859 and parameters 
[ 1.24075897  2.23743464 -2.12427943 -0.11653082  0.55388708 -2.77010918
  3.06858477  2.18960145  1.1855202  -1.0664833 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:39:21 2024]  Iteration number: 0 with current cost as 0.21892701612491255 and parameters 
[ 1.17066511  2.23743443 -2.12427943 -0.11653123  0.55388708 -2.77010918
  3.06858477  2.18960145  1.18552019 -1.06648329]. 
Working on 1.0 fold... 
[Sun Mar 24 18:42:18 2024]  Iteration number: 0 with current cost as 0.2150946344684368 and parameters 
[ 1.34838651  2.23743484 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18552019 -1.0664835 ]. 
Training complete taking 750.6441059112549 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 18:44:07 2024]  Iteration number: 0 with current cost as 0.5266736777098473 and parameters 
[-1.50725454  2.23743464 -2.1242794  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.1855201  -1.06648308]. 
Working on 0.4 fold... 
[Sun Mar 24 18:46:40 2024]  Iteration number: 0 with current cost as 0.21738920405150636 and parameters 
[ 1.20231152  2.23743447 -2.12427964 -0.11653103  0.55388691 -2.77010914
  3.06858465  2.18960145  1.18551998 -1.06648342]. 
Working on 0.6 fold... 
[Sun Mar 24 18:49:23 2024]  Iteration number: 0 with current cost as 0.21359071537449859 and parameters 
[ 1.24075897  2.23743464 -2.12427943 -0.11653082  0.55388708 -2.77010918
  3.06858477  2.18960145  1.1855202  -1.0664833 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:52:04 2024]  Iteration number: 0 with current cost as 0.21892701612491255 and parameters 
[ 1.17066511  2.23743443 -2.12427943 -0.11653123  0.55388708 -2.77010918
  3.06858477  2.18960145  1.18552019 -1.06648329]. 
Working on 1.0 fold... 
[Sun Mar 24 18:55:03 2024]  Iteration number: 0 with current cost as 0.2150946344684368 and parameters 
[ 1.34838651  2.23743484 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18552019 -1.0664835 ]. 
Training complete taking 767.5650413036346 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 18:56:54 2024]  Iteration number: 0 with current cost as 0.5266736777098473 and parameters 
[-1.50725454  2.23743464 -2.1242794  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.1855201  -1.06648308]. 
Working on 0.4 fold... 
[Sun Mar 24 18:59:31 2024]  Iteration number: 0 with current cost as 0.21738920405150636 and parameters 
[ 1.20231152  2.23743447 -2.12427964 -0.11653103  0.55388691 -2.77010914
  3.06858465  2.18960145  1.18551998 -1.06648342]. 
Working on 0.6 fold... 
[Sun Mar 24 19:02:14 2024]  Iteration number: 0 with current cost as 0.21359071537449859 and parameters 
[ 1.24075897  2.23743464 -2.12427943 -0.11653082  0.55388708 -2.77010918
  3.06858477  2.18960145  1.1855202  -1.0664833 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:04:58 2024]  Iteration number: 0 with current cost as 0.21892701612491255 and parameters 
[ 1.17066511  2.23743443 -2.12427943 -0.11653123  0.55388708 -2.77010918
  3.06858477  2.18960145  1.18552019 -1.06648329]. 
Working on 1.0 fold... 
[Sun Mar 24 19:08:07 2024]  Iteration number: 0 with current cost as 0.2150946344684368 and parameters 
[ 1.34838651  2.23743484 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18552019 -1.0664835 ]. 
Training complete taking 785.0335605144501 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 19:10:04 2024]  Iteration number: 0 with current cost as 0.5266736777098473 and parameters 
[-1.50725454  2.23743464 -2.1242794  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.1855201  -1.06648308]. 
Working on 0.4 fold... 
[Sun Mar 24 19:12:44 2024]  Iteration number: 0 with current cost as 0.21738920405150636 and parameters 
[ 1.20231152  2.23743447 -2.12427964 -0.11653103  0.55388691 -2.77010914
  3.06858465  2.18960145  1.18551998 -1.06648342]. 
Working on 0.6 fold... 
[Sun Mar 24 19:15:35 2024]  Iteration number: 0 with current cost as 0.21359071537449859 and parameters 
[ 1.24075897  2.23743464 -2.12427943 -0.11653082  0.55388708 -2.77010918
  3.06858477  2.18960145  1.1855202  -1.0664833 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:18:31 2024]  Iteration number: 0 with current cost as 0.21892701612491255 and parameters 
[ 1.17066511  2.23743443 -2.12427943 -0.11653123  0.55388708 -2.77010918
  3.06858477  2.18960145  1.18552019 -1.06648329]. 
Working on 1.0 fold... 
[Sun Mar 24 19:21:36 2024]  Iteration number: 0 with current cost as 0.2150946344684368 and parameters 
[ 1.34838651  2.23743484 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18552019 -1.0664835 ]. 
Training complete taking 812.7354912757874 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 19:23:33 2024]  Iteration number: 0 with current cost as 0.5266736777098473 and parameters 
[-1.50725454  2.23743464 -2.1242794  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.1855201  -1.06648308]. 
Working on 0.4 fold... 
[Sun Mar 24 19:26:14 2024]  Iteration number: 0 with current cost as 0.21738920405150636 and parameters 
[ 1.20231152  2.23743447 -2.12427964 -0.11653103  0.55388691 -2.77010914
  3.06858465  2.18960145  1.18551998 -1.06648342]. 
Working on 0.6 fold... 
[Sun Mar 24 19:29:23 2024]  Iteration number: 0 with current cost as 0.21359071537449859 and parameters 
[ 1.24075897  2.23743464 -2.12427943 -0.11653082  0.55388708 -2.77010918
  3.06858477  2.18960145  1.1855202  -1.0664833 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:32:00 2024]  Iteration number: 0 with current cost as 0.21892701612491255 and parameters 
[ 1.17066511  2.23743443 -2.12427943 -0.11653123  0.55388708 -2.77010918
  3.06858477  2.18960145  1.18552019 -1.06648329]. 
Working on 1.0 fold... 
[Sun Mar 24 19:34:59 2024]  Iteration number: 0 with current cost as 0.2150946344684368 and parameters 
[ 1.34838651  2.23743484 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18552019 -1.0664835 ]. 
Training complete taking 793.45077085495 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 19:36:39 2024]  Iteration number: 0 with current cost as 0.5266736777098473 and parameters 
[-1.50725454  2.23743464 -2.1242794  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.1855201  -1.06648308]. 
Working on 0.4 fold... 
[Sun Mar 24 19:38:59 2024]  Iteration number: 0 with current cost as 0.21738920405150636 and parameters 
[ 1.20231152  2.23743447 -2.12427964 -0.11653103  0.55388691 -2.77010914
  3.06858465  2.18960145  1.18551998 -1.06648342]. 
Working on 0.6 fold... 
[Sun Mar 24 19:41:32 2024]  Iteration number: 0 with current cost as 0.21359071537449859 and parameters 
[ 1.24075897  2.23743464 -2.12427943 -0.11653082  0.55388708 -2.77010918
  3.06858477  2.18960145  1.1855202  -1.0664833 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:44:07 2024]  Iteration number: 0 with current cost as 0.21892701612491255 and parameters 
[ 1.17066511  2.23743443 -2.12427943 -0.11653123  0.55388708 -2.77010918
  3.06858477  2.18960145  1.18552019 -1.06648329]. 
Working on 1.0 fold... 
[Sun Mar 24 19:47:04 2024]  Iteration number: 0 with current cost as 0.2150946344684368 and parameters 
[ 1.34838651  2.23743484 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18552019 -1.0664835 ]. 
Training complete taking 726.7187216281891 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 19:48:49 2024]  Iteration number: 0 with current cost as 0.5266736777098473 and parameters 
[-1.50725454  2.23743464 -2.1242794  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.1855201  -1.06648308]. 
Working on 0.4 fold... 
[Sun Mar 24 19:51:16 2024]  Iteration number: 0 with current cost as 0.21738920405150636 and parameters 
[ 1.20231152  2.23743447 -2.12427964 -0.11653103  0.55388691 -2.77010914
  3.06858465  2.18960145  1.18551998 -1.06648342]. 
Working on 0.6 fold... 
[Sun Mar 24 19:53:52 2024]  Iteration number: 0 with current cost as 0.21359071537449859 and parameters 
[ 1.24075897  2.23743464 -2.12427943 -0.11653082  0.55388708 -2.77010918
  3.06858477  2.18960145  1.1855202  -1.0664833 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:56:34 2024]  Iteration number: 0 with current cost as 0.21892701612491255 and parameters 
[ 1.17066511  2.23743443 -2.12427943 -0.11653123  0.55388708 -2.77010918
  3.06858477  2.18960145  1.18552019 -1.06648329]. 
Working on 1.0 fold... 
[Sun Mar 24 19:59:29 2024]  Iteration number: 0 with current cost as 0.2150946344684368 and parameters 
[ 1.34838651  2.23743484 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18552019 -1.0664835 ]. 
Training complete taking 747.1520884037018 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 20:01:13 2024]  Iteration number: 0 with current cost as 0.5266736777098473 and parameters 
[-1.50725454  2.23743464 -2.1242794  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.1855201  -1.06648308]. 
Working on 0.4 fold... 
[Sun Mar 24 20:03:33 2024]  Iteration number: 0 with current cost as 0.21738920405150636 and parameters 
[ 1.20231152  2.23743447 -2.12427964 -0.11653103  0.55388691 -2.77010914
  3.06858465  2.18960145  1.18551998 -1.06648342]. 
Working on 0.6 fold... 
[Sun Mar 24 20:06:09 2024]  Iteration number: 0 with current cost as 0.21359071537449859 and parameters 
[ 1.24075897  2.23743464 -2.12427943 -0.11653082  0.55388708 -2.77010918
  3.06858477  2.18960145  1.1855202  -1.0664833 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:08:47 2024]  Iteration number: 0 with current cost as 0.21892701612491255 and parameters 
[ 1.17066511  2.23743443 -2.12427943 -0.11653123  0.55388708 -2.77010918
  3.06858477  2.18960145  1.18552019 -1.06648329]. 
Working on 1.0 fold... 
[Sun Mar 24 20:11:30 2024]  Iteration number: 0 with current cost as 0.2150946344684368 and parameters 
[ 1.34838651  2.23743484 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18552019 -1.0664835 ]. 
Training complete taking 718.5512697696686 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 20:13:13 2024]  Iteration number: 0 with current cost as 0.5266736777098473 and parameters 
[-1.50725454  2.23743464 -2.1242794  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.1855201  -1.06648308]. 
Working on 0.4 fold... 
[Sun Mar 24 20:15:33 2024]  Iteration number: 0 with current cost as 0.21738920405150636 and parameters 
[ 1.20231152  2.23743447 -2.12427964 -0.11653103  0.55388691 -2.77010914
  3.06858465  2.18960145  1.18551998 -1.06648342]. 
Working on 0.6 fold... 
[Sun Mar 24 20:18:09 2024]  Iteration number: 0 with current cost as 0.21359071537449859 and parameters 
[ 1.24075897  2.23743464 -2.12427943 -0.11653082  0.55388708 -2.77010918
  3.06858477  2.18960145  1.1855202  -1.0664833 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:20:43 2024]  Iteration number: 0 with current cost as 0.21892701612491255 and parameters 
[ 1.17066511  2.23743443 -2.12427943 -0.11653123  0.55388708 -2.77010918
  3.06858477  2.18960145  1.18552019 -1.06648329]. 
Working on 1.0 fold... 
[Sun Mar 24 20:23:37 2024]  Iteration number: 0 with current cost as 0.2150946344684368 and parameters 
[ 1.34838651  2.23743484 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18552019 -1.0664835 ]. 
Training complete taking 725.6607375144958 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 20:25:19 2024]  Iteration number: 0 with current cost as 0.5266736777098473 and parameters 
[-1.50725454  2.23743464 -2.1242794  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.1855201  -1.06648308]. 
Working on 0.4 fold... 
[Sun Mar 24 20:27:42 2024]  Iteration number: 0 with current cost as 0.21738920405150636 and parameters 
[ 1.20231152  2.23743447 -2.12427964 -0.11653103  0.55388691 -2.77010914
  3.06858465  2.18960145  1.18551998 -1.06648342]. 
Working on 0.6 fold... 
[Sun Mar 24 20:30:18 2024]  Iteration number: 0 with current cost as 0.21359071537449859 and parameters 
[ 1.24075897  2.23743464 -2.12427943 -0.11653082  0.55388708 -2.77010918
  3.06858477  2.18960145  1.1855202  -1.0664833 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:32:52 2024]  Iteration number: 0 with current cost as 0.21892701612491255 and parameters 
[ 1.17066511  2.23743443 -2.12427943 -0.11653123  0.55388708 -2.77010918
  3.06858477  2.18960145  1.18552019 -1.06648329]. 
Working on 1.0 fold... 
[Sun Mar 24 20:35:43 2024]  Iteration number: 0 with current cost as 0.2150946344684368 and parameters 
[ 1.34838651  2.23743484 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18552019 -1.0664835 ]. 
Training complete taking 725.3930456638336 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 20:37:23 2024]  Iteration number: 0 with current cost as 0.5266736777098473 and parameters 
[-1.50725454  2.23743464 -2.1242794  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.1855201  -1.06648308]. 
Working on 0.4 fold... 
[Sun Mar 24 20:39:44 2024]  Iteration number: 0 with current cost as 0.21738920405150636 and parameters 
[ 1.20231152  2.23743447 -2.12427964 -0.11653103  0.55388691 -2.77010914
  3.06858465  2.18960145  1.18551998 -1.06648342]. 
Working on 0.6 fold... 
[Sun Mar 24 20:42:19 2024]  Iteration number: 0 with current cost as 0.21359071537449859 and parameters 
[ 1.24075897  2.23743464 -2.12427943 -0.11653082  0.55388708 -2.77010918
  3.06858477  2.18960145  1.1855202  -1.0664833 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:44:56 2024]  Iteration number: 0 with current cost as 0.21892701612491255 and parameters 
[ 1.17066511  2.23743443 -2.12427943 -0.11653123  0.55388708 -2.77010918
  3.06858477  2.18960145  1.18552019 -1.06648329]. 
Working on 1.0 fold... 
[Sun Mar 24 20:47:40 2024]  Iteration number: 0 with current cost as 0.2150946344684368 and parameters 
[ 1.34838651  2.23743484 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18552019 -1.0664835 ]. 
Training complete taking 722.235951423645 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 20:49:26 2024]  Iteration number: 0 with current cost as 0.5266736777098473 and parameters 
[-1.50725454  2.23743464 -2.1242794  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.1855201  -1.06648308]. 
Working on 0.4 fold... 
[Sun Mar 24 20:51:41 2024]  Iteration number: 0 with current cost as 0.21738920405150636 and parameters 
[ 1.20231152  2.23743447 -2.12427964 -0.11653103  0.55388691 -2.77010914
  3.06858465  2.18960145  1.18551998 -1.06648342]. 
Working on 0.6 fold... 
[Sun Mar 24 20:54:19 2024]  Iteration number: 0 with current cost as 0.21359071537449859 and parameters 
[ 1.24075897  2.23743464 -2.12427943 -0.11653082  0.55388708 -2.77010918
  3.06858477  2.18960145  1.1855202  -1.0664833 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:57:00 2024]  Iteration number: 0 with current cost as 0.21892701612491255 and parameters 
[ 1.17066511  2.23743443 -2.12427943 -0.11653123  0.55388708 -2.77010918
  3.06858477  2.18960145  1.18552019 -1.06648329]. 
Working on 1.0 fold... 
[Sun Mar 24 20:59:51 2024]  Iteration number: 0 with current cost as 0.2150946344684368 and parameters 
[ 1.34838651  2.23743484 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18552019 -1.0664835 ]. 
Training complete taking 726.2210087776184 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 21:01:34 2024]  Iteration number: 0 with current cost as 0.5266736777098473 and parameters 
[-1.50725454  2.23743464 -2.1242794  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.1855201  -1.06648308]. 
Working on 0.4 fold... 
[Sun Mar 24 21:04:02 2024]  Iteration number: 0 with current cost as 0.21738920405150636 and parameters 
[ 1.20231152  2.23743447 -2.12427964 -0.11653103  0.55388691 -2.77010914
  3.06858465  2.18960145  1.18551998 -1.06648342]. 
Working on 0.6 fold... 
[Sun Mar 24 21:06:32 2024]  Iteration number: 0 with current cost as 0.21359071537449859 and parameters 
[ 1.24075897  2.23743464 -2.12427943 -0.11653082  0.55388708 -2.77010918
  3.06858477  2.18960145  1.1855202  -1.0664833 ]. 
Working on 0.8 fold... 
[Sun Mar 24 21:09:01 2024]  Iteration number: 0 with current cost as 0.21892701612491255 and parameters 
[ 1.17066511  2.23743443 -2.12427943 -0.11653123  0.55388708 -2.77010918
  3.06858477  2.18960145  1.18552019 -1.06648329]. 
Working on 1.0 fold... 
[Sun Mar 24 21:11:53 2024]  Iteration number: 0 with current cost as 0.2150946344684368 and parameters 
[ 1.34838651  2.23743484 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18552019 -1.0664835 ]. 
Training complete taking 727.5800178050995 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 21:13:38 2024]  Iteration number: 0 with current cost as 0.5266736777098473 and parameters 
[-1.50725454  2.23743464 -2.1242794  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.1855201  -1.06648308]. 
Working on 0.4 fold... 
[Sun Mar 24 21:16:05 2024]  Iteration number: 0 with current cost as 0.21738920405150636 and parameters 
[ 1.20231152  2.23743447 -2.12427964 -0.11653103  0.55388691 -2.77010914
  3.06858465  2.18960145  1.18551998 -1.06648342]. 
Working on 0.6 fold... 
[Sun Mar 24 21:18:59 2024]  Iteration number: 0 with current cost as 0.21359071537449859 and parameters 
[ 1.24075897  2.23743464 -2.12427943 -0.11653082  0.55388708 -2.77010918
  3.06858477  2.18960145  1.1855202  -1.0664833 ]. 
Working on 0.8 fold... 
[Sun Mar 24 21:21:44 2024]  Iteration number: 0 with current cost as 0.21892701612491255 and parameters 
[ 1.17066511  2.23743443 -2.12427943 -0.11653123  0.55388708 -2.77010918
  3.06858477  2.18960145  1.18552019 -1.06648329]. 
Working on 1.0 fold... 
[Sun Mar 24 21:24:48 2024]  Iteration number: 0 with current cost as 0.2150946344684368 and parameters 
[ 1.34838651  2.23743484 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18552019 -1.0664835 ]. 
Training complete taking 773.2186090946198 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 21:26:39 2024]  Iteration number: 0 with current cost as 0.5266736777098473 and parameters 
[-1.50725454  2.23743464 -2.1242794  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.1855201  -1.06648308]. 
Working on 0.4 fold... 
[Sun Mar 24 21:29:21 2024]  Iteration number: 0 with current cost as 0.21738920405150636 and parameters 
[ 1.20231152  2.23743447 -2.12427964 -0.11653103  0.55388691 -2.77010914
  3.06858465  2.18960145  1.18551998 -1.06648342]. 
Working on 0.6 fold... 
[Sun Mar 24 21:32:14 2024]  Iteration number: 0 with current cost as 0.21359071537449859 and parameters 
[ 1.24075897  2.23743464 -2.12427943 -0.11653082  0.55388708 -2.77010918
  3.06858477  2.18960145  1.1855202  -1.0664833 ]. 
Working on 0.8 fold... 
[Sun Mar 24 21:34:45 2024]  Iteration number: 0 with current cost as 0.21892701612491255 and parameters 
[ 1.17066511  2.23743443 -2.12427943 -0.11653123  0.55388708 -2.77010918
  3.06858477  2.18960145  1.18552019 -1.06648329]. 
Working on 1.0 fold... 
[Sun Mar 24 21:37:32 2024]  Iteration number: 0 with current cost as 0.2150946344684368 and parameters 
[ 1.34838651  2.23743484 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18552019 -1.0664835 ]. 
Training complete taking 761.5646066665649 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 21:39:14 2024]  Iteration number: 0 with current cost as 0.5266736777098473 and parameters 
[-1.50725454  2.23743464 -2.1242794  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.1855201  -1.06648308]. 
Working on 0.4 fold... 
[Sun Mar 24 21:41:42 2024]  Iteration number: 0 with current cost as 0.21738920405150636 and parameters 
[ 1.20231152  2.23743447 -2.12427964 -0.11653103  0.55388691 -2.77010914
  3.06858465  2.18960145  1.18551998 -1.06648342]. 
Working on 0.6 fold... 
[Sun Mar 24 21:44:18 2024]  Iteration number: 0 with current cost as 0.21359071537449859 and parameters 
[ 1.24075897  2.23743464 -2.12427943 -0.11653082  0.55388708 -2.77010918
  3.06858477  2.18960145  1.1855202  -1.0664833 ]. 
Working on 0.8 fold... 
[Sun Mar 24 21:46:52 2024]  Iteration number: 0 with current cost as 0.21892701612491255 and parameters 
[ 1.17066511  2.23743443 -2.12427943 -0.11653123  0.55388708 -2.77010918
  3.06858477  2.18960145  1.18552019 -1.06648329]. 
Working on 1.0 fold... 
[Sun Mar 24 21:49:44 2024]  Iteration number: 0 with current cost as 0.2150946344684368 and parameters 
[ 1.34838651  2.23743484 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18552019 -1.0664835 ]. 
Training complete taking 736.5916247367859 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 21:51:33 2024]  Iteration number: 0 with current cost as 0.5266736777098473 and parameters 
[-1.50725454  2.23743464 -2.1242794  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.1855201  -1.06648308]. 
Working on 0.4 fold... 
[Sun Mar 24 21:53:56 2024]  Iteration number: 0 with current cost as 0.21738920405150636 and parameters 
[ 1.20231152  2.23743447 -2.12427964 -0.11653103  0.55388691 -2.77010914
  3.06858465  2.18960145  1.18551998 -1.06648342]. 
Working on 0.6 fold... 
[Sun Mar 24 21:56:25 2024]  Iteration number: 0 with current cost as 0.21359071537449859 and parameters 
[ 1.24075897  2.23743464 -2.12427943 -0.11653082  0.55388708 -2.77010918
  3.06858477  2.18960145  1.1855202  -1.0664833 ]. 
Working on 0.8 fold... 
[Sun Mar 24 21:58:56 2024]  Iteration number: 0 with current cost as 0.21892701612491255 and parameters 
[ 1.17066511  2.23743443 -2.12427943 -0.11653123  0.55388708 -2.77010918
  3.06858477  2.18960145  1.18552019 -1.06648329]. 
Working on 1.0 fold... 
[Sun Mar 24 22:01:40 2024]  Iteration number: 0 with current cost as 0.2150946344684368 and parameters 
[ 1.34838651  2.23743484 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18552019 -1.0664835 ]. 
Training complete taking 716.0436515808105 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 22:03:27 2024]  Iteration number: 0 with current cost as 0.5266736777098473 and parameters 
[-1.50725454  2.23743464 -2.1242794  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.1855201  -1.06648308]. 
Working on 0.4 fold... 
[Sun Mar 24 22:05:47 2024]  Iteration number: 0 with current cost as 0.21738920405150636 and parameters 
[ 1.20231152  2.23743447 -2.12427964 -0.11653103  0.55388691 -2.77010914
  3.06858465  2.18960145  1.18551998 -1.06648342]. 
Working on 0.6 fold... 
[Sun Mar 24 22:08:23 2024]  Iteration number: 0 with current cost as 0.21359071537449859 and parameters 
[ 1.24075897  2.23743464 -2.12427943 -0.11653082  0.55388708 -2.77010918
  3.06858477  2.18960145  1.1855202  -1.0664833 ]. 
Working on 0.8 fold... 
[Sun Mar 24 22:11:01 2024]  Iteration number: 0 with current cost as 0.21892701612491255 and parameters 
[ 1.17066511  2.23743443 -2.12427943 -0.11653123  0.55388708 -2.77010918
  3.06858477  2.18960145  1.18552019 -1.06648329]. 
Working on 1.0 fold... 
[Sun Mar 24 22:13:43 2024]  Iteration number: 0 with current cost as 0.2150946344684368 and parameters 
[ 1.34838651  2.23743484 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18552019 -1.0664835 ]. 
Training complete taking 719.4752621650696 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 22:15:25 2024]  Iteration number: 0 with current cost as 0.5266736777098473 and parameters 
[-1.50725454  2.23743464 -2.1242794  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.1855201  -1.06648308]. 
Working on 0.4 fold... 
[Sun Mar 24 22:17:44 2024]  Iteration number: 0 with current cost as 0.21738920405150636 and parameters 
[ 1.20231152  2.23743447 -2.12427964 -0.11653103  0.55388691 -2.77010914
  3.06858465  2.18960145  1.18551998 -1.06648342]. 
Working on 0.6 fold... 
[Sun Mar 24 22:20:16 2024]  Iteration number: 0 with current cost as 0.21359071537449859 and parameters 
[ 1.24075897  2.23743464 -2.12427943 -0.11653082  0.55388708 -2.77010918
  3.06858477  2.18960145  1.1855202  -1.0664833 ]. 
Working on 0.8 fold... 
[Sun Mar 24 22:22:51 2024]  Iteration number: 0 with current cost as 0.21892701612491255 and parameters 
[ 1.17066511  2.23743443 -2.12427943 -0.11653123  0.55388708 -2.77010918
  3.06858477  2.18960145  1.18552019 -1.06648329]. 
Working on 1.0 fold... 
[Sun Mar 24 22:25:41 2024]  Iteration number: 0 with current cost as 0.2150946344684368 and parameters 
[ 1.34838651  2.23743484 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18552019 -1.0664835 ]. 
Training complete taking 715.4427306652069 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 22:27:23 2024]  Iteration number: 0 with current cost as 0.5266736777098473 and parameters 
[-1.50725454  2.23743464 -2.1242794  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.1855201  -1.06648308]. 
Working on 0.4 fold... 
[Sun Mar 24 22:29:40 2024]  Iteration number: 0 with current cost as 0.21738920405150636 and parameters 
[ 1.20231152  2.23743447 -2.12427964 -0.11653103  0.55388691 -2.77010914
  3.06858465  2.18960145  1.18551998 -1.06648342]. 
Working on 0.6 fold... 
[Sun Mar 24 22:32:13 2024]  Iteration number: 0 with current cost as 0.21359071537449859 and parameters 
[ 1.24075897  2.23743464 -2.12427943 -0.11653082  0.55388708 -2.77010918
  3.06858477  2.18960145  1.1855202  -1.0664833 ]. 
Working on 0.8 fold... 
[Sun Mar 24 22:34:49 2024]  Iteration number: 0 with current cost as 0.21892701612491255 and parameters 
[ 1.17066511  2.23743443 -2.12427943 -0.11653123  0.55388708 -2.77010918
  3.06858477  2.18960145  1.18552019 -1.06648329]. 
Working on 1.0 fold... 
[Sun Mar 24 22:37:34 2024]  Iteration number: 0 with current cost as 0.2150946344684368 and parameters 
[ 1.34838651  2.23743484 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18552019 -1.0664835 ]. 
Training complete taking 716.2035784721375 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 22:39:15 2024]  Iteration number: 0 with current cost as 0.5266736777098473 and parameters 
[-1.50725454  2.23743464 -2.1242794  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.1855201  -1.06648308]. 
Working on 0.4 fold... 
[Sun Mar 24 22:41:34 2024]  Iteration number: 0 with current cost as 0.21738920405150636 and parameters 
[ 1.20231152  2.23743447 -2.12427964 -0.11653103  0.55388691 -2.77010914
  3.06858465  2.18960145  1.18551998 -1.06648342]. 
Working on 0.6 fold... 
[Sun Mar 24 22:44:11 2024]  Iteration number: 0 with current cost as 0.21359071537449859 and parameters 
[ 1.24075897  2.23743464 -2.12427943 -0.11653082  0.55388708 -2.77010918
  3.06858477  2.18960145  1.1855202  -1.0664833 ]. 
Working on 0.8 fold... 
[Sun Mar 24 22:46:52 2024]  Iteration number: 0 with current cost as 0.21892701612491255 and parameters 
[ 1.17066511  2.23743443 -2.12427943 -0.11653123  0.55388708 -2.77010918
  3.06858477  2.18960145  1.18552019 -1.06648329]. 
Working on 1.0 fold... 
[Sun Mar 24 22:50:03 2024]  Iteration number: 0 with current cost as 0.2150946344684368 and parameters 
[ 1.34838651  2.23743484 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18552019 -1.0664835 ]. 
Training complete taking 749.6646661758423 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 22:51:58 2024]  Iteration number: 0 with current cost as 0.5266736777098473 and parameters 
[-1.50725454  2.23743464 -2.1242794  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.1855201  -1.06648308]. 
Working on 0.4 fold... 
[Sun Mar 24 22:54:42 2024]  Iteration number: 0 with current cost as 0.21738920405150636 and parameters 
[ 1.20231152  2.23743447 -2.12427964 -0.11653103  0.55388691 -2.77010914
  3.06858465  2.18960145  1.18551998 -1.06648342]. 
Working on 0.6 fold... 
[Sun Mar 24 22:57:35 2024]  Iteration number: 0 with current cost as 0.21359071537449859 and parameters 
[ 1.24075897  2.23743464 -2.12427943 -0.11653082  0.55388708 -2.77010918
  3.06858477  2.18960145  1.1855202  -1.0664833 ]. 
Working on 0.8 fold... 
[Sun Mar 24 23:00:22 2024]  Iteration number: 0 with current cost as 0.21892701612491255 and parameters 
[ 1.17066511  2.23743443 -2.12427943 -0.11653123  0.55388708 -2.77010918
  3.06858477  2.18960145  1.18552019 -1.06648329]. 
Working on 1.0 fold... 
[Sun Mar 24 23:03:35 2024]  Iteration number: 0 with current cost as 0.2150946344684368 and parameters 
[ 1.34838651  2.23743484 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18552019 -1.0664835 ]. 
Training complete taking 813.6625854969025 seconds. 
Discarding model... 

Training complete taking 18736.13564157486 total seconds. 
Now scoring model... 
Scoring complete taking 2.2140285968780518 seconds. 
Saved predicted values as A2-A2-CNOT_ESU2_predicted_values.csv
Model scores: {'MSE_train': (135.81936823238854,), 'R2_train': 0.3420007440560898, 'MAE_train': 10.744794510050792, 'MSE_test': 137.88459699081528, 'R2_test': 0.1688573082784699, 'MAE_test': 10.617140453758406}. 
Saved model results as A2-A2-CNOT_ESU2_results.json. 
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Thu Apr  4 12:26:48 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 12:27:53 2024]  Iteration number: 0 with current cost as 0.5266736777098473 and parameters 
[-1.50725454  2.23743464 -2.1242794  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.1855201  -1.06648308]. 
Working on 0.4 fold... 
[Thu Apr  4 12:30:13 2024]  Iteration number: 0 with current cost as 0.21738920405150636 and parameters 
[ 1.20231152  2.23743447 -2.12427964 -0.11653103  0.55388691 -2.77010914
  3.06858465  2.18960145  1.18551998 -1.06648342]. 
Working on 0.6 fold... 
[Thu Apr  4 12:32:44 2024]  Iteration number: 0 with current cost as 0.21359071537449859 and parameters 
[ 1.24075897  2.23743464 -2.12427943 -0.11653082  0.55388708 -2.77010918
  3.06858477  2.18960145  1.1855202  -1.0664833 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:35:18 2024]  Iteration number: 0 with current cost as 0.21892701612491255 and parameters 
[ 1.17066511  2.23743443 -2.12427943 -0.11653123  0.55388708 -2.77010918
  3.06858477  2.18960145  1.18552019 -1.06648329]. 
Working on 1.0 fold... 
[Thu Apr  4 12:38:03 2024]  Iteration number: 0 with current cost as 0.2150946344684368 and parameters 
[ 1.34838651  2.23743484 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18552019 -1.0664835 ]. 
Training complete taking 713.2909109592438 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 12:39:45 2024]  Iteration number: 0 with current cost as 0.5266736777098473 and parameters 
[-1.50725454  2.23743464 -2.1242794  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.1855201  -1.06648308]. 
Working on 0.4 fold... 
[Thu Apr  4 12:42:06 2024]  Iteration number: 0 with current cost as 0.21738920405150636 and parameters 
[ 1.20231152  2.23743447 -2.12427964 -0.11653103  0.55388691 -2.77010914
  3.06858465  2.18960145  1.18551998 -1.06648342]. 
Working on 0.6 fold... 
[Thu Apr  4 12:44:40 2024]  Iteration number: 0 with current cost as 0.21359071537449859 and parameters 
[ 1.24075897  2.23743464 -2.12427943 -0.11653082  0.55388708 -2.77010918
  3.06858477  2.18960145  1.1855202  -1.0664833 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:47:13 2024]  Iteration number: 0 with current cost as 0.21892701612491255 and parameters 
[ 1.17066511  2.23743443 -2.12427943 -0.11653123  0.55388708 -2.77010918
  3.06858477  2.18960145  1.18552019 -1.06648329]. 
Working on 1.0 fold... 
[Thu Apr  4 12:50:00 2024]  Iteration number: 0 with current cost as 0.2150946344684368 and parameters 
[ 1.34838651  2.23743484 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18552019 -1.0664835 ]. 
Training complete taking 716.4285604953766 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 12:51:40 2024]  Iteration number: 0 with current cost as 0.5266736777098473 and parameters 
[-1.50725454  2.23743464 -2.1242794  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.1855201  -1.06648308]. 
Working on 0.4 fold... 
[Thu Apr  4 12:54:01 2024]  Iteration number: 0 with current cost as 0.21738920405150636 and parameters 
[ 1.20231152  2.23743447 -2.12427964 -0.11653103  0.55388691 -2.77010914
  3.06858465  2.18960145  1.18551998 -1.06648342]. 
Working on 0.6 fold... 
[Thu Apr  4 12:56:31 2024]  Iteration number: 0 with current cost as 0.21359071537449859 and parameters 
[ 1.24075897  2.23743464 -2.12427943 -0.11653082  0.55388708 -2.77010918
  3.06858477  2.18960145  1.1855202  -1.0664833 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:59:03 2024]  Iteration number: 0 with current cost as 0.21892701612491255 and parameters 
[ 1.17066511  2.23743443 -2.12427943 -0.11653123  0.55388708 -2.77010918
  3.06858477  2.18960145  1.18552019 -1.06648329]. 
Working on 1.0 fold... 
[Thu Apr  4 13:01:50 2024]  Iteration number: 0 with current cost as 0.2150946344684368 and parameters 
[ 1.34838651  2.23743484 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18552019 -1.0664835 ]. 
Training complete taking 711.5611114501953 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 13:03:35 2024]  Iteration number: 0 with current cost as 0.5266736777098473 and parameters 
[-1.50725454  2.23743464 -2.1242794  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.1855201  -1.06648308]. 
Working on 0.4 fold... 
[Thu Apr  4 13:05:58 2024]  Iteration number: 0 with current cost as 0.21738920405150636 and parameters 
[ 1.20231152  2.23743447 -2.12427964 -0.11653103  0.55388691 -2.77010914
  3.06858465  2.18960145  1.18551998 -1.06648342]. 
Working on 0.6 fold... 
[Thu Apr  4 13:08:32 2024]  Iteration number: 0 with current cost as 0.21359071537449859 and parameters 
[ 1.24075897  2.23743464 -2.12427943 -0.11653082  0.55388708 -2.77010918
  3.06858477  2.18960145  1.1855202  -1.0664833 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:11:04 2024]  Iteration number: 0 with current cost as 0.21892701612491255 and parameters 
[ 1.17066511  2.23743443 -2.12427943 -0.11653123  0.55388708 -2.77010918
  3.06858477  2.18960145  1.18552019 -1.06648329]. 
Working on 1.0 fold... 
[Thu Apr  4 13:13:48 2024]  Iteration number: 0 with current cost as 0.2150946344684368 and parameters 
[ 1.34838651  2.23743484 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18552019 -1.0664835 ]. 
Training complete taking 718.0660746097565 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 13:15:29 2024]  Iteration number: 0 with current cost as 0.5266736777098473 and parameters 
[-1.50725454  2.23743464 -2.1242794  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.1855201  -1.06648308]. 
Working on 0.4 fold... 
[Thu Apr  4 13:17:51 2024]  Iteration number: 0 with current cost as 0.21738920405150636 and parameters 
[ 1.20231152  2.23743447 -2.12427964 -0.11653103  0.55388691 -2.77010914
  3.06858465  2.18960145  1.18551998 -1.06648342]. 
Working on 0.6 fold... 
[Thu Apr  4 13:20:24 2024]  Iteration number: 0 with current cost as 0.21359071537449859 and parameters 
[ 1.24075897  2.23743464 -2.12427943 -0.11653082  0.55388708 -2.77010918
  3.06858477  2.18960145  1.1855202  -1.0664833 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:22:58 2024]  Iteration number: 0 with current cost as 0.21892701612491255 and parameters 
[ 1.17066511  2.23743443 -2.12427943 -0.11653123  0.55388708 -2.77010918
  3.06858477  2.18960145  1.18552019 -1.06648329]. 
Working on 1.0 fold... 
[Thu Apr  4 13:25:46 2024]  Iteration number: 0 with current cost as 0.2150946344684368 and parameters 
[ 1.34838651  2.23743484 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18552019 -1.0664835 ]. 
Training complete taking 718.5639998912811 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 13:27:30 2024]  Iteration number: 0 with current cost as 0.5266736777098473 and parameters 
[-1.50725454  2.23743464 -2.1242794  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.1855201  -1.06648308]. 
Working on 0.4 fold... 
[Thu Apr  4 13:29:50 2024]  Iteration number: 0 with current cost as 0.21738920405150636 and parameters 
[ 1.20231152  2.23743447 -2.12427964 -0.11653103  0.55388691 -2.77010914
  3.06858465  2.18960145  1.18551998 -1.06648342]. 
Working on 0.6 fold... 
[Thu Apr  4 13:32:18 2024]  Iteration number: 0 with current cost as 0.21359071537449859 and parameters 
[ 1.24075897  2.23743464 -2.12427943 -0.11653082  0.55388708 -2.77010918
  3.06858477  2.18960145  1.1855202  -1.0664833 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:34:50 2024]  Iteration number: 0 with current cost as 0.21892701612491255 and parameters 
[ 1.17066511  2.23743443 -2.12427943 -0.11653123  0.55388708 -2.77010918
  3.06858477  2.18960145  1.18552019 -1.06648329]. 
Working on 1.0 fold... 
[Thu Apr  4 13:37:36 2024]  Iteration number: 0 with current cost as 0.2150946344684368 and parameters 
[ 1.34838651  2.23743484 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18552019 -1.0664835 ]. 
Training complete taking 707.9730920791626 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 13:39:19 2024]  Iteration number: 0 with current cost as 0.5266736777098473 and parameters 
[-1.50725454  2.23743464 -2.1242794  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.1855201  -1.06648308]. 
Working on 0.4 fold... 
[Thu Apr  4 13:41:38 2024]  Iteration number: 0 with current cost as 0.21738920405150636 and parameters 
[ 1.20231152  2.23743447 -2.12427964 -0.11653103  0.55388691 -2.77010914
  3.06858465  2.18960145  1.18551998 -1.06648342]. 
Working on 0.6 fold... 
[Thu Apr  4 13:44:12 2024]  Iteration number: 0 with current cost as 0.21359071537449859 and parameters 
[ 1.24075897  2.23743464 -2.12427943 -0.11653082  0.55388708 -2.77010918
  3.06858477  2.18960145  1.1855202  -1.0664833 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:46:45 2024]  Iteration number: 0 with current cost as 0.21892701612491255 and parameters 
[ 1.17066511  2.23743443 -2.12427943 -0.11653123  0.55388708 -2.77010918
  3.06858477  2.18960145  1.18552019 -1.06648329]. 
Working on 1.0 fold... 
[Thu Apr  4 13:49:33 2024]  Iteration number: 0 with current cost as 0.2150946344684368 and parameters 
[ 1.34838651  2.23743484 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18552019 -1.0664835 ]. 
Training complete taking 718.5622591972351 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 13:51:18 2024]  Iteration number: 0 with current cost as 0.5266736777098473 and parameters 
[-1.50725454  2.23743464 -2.1242794  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.1855201  -1.06648308]. 
Working on 0.4 fold... 
[Thu Apr  4 13:53:40 2024]  Iteration number: 0 with current cost as 0.21738920405150636 and parameters 
[ 1.20231152  2.23743447 -2.12427964 -0.11653103  0.55388691 -2.77010914
  3.06858465  2.18960145  1.18551998 -1.06648342]. 
Working on 0.6 fold... 
[Thu Apr  4 13:56:13 2024]  Iteration number: 0 with current cost as 0.21359071537449859 and parameters 
[ 1.24075897  2.23743464 -2.12427943 -0.11653082  0.55388708 -2.77010918
  3.06858477  2.18960145  1.1855202  -1.0664833 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:58:44 2024]  Iteration number: 0 with current cost as 0.21892701612491255 and parameters 
[ 1.17066511  2.23743443 -2.12427943 -0.11653123  0.55388708 -2.77010918
  3.06858477  2.18960145  1.18552019 -1.06648329]. 
Working on 1.0 fold... 
[Thu Apr  4 14:01:28 2024]  Iteration number: 0 with current cost as 0.2150946344684368 and parameters 
[ 1.34838651  2.23743484 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18552019 -1.0664835 ]. 
Training complete taking 712.8992459774017 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 14:03:09 2024]  Iteration number: 0 with current cost as 0.5266736777098473 and parameters 
[-1.50725454  2.23743464 -2.1242794  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.1855201  -1.06648308]. 
Working on 0.4 fold... 
[Thu Apr  4 14:05:28 2024]  Iteration number: 0 with current cost as 0.21738920405150636 and parameters 
[ 1.20231152  2.23743447 -2.12427964 -0.11653103  0.55388691 -2.77010914
  3.06858465  2.18960145  1.18551998 -1.06648342]. 
Working on 0.6 fold... 
[Thu Apr  4 14:08:00 2024]  Iteration number: 0 with current cost as 0.21359071537449859 and parameters 
[ 1.24075897  2.23743464 -2.12427943 -0.11653082  0.55388708 -2.77010918
  3.06858477  2.18960145  1.1855202  -1.0664833 ]. 
Working on 0.8 fold... 
[Thu Apr  4 14:10:34 2024]  Iteration number: 0 with current cost as 0.21892701612491255 and parameters 
[ 1.17066511  2.23743443 -2.12427943 -0.11653123  0.55388708 -2.77010918
  3.06858477  2.18960145  1.18552019 -1.06648329]. 
Working on 1.0 fold... 
[Thu Apr  4 14:13:20 2024]  Iteration number: 0 with current cost as 0.2150946344684368 and parameters 
[ 1.34838651  2.23743484 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18552019 -1.0664835 ]. 
Training complete taking 712.15229845047 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 14:15:03 2024]  Iteration number: 0 with current cost as 0.5266736777098473 and parameters 
[-1.50725454  2.23743464 -2.1242794  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.1855201  -1.06648308]. 
Working on 0.4 fold... 
[Thu Apr  4 14:17:28 2024]  Iteration number: 0 with current cost as 0.21738920405150636 and parameters 
[ 1.20231152  2.23743447 -2.12427964 -0.11653103  0.55388691 -2.77010914
  3.06858465  2.18960145  1.18551998 -1.06648342]. 
Working on 0.6 fold... 
[Thu Apr  4 14:19:59 2024]  Iteration number: 0 with current cost as 0.21359071537449859 and parameters 
[ 1.24075897  2.23743464 -2.12427943 -0.11653082  0.55388708 -2.77010918
  3.06858477  2.18960145  1.1855202  -1.0664833 ]. 
Working on 0.8 fold... 
[Thu Apr  4 14:22:35 2024]  Iteration number: 0 with current cost as 0.21892701612491255 and parameters 
[ 1.17066511  2.23743443 -2.12427943 -0.11653123  0.55388708 -2.77010918
  3.06858477  2.18960145  1.18552019 -1.06648329]. 
Working on 1.0 fold... 
[Thu Apr  4 14:25:22 2024]  Iteration number: 0 with current cost as 0.2150946344684368 and parameters 
[ 1.34838651  2.23743484 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18552019 -1.0664835 ]. 
Training complete taking 721.8592808246613 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 14:27:00 2024]  Iteration number: 0 with current cost as 0.5266736777098473 and parameters 
[-1.50725454  2.23743464 -2.1242794  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.1855201  -1.06648308]. 
Working on 0.4 fold... 
[Thu Apr  4 14:29:16 2024]  Iteration number: 0 with current cost as 0.21738920405150636 and parameters 
[ 1.20231152  2.23743447 -2.12427964 -0.11653103  0.55388691 -2.77010914
  3.06858465  2.18960145  1.18551998 -1.06648342]. 
Working on 0.6 fold... 
[Thu Apr  4 14:31:49 2024]  Iteration number: 0 with current cost as 0.21359071537449859 and parameters 
[ 1.24075897  2.23743464 -2.12427943 -0.11653082  0.55388708 -2.77010918
  3.06858477  2.18960145  1.1855202  -1.0664833 ]. 
Working on 0.8 fold... 
[Thu Apr  4 14:34:26 2024]  Iteration number: 0 with current cost as 0.21892701612491255 and parameters 
[ 1.17066511  2.23743443 -2.12427943 -0.11653123  0.55388708 -2.77010918
  3.06858477  2.18960145  1.18552019 -1.06648329]. 
Working on 1.0 fold... 
[Thu Apr  4 14:37:09 2024]  Iteration number: 0 with current cost as 0.2150946344684368 and parameters 
[ 1.34838651  2.23743484 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18552019 -1.0664835 ]. 
Training complete taking 707.5625977516174 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 14:38:53 2024]  Iteration number: 0 with current cost as 0.5266736777098473 and parameters 
[-1.50725454  2.23743464 -2.1242794  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.1855201  -1.06648308]. 
Working on 0.4 fold... 
[Thu Apr  4 14:41:10 2024]  Iteration number: 0 with current cost as 0.21738920405150636 and parameters 
[ 1.20231152  2.23743447 -2.12427964 -0.11653103  0.55388691 -2.77010914
  3.06858465  2.18960145  1.18551998 -1.06648342]. 
Working on 0.6 fold... 
[Thu Apr  4 14:43:45 2024]  Iteration number: 0 with current cost as 0.21359071537449859 and parameters 
[ 1.24075897  2.23743464 -2.12427943 -0.11653082  0.55388708 -2.77010918
  3.06858477  2.18960145  1.1855202  -1.0664833 ]. 
Working on 0.8 fold... 
[Thu Apr  4 14:46:15 2024]  Iteration number: 0 with current cost as 0.21892701612491255 and parameters 
[ 1.17066511  2.23743443 -2.12427943 -0.11653123  0.55388708 -2.77010918
  3.06858477  2.18960145  1.18552019 -1.06648329]. 
Working on 1.0 fold... 
[Thu Apr  4 14:49:00 2024]  Iteration number: 0 with current cost as 0.2150946344684368 and parameters 
[ 1.34838651  2.23743484 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18552019 -1.0664835 ]. 
Training complete taking 709.9962043762207 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 14:50:43 2024]  Iteration number: 0 with current cost as 0.5266736777098473 and parameters 
[-1.50725454  2.23743464 -2.1242794  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.1855201  -1.06648308]. 
Working on 0.4 fold... 
[Thu Apr  4 14:53:04 2024]  Iteration number: 0 with current cost as 0.21738920405150636 and parameters 
[ 1.20231152  2.23743447 -2.12427964 -0.11653103  0.55388691 -2.77010914
  3.06858465  2.18960145  1.18551998 -1.06648342]. 
Working on 0.6 fold... 
[Thu Apr  4 14:55:42 2024]  Iteration number: 0 with current cost as 0.21359071537449859 and parameters 
[ 1.24075897  2.23743464 -2.12427943 -0.11653082  0.55388708 -2.77010918
  3.06858477  2.18960145  1.1855202  -1.0664833 ]. 
Working on 0.8 fold... 
[Thu Apr  4 14:58:17 2024]  Iteration number: 0 with current cost as 0.21892701612491255 and parameters 
[ 1.17066511  2.23743443 -2.12427943 -0.11653123  0.55388708 -2.77010918
  3.06858477  2.18960145  1.18552019 -1.06648329]. 
Working on 1.0 fold... 
[Thu Apr  4 15:01:05 2024]  Iteration number: 0 with current cost as 0.2150946344684368 and parameters 
[ 1.34838651  2.23743484 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18552019 -1.0664835 ]. 
Training complete taking 725.587673664093 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 15:02:50 2024]  Iteration number: 0 with current cost as 0.5266736777098473 and parameters 
[-1.50725454  2.23743464 -2.1242794  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.1855201  -1.06648308]. 
Working on 0.4 fold... 
[Thu Apr  4 15:05:09 2024]  Iteration number: 0 with current cost as 0.21738920405150636 and parameters 
[ 1.20231152  2.23743447 -2.12427964 -0.11653103  0.55388691 -2.77010914
  3.06858465  2.18960145  1.18551998 -1.06648342]. 
Working on 0.6 fold... 
[Thu Apr  4 15:07:40 2024]  Iteration number: 0 with current cost as 0.21359071537449859 and parameters 
[ 1.24075897  2.23743464 -2.12427943 -0.11653082  0.55388708 -2.77010918
  3.06858477  2.18960145  1.1855202  -1.0664833 ]. 
Working on 0.8 fold... 
[Thu Apr  4 15:10:14 2024]  Iteration number: 0 with current cost as 0.21892701612491255 and parameters 
[ 1.17066511  2.23743443 -2.12427943 -0.11653123  0.55388708 -2.77010918
  3.06858477  2.18960145  1.18552019 -1.06648329]. 
Working on 1.0 fold... 
[Thu Apr  4 15:12:51 2024]  Iteration number: 0 with current cost as 0.2150946344684368 and parameters 
[ 1.34838651  2.23743484 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18552019 -1.0664835 ]. 
Training complete taking 708.1870558261871 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 15:14:34 2024]  Iteration number: 0 with current cost as 0.5266736777098473 and parameters 
[-1.50725454  2.23743464 -2.1242794  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.1855201  -1.06648308]. 
Working on 0.4 fold... 
[Thu Apr  4 15:16:50 2024]  Iteration number: 0 with current cost as 0.21738920405150636 and parameters 
[ 1.20231152  2.23743447 -2.12427964 -0.11653103  0.55388691 -2.77010914
  3.06858465  2.18960145  1.18551998 -1.06648342]. 
Working on 0.6 fold... 
[Thu Apr  4 15:19:25 2024]  Iteration number: 0 with current cost as 0.21359071537449859 and parameters 
[ 1.24075897  2.23743464 -2.12427943 -0.11653082  0.55388708 -2.77010918
  3.06858477  2.18960145  1.1855202  -1.0664833 ]. 
Working on 0.8 fold... 
[Thu Apr  4 15:21:59 2024]  Iteration number: 0 with current cost as 0.21892701612491255 and parameters 
[ 1.17066511  2.23743443 -2.12427943 -0.11653123  0.55388708 -2.77010918
  3.06858477  2.18960145  1.18552019 -1.06648329]. 
Working on 1.0 fold... 
[Thu Apr  4 15:24:44 2024]  Iteration number: 0 with current cost as 0.2150946344684368 and parameters 
[ 1.34838651  2.23743484 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18552019 -1.0664835 ]. 
Training complete taking 710.7319893836975 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 15:26:26 2024]  Iteration number: 0 with current cost as 0.5266736777098473 and parameters 
[-1.50725454  2.23743464 -2.1242794  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.1855201  -1.06648308]. 
Working on 0.4 fold... 
[Thu Apr  4 15:28:48 2024]  Iteration number: 0 with current cost as 0.21738920405150636 and parameters 
[ 1.20231152  2.23743447 -2.12427964 -0.11653103  0.55388691 -2.77010914
  3.06858465  2.18960145  1.18551998 -1.06648342]. 
Working on 0.6 fold... 
[Thu Apr  4 15:31:22 2024]  Iteration number: 0 with current cost as 0.21359071537449859 and parameters 
[ 1.24075897  2.23743464 -2.12427943 -0.11653082  0.55388708 -2.77010918
  3.06858477  2.18960145  1.1855202  -1.0664833 ]. 
Working on 0.8 fold... 
[Thu Apr  4 15:33:54 2024]  Iteration number: 0 with current cost as 0.21892701612491255 and parameters 
[ 1.17066511  2.23743443 -2.12427943 -0.11653123  0.55388708 -2.77010918
  3.06858477  2.18960145  1.18552019 -1.06648329]. 
Working on 1.0 fold... 
[Thu Apr  4 15:36:38 2024]  Iteration number: 0 with current cost as 0.2150946344684368 and parameters 
[ 1.34838651  2.23743484 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18552019 -1.0664835 ]. 
Training complete taking 713.993376493454 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 15:38:17 2024]  Iteration number: 0 with current cost as 0.5266736777098473 and parameters 
[-1.50725454  2.23743464 -2.1242794  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.1855201  -1.06648308]. 
Working on 0.4 fold... 
[Thu Apr  4 15:40:39 2024]  Iteration number: 0 with current cost as 0.21738920405150636 and parameters 
[ 1.20231152  2.23743447 -2.12427964 -0.11653103  0.55388691 -2.77010914
  3.06858465  2.18960145  1.18551998 -1.06648342]. 
Working on 0.6 fold... 
[Thu Apr  4 15:43:13 2024]  Iteration number: 0 with current cost as 0.21359071537449859 and parameters 
[ 1.24075897  2.23743464 -2.12427943 -0.11653082  0.55388708 -2.77010918
  3.06858477  2.18960145  1.1855202  -1.0664833 ]. 
Working on 0.8 fold... 
[Thu Apr  4 15:45:50 2024]  Iteration number: 0 with current cost as 0.21892701612491255 and parameters 
[ 1.17066511  2.23743443 -2.12427943 -0.11653123  0.55388708 -2.77010918
  3.06858477  2.18960145  1.18552019 -1.06648329]. 
Working on 1.0 fold... 
[Thu Apr  4 15:48:37 2024]  Iteration number: 0 with current cost as 0.2150946344684368 and parameters 
[ 1.34838651  2.23743484 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18552019 -1.0664835 ]. 
Training complete taking 719.9155035018921 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 15:50:19 2024]  Iteration number: 0 with current cost as 0.5266736777098473 and parameters 
[-1.50725454  2.23743464 -2.1242794  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.1855201  -1.06648308]. 
Working on 0.4 fold... 
[Thu Apr  4 15:52:45 2024]  Iteration number: 0 with current cost as 0.21738920405150636 and parameters 
[ 1.20231152  2.23743447 -2.12427964 -0.11653103  0.55388691 -2.77010914
  3.06858465  2.18960145  1.18551998 -1.06648342]. 
Working on 0.6 fold... 
[Thu Apr  4 15:55:21 2024]  Iteration number: 0 with current cost as 0.21359071537449859 and parameters 
[ 1.24075897  2.23743464 -2.12427943 -0.11653082  0.55388708 -2.77010918
  3.06858477  2.18960145  1.1855202  -1.0664833 ]. 
Working on 0.8 fold... 
[Thu Apr  4 15:57:57 2024]  Iteration number: 0 with current cost as 0.21892701612491255 and parameters 
[ 1.17066511  2.23743443 -2.12427943 -0.11653123  0.55388708 -2.77010918
  3.06858477  2.18960145  1.18552019 -1.06648329]. 
Working on 1.0 fold... 
[Thu Apr  4 16:00:48 2024]  Iteration number: 0 with current cost as 0.2150946344684368 and parameters 
[ 1.34838651  2.23743484 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18552019 -1.0664835 ]. 
Training complete taking 731.6201121807098 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 16:02:33 2024]  Iteration number: 0 with current cost as 0.5266736777098473 and parameters 
[-1.50725454  2.23743464 -2.1242794  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.1855201  -1.06648308]. 
Working on 0.4 fold... 
[Thu Apr  4 16:04:53 2024]  Iteration number: 0 with current cost as 0.21738920405150636 and parameters 
[ 1.20231152  2.23743447 -2.12427964 -0.11653103  0.55388691 -2.77010914
  3.06858465  2.18960145  1.18551998 -1.06648342]. 
Working on 0.6 fold... 
[Thu Apr  4 16:07:30 2024]  Iteration number: 0 with current cost as 0.21359071537449859 and parameters 
[ 1.24075897  2.23743464 -2.12427943 -0.11653082  0.55388708 -2.77010918
  3.06858477  2.18960145  1.1855202  -1.0664833 ]. 
Working on 0.8 fold... 
[Thu Apr  4 16:10:01 2024]  Iteration number: 0 with current cost as 0.21892701612491255 and parameters 
[ 1.17066511  2.23743443 -2.12427943 -0.11653123  0.55388708 -2.77010918
  3.06858477  2.18960145  1.18552019 -1.06648329]. 
Working on 1.0 fold... 
[Thu Apr  4 16:12:47 2024]  Iteration number: 0 with current cost as 0.2150946344684368 and parameters 
[ 1.34838651  2.23743484 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18552019 -1.0664835 ]. 
Training complete taking 718.4539158344269 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 16:14:32 2024]  Iteration number: 0 with current cost as 0.5266736777098473 and parameters 
[-1.50725454  2.23743464 -2.1242794  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.1855201  -1.06648308]. 
Working on 0.4 fold... 
[Thu Apr  4 16:16:54 2024]  Iteration number: 0 with current cost as 0.21738920405150636 and parameters 
[ 1.20231152  2.23743447 -2.12427964 -0.11653103  0.55388691 -2.77010914
  3.06858465  2.18960145  1.18551998 -1.06648342]. 
Working on 0.6 fold... 
[Thu Apr  4 16:19:32 2024]  Iteration number: 0 with current cost as 0.21359071537449859 and parameters 
[ 1.24075897  2.23743464 -2.12427943 -0.11653082  0.55388708 -2.77010918
  3.06858477  2.18960145  1.1855202  -1.0664833 ]. 
Working on 0.8 fold... 
[Thu Apr  4 16:22:04 2024]  Iteration number: 0 with current cost as 0.21892701612491255 and parameters 
[ 1.17066511  2.23743443 -2.12427943 -0.11653123  0.55388708 -2.77010918
  3.06858477  2.18960145  1.18552019 -1.06648329]. 
Working on 1.0 fold... 
[Thu Apr  4 16:24:50 2024]  Iteration number: 0 with current cost as 0.2150946344684368 and parameters 
[ 1.34838651  2.23743484 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18552019 -1.0664835 ]. 
Training complete taking 720.3533716201782 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 16:26:32 2024]  Iteration number: 0 with current cost as 0.5266736777098473 and parameters 
[-1.50725454  2.23743464 -2.1242794  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.1855201  -1.06648308]. 
Working on 0.4 fold... 
[Thu Apr  4 16:28:54 2024]  Iteration number: 0 with current cost as 0.21738920405150636 and parameters 
[ 1.20231152  2.23743447 -2.12427964 -0.11653103  0.55388691 -2.77010914
  3.06858465  2.18960145  1.18551998 -1.06648342]. 
Working on 0.6 fold... 
[Thu Apr  4 16:31:27 2024]  Iteration number: 0 with current cost as 0.21359071537449859 and parameters 
[ 1.24075897  2.23743464 -2.12427943 -0.11653082  0.55388708 -2.77010918
  3.06858477  2.18960145  1.1855202  -1.0664833 ]. 
Working on 0.8 fold... 
[Thu Apr  4 16:34:07 2024]  Iteration number: 0 with current cost as 0.21892701612491255 and parameters 
[ 1.17066511  2.23743443 -2.12427943 -0.11653123  0.55388708 -2.77010918
  3.06858477  2.18960145  1.18552019 -1.06648329]. 
Working on 1.0 fold... 
[Thu Apr  4 16:36:59 2024]  Iteration number: 0 with current cost as 0.2150946344684368 and parameters 
[ 1.34838651  2.23743484 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18552019 -1.0664835 ]. 
Training complete taking 732.1698582172394 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 16:38:42 2024]  Iteration number: 0 with current cost as 0.5266736777098473 and parameters 
[-1.50725454  2.23743464 -2.1242794  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.1855201  -1.06648308]. 
Working on 0.4 fold... 
[Thu Apr  4 16:41:03 2024]  Iteration number: 0 with current cost as 0.21738920405150636 and parameters 
[ 1.20231152  2.23743447 -2.12427964 -0.11653103  0.55388691 -2.77010914
  3.06858465  2.18960145  1.18551998 -1.06648342]. 
Working on 0.6 fold... 
[Thu Apr  4 16:43:33 2024]  Iteration number: 0 with current cost as 0.21359071537449859 and parameters 
[ 1.24075897  2.23743464 -2.12427943 -0.11653082  0.55388708 -2.77010918
  3.06858477  2.18960145  1.1855202  -1.0664833 ]. 
Working on 0.8 fold... 
[Thu Apr  4 16:46:08 2024]  Iteration number: 0 with current cost as 0.21892701612491255 and parameters 
[ 1.17066511  2.23743443 -2.12427943 -0.11653123  0.55388708 -2.77010918
  3.06858477  2.18960145  1.18552019 -1.06648329]. 
Working on 1.0 fold... 
[Thu Apr  4 16:48:57 2024]  Iteration number: 0 with current cost as 0.2150946344684368 and parameters 
[ 1.34838651  2.23743484 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18552019 -1.0664835 ]. 
Training complete taking 719.7110407352448 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 16:50:42 2024]  Iteration number: 0 with current cost as 0.5266736777098473 and parameters 
[-1.50725454  2.23743464 -2.1242794  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.1855201  -1.06648308]. 
Working on 0.4 fold... 
[Thu Apr  4 16:53:01 2024]  Iteration number: 0 with current cost as 0.21738920405150636 and parameters 
[ 1.20231152  2.23743447 -2.12427964 -0.11653103  0.55388691 -2.77010914
  3.06858465  2.18960145  1.18551998 -1.06648342]. 
Working on 0.6 fold... 
[Thu Apr  4 16:55:36 2024]  Iteration number: 0 with current cost as 0.21359071537449859 and parameters 
[ 1.24075897  2.23743464 -2.12427943 -0.11653082  0.55388708 -2.77010918
  3.06858477  2.18960145  1.1855202  -1.0664833 ]. 
Working on 0.8 fold... 
[Thu Apr  4 16:58:09 2024]  Iteration number: 0 with current cost as 0.21892701612491255 and parameters 
[ 1.17066511  2.23743443 -2.12427943 -0.11653123  0.55388708 -2.77010918
  3.06858477  2.18960145  1.18552019 -1.06648329]. 
Working on 1.0 fold... 
[Thu Apr  4 17:00:54 2024]  Iteration number: 0 with current cost as 0.2150946344684368 and parameters 
[ 1.34838651  2.23743484 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18552019 -1.0664835 ]. 
Training complete taking 713.7660355567932 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 17:02:37 2024]  Iteration number: 0 with current cost as 0.5266736777098473 and parameters 
[-1.50725454  2.23743464 -2.1242794  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.1855201  -1.06648308]. 
Working on 0.4 fold... 
[Thu Apr  4 17:04:56 2024]  Iteration number: 0 with current cost as 0.21738920405150636 and parameters 
[ 1.20231152  2.23743447 -2.12427964 -0.11653103  0.55388691 -2.77010914
  3.06858465  2.18960145  1.18551998 -1.06648342]. 
Working on 0.6 fold... 
[Thu Apr  4 17:07:30 2024]  Iteration number: 0 with current cost as 0.21359071537449859 and parameters 
[ 1.24075897  2.23743464 -2.12427943 -0.11653082  0.55388708 -2.77010918
  3.06858477  2.18960145  1.1855202  -1.0664833 ]. 
Working on 0.8 fold... 
[Thu Apr  4 17:10:04 2024]  Iteration number: 0 with current cost as 0.21892701612491255 and parameters 
[ 1.17066511  2.23743443 -2.12427943 -0.11653123  0.55388708 -2.77010918
  3.06858477  2.18960145  1.18552019 -1.06648329]. 
Working on 1.0 fold... 
[Thu Apr  4 17:12:51 2024]  Iteration number: 0 with current cost as 0.2150946344684368 and parameters 
[ 1.34838651  2.23743484 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18552019 -1.0664835 ]. 
Training complete taking 718.3915920257568 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 17:14:36 2024]  Iteration number: 0 with current cost as 0.5266736777098473 and parameters 
[-1.50725454  2.23743464 -2.1242794  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.1855201  -1.06648308]. 
Working on 0.4 fold... 
[Thu Apr  4 17:16:57 2024]  Iteration number: 0 with current cost as 0.21738920405150636 and parameters 
[ 1.20231152  2.23743447 -2.12427964 -0.11653103  0.55388691 -2.77010914
  3.06858465  2.18960145  1.18551998 -1.06648342]. 
Working on 0.6 fold... 
[Thu Apr  4 17:19:25 2024]  Iteration number: 0 with current cost as 0.21359071537449859 and parameters 
[ 1.24075897  2.23743464 -2.12427943 -0.11653082  0.55388708 -2.77010918
  3.06858477  2.18960145  1.1855202  -1.0664833 ]. 
Working on 0.8 fold... 
[Thu Apr  4 17:22:05 2024]  Iteration number: 0 with current cost as 0.21892701612491255 and parameters 
[ 1.17066511  2.23743443 -2.12427943 -0.11653123  0.55388708 -2.77010918
  3.06858477  2.18960145  1.18552019 -1.06648329]. 
Working on 1.0 fold... 
[Thu Apr  4 17:24:57 2024]  Iteration number: 0 with current cost as 0.2150946344684368 and parameters 
[ 1.34838651  2.23743484 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18552019 -1.0664835 ]. 
Training complete taking 726.3073763847351 seconds. 
Discarding model... 

Training complete taking 17928.106492757797 total seconds. 
Now scoring model... 
Scoring complete taking 2.004173517227173 seconds. 
Saved predicted values as A2-A2-CNOT_ESU2_predicted_values.csv
Model scores: {'MSE_train': (135.81936823238854,), 'R2_train': 0.3420007440560898, 'MAE_train': 10.744794510050792, 'MSE_test': 137.88459699081528, 'R2_test': 0.1688573082784699, 'MAE_test': 10.617140453758406}. 
Saved model results as A2-A2-CNOT_ESU2_results.json. 
