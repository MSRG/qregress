/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Sun Mar 24 17:29:55 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 17:30:04 2024]  Iteration number: 0 with current cost as 0.4165152327167121 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.14430074  0.57258968 -2.77306911
  3.01337485  2.27742411  1.23068477 -1.13312752  0.73168749  1.10374975
  1.39385116 -1.77804267  0.79131258]. 
Working on 0.4 fold... 
[Sun Mar 24 17:32:06 2024]  Iteration number: 0 with current cost as 0.3644036784353097 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.13875475  0.56688959 -2.77559735
  3.04757655  2.24828578  1.18831688 -1.11791926  0.69811373  1.11171379
  1.3910968  -1.77867102  0.79830139]. 
Working on 0.6 fold... 
[Sun Mar 24 17:34:00 2024]  Iteration number: 0 with current cost as 0.3729845469139776 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.14247765  0.56981354 -2.7753328
  3.03234359  2.26165013  1.20696683 -1.12598885  0.71371071  1.10679473
  1.38609503 -1.78513494  0.79193706]. 
Working on 0.8 fold... 
[Sun Mar 24 17:35:59 2024]  Iteration number: 0 with current cost as 0.37643825803597736 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.14010928  0.56944789 -2.77312857
  3.03374532  2.26385027  1.20329732 -1.12302748  0.71156208  1.1097177
  1.38560682 -1.78672016  0.78789953]. 
Working on 1.0 fold... 
[Sun Mar 24 17:37:57 2024]  Iteration number: 0 with current cost as 0.37882592139210236 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.14124975  0.56916588 -2.77491723
  3.03470979  2.25857258  1.20464027 -1.12359283  0.71075893  1.10878032
  1.38915829 -1.78122717  0.79565233]. 
Training complete taking 586.329936504364 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 17:39:49 2024]  Iteration number: 0 with current cost as 0.4165152327167121 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.14430074  0.57258968 -2.77306911
  3.01337485  2.27742411  1.23068477 -1.13312752  0.73168749  1.10374975
  1.39385116 -1.77804267  0.79131258]. 
Working on 0.4 fold... 
[Sun Mar 24 17:41:50 2024]  Iteration number: 0 with current cost as 0.3644036784353097 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.13875475  0.56688959 -2.77559735
  3.04757655  2.24828578  1.18831688 -1.11791926  0.69811373  1.11171379
  1.3910968  -1.77867102  0.79830139]. 
Working on 0.6 fold... 
[Sun Mar 24 17:43:45 2024]  Iteration number: 0 with current cost as 0.3729845469139776 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.14247765  0.56981354 -2.7753328
  3.03234359  2.26165013  1.20696683 -1.12598885  0.71371071  1.10679473
  1.38609503 -1.78513494  0.79193706]. 
Working on 0.8 fold... 
[Sun Mar 24 17:45:50 2024]  Iteration number: 0 with current cost as 0.37643825803597736 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.14010928  0.56944789 -2.77312857
  3.03374532  2.26385027  1.20329732 -1.12302748  0.71156208  1.1097177
  1.38560682 -1.78672016  0.78789953]. 
Working on 1.0 fold... 
[Sun Mar 24 17:47:48 2024]  Iteration number: 0 with current cost as 0.37882592139210236 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.14124975  0.56916588 -2.77491723
  3.03470979  2.25857258  1.20464027 -1.12359283  0.71075893  1.10878032
  1.38915829 -1.78122717  0.79565233]. 
Training complete taking 589.2421851158142 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 17:49:40 2024]  Iteration number: 0 with current cost as 0.4165152327167121 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.14430074  0.57258968 -2.77306911
  3.01337485  2.27742411  1.23068477 -1.13312752  0.73168749  1.10374975
  1.39385116 -1.77804267  0.79131258]. 
Working on 0.4 fold... 
[Sun Mar 24 17:51:39 2024]  Iteration number: 0 with current cost as 0.3644036784353097 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.13875475  0.56688959 -2.77559735
  3.04757655  2.24828578  1.18831688 -1.11791926  0.69811373  1.11171379
  1.3910968  -1.77867102  0.79830139]. 
Working on 0.6 fold... 
[Sun Mar 24 17:53:33 2024]  Iteration number: 0 with current cost as 0.3729845469139776 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.14247765  0.56981354 -2.7753328
  3.03234359  2.26165013  1.20696683 -1.12598885  0.71371071  1.10679473
  1.38609503 -1.78513494  0.79193706]. 
Working on 0.8 fold... 
[Sun Mar 24 17:55:30 2024]  Iteration number: 0 with current cost as 0.37643825803597736 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.14010928  0.56944789 -2.77312857
  3.03374532  2.26385027  1.20329732 -1.12302748  0.71156208  1.1097177
  1.38560682 -1.78672016  0.78789953]. 
Working on 1.0 fold... 
[Sun Mar 24 17:57:28 2024]  Iteration number: 0 with current cost as 0.37882592139210236 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.14124975  0.56916588 -2.77491723
  3.03470979  2.25857258  1.20464027 -1.12359283  0.71075893  1.10878032
  1.38915829 -1.78122717  0.79565233]. 
Training complete taking 581.6056780815125 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 17:59:19 2024]  Iteration number: 0 with current cost as 0.4165152327167121 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.14430074  0.57258968 -2.77306911
  3.01337485  2.27742411  1.23068477 -1.13312752  0.73168749  1.10374975
  1.39385116 -1.77804267  0.79131258]. 
Working on 0.4 fold... 
[Sun Mar 24 18:01:19 2024]  Iteration number: 0 with current cost as 0.3644036784353097 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.13875475  0.56688959 -2.77559735
  3.04757655  2.24828578  1.18831688 -1.11791926  0.69811373  1.11171379
  1.3910968  -1.77867102  0.79830139]. 
Working on 0.6 fold... 
[Sun Mar 24 18:03:15 2024]  Iteration number: 0 with current cost as 0.3729845469139776 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.14247765  0.56981354 -2.7753328
  3.03234359  2.26165013  1.20696683 -1.12598885  0.71371071  1.10679473
  1.38609503 -1.78513494  0.79193706]. 
Working on 0.8 fold... 
[Sun Mar 24 18:05:14 2024]  Iteration number: 0 with current cost as 0.37643825803597736 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.14010928  0.56944789 -2.77312857
  3.03374532  2.26385027  1.20329732 -1.12302748  0.71156208  1.1097177
  1.38560682 -1.78672016  0.78789953]. 
Working on 1.0 fold... 
[Sun Mar 24 18:07:14 2024]  Iteration number: 0 with current cost as 0.37882592139210236 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.14124975  0.56916588 -2.77491723
  3.03470979  2.25857258  1.20464027 -1.12359283  0.71075893  1.10878032
  1.38915829 -1.78122717  0.79565233]. 
Training complete taking 583.5188233852386 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 18:09:05 2024]  Iteration number: 0 with current cost as 0.4165152327167121 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.14430074  0.57258968 -2.77306911
  3.01337485  2.27742411  1.23068477 -1.13312752  0.73168749  1.10374975
  1.39385116 -1.77804267  0.79131258]. 
Working on 0.4 fold... 
[Sun Mar 24 18:11:02 2024]  Iteration number: 0 with current cost as 0.3644036784353097 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.13875475  0.56688959 -2.77559735
  3.04757655  2.24828578  1.18831688 -1.11791926  0.69811373  1.11171379
  1.3910968  -1.77867102  0.79830139]. 
Working on 0.6 fold... 
[Sun Mar 24 18:13:14 2024]  Iteration number: 0 with current cost as 0.3729845469139776 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.14247765  0.56981354 -2.7753328
  3.03234359  2.26165013  1.20696683 -1.12598885  0.71371071  1.10679473
  1.38609503 -1.78513494  0.79193706]. 
Working on 0.8 fold... 
[Sun Mar 24 18:15:13 2024]  Iteration number: 0 with current cost as 0.37643825803597736 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.14010928  0.56944789 -2.77312857
  3.03374532  2.26385027  1.20329732 -1.12302748  0.71156208  1.1097177
  1.38560682 -1.78672016  0.78789953]. 
Working on 1.0 fold... 
[Sun Mar 24 18:17:12 2024]  Iteration number: 0 with current cost as 0.37882592139210236 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.14124975  0.56916588 -2.77491723
  3.03470979  2.25857258  1.20464027 -1.12359283  0.71075893  1.10878032
  1.38915829 -1.78122717  0.79565233]. 
Training complete taking 598.4359111785889 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 18:19:01 2024]  Iteration number: 0 with current cost as 0.4165152327167121 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.14430074  0.57258968 -2.77306911
  3.01337485  2.27742411  1.23068477 -1.13312752  0.73168749  1.10374975
  1.39385116 -1.77804267  0.79131258]. 
Working on 0.4 fold... 
[Sun Mar 24 18:21:01 2024]  Iteration number: 0 with current cost as 0.3644036784353097 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.13875475  0.56688959 -2.77559735
  3.04757655  2.24828578  1.18831688 -1.11791926  0.69811373  1.11171379
  1.3910968  -1.77867102  0.79830139]. 
Working on 0.6 fold... 
[Sun Mar 24 18:22:56 2024]  Iteration number: 0 with current cost as 0.3729845469139776 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.14247765  0.56981354 -2.7753328
  3.03234359  2.26165013  1.20696683 -1.12598885  0.71371071  1.10679473
  1.38609503 -1.78513494  0.79193706]. 
Working on 0.8 fold... 
[Sun Mar 24 18:24:56 2024]  Iteration number: 0 with current cost as 0.37643825803597736 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.14010928  0.56944789 -2.77312857
  3.03374532  2.26385027  1.20329732 -1.12302748  0.71156208  1.1097177
  1.38560682 -1.78672016  0.78789953]. 
Working on 1.0 fold... 
[Sun Mar 24 18:26:57 2024]  Iteration number: 0 with current cost as 0.37882592139210236 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.14124975  0.56916588 -2.77491723
  3.03470979  2.25857258  1.20464027 -1.12359283  0.71075893  1.10878032
  1.38915829 -1.78122717  0.79565233]. 
Training complete taking 587.828976392746 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 18:28:49 2024]  Iteration number: 0 with current cost as 0.4165152327167121 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.14430074  0.57258968 -2.77306911
  3.01337485  2.27742411  1.23068477 -1.13312752  0.73168749  1.10374975
  1.39385116 -1.77804267  0.79131258]. 
Working on 0.4 fold... 
[Sun Mar 24 18:30:48 2024]  Iteration number: 0 with current cost as 0.3644036784353097 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.13875475  0.56688959 -2.77559735
  3.04757655  2.24828578  1.18831688 -1.11791926  0.69811373  1.11171379
  1.3910968  -1.77867102  0.79830139]. 
Working on 0.6 fold... 
[Sun Mar 24 18:32:44 2024]  Iteration number: 0 with current cost as 0.3729845469139776 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.14247765  0.56981354 -2.7753328
  3.03234359  2.26165013  1.20696683 -1.12598885  0.71371071  1.10679473
  1.38609503 -1.78513494  0.79193706]. 
Working on 0.8 fold... 
[Sun Mar 24 18:34:43 2024]  Iteration number: 0 with current cost as 0.37643825803597736 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.14010928  0.56944789 -2.77312857
  3.03374532  2.26385027  1.20329732 -1.12302748  0.71156208  1.1097177
  1.38560682 -1.78672016  0.78789953]. 
Working on 1.0 fold... 
[Sun Mar 24 18:36:42 2024]  Iteration number: 0 with current cost as 0.37882592139210236 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.14124975  0.56916588 -2.77491723
  3.03470979  2.25857258  1.20464027 -1.12359283  0.71075893  1.10878032
  1.38915829 -1.78122717  0.79565233]. 
Training complete taking 582.373893737793 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 18:38:31 2024]  Iteration number: 0 with current cost as 0.4165152327167121 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.14430074  0.57258968 -2.77306911
  3.01337485  2.27742411  1.23068477 -1.13312752  0.73168749  1.10374975
  1.39385116 -1.77804267  0.79131258]. 
Working on 0.4 fold... 
[Sun Mar 24 18:40:30 2024]  Iteration number: 0 with current cost as 0.3644036784353097 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.13875475  0.56688959 -2.77559735
  3.04757655  2.24828578  1.18831688 -1.11791926  0.69811373  1.11171379
  1.3910968  -1.77867102  0.79830139]. 
Working on 0.6 fold... 
[Sun Mar 24 18:42:26 2024]  Iteration number: 0 with current cost as 0.3729845469139776 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.14247765  0.56981354 -2.7753328
  3.03234359  2.26165013  1.20696683 -1.12598885  0.71371071  1.10679473
  1.38609503 -1.78513494  0.79193706]. 
Working on 0.8 fold... 
[Sun Mar 24 18:44:26 2024]  Iteration number: 0 with current cost as 0.37643825803597736 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.14010928  0.56944789 -2.77312857
  3.03374532  2.26385027  1.20329732 -1.12302748  0.71156208  1.1097177
  1.38560682 -1.78672016  0.78789953]. 
Working on 1.0 fold... 
[Sun Mar 24 18:46:25 2024]  Iteration number: 0 with current cost as 0.37882592139210236 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.14124975  0.56916588 -2.77491723
  3.03470979  2.25857258  1.20464027 -1.12359283  0.71075893  1.10878032
  1.38915829 -1.78122717  0.79565233]. 
Training complete taking 585.6755549907684 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 18:48:17 2024]  Iteration number: 0 with current cost as 0.4165152327167121 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.14430074  0.57258968 -2.77306911
  3.01337485  2.27742411  1.23068477 -1.13312752  0.73168749  1.10374975
  1.39385116 -1.77804267  0.79131258]. 
Working on 0.4 fold... 
[Sun Mar 24 18:50:22 2024]  Iteration number: 0 with current cost as 0.3644036784353097 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.13875475  0.56688959 -2.77559735
  3.04757655  2.24828578  1.18831688 -1.11791926  0.69811373  1.11171379
  1.3910968  -1.77867102  0.79830139]. 
Working on 0.6 fold... 
[Sun Mar 24 18:52:28 2024]  Iteration number: 0 with current cost as 0.3729845469139776 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.14247765  0.56981354 -2.7753328
  3.03234359  2.26165013  1.20696683 -1.12598885  0.71371071  1.10679473
  1.38609503 -1.78513494  0.79193706]. 
Working on 0.8 fold... 
[Sun Mar 24 18:54:28 2024]  Iteration number: 0 with current cost as 0.37643825803597736 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.14010928  0.56944789 -2.77312857
  3.03374532  2.26385027  1.20329732 -1.12302748  0.71156208  1.1097177
  1.38560682 -1.78672016  0.78789953]. 
Working on 1.0 fold... 
[Sun Mar 24 18:56:31 2024]  Iteration number: 0 with current cost as 0.37882592139210236 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.14124975  0.56916588 -2.77491723
  3.03470979  2.25857258  1.20464027 -1.12359283  0.71075893  1.10878032
  1.38915829 -1.78122717  0.79565233]. 
Training complete taking 605.7061402797699 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 18:58:23 2024]  Iteration number: 0 with current cost as 0.4165152327167121 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.14430074  0.57258968 -2.77306911
  3.01337485  2.27742411  1.23068477 -1.13312752  0.73168749  1.10374975
  1.39385116 -1.77804267  0.79131258]. 
Working on 0.4 fold... 
[Sun Mar 24 19:00:22 2024]  Iteration number: 0 with current cost as 0.3644036784353097 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.13875475  0.56688959 -2.77559735
  3.04757655  2.24828578  1.18831688 -1.11791926  0.69811373  1.11171379
  1.3910968  -1.77867102  0.79830139]. 
Working on 0.6 fold... 
[Sun Mar 24 19:02:18 2024]  Iteration number: 0 with current cost as 0.3729845469139776 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.14247765  0.56981354 -2.7753328
  3.03234359  2.26165013  1.20696683 -1.12598885  0.71371071  1.10679473
  1.38609503 -1.78513494  0.79193706]. 
Working on 0.8 fold... 
[Sun Mar 24 19:04:18 2024]  Iteration number: 0 with current cost as 0.37643825803597736 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.14010928  0.56944789 -2.77312857
  3.03374532  2.26385027  1.20329732 -1.12302748  0.71156208  1.1097177
  1.38560682 -1.78672016  0.78789953]. 
Working on 1.0 fold... 
[Sun Mar 24 19:06:17 2024]  Iteration number: 0 with current cost as 0.37882592139210236 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.14124975  0.56916588 -2.77491723
  3.03470979  2.25857258  1.20464027 -1.12359283  0.71075893  1.10878032
  1.38915829 -1.78122717  0.79565233]. 
Training complete taking 584.3958349227905 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 19:08:09 2024]  Iteration number: 0 with current cost as 0.4165152327167121 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.14430074  0.57258968 -2.77306911
  3.01337485  2.27742411  1.23068477 -1.13312752  0.73168749  1.10374975
  1.39385116 -1.77804267  0.79131258]. 
Working on 0.4 fold... 
[Sun Mar 24 19:10:08 2024]  Iteration number: 0 with current cost as 0.3644036784353097 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.13875475  0.56688959 -2.77559735
  3.04757655  2.24828578  1.18831688 -1.11791926  0.69811373  1.11171379
  1.3910968  -1.77867102  0.79830139]. 
Working on 0.6 fold... 
[Sun Mar 24 19:12:02 2024]  Iteration number: 0 with current cost as 0.3729845469139776 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.14247765  0.56981354 -2.7753328
  3.03234359  2.26165013  1.20696683 -1.12598885  0.71371071  1.10679473
  1.38609503 -1.78513494  0.79193706]. 
Working on 0.8 fold... 
[Sun Mar 24 19:14:02 2024]  Iteration number: 0 with current cost as 0.37643825803597736 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.14010928  0.56944789 -2.77312857
  3.03374532  2.26385027  1.20329732 -1.12302748  0.71156208  1.1097177
  1.38560682 -1.78672016  0.78789953]. 
Working on 1.0 fold... 
[Sun Mar 24 19:16:01 2024]  Iteration number: 0 with current cost as 0.37882592139210236 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.14124975  0.56916588 -2.77491723
  3.03470979  2.25857258  1.20464027 -1.12359283  0.71075893  1.10878032
  1.38915829 -1.78122717  0.79565233]. 
Training complete taking 593.2990264892578 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 19:18:01 2024]  Iteration number: 0 with current cost as 0.4165152327167121 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.14430074  0.57258968 -2.77306911
  3.01337485  2.27742411  1.23068477 -1.13312752  0.73168749  1.10374975
  1.39385116 -1.77804267  0.79131258]. 
Working on 0.4 fold... 
[Sun Mar 24 19:20:00 2024]  Iteration number: 0 with current cost as 0.3644036784353097 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.13875475  0.56688959 -2.77559735
  3.04757655  2.24828578  1.18831688 -1.11791926  0.69811373  1.11171379
  1.3910968  -1.77867102  0.79830139]. 
Working on 0.6 fold... 
[Sun Mar 24 19:21:56 2024]  Iteration number: 0 with current cost as 0.3729845469139776 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.14247765  0.56981354 -2.7753328
  3.03234359  2.26165013  1.20696683 -1.12598885  0.71371071  1.10679473
  1.38609503 -1.78513494  0.79193706]. 
Working on 0.8 fold... 
[Sun Mar 24 19:24:18 2024]  Iteration number: 0 with current cost as 0.37643825803597736 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.14010928  0.56944789 -2.77312857
  3.03374532  2.26385027  1.20329732 -1.12302748  0.71156208  1.1097177
  1.38560682 -1.78672016  0.78789953]. 
Working on 1.0 fold... 
[Sun Mar 24 19:26:17 2024]  Iteration number: 0 with current cost as 0.37882592139210236 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.14124975  0.56916588 -2.77491723
  3.03470979  2.25857258  1.20464027 -1.12359283  0.71075893  1.10878032
  1.38915829 -1.78122717  0.79565233]. 
Training complete taking 606.5275321006775 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 19:28:07 2024]  Iteration number: 0 with current cost as 0.4165152327167121 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.14430074  0.57258968 -2.77306911
  3.01337485  2.27742411  1.23068477 -1.13312752  0.73168749  1.10374975
  1.39385116 -1.77804267  0.79131258]. 
Working on 0.4 fold... 
[Sun Mar 24 19:30:06 2024]  Iteration number: 0 with current cost as 0.3644036784353097 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.13875475  0.56688959 -2.77559735
  3.04757655  2.24828578  1.18831688 -1.11791926  0.69811373  1.11171379
  1.3910968  -1.77867102  0.79830139]. 
Working on 0.6 fold... 
[Sun Mar 24 19:32:05 2024]  Iteration number: 0 with current cost as 0.3729845469139776 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.14247765  0.56981354 -2.7753328
  3.03234359  2.26165013  1.20696683 -1.12598885  0.71371071  1.10679473
  1.38609503 -1.78513494  0.79193706]. 
Working on 0.8 fold... 
[Sun Mar 24 19:34:19 2024]  Iteration number: 0 with current cost as 0.37643825803597736 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.14010928  0.56944789 -2.77312857
  3.03374532  2.26385027  1.20329732 -1.12302748  0.71156208  1.1097177
  1.38560682 -1.78672016  0.78789953]. 
Working on 1.0 fold... 
[Sun Mar 24 19:36:35 2024]  Iteration number: 0 with current cost as 0.37882592139210236 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.14124975  0.56916588 -2.77491723
  3.03470979  2.25857258  1.20464027 -1.12359283  0.71075893  1.10878032
  1.38915829 -1.78122717  0.79565233]. 
Training complete taking 623.8887214660645 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 19:38:31 2024]  Iteration number: 0 with current cost as 0.4165152327167121 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.14430074  0.57258968 -2.77306911
  3.01337485  2.27742411  1.23068477 -1.13312752  0.73168749  1.10374975
  1.39385116 -1.77804267  0.79131258]. 
Working on 0.4 fold... 
[Sun Mar 24 19:40:31 2024]  Iteration number: 0 with current cost as 0.3644036784353097 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.13875475  0.56688959 -2.77559735
  3.04757655  2.24828578  1.18831688 -1.11791926  0.69811373  1.11171379
  1.3910968  -1.77867102  0.79830139]. 
Working on 0.6 fold... 
[Sun Mar 24 19:42:26 2024]  Iteration number: 0 with current cost as 0.3729845469139776 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.14247765  0.56981354 -2.7753328
  3.03234359  2.26165013  1.20696683 -1.12598885  0.71371071  1.10679473
  1.38609503 -1.78513494  0.79193706]. 
Working on 0.8 fold... 
[Sun Mar 24 19:44:24 2024]  Iteration number: 0 with current cost as 0.37643825803597736 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.14010928  0.56944789 -2.77312857
  3.03374532  2.26385027  1.20329732 -1.12302748  0.71156208  1.1097177
  1.38560682 -1.78672016  0.78789953]. 
Working on 1.0 fold... 
[Sun Mar 24 19:46:22 2024]  Iteration number: 0 with current cost as 0.37882592139210236 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.14124975  0.56916588 -2.77491723
  3.03470979  2.25857258  1.20464027 -1.12359283  0.71075893  1.10878032
  1.38915829 -1.78122717  0.79565233]. 
Training complete taking 579.3640878200531 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 19:48:12 2024]  Iteration number: 0 with current cost as 0.4165152327167121 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.14430074  0.57258968 -2.77306911
  3.01337485  2.27742411  1.23068477 -1.13312752  0.73168749  1.10374975
  1.39385116 -1.77804267  0.79131258]. 
Working on 0.4 fold... 
[Sun Mar 24 19:50:11 2024]  Iteration number: 0 with current cost as 0.3644036784353097 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.13875475  0.56688959 -2.77559735
  3.04757655  2.24828578  1.18831688 -1.11791926  0.69811373  1.11171379
  1.3910968  -1.77867102  0.79830139]. 
Working on 0.6 fold... 
[Sun Mar 24 19:52:04 2024]  Iteration number: 0 with current cost as 0.3729845469139776 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.14247765  0.56981354 -2.7753328
  3.03234359  2.26165013  1.20696683 -1.12598885  0.71371071  1.10679473
  1.38609503 -1.78513494  0.79193706]. 
Working on 0.8 fold... 
[Sun Mar 24 19:54:01 2024]  Iteration number: 0 with current cost as 0.37643825803597736 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.14010928  0.56944789 -2.77312857
  3.03374532  2.26385027  1.20329732 -1.12302748  0.71156208  1.1097177
  1.38560682 -1.78672016  0.78789953]. 
Working on 1.0 fold... 
[Sun Mar 24 19:55:58 2024]  Iteration number: 0 with current cost as 0.37882592139210236 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.14124975  0.56916588 -2.77491723
  3.03470979  2.25857258  1.20464027 -1.12359283  0.71075893  1.10878032
  1.38915829 -1.78122717  0.79565233]. 
Training complete taking 576.4834578037262 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 19:58:02 2024]  Iteration number: 0 with current cost as 0.4165152327167121 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.14430074  0.57258968 -2.77306911
  3.01337485  2.27742411  1.23068477 -1.13312752  0.73168749  1.10374975
  1.39385116 -1.77804267  0.79131258]. 
Working on 0.4 fold... 
[Sun Mar 24 20:00:00 2024]  Iteration number: 0 with current cost as 0.3644036784353097 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.13875475  0.56688959 -2.77559735
  3.04757655  2.24828578  1.18831688 -1.11791926  0.69811373  1.11171379
  1.3910968  -1.77867102  0.79830139]. 
Working on 0.6 fold... 
[Sun Mar 24 20:02:08 2024]  Iteration number: 0 with current cost as 0.3729845469139776 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.14247765  0.56981354 -2.7753328
  3.03234359  2.26165013  1.20696683 -1.12598885  0.71371071  1.10679473
  1.38609503 -1.78513494  0.79193706]. 
Working on 0.8 fold... 
[Sun Mar 24 20:04:07 2024]  Iteration number: 0 with current cost as 0.37643825803597736 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.14010928  0.56944789 -2.77312857
  3.03374532  2.26385027  1.20329732 -1.12302748  0.71156208  1.1097177
  1.38560682 -1.78672016  0.78789953]. 
Working on 1.0 fold... 
[Sun Mar 24 20:06:04 2024]  Iteration number: 0 with current cost as 0.37882592139210236 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.14124975  0.56916588 -2.77491723
  3.03470979  2.25857258  1.20464027 -1.12359283  0.71075893  1.10878032
  1.38915829 -1.78122717  0.79565233]. 
Training complete taking 607.4640922546387 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 20:07:54 2024]  Iteration number: 0 with current cost as 0.4165152327167121 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.14430074  0.57258968 -2.77306911
  3.01337485  2.27742411  1.23068477 -1.13312752  0.73168749  1.10374975
  1.39385116 -1.77804267  0.79131258]. 
Working on 0.4 fold... 
[Sun Mar 24 20:09:55 2024]  Iteration number: 0 with current cost as 0.3644036784353097 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.13875475  0.56688959 -2.77559735
  3.04757655  2.24828578  1.18831688 -1.11791926  0.69811373  1.11171379
  1.3910968  -1.77867102  0.79830139]. 
Working on 0.6 fold... 
[Sun Mar 24 20:12:00 2024]  Iteration number: 0 with current cost as 0.3729845469139776 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.14247765  0.56981354 -2.7753328
  3.03234359  2.26165013  1.20696683 -1.12598885  0.71371071  1.10679473
  1.38609503 -1.78513494  0.79193706]. 
Working on 0.8 fold... 
[Sun Mar 24 20:13:58 2024]  Iteration number: 0 with current cost as 0.37643825803597736 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.14010928  0.56944789 -2.77312857
  3.03374532  2.26385027  1.20329732 -1.12302748  0.71156208  1.1097177
  1.38560682 -1.78672016  0.78789953]. 
Working on 1.0 fold... 
[Sun Mar 24 20:16:02 2024]  Iteration number: 0 with current cost as 0.37882592139210236 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.14124975  0.56916588 -2.77491723
  3.03470979  2.25857258  1.20464027 -1.12359283  0.71075893  1.10878032
  1.38915829 -1.78122717  0.79565233]. 
Training complete taking 600.2923402786255 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 20:17:55 2024]  Iteration number: 0 with current cost as 0.4165152327167121 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.14430074  0.57258968 -2.77306911
  3.01337485  2.27742411  1.23068477 -1.13312752  0.73168749  1.10374975
  1.39385116 -1.77804267  0.79131258]. 
Working on 0.4 fold... 
[Sun Mar 24 20:19:53 2024]  Iteration number: 0 with current cost as 0.3644036784353097 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.13875475  0.56688959 -2.77559735
  3.04757655  2.24828578  1.18831688 -1.11791926  0.69811373  1.11171379
  1.3910968  -1.77867102  0.79830139]. 
Working on 0.6 fold... 
[Sun Mar 24 20:21:49 2024]  Iteration number: 0 with current cost as 0.3729845469139776 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.14247765  0.56981354 -2.7753328
  3.03234359  2.26165013  1.20696683 -1.12598885  0.71371071  1.10679473
  1.38609503 -1.78513494  0.79193706]. 
Working on 0.8 fold... 
[Sun Mar 24 20:23:55 2024]  Iteration number: 0 with current cost as 0.37643825803597736 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.14010928  0.56944789 -2.77312857
  3.03374532  2.26385027  1.20329732 -1.12302748  0.71156208  1.1097177
  1.38560682 -1.78672016  0.78789953]. 
Working on 1.0 fold... 
[Sun Mar 24 20:25:53 2024]  Iteration number: 0 with current cost as 0.37882592139210236 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.14124975  0.56916588 -2.77491723
  3.03470979  2.25857258  1.20464027 -1.12359283  0.71075893  1.10878032
  1.38915829 -1.78122717  0.79565233]. 
Training complete taking 586.2712228298187 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 20:27:41 2024]  Iteration number: 0 with current cost as 0.4165152327167121 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.14430074  0.57258968 -2.77306911
  3.01337485  2.27742411  1.23068477 -1.13312752  0.73168749  1.10374975
  1.39385116 -1.77804267  0.79131258]. 
Working on 0.4 fold... 
[Sun Mar 24 20:29:38 2024]  Iteration number: 0 with current cost as 0.3644036784353097 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.13875475  0.56688959 -2.77559735
  3.04757655  2.24828578  1.18831688 -1.11791926  0.69811373  1.11171379
  1.3910968  -1.77867102  0.79830139]. 
Working on 0.6 fold... 
[Sun Mar 24 20:31:33 2024]  Iteration number: 0 with current cost as 0.3729845469139776 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.14247765  0.56981354 -2.7753328
  3.03234359  2.26165013  1.20696683 -1.12598885  0.71371071  1.10679473
  1.38609503 -1.78513494  0.79193706]. 
Working on 0.8 fold... 
[Sun Mar 24 20:33:35 2024]  Iteration number: 0 with current cost as 0.37643825803597736 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.14010928  0.56944789 -2.77312857
  3.03374532  2.26385027  1.20329732 -1.12302748  0.71156208  1.1097177
  1.38560682 -1.78672016  0.78789953]. 
Working on 1.0 fold... 
[Sun Mar 24 20:35:33 2024]  Iteration number: 0 with current cost as 0.37882592139210236 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.14124975  0.56916588 -2.77491723
  3.03470979  2.25857258  1.20464027 -1.12359283  0.71075893  1.10878032
  1.38915829 -1.78122717  0.79565233]. 
Training complete taking 582.5431275367737 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 20:37:23 2024]  Iteration number: 0 with current cost as 0.4165152327167121 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.14430074  0.57258968 -2.77306911
  3.01337485  2.27742411  1.23068477 -1.13312752  0.73168749  1.10374975
  1.39385116 -1.77804267  0.79131258]. 
Working on 0.4 fold... 
[Sun Mar 24 20:39:21 2024]  Iteration number: 0 with current cost as 0.3644036784353097 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.13875475  0.56688959 -2.77559735
  3.04757655  2.24828578  1.18831688 -1.11791926  0.69811373  1.11171379
  1.3910968  -1.77867102  0.79830139]. 
Working on 0.6 fold... 
[Sun Mar 24 20:41:15 2024]  Iteration number: 0 with current cost as 0.3729845469139776 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.14247765  0.56981354 -2.7753328
  3.03234359  2.26165013  1.20696683 -1.12598885  0.71371071  1.10679473
  1.38609503 -1.78513494  0.79193706]. 
Working on 0.8 fold... 
[Sun Mar 24 20:43:12 2024]  Iteration number: 0 with current cost as 0.37643825803597736 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.14010928  0.56944789 -2.77312857
  3.03374532  2.26385027  1.20329732 -1.12302748  0.71156208  1.1097177
  1.38560682 -1.78672016  0.78789953]. 
Working on 1.0 fold... 
[Sun Mar 24 20:45:10 2024]  Iteration number: 0 with current cost as 0.37882592139210236 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.14124975  0.56916588 -2.77491723
  3.03470979  2.25857258  1.20464027 -1.12359283  0.71075893  1.10878032
  1.38915829 -1.78122717  0.79565233]. 
Training complete taking 577.2297887802124 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 20:47:01 2024]  Iteration number: 0 with current cost as 0.4165152327167121 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.14430074  0.57258968 -2.77306911
  3.01337485  2.27742411  1.23068477 -1.13312752  0.73168749  1.10374975
  1.39385116 -1.77804267  0.79131258]. 
Working on 0.4 fold... 
[Sun Mar 24 20:49:01 2024]  Iteration number: 0 with current cost as 0.3644036784353097 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.13875475  0.56688959 -2.77559735
  3.04757655  2.24828578  1.18831688 -1.11791926  0.69811373  1.11171379
  1.3910968  -1.77867102  0.79830139]. 
Working on 0.6 fold... 
[Sun Mar 24 20:51:09 2024]  Iteration number: 0 with current cost as 0.3729845469139776 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.14247765  0.56981354 -2.7753328
  3.03234359  2.26165013  1.20696683 -1.12598885  0.71371071  1.10679473
  1.38609503 -1.78513494  0.79193706]. 
Working on 0.8 fold... 
[Sun Mar 24 20:53:10 2024]  Iteration number: 0 with current cost as 0.37643825803597736 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.14010928  0.56944789 -2.77312857
  3.03374532  2.26385027  1.20329732 -1.12302748  0.71156208  1.1097177
  1.38560682 -1.78672016  0.78789953]. 
Working on 1.0 fold... 
[Sun Mar 24 20:55:09 2024]  Iteration number: 0 with current cost as 0.37882592139210236 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.14124975  0.56916588 -2.77491723
  3.03470979  2.25857258  1.20464027 -1.12359283  0.71075893  1.10878032
  1.38915829 -1.78122717  0.79565233]. 
Training complete taking 598.8088984489441 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 20:56:59 2024]  Iteration number: 0 with current cost as 0.4165152327167121 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.14430074  0.57258968 -2.77306911
  3.01337485  2.27742411  1.23068477 -1.13312752  0.73168749  1.10374975
  1.39385116 -1.77804267  0.79131258]. 
Working on 0.4 fold... 
[Sun Mar 24 20:59:06 2024]  Iteration number: 0 with current cost as 0.3644036784353097 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.13875475  0.56688959 -2.77559735
  3.04757655  2.24828578  1.18831688 -1.11791926  0.69811373  1.11171379
  1.3910968  -1.77867102  0.79830139]. 
Working on 0.6 fold... 
[Sun Mar 24 21:00:59 2024]  Iteration number: 0 with current cost as 0.3729845469139776 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.14247765  0.56981354 -2.7753328
  3.03234359  2.26165013  1.20696683 -1.12598885  0.71371071  1.10679473
  1.38609503 -1.78513494  0.79193706]. 
Working on 0.8 fold... 
[Sun Mar 24 21:02:58 2024]  Iteration number: 0 with current cost as 0.37643825803597736 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.14010928  0.56944789 -2.77312857
  3.03374532  2.26385027  1.20329732 -1.12302748  0.71156208  1.1097177
  1.38560682 -1.78672016  0.78789953]. 
Working on 1.0 fold... 
[Sun Mar 24 21:04:59 2024]  Iteration number: 0 with current cost as 0.37882592139210236 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.14124975  0.56916588 -2.77491723
  3.03470979  2.25857258  1.20464027 -1.12359283  0.71075893  1.10878032
  1.38915829 -1.78122717  0.79565233]. 
Training complete taking 588.3725771903992 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 21:06:49 2024]  Iteration number: 0 with current cost as 0.4165152327167121 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.14430074  0.57258968 -2.77306911
  3.01337485  2.27742411  1.23068477 -1.13312752  0.73168749  1.10374975
  1.39385116 -1.77804267  0.79131258]. 
Working on 0.4 fold... 
[Sun Mar 24 21:08:47 2024]  Iteration number: 0 with current cost as 0.3644036784353097 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.13875475  0.56688959 -2.77559735
  3.04757655  2.24828578  1.18831688 -1.11791926  0.69811373  1.11171379
  1.3910968  -1.77867102  0.79830139]. 
Working on 0.6 fold... 
[Sun Mar 24 21:10:40 2024]  Iteration number: 0 with current cost as 0.3729845469139776 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.14247765  0.56981354 -2.7753328
  3.03234359  2.26165013  1.20696683 -1.12598885  0.71371071  1.10679473
  1.38609503 -1.78513494  0.79193706]. 
Working on 0.8 fold... 
[Sun Mar 24 21:12:42 2024]  Iteration number: 0 with current cost as 0.37643825803597736 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.14010928  0.56944789 -2.77312857
  3.03374532  2.26385027  1.20329732 -1.12302748  0.71156208  1.1097177
  1.38560682 -1.78672016  0.78789953]. 
Working on 1.0 fold... 
[Sun Mar 24 21:14:41 2024]  Iteration number: 0 with current cost as 0.37882592139210236 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.14124975  0.56916588 -2.77491723
  3.03470979  2.25857258  1.20464027 -1.12359283  0.71075893  1.10878032
  1.38915829 -1.78122717  0.79565233]. 
Training complete taking 584.377158164978 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 21:16:32 2024]  Iteration number: 0 with current cost as 0.4165152327167121 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.14430074  0.57258968 -2.77306911
  3.01337485  2.27742411  1.23068477 -1.13312752  0.73168749  1.10374975
  1.39385116 -1.77804267  0.79131258]. 
Working on 0.4 fold... 
[Sun Mar 24 21:18:30 2024]  Iteration number: 0 with current cost as 0.3644036784353097 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.13875475  0.56688959 -2.77559735
  3.04757655  2.24828578  1.18831688 -1.11791926  0.69811373  1.11171379
  1.3910968  -1.77867102  0.79830139]. 
Working on 0.6 fold... 
[Sun Mar 24 21:20:25 2024]  Iteration number: 0 with current cost as 0.3729845469139776 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.14247765  0.56981354 -2.7753328
  3.03234359  2.26165013  1.20696683 -1.12598885  0.71371071  1.10679473
  1.38609503 -1.78513494  0.79193706]. 
Working on 0.8 fold... 
[Sun Mar 24 21:22:23 2024]  Iteration number: 0 with current cost as 0.37643825803597736 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.14010928  0.56944789 -2.77312857
  3.03374532  2.26385027  1.20329732 -1.12302748  0.71156208  1.1097177
  1.38560682 -1.78672016  0.78789953]. 
Working on 1.0 fold... 
[Sun Mar 24 21:24:21 2024]  Iteration number: 0 with current cost as 0.37882592139210236 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.14124975  0.56916588 -2.77491723
  3.03470979  2.25857258  1.20464027 -1.12359283  0.71075893  1.10878032
  1.38915829 -1.78122717  0.79565233]. 
Training complete taking 578.5486350059509 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 21:26:11 2024]  Iteration number: 0 with current cost as 0.4165152327167121 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.14430074  0.57258968 -2.77306911
  3.01337485  2.27742411  1.23068477 -1.13312752  0.73168749  1.10374975
  1.39385116 -1.77804267  0.79131258]. 
Working on 0.4 fold... 
[Sun Mar 24 21:28:08 2024]  Iteration number: 0 with current cost as 0.3644036784353097 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.13875475  0.56688959 -2.77559735
  3.04757655  2.24828578  1.18831688 -1.11791926  0.69811373  1.11171379
  1.3910968  -1.77867102  0.79830139]. 
Working on 0.6 fold... 
[Sun Mar 24 21:30:14 2024]  Iteration number: 0 with current cost as 0.3729845469139776 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.14247765  0.56981354 -2.7753328
  3.03234359  2.26165013  1.20696683 -1.12598885  0.71371071  1.10679473
  1.38609503 -1.78513494  0.79193706]. 
Working on 0.8 fold... 
[Sun Mar 24 21:32:14 2024]  Iteration number: 0 with current cost as 0.37643825803597736 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.14010928  0.56944789 -2.77312857
  3.03374532  2.26385027  1.20329732 -1.12302748  0.71156208  1.1097177
  1.38560682 -1.78672016  0.78789953]. 
Working on 1.0 fold... 
[Sun Mar 24 21:34:12 2024]  Iteration number: 0 with current cost as 0.37882592139210236 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.14124975  0.56916588 -2.77491723
  3.03470979  2.25857258  1.20464027 -1.12359283  0.71075893  1.10878032
  1.38915829 -1.78122717  0.79565233]. 
Training complete taking 591.6858580112457 seconds. 
Discarding model... 

Training complete taking 14760.271299123764 total seconds. 
Now scoring model... 
Scoring complete taking 0.7729754447937012 seconds. 
Saved predicted values as A2_HWE-CNOT_predicted_values.csv
Model scores: {'MSE_train': (70.79353493649862,), 'R2_train': 0.6570290826699121, 'MAE_train': 6.830832110377739, 'MSE_test': 74.76713612044873, 'R2_test': 0.5493176168792864, 'MAE_test': 6.681216664864879}. 
Saved model results as A2_HWE-CNOT_results.json. 
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Thu Apr  4 11:28:41 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 11:28:50 2024]  Iteration number: 0 with current cost as 0.4165152327167121 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.14430074  0.57258968 -2.77306911
  3.01337485  2.27742411  1.23068477 -1.13312752  0.73168749  1.10374975
  1.39385116 -1.77804267  0.79131258]. 
Working on 0.4 fold... 
[Thu Apr  4 11:30:51 2024]  Iteration number: 0 with current cost as 0.3644036784353097 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.13875475  0.56688959 -2.77559735
  3.04757655  2.24828578  1.18831688 -1.11791926  0.69811373  1.11171379
  1.3910968  -1.77867102  0.79830139]. 
Working on 0.6 fold... 
[Thu Apr  4 11:32:46 2024]  Iteration number: 0 with current cost as 0.3729845469139776 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.14247765  0.56981354 -2.7753328
  3.03234359  2.26165013  1.20696683 -1.12598885  0.71371071  1.10679473
  1.38609503 -1.78513494  0.79193706]. 
Working on 0.8 fold... 
[Thu Apr  4 11:34:44 2024]  Iteration number: 0 with current cost as 0.37643825803597736 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.14010928  0.56944789 -2.77312857
  3.03374532  2.26385027  1.20329732 -1.12302748  0.71156208  1.1097177
  1.38560682 -1.78672016  0.78789953]. 
Working on 1.0 fold... 
[Thu Apr  4 11:36:43 2024]  Iteration number: 0 with current cost as 0.37882592139210236 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.14124975  0.56916588 -2.77491723
  3.03470979  2.25857258  1.20464027 -1.12359283  0.71075893  1.10878032
  1.38915829 -1.78122717  0.79565233]. 
Training complete taking 585.2880809307098 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 11:38:33 2024]  Iteration number: 0 with current cost as 0.4165152327167121 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.14430074  0.57258968 -2.77306911
  3.01337485  2.27742411  1.23068477 -1.13312752  0.73168749  1.10374975
  1.39385116 -1.77804267  0.79131258]. 
Working on 0.4 fold... 
[Thu Apr  4 11:40:32 2024]  Iteration number: 0 with current cost as 0.3644036784353097 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.13875475  0.56688959 -2.77559735
  3.04757655  2.24828578  1.18831688 -1.11791926  0.69811373  1.11171379
  1.3910968  -1.77867102  0.79830139]. 
Working on 0.6 fold... 
[Thu Apr  4 11:42:33 2024]  Iteration number: 0 with current cost as 0.3729845469139776 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.14247765  0.56981354 -2.7753328
  3.03234359  2.26165013  1.20696683 -1.12598885  0.71371071  1.10679473
  1.38609503 -1.78513494  0.79193706]. 
Working on 0.8 fold... 
[Thu Apr  4 11:44:32 2024]  Iteration number: 0 with current cost as 0.37643825803597736 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.14010928  0.56944789 -2.77312857
  3.03374532  2.26385027  1.20329732 -1.12302748  0.71156208  1.1097177
  1.38560682 -1.78672016  0.78789953]. 
Working on 1.0 fold... 
[Thu Apr  4 11:46:31 2024]  Iteration number: 0 with current cost as 0.37882592139210236 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.14124975  0.56916588 -2.77491723
  3.03470979  2.25857258  1.20464027 -1.12359283  0.71075893  1.10878032
  1.38915829 -1.78122717  0.79565233]. 
Training complete taking 586.4700083732605 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 11:48:21 2024]  Iteration number: 0 with current cost as 0.4165152327167121 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.14430074  0.57258968 -2.77306911
  3.01337485  2.27742411  1.23068477 -1.13312752  0.73168749  1.10374975
  1.39385116 -1.77804267  0.79131258]. 
Working on 0.4 fold... 
[Thu Apr  4 11:50:22 2024]  Iteration number: 0 with current cost as 0.3644036784353097 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.13875475  0.56688959 -2.77559735
  3.04757655  2.24828578  1.18831688 -1.11791926  0.69811373  1.11171379
  1.3910968  -1.77867102  0.79830139]. 
Working on 0.6 fold... 
[Thu Apr  4 11:52:23 2024]  Iteration number: 0 with current cost as 0.3729845469139776 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.14247765  0.56981354 -2.7753328
  3.03234359  2.26165013  1.20696683 -1.12598885  0.71371071  1.10679473
  1.38609503 -1.78513494  0.79193706]. 
Working on 0.8 fold... 
[Thu Apr  4 11:54:20 2024]  Iteration number: 0 with current cost as 0.37643825803597736 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.14010928  0.56944789 -2.77312857
  3.03374532  2.26385027  1.20329732 -1.12302748  0.71156208  1.1097177
  1.38560682 -1.78672016  0.78789953]. 
Working on 1.0 fold... 
[Thu Apr  4 11:56:18 2024]  Iteration number: 0 with current cost as 0.37882592139210236 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.14124975  0.56916588 -2.77491723
  3.03470979  2.25857258  1.20464027 -1.12359283  0.71075893  1.10878032
  1.38915829 -1.78122717  0.79565233]. 
Training complete taking 589.4536747932434 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 11:58:09 2024]  Iteration number: 0 with current cost as 0.4165152327167121 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.14430074  0.57258968 -2.77306911
  3.01337485  2.27742411  1.23068477 -1.13312752  0.73168749  1.10374975
  1.39385116 -1.77804267  0.79131258]. 
Working on 0.4 fold... 
[Thu Apr  4 12:00:09 2024]  Iteration number: 0 with current cost as 0.3644036784353097 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.13875475  0.56688959 -2.77559735
  3.04757655  2.24828578  1.18831688 -1.11791926  0.69811373  1.11171379
  1.3910968  -1.77867102  0.79830139]. 
Working on 0.6 fold... 
[Thu Apr  4 12:02:04 2024]  Iteration number: 0 with current cost as 0.3729845469139776 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.14247765  0.56981354 -2.7753328
  3.03234359  2.26165013  1.20696683 -1.12598885  0.71371071  1.10679473
  1.38609503 -1.78513494  0.79193706]. 
Working on 0.8 fold... 
[Thu Apr  4 12:04:03 2024]  Iteration number: 0 with current cost as 0.37643825803597736 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.14010928  0.56944789 -2.77312857
  3.03374532  2.26385027  1.20329732 -1.12302748  0.71156208  1.1097177
  1.38560682 -1.78672016  0.78789953]. 
Working on 1.0 fold... 
[Thu Apr  4 12:06:02 2024]  Iteration number: 0 with current cost as 0.37882592139210236 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.14124975  0.56916588 -2.77491723
  3.03470979  2.25857258  1.20464027 -1.12359283  0.71075893  1.10878032
  1.38915829 -1.78122717  0.79565233]. 
Training complete taking 583.9313490390778 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 12:07:55 2024]  Iteration number: 0 with current cost as 0.4165152327167121 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.14430074  0.57258968 -2.77306911
  3.01337485  2.27742411  1.23068477 -1.13312752  0.73168749  1.10374975
  1.39385116 -1.77804267  0.79131258]. 
Working on 0.4 fold... 
[Thu Apr  4 12:10:04 2024]  Iteration number: 0 with current cost as 0.3644036784353097 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.13875475  0.56688959 -2.77559735
  3.04757655  2.24828578  1.18831688 -1.11791926  0.69811373  1.11171379
  1.3910968  -1.77867102  0.79830139]. 
Working on 0.6 fold... 
[Thu Apr  4 12:12:04 2024]  Iteration number: 0 with current cost as 0.3729845469139776 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.14247765  0.56981354 -2.7753328
  3.03234359  2.26165013  1.20696683 -1.12598885  0.71371071  1.10679473
  1.38609503 -1.78513494  0.79193706]. 
Working on 0.8 fold... 
[Thu Apr  4 12:14:05 2024]  Iteration number: 0 with current cost as 0.37643825803597736 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.14010928  0.56944789 -2.77312857
  3.03374532  2.26385027  1.20329732 -1.12302748  0.71156208  1.1097177
  1.38560682 -1.78672016  0.78789953]. 
Working on 1.0 fold... 
[Thu Apr  4 12:16:04 2024]  Iteration number: 0 with current cost as 0.37882592139210236 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.14124975  0.56916588 -2.77491723
  3.03470979  2.25857258  1.20464027 -1.12359283  0.71075893  1.10878032
  1.38915829 -1.78122717  0.79565233]. 
Training complete taking 602.0463440418243 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 12:17:58 2024]  Iteration number: 0 with current cost as 0.4165152327167121 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.14430074  0.57258968 -2.77306911
  3.01337485  2.27742411  1.23068477 -1.13312752  0.73168749  1.10374975
  1.39385116 -1.77804267  0.79131258]. 
Working on 0.4 fold... 
[Thu Apr  4 12:19:56 2024]  Iteration number: 0 with current cost as 0.3644036784353097 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.13875475  0.56688959 -2.77559735
  3.04757655  2.24828578  1.18831688 -1.11791926  0.69811373  1.11171379
  1.3910968  -1.77867102  0.79830139]. 
Working on 0.6 fold... 
[Thu Apr  4 12:21:50 2024]  Iteration number: 0 with current cost as 0.3729845469139776 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.14247765  0.56981354 -2.7753328
  3.03234359  2.26165013  1.20696683 -1.12598885  0.71371071  1.10679473
  1.38609503 -1.78513494  0.79193706]. 
Working on 0.8 fold... 
[Thu Apr  4 12:23:48 2024]  Iteration number: 0 with current cost as 0.37643825803597736 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.14010928  0.56944789 -2.77312857
  3.03374532  2.26385027  1.20329732 -1.12302748  0.71156208  1.1097177
  1.38560682 -1.78672016  0.78789953]. 
Working on 1.0 fold... 
[Thu Apr  4 12:25:47 2024]  Iteration number: 0 with current cost as 0.37882592139210236 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.14124975  0.56916588 -2.77491723
  3.03470979  2.25857258  1.20464027 -1.12359283  0.71075893  1.10878032
  1.38915829 -1.78122717  0.79565233]. 
Training complete taking 582.7323844432831 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 12:27:38 2024]  Iteration number: 0 with current cost as 0.4165152327167121 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.14430074  0.57258968 -2.77306911
  3.01337485  2.27742411  1.23068477 -1.13312752  0.73168749  1.10374975
  1.39385116 -1.77804267  0.79131258]. 
Working on 0.4 fold... 
[Thu Apr  4 12:29:39 2024]  Iteration number: 0 with current cost as 0.3644036784353097 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.13875475  0.56688959 -2.77559735
  3.04757655  2.24828578  1.18831688 -1.11791926  0.69811373  1.11171379
  1.3910968  -1.77867102  0.79830139]. 
Working on 0.6 fold... 
[Thu Apr  4 12:31:35 2024]  Iteration number: 0 with current cost as 0.3729845469139776 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.14247765  0.56981354 -2.7753328
  3.03234359  2.26165013  1.20696683 -1.12598885  0.71371071  1.10679473
  1.38609503 -1.78513494  0.79193706]. 
Working on 0.8 fold... 
[Thu Apr  4 12:33:34 2024]  Iteration number: 0 with current cost as 0.37643825803597736 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.14010928  0.56944789 -2.77312857
  3.03374532  2.26385027  1.20329732 -1.12302748  0.71156208  1.1097177
  1.38560682 -1.78672016  0.78789953]. 
Working on 1.0 fold... 
[Thu Apr  4 12:35:33 2024]  Iteration number: 0 with current cost as 0.37882592139210236 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.14124975  0.56916588 -2.77491723
  3.03470979  2.25857258  1.20464027 -1.12359283  0.71075893  1.10878032
  1.38915829 -1.78122717  0.79565233]. 
Training complete taking 602.1865046024323 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 12:37:40 2024]  Iteration number: 0 with current cost as 0.4165152327167121 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.14430074  0.57258968 -2.77306911
  3.01337485  2.27742411  1.23068477 -1.13312752  0.73168749  1.10374975
  1.39385116 -1.77804267  0.79131258]. 
Working on 0.4 fold... 
[Thu Apr  4 12:39:38 2024]  Iteration number: 0 with current cost as 0.3644036784353097 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.13875475  0.56688959 -2.77559735
  3.04757655  2.24828578  1.18831688 -1.11791926  0.69811373  1.11171379
  1.3910968  -1.77867102  0.79830139]. 
Working on 0.6 fold... 
[Thu Apr  4 12:41:33 2024]  Iteration number: 0 with current cost as 0.3729845469139776 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.14247765  0.56981354 -2.7753328
  3.03234359  2.26165013  1.20696683 -1.12598885  0.71371071  1.10679473
  1.38609503 -1.78513494  0.79193706]. 
Working on 0.8 fold... 
[Thu Apr  4 12:43:32 2024]  Iteration number: 0 with current cost as 0.37643825803597736 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.14010928  0.56944789 -2.77312857
  3.03374532  2.26385027  1.20329732 -1.12302748  0.71156208  1.1097177
  1.38560682 -1.78672016  0.78789953]. 
Working on 1.0 fold... 
[Thu Apr  4 12:45:31 2024]  Iteration number: 0 with current cost as 0.37882592139210236 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.14124975  0.56916588 -2.77491723
  3.03470979  2.25857258  1.20464027 -1.12359283  0.71075893  1.10878032
  1.38915829 -1.78122717  0.79565233]. 
Training complete taking 604.4126915931702 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 12:47:45 2024]  Iteration number: 0 with current cost as 0.4165152327167121 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.14430074  0.57258968 -2.77306911
  3.01337485  2.27742411  1.23068477 -1.13312752  0.73168749  1.10374975
  1.39385116 -1.77804267  0.79131258]. 
Working on 0.4 fold... 
[Thu Apr  4 12:49:43 2024]  Iteration number: 0 with current cost as 0.3644036784353097 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.13875475  0.56688959 -2.77559735
  3.04757655  2.24828578  1.18831688 -1.11791926  0.69811373  1.11171379
  1.3910968  -1.77867102  0.79830139]. 
Working on 0.6 fold... 
[Thu Apr  4 12:51:39 2024]  Iteration number: 0 with current cost as 0.3729845469139776 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.14247765  0.56981354 -2.7753328
  3.03234359  2.26165013  1.20696683 -1.12598885  0.71371071  1.10679473
  1.38609503 -1.78513494  0.79193706]. 
Working on 0.8 fold... 
[Thu Apr  4 12:53:46 2024]  Iteration number: 0 with current cost as 0.37643825803597736 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.14010928  0.56944789 -2.77312857
  3.03374532  2.26385027  1.20329732 -1.12302748  0.71156208  1.1097177
  1.38560682 -1.78672016  0.78789953]. 
Working on 1.0 fold... 
[Thu Apr  4 12:55:48 2024]  Iteration number: 0 with current cost as 0.37882592139210236 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.14124975  0.56916588 -2.77491723
  3.03470979  2.25857258  1.20464027 -1.12359283  0.71075893  1.10878032
  1.38915829 -1.78122717  0.79565233]. 
Training complete taking 594.692369222641 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 12:57:39 2024]  Iteration number: 0 with current cost as 0.4165152327167121 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.14430074  0.57258968 -2.77306911
  3.01337485  2.27742411  1.23068477 -1.13312752  0.73168749  1.10374975
  1.39385116 -1.77804267  0.79131258]. 
Working on 0.4 fold... 
[Thu Apr  4 12:59:46 2024]  Iteration number: 0 with current cost as 0.3644036784353097 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.13875475  0.56688959 -2.77559735
  3.04757655  2.24828578  1.18831688 -1.11791926  0.69811373  1.11171379
  1.3910968  -1.77867102  0.79830139]. 
Working on 0.6 fold... 
[Thu Apr  4 13:01:41 2024]  Iteration number: 0 with current cost as 0.3729845469139776 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.14247765  0.56981354 -2.7753328
  3.03234359  2.26165013  1.20696683 -1.12598885  0.71371071  1.10679473
  1.38609503 -1.78513494  0.79193706]. 
Working on 0.8 fold... 
[Thu Apr  4 13:03:41 2024]  Iteration number: 0 with current cost as 0.37643825803597736 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.14010928  0.56944789 -2.77312857
  3.03374532  2.26385027  1.20329732 -1.12302748  0.71156208  1.1097177
  1.38560682 -1.78672016  0.78789953]. 
Working on 1.0 fold... 
[Thu Apr  4 13:05:41 2024]  Iteration number: 0 with current cost as 0.37882592139210236 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.14124975  0.56916588 -2.77491723
  3.03470979  2.25857258  1.20464027 -1.12359283  0.71075893  1.10878032
  1.38915829 -1.78122717  0.79565233]. 
Training complete taking 590.7766108512878 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 13:07:32 2024]  Iteration number: 0 with current cost as 0.4165152327167121 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.14430074  0.57258968 -2.77306911
  3.01337485  2.27742411  1.23068477 -1.13312752  0.73168749  1.10374975
  1.39385116 -1.77804267  0.79131258]. 
Working on 0.4 fold... 
[Thu Apr  4 13:09:29 2024]  Iteration number: 0 with current cost as 0.3644036784353097 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.13875475  0.56688959 -2.77559735
  3.04757655  2.24828578  1.18831688 -1.11791926  0.69811373  1.11171379
  1.3910968  -1.77867102  0.79830139]. 
Working on 0.6 fold... 
[Thu Apr  4 13:11:21 2024]  Iteration number: 0 with current cost as 0.3729845469139776 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.14247765  0.56981354 -2.7753328
  3.03234359  2.26165013  1.20696683 -1.12598885  0.71371071  1.10679473
  1.38609503 -1.78513494  0.79193706]. 
Working on 0.8 fold... 
[Thu Apr  4 13:13:20 2024]  Iteration number: 0 with current cost as 0.37643825803597736 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.14010928  0.56944789 -2.77312857
  3.03374532  2.26385027  1.20329732 -1.12302748  0.71156208  1.1097177
  1.38560682 -1.78672016  0.78789953]. 
Working on 1.0 fold... 
[Thu Apr  4 13:15:17 2024]  Iteration number: 0 with current cost as 0.37882592139210236 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.14124975  0.56916588 -2.77491723
  3.03470979  2.25857258  1.20464027 -1.12359283  0.71075893  1.10878032
  1.38915829 -1.78122717  0.79565233]. 
Training complete taking 578.2735278606415 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 13:17:08 2024]  Iteration number: 0 with current cost as 0.4165152327167121 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.14430074  0.57258968 -2.77306911
  3.01337485  2.27742411  1.23068477 -1.13312752  0.73168749  1.10374975
  1.39385116 -1.77804267  0.79131258]. 
Working on 0.4 fold... 
[Thu Apr  4 13:19:07 2024]  Iteration number: 0 with current cost as 0.3644036784353097 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.13875475  0.56688959 -2.77559735
  3.04757655  2.24828578  1.18831688 -1.11791926  0.69811373  1.11171379
  1.3910968  -1.77867102  0.79830139]. 
Working on 0.6 fold... 
[Thu Apr  4 13:21:01 2024]  Iteration number: 0 with current cost as 0.3729845469139776 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.14247765  0.56981354 -2.7753328
  3.03234359  2.26165013  1.20696683 -1.12598885  0.71371071  1.10679473
  1.38609503 -1.78513494  0.79193706]. 
Working on 0.8 fold... 
[Thu Apr  4 13:22:59 2024]  Iteration number: 0 with current cost as 0.37643825803597736 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.14010928  0.56944789 -2.77312857
  3.03374532  2.26385027  1.20329732 -1.12302748  0.71156208  1.1097177
  1.38560682 -1.78672016  0.78789953]. 
Working on 1.0 fold... 
[Thu Apr  4 13:24:58 2024]  Iteration number: 0 with current cost as 0.37882592139210236 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.14124975  0.56916588 -2.77491723
  3.03470979  2.25857258  1.20464027 -1.12359283  0.71075893  1.10878032
  1.38915829 -1.78122717  0.79565233]. 
Training complete taking 578.2618026733398 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 13:26:47 2024]  Iteration number: 0 with current cost as 0.4165152327167121 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.14430074  0.57258968 -2.77306911
  3.01337485  2.27742411  1.23068477 -1.13312752  0.73168749  1.10374975
  1.39385116 -1.77804267  0.79131258]. 
Working on 0.4 fold... 
[Thu Apr  4 13:28:44 2024]  Iteration number: 0 with current cost as 0.3644036784353097 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.13875475  0.56688959 -2.77559735
  3.04757655  2.24828578  1.18831688 -1.11791926  0.69811373  1.11171379
  1.3910968  -1.77867102  0.79830139]. 
Working on 0.6 fold... 
[Thu Apr  4 13:30:46 2024]  Iteration number: 0 with current cost as 0.3729845469139776 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.14247765  0.56981354 -2.7753328
  3.03234359  2.26165013  1.20696683 -1.12598885  0.71371071  1.10679473
  1.38609503 -1.78513494  0.79193706]. 
Working on 0.8 fold... 
[Thu Apr  4 13:32:44 2024]  Iteration number: 0 with current cost as 0.37643825803597736 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.14010928  0.56944789 -2.77312857
  3.03374532  2.26385027  1.20329732 -1.12302748  0.71156208  1.1097177
  1.38560682 -1.78672016  0.78789953]. 
Working on 1.0 fold... 
[Thu Apr  4 13:34:43 2024]  Iteration number: 0 with current cost as 0.37882592139210236 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.14124975  0.56916588 -2.77491723
  3.03470979  2.25857258  1.20464027 -1.12359283  0.71075893  1.10878032
  1.38915829 -1.78122717  0.79565233]. 
Training complete taking 585.061140537262 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 13:36:33 2024]  Iteration number: 0 with current cost as 0.4165152327167121 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.14430074  0.57258968 -2.77306911
  3.01337485  2.27742411  1.23068477 -1.13312752  0.73168749  1.10374975
  1.39385116 -1.77804267  0.79131258]. 
Working on 0.4 fold... 
[Thu Apr  4 13:38:31 2024]  Iteration number: 0 with current cost as 0.3644036784353097 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.13875475  0.56688959 -2.77559735
  3.04757655  2.24828578  1.18831688 -1.11791926  0.69811373  1.11171379
  1.3910968  -1.77867102  0.79830139]. 
Working on 0.6 fold... 
[Thu Apr  4 13:40:24 2024]  Iteration number: 0 with current cost as 0.3729845469139776 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.14247765  0.56981354 -2.7753328
  3.03234359  2.26165013  1.20696683 -1.12598885  0.71371071  1.10679473
  1.38609503 -1.78513494  0.79193706]. 
Working on 0.8 fold... 
[Thu Apr  4 13:42:22 2024]  Iteration number: 0 with current cost as 0.37643825803597736 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.14010928  0.56944789 -2.77312857
  3.03374532  2.26385027  1.20329732 -1.12302748  0.71156208  1.1097177
  1.38560682 -1.78672016  0.78789953]. 
Working on 1.0 fold... 
[Thu Apr  4 13:44:20 2024]  Iteration number: 0 with current cost as 0.37882592139210236 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.14124975  0.56916588 -2.77491723
  3.03470979  2.25857258  1.20464027 -1.12359283  0.71075893  1.10878032
  1.38915829 -1.78122717  0.79565233]. 
Training complete taking 577.8874471187592 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 13:46:10 2024]  Iteration number: 0 with current cost as 0.4165152327167121 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.14430074  0.57258968 -2.77306911
  3.01337485  2.27742411  1.23068477 -1.13312752  0.73168749  1.10374975
  1.39385116 -1.77804267  0.79131258]. 
Working on 0.4 fold... 
[Thu Apr  4 13:48:07 2024]  Iteration number: 0 with current cost as 0.3644036784353097 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.13875475  0.56688959 -2.77559735
  3.04757655  2.24828578  1.18831688 -1.11791926  0.69811373  1.11171379
  1.3910968  -1.77867102  0.79830139]. 
Working on 0.6 fold... 
[Thu Apr  4 13:50:00 2024]  Iteration number: 0 with current cost as 0.3729845469139776 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.14247765  0.56981354 -2.7753328
  3.03234359  2.26165013  1.20696683 -1.12598885  0.71371071  1.10679473
  1.38609503 -1.78513494  0.79193706]. 
Working on 0.8 fold... 
[Thu Apr  4 13:51:56 2024]  Iteration number: 0 with current cost as 0.37643825803597736 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.14010928  0.56944789 -2.77312857
  3.03374532  2.26385027  1.20329732 -1.12302748  0.71156208  1.1097177
  1.38560682 -1.78672016  0.78789953]. 
Working on 1.0 fold... 
[Thu Apr  4 13:53:53 2024]  Iteration number: 0 with current cost as 0.37882592139210236 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.14124975  0.56916588 -2.77491723
  3.03470979  2.25857258  1.20464027 -1.12359283  0.71075893  1.10878032
  1.38915829 -1.78122717  0.79565233]. 
Training complete taking 574.692152261734 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 13:55:44 2024]  Iteration number: 0 with current cost as 0.4165152327167121 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.14430074  0.57258968 -2.77306911
  3.01337485  2.27742411  1.23068477 -1.13312752  0.73168749  1.10374975
  1.39385116 -1.77804267  0.79131258]. 
Working on 0.4 fold... 
[Thu Apr  4 13:57:43 2024]  Iteration number: 0 with current cost as 0.3644036784353097 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.13875475  0.56688959 -2.77559735
  3.04757655  2.24828578  1.18831688 -1.11791926  0.69811373  1.11171379
  1.3910968  -1.77867102  0.79830139]. 
Working on 0.6 fold... 
[Thu Apr  4 13:59:37 2024]  Iteration number: 0 with current cost as 0.3729845469139776 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.14247765  0.56981354 -2.7753328
  3.03234359  2.26165013  1.20696683 -1.12598885  0.71371071  1.10679473
  1.38609503 -1.78513494  0.79193706]. 
Working on 0.8 fold... 
[Thu Apr  4 14:01:36 2024]  Iteration number: 0 with current cost as 0.37643825803597736 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.14010928  0.56944789 -2.77312857
  3.03374532  2.26385027  1.20329732 -1.12302748  0.71156208  1.1097177
  1.38560682 -1.78672016  0.78789953]. 
Working on 1.0 fold... 
[Thu Apr  4 14:03:34 2024]  Iteration number: 0 with current cost as 0.37882592139210236 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.14124975  0.56916588 -2.77491723
  3.03470979  2.25857258  1.20464027 -1.12359283  0.71075893  1.10878032
  1.38915829 -1.78122717  0.79565233]. 
Training complete taking 578.3069047927856 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 14:05:23 2024]  Iteration number: 0 with current cost as 0.4165152327167121 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.14430074  0.57258968 -2.77306911
  3.01337485  2.27742411  1.23068477 -1.13312752  0.73168749  1.10374975
  1.39385116 -1.77804267  0.79131258]. 
Working on 0.4 fold... 
[Thu Apr  4 14:07:28 2024]  Iteration number: 0 with current cost as 0.3644036784353097 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.13875475  0.56688959 -2.77559735
  3.04757655  2.24828578  1.18831688 -1.11791926  0.69811373  1.11171379
  1.3910968  -1.77867102  0.79830139]. 
Working on 0.6 fold... 
[Thu Apr  4 14:09:21 2024]  Iteration number: 0 with current cost as 0.3729845469139776 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.14247765  0.56981354 -2.7753328
  3.03234359  2.26165013  1.20696683 -1.12598885  0.71371071  1.10679473
  1.38609503 -1.78513494  0.79193706]. 
Working on 0.8 fold... 
[Thu Apr  4 14:11:18 2024]  Iteration number: 0 with current cost as 0.37643825803597736 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.14010928  0.56944789 -2.77312857
  3.03374532  2.26385027  1.20329732 -1.12302748  0.71156208  1.1097177
  1.38560682 -1.78672016  0.78789953]. 
Working on 1.0 fold... 
[Thu Apr  4 14:13:17 2024]  Iteration number: 0 with current cost as 0.37882592139210236 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.14124975  0.56916588 -2.77491723
  3.03470979  2.25857258  1.20464027 -1.12359283  0.71075893  1.10878032
  1.38915829 -1.78122717  0.79565233]. 
Training complete taking 582.4204981327057 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 14:15:07 2024]  Iteration number: 0 with current cost as 0.4165152327167121 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.14430074  0.57258968 -2.77306911
  3.01337485  2.27742411  1.23068477 -1.13312752  0.73168749  1.10374975
  1.39385116 -1.77804267  0.79131258]. 
Working on 0.4 fold... 
[Thu Apr  4 14:17:16 2024]  Iteration number: 0 with current cost as 0.3644036784353097 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.13875475  0.56688959 -2.77559735
  3.04757655  2.24828578  1.18831688 -1.11791926  0.69811373  1.11171379
  1.3910968  -1.77867102  0.79830139]. 
Working on 0.6 fold... 
[Thu Apr  4 14:19:14 2024]  Iteration number: 0 with current cost as 0.3729845469139776 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.14247765  0.56981354 -2.7753328
  3.03234359  2.26165013  1.20696683 -1.12598885  0.71371071  1.10679473
  1.38609503 -1.78513494  0.79193706]. 
Working on 0.8 fold... 
[Thu Apr  4 14:21:12 2024]  Iteration number: 0 with current cost as 0.37643825803597736 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.14010928  0.56944789 -2.77312857
  3.03374532  2.26385027  1.20329732 -1.12302748  0.71156208  1.1097177
  1.38560682 -1.78672016  0.78789953]. 
Working on 1.0 fold... 
[Thu Apr  4 14:23:10 2024]  Iteration number: 0 with current cost as 0.37882592139210236 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.14124975  0.56916588 -2.77491723
  3.03470979  2.25857258  1.20464027 -1.12359283  0.71075893  1.10878032
  1.38915829 -1.78122717  0.79565233]. 
Training complete taking 595.4815201759338 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 14:25:03 2024]  Iteration number: 0 with current cost as 0.4165152327167121 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.14430074  0.57258968 -2.77306911
  3.01337485  2.27742411  1.23068477 -1.13312752  0.73168749  1.10374975
  1.39385116 -1.77804267  0.79131258]. 
Working on 0.4 fold... 
[Thu Apr  4 14:26:58 2024]  Iteration number: 0 with current cost as 0.3644036784353097 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.13875475  0.56688959 -2.77559735
  3.04757655  2.24828578  1.18831688 -1.11791926  0.69811373  1.11171379
  1.3910968  -1.77867102  0.79830139]. 
Working on 0.6 fold... 
[Thu Apr  4 14:28:52 2024]  Iteration number: 0 with current cost as 0.3729845469139776 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.14247765  0.56981354 -2.7753328
  3.03234359  2.26165013  1.20696683 -1.12598885  0.71371071  1.10679473
  1.38609503 -1.78513494  0.79193706]. 
Working on 0.8 fold... 
[Thu Apr  4 14:30:51 2024]  Iteration number: 0 with current cost as 0.37643825803597736 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.14010928  0.56944789 -2.77312857
  3.03374532  2.26385027  1.20329732 -1.12302748  0.71156208  1.1097177
  1.38560682 -1.78672016  0.78789953]. 
Working on 1.0 fold... 
[Thu Apr  4 14:32:48 2024]  Iteration number: 0 with current cost as 0.37882592139210236 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.14124975  0.56916588 -2.77491723
  3.03470979  2.25857258  1.20464027 -1.12359283  0.71075893  1.10878032
  1.38915829 -1.78122717  0.79565233]. 
Training complete taking 578.3000528812408 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 14:34:39 2024]  Iteration number: 0 with current cost as 0.4165152327167121 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.14430074  0.57258968 -2.77306911
  3.01337485  2.27742411  1.23068477 -1.13312752  0.73168749  1.10374975
  1.39385116 -1.77804267  0.79131258]. 
Working on 0.4 fold... 
[Thu Apr  4 14:36:38 2024]  Iteration number: 0 with current cost as 0.3644036784353097 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.13875475  0.56688959 -2.77559735
  3.04757655  2.24828578  1.18831688 -1.11791926  0.69811373  1.11171379
  1.3910968  -1.77867102  0.79830139]. 
Working on 0.6 fold... 
[Thu Apr  4 14:38:32 2024]  Iteration number: 0 with current cost as 0.3729845469139776 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.14247765  0.56981354 -2.7753328
  3.03234359  2.26165013  1.20696683 -1.12598885  0.71371071  1.10679473
  1.38609503 -1.78513494  0.79193706]. 
Working on 0.8 fold... 
[Thu Apr  4 14:40:28 2024]  Iteration number: 0 with current cost as 0.37643825803597736 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.14010928  0.56944789 -2.77312857
  3.03374532  2.26385027  1.20329732 -1.12302748  0.71156208  1.1097177
  1.38560682 -1.78672016  0.78789953]. 
Working on 1.0 fold... 
[Thu Apr  4 14:42:50 2024]  Iteration number: 0 with current cost as 0.37882592139210236 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.14124975  0.56916588 -2.77491723
  3.03470979  2.25857258  1.20464027 -1.12359283  0.71075893  1.10878032
  1.38915829 -1.78122717  0.79565233]. 
Training complete taking 602.9278228282928 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 14:44:42 2024]  Iteration number: 0 with current cost as 0.4165152327167121 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.14430074  0.57258968 -2.77306911
  3.01337485  2.27742411  1.23068477 -1.13312752  0.73168749  1.10374975
  1.39385116 -1.77804267  0.79131258]. 
Working on 0.4 fold... 
[Thu Apr  4 14:46:40 2024]  Iteration number: 0 with current cost as 0.3644036784353097 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.13875475  0.56688959 -2.77559735
  3.04757655  2.24828578  1.18831688 -1.11791926  0.69811373  1.11171379
  1.3910968  -1.77867102  0.79830139]. 
Working on 0.6 fold... 
[Thu Apr  4 14:48:37 2024]  Iteration number: 0 with current cost as 0.3729845469139776 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.14247765  0.56981354 -2.7753328
  3.03234359  2.26165013  1.20696683 -1.12598885  0.71371071  1.10679473
  1.38609503 -1.78513494  0.79193706]. 
Working on 0.8 fold... 
[Thu Apr  4 14:50:34 2024]  Iteration number: 0 with current cost as 0.37643825803597736 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.14010928  0.56944789 -2.77312857
  3.03374532  2.26385027  1.20329732 -1.12302748  0.71156208  1.1097177
  1.38560682 -1.78672016  0.78789953]. 
Working on 1.0 fold... 
[Thu Apr  4 14:52:30 2024]  Iteration number: 0 with current cost as 0.37882592139210236 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.14124975  0.56916588 -2.77491723
  3.03470979  2.25857258  1.20464027 -1.12359283  0.71075893  1.10878032
  1.38915829 -1.78122717  0.79565233]. 
Training complete taking 579.1022219657898 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 14:54:21 2024]  Iteration number: 0 with current cost as 0.4165152327167121 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.14430074  0.57258968 -2.77306911
  3.01337485  2.27742411  1.23068477 -1.13312752  0.73168749  1.10374975
  1.39385116 -1.77804267  0.79131258]. 
Working on 0.4 fold... 
[Thu Apr  4 14:56:19 2024]  Iteration number: 0 with current cost as 0.3644036784353097 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.13875475  0.56688959 -2.77559735
  3.04757655  2.24828578  1.18831688 -1.11791926  0.69811373  1.11171379
  1.3910968  -1.77867102  0.79830139]. 
Working on 0.6 fold... 
[Thu Apr  4 14:58:12 2024]  Iteration number: 0 with current cost as 0.3729845469139776 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.14247765  0.56981354 -2.7753328
  3.03234359  2.26165013  1.20696683 -1.12598885  0.71371071  1.10679473
  1.38609503 -1.78513494  0.79193706]. 
Working on 0.8 fold... 
[Thu Apr  4 15:00:09 2024]  Iteration number: 0 with current cost as 0.37643825803597736 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.14010928  0.56944789 -2.77312857
  3.03374532  2.26385027  1.20329732 -1.12302748  0.71156208  1.1097177
  1.38560682 -1.78672016  0.78789953]. 
Working on 1.0 fold... 
[Thu Apr  4 15:02:08 2024]  Iteration number: 0 with current cost as 0.37882592139210236 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.14124975  0.56916588 -2.77491723
  3.03470979  2.25857258  1.20464027 -1.12359283  0.71075893  1.10878032
  1.38915829 -1.78122717  0.79565233]. 
Training complete taking 575.3591783046722 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 15:03:56 2024]  Iteration number: 0 with current cost as 0.4165152327167121 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.14430074  0.57258968 -2.77306911
  3.01337485  2.27742411  1.23068477 -1.13312752  0.73168749  1.10374975
  1.39385116 -1.77804267  0.79131258]. 
Working on 0.4 fold... 
[Thu Apr  4 15:05:54 2024]  Iteration number: 0 with current cost as 0.3644036784353097 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.13875475  0.56688959 -2.77559735
  3.04757655  2.24828578  1.18831688 -1.11791926  0.69811373  1.11171379
  1.3910968  -1.77867102  0.79830139]. 
Working on 0.6 fold... 
[Thu Apr  4 15:07:48 2024]  Iteration number: 0 with current cost as 0.3729845469139776 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.14247765  0.56981354 -2.7753328
  3.03234359  2.26165013  1.20696683 -1.12598885  0.71371071  1.10679473
  1.38609503 -1.78513494  0.79193706]. 
Working on 0.8 fold... 
[Thu Apr  4 15:09:46 2024]  Iteration number: 0 with current cost as 0.37643825803597736 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.14010928  0.56944789 -2.77312857
  3.03374532  2.26385027  1.20329732 -1.12302748  0.71156208  1.1097177
  1.38560682 -1.78672016  0.78789953]. 
Working on 1.0 fold... 
[Thu Apr  4 15:11:44 2024]  Iteration number: 0 with current cost as 0.37882592139210236 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.14124975  0.56916588 -2.77491723
  3.03470979  2.25857258  1.20464027 -1.12359283  0.71075893  1.10878032
  1.38915829 -1.78122717  0.79565233]. 
Training complete taking 577.0699994564056 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 15:13:35 2024]  Iteration number: 0 with current cost as 0.4165152327167121 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.14430074  0.57258968 -2.77306911
  3.01337485  2.27742411  1.23068477 -1.13312752  0.73168749  1.10374975
  1.39385116 -1.77804267  0.79131258]. 
Working on 0.4 fold... 
[Thu Apr  4 15:15:31 2024]  Iteration number: 0 with current cost as 0.3644036784353097 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.13875475  0.56688959 -2.77559735
  3.04757655  2.24828578  1.18831688 -1.11791926  0.69811373  1.11171379
  1.3910968  -1.77867102  0.79830139]. 
Working on 0.6 fold... 
[Thu Apr  4 15:17:26 2024]  Iteration number: 0 with current cost as 0.3729845469139776 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.14247765  0.56981354 -2.7753328
  3.03234359  2.26165013  1.20696683 -1.12598885  0.71371071  1.10679473
  1.38609503 -1.78513494  0.79193706]. 
Working on 0.8 fold... 
[Thu Apr  4 15:19:37 2024]  Iteration number: 0 with current cost as 0.37643825803597736 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.14010928  0.56944789 -2.77312857
  3.03374532  2.26385027  1.20329732 -1.12302748  0.71156208  1.1097177
  1.38560682 -1.78672016  0.78789953]. 
Working on 1.0 fold... 
[Thu Apr  4 15:21:35 2024]  Iteration number: 0 with current cost as 0.37882592139210236 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.14124975  0.56916588 -2.77491723
  3.03470979  2.25857258  1.20464027 -1.12359283  0.71075893  1.10878032
  1.38915829 -1.78122717  0.79565233]. 
Training complete taking 590.2910666465759 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 15:23:24 2024]  Iteration number: 0 with current cost as 0.4165152327167121 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.14430074  0.57258968 -2.77306911
  3.01337485  2.27742411  1.23068477 -1.13312752  0.73168749  1.10374975
  1.39385116 -1.77804267  0.79131258]. 
Working on 0.4 fold... 
[Thu Apr  4 15:25:24 2024]  Iteration number: 0 with current cost as 0.3644036784353097 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.13875475  0.56688959 -2.77559735
  3.04757655  2.24828578  1.18831688 -1.11791926  0.69811373  1.11171379
  1.3910968  -1.77867102  0.79830139]. 
Working on 0.6 fold... 
[Thu Apr  4 15:27:20 2024]  Iteration number: 0 with current cost as 0.3729845469139776 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.14247765  0.56981354 -2.7753328
  3.03234359  2.26165013  1.20696683 -1.12598885  0.71371071  1.10679473
  1.38609503 -1.78513494  0.79193706]. 
Working on 0.8 fold... 
[Thu Apr  4 15:29:23 2024]  Iteration number: 0 with current cost as 0.37643825803597736 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.14010928  0.56944789 -2.77312857
  3.03374532  2.26385027  1.20329732 -1.12302748  0.71156208  1.1097177
  1.38560682 -1.78672016  0.78789953]. 
Working on 1.0 fold... 
[Thu Apr  4 15:31:20 2024]  Iteration number: 0 with current cost as 0.37882592139210236 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.14124975  0.56916588 -2.77491723
  3.03470979  2.25857258  1.20464027 -1.12359283  0.71075893  1.10878032
  1.38915829 -1.78122717  0.79565233]. 
Training complete taking 585.242018699646 seconds. 
Discarding model... 

Training complete taking 14660.669401407242 total seconds. 
Now scoring model... 
Scoring complete taking 0.7687971591949463 seconds. 
Saved predicted values as A2_HWE-CNOT_predicted_values.csv
Model scores: {'MSE_train': (70.79353493649862,), 'R2_train': 0.6570290826699121, 'MAE_train': 6.830832110377739, 'MSE_test': 74.76713612044873, 'R2_test': 0.5493176168792864, 'MAE_test': 6.681216664864879}. 
Saved model results as A2_HWE-CNOT_results.json. 
