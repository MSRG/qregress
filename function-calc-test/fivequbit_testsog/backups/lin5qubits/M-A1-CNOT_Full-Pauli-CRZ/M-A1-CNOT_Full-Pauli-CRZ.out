/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/circuits/Encoders.py:12: RuntimeWarning: invalid value encountered in arcsin
  if np.isnan(np.arcsin(features[feature_idx])) or np.isnan(np.arccos(features[feature_idx])):
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Sun Mar 24 17:29:52 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 17:30:14 2024]  Iteration number: 0 with current cost as 0.3246559745236599 and parameters 
[-3.29181247  2.4906329  -2.10793542 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.69764308  1.14432445
  1.7223314  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 17:36:06 2024]  Iteration number: 0 with current cost as 0.3000099586663876 and parameters 
[-3.34804442  2.39267832 -2.11737085 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551998 -1.06648308  0.66975628  1.14432446
  1.80311613 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 17:42:00 2024]  Iteration number: 0 with current cost as 0.30330284663373347 and parameters 
[-3.33246689  2.44212326 -2.1083687  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6772108   1.14432445
  1.77615035 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 17:48:31 2024]  Iteration number: 0 with current cost as 0.3010396322898269 and parameters 
[-3.34429392  2.44727817 -2.11084642 -0.11653103  0.55388707 -2.77010897
  3.06858498  2.18960144  1.18551998 -1.06648309  0.68622881  1.14432445
  1.7889791  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 17:54:39 2024]  Iteration number: 0 with current cost as 0.31123135553699066 and parameters 
[-3.33407814  2.42545831 -2.11166345 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.67805487  1.14432447
  1.78118207 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Training complete taking 1806.9148137569427 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 18:00:19 2024]  Iteration number: 0 with current cost as 0.3246559745236599 and parameters 
[-3.29181247  2.4906329  -2.10793542 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.69764308  1.14432445
  1.7223314  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 18:06:03 2024]  Iteration number: 0 with current cost as 0.3000099586663876 and parameters 
[-3.34804442  2.39267832 -2.11737085 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551998 -1.06648308  0.66975628  1.14432446
  1.80311613 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 18:12:00 2024]  Iteration number: 0 with current cost as 0.30330284663373347 and parameters 
[-3.33246689  2.44212326 -2.1083687  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6772108   1.14432445
  1.77615035 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 18:18:27 2024]  Iteration number: 0 with current cost as 0.3010396322898269 and parameters 
[-3.34429392  2.44727817 -2.11084642 -0.11653103  0.55388707 -2.77010897
  3.06858498  2.18960144  1.18551998 -1.06648309  0.68622881  1.14432445
  1.7889791  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 18:24:42 2024]  Iteration number: 0 with current cost as 0.31123135553699066 and parameters 
[-3.33407814  2.42545831 -2.11166345 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.67805487  1.14432447
  1.78118207 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Training complete taking 1805.8789150714874 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 18:30:25 2024]  Iteration number: 0 with current cost as 0.3246559745236599 and parameters 
[-3.29181247  2.4906329  -2.10793542 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.69764308  1.14432445
  1.7223314  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 18:36:14 2024]  Iteration number: 0 with current cost as 0.3000099586663876 and parameters 
[-3.34804442  2.39267832 -2.11737085 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551998 -1.06648308  0.66975628  1.14432446
  1.80311613 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 18:42:31 2024]  Iteration number: 0 with current cost as 0.30330284663373347 and parameters 
[-3.33246689  2.44212326 -2.1083687  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6772108   1.14432445
  1.77615035 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 18:48:58 2024]  Iteration number: 0 with current cost as 0.3010396322898269 and parameters 
[-3.34429392  2.44727817 -2.11084642 -0.11653103  0.55388707 -2.77010897
  3.06858498  2.18960144  1.18551998 -1.06648309  0.68622881  1.14432445
  1.7889791  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 18:54:59 2024]  Iteration number: 0 with current cost as 0.31123135553699066 and parameters 
[-3.33407814  2.42545831 -2.11166345 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.67805487  1.14432447
  1.78118207 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Training complete taking 1806.327803850174 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 19:00:31 2024]  Iteration number: 0 with current cost as 0.3246559745236599 and parameters 
[-3.29181247  2.4906329  -2.10793542 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.69764308  1.14432445
  1.7223314  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 19:06:26 2024]  Iteration number: 0 with current cost as 0.3000099586663876 and parameters 
[-3.34804442  2.39267832 -2.11737085 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551998 -1.06648308  0.66975628  1.14432446
  1.80311613 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 19:12:23 2024]  Iteration number: 0 with current cost as 0.30330284663373347 and parameters 
[-3.33246689  2.44212326 -2.1083687  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6772108   1.14432445
  1.77615035 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 19:18:47 2024]  Iteration number: 0 with current cost as 0.3010396322898269 and parameters 
[-3.34429392  2.44727817 -2.11084642 -0.11653103  0.55388707 -2.77010897
  3.06858498  2.18960144  1.18551998 -1.06648309  0.68622881  1.14432445
  1.7889791  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 19:24:51 2024]  Iteration number: 0 with current cost as 0.31123135553699066 and parameters 
[-3.33407814  2.42545831 -2.11166345 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.67805487  1.14432447
  1.78118207 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Training complete taking 1796.1630840301514 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 19:30:27 2024]  Iteration number: 0 with current cost as 0.3246559745236599 and parameters 
[-3.29181247  2.4906329  -2.10793542 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.69764308  1.14432445
  1.7223314  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 19:36:10 2024]  Iteration number: 0 with current cost as 0.3000099586663876 and parameters 
[-3.34804442  2.39267832 -2.11737085 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551998 -1.06648308  0.66975628  1.14432446
  1.80311613 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 19:42:21 2024]  Iteration number: 0 with current cost as 0.30330284663373347 and parameters 
[-3.33246689  2.44212326 -2.1083687  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6772108   1.14432445
  1.77615035 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 19:48:40 2024]  Iteration number: 0 with current cost as 0.3010396322898269 and parameters 
[-3.34429392  2.44727817 -2.11084642 -0.11653103  0.55388707 -2.77010897
  3.06858498  2.18960144  1.18551998 -1.06648309  0.68622881  1.14432445
  1.7889791  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 19:54:44 2024]  Iteration number: 0 with current cost as 0.31123135553699066 and parameters 
[-3.33407814  2.42545831 -2.11166345 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.67805487  1.14432447
  1.78118207 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Training complete taking 1807.5068299770355 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 20:00:34 2024]  Iteration number: 0 with current cost as 0.3246559745236599 and parameters 
[-3.29181247  2.4906329  -2.10793542 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.69764308  1.14432445
  1.7223314  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 20:06:41 2024]  Iteration number: 0 with current cost as 0.3000099586663876 and parameters 
[-3.34804442  2.39267832 -2.11737085 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551998 -1.06648308  0.66975628  1.14432446
  1.80311613 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 20:12:35 2024]  Iteration number: 0 with current cost as 0.30330284663373347 and parameters 
[-3.33246689  2.44212326 -2.1083687  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6772108   1.14432445
  1.77615035 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 20:18:56 2024]  Iteration number: 0 with current cost as 0.3010396322898269 and parameters 
[-3.34429392  2.44727817 -2.11084642 -0.11653103  0.55388707 -2.77010897
  3.06858498  2.18960144  1.18551998 -1.06648309  0.68622881  1.14432445
  1.7889791  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 20:25:05 2024]  Iteration number: 0 with current cost as 0.31123135553699066 and parameters 
[-3.33407814  2.42545831 -2.11166345 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.67805487  1.14432447
  1.78118207 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Training complete taking 1814.5184783935547 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 20:30:48 2024]  Iteration number: 0 with current cost as 0.3246559745236599 and parameters 
[-3.29181247  2.4906329  -2.10793542 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.69764308  1.14432445
  1.7223314  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 20:36:34 2024]  Iteration number: 0 with current cost as 0.3000099586663876 and parameters 
[-3.34804442  2.39267832 -2.11737085 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551998 -1.06648308  0.66975628  1.14432446
  1.80311613 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 20:42:26 2024]  Iteration number: 0 with current cost as 0.30330284663373347 and parameters 
[-3.33246689  2.44212326 -2.1083687  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6772108   1.14432445
  1.77615035 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 20:48:45 2024]  Iteration number: 0 with current cost as 0.3010396322898269 and parameters 
[-3.34429392  2.44727817 -2.11084642 -0.11653103  0.55388707 -2.77010897
  3.06858498  2.18960144  1.18551998 -1.06648309  0.68622881  1.14432445
  1.7889791  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 20:54:47 2024]  Iteration number: 0 with current cost as 0.31123135553699066 and parameters 
[-3.33407814  2.42545831 -2.11166345 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.67805487  1.14432447
  1.78118207 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Training complete taking 1802.4477055072784 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 21:00:50 2024]  Iteration number: 0 with current cost as 0.3246559745236599 and parameters 
[-3.29181247  2.4906329  -2.10793542 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.69764308  1.14432445
  1.7223314  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 21:06:35 2024]  Iteration number: 0 with current cost as 0.3000099586663876 and parameters 
[-3.34804442  2.39267832 -2.11737085 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551998 -1.06648308  0.66975628  1.14432446
  1.80311613 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 21:12:52 2024]  Iteration number: 0 with current cost as 0.30330284663373347 and parameters 
[-3.33246689  2.44212326 -2.1083687  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6772108   1.14432445
  1.77615035 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 21:19:12 2024]  Iteration number: 0 with current cost as 0.3010396322898269 and parameters 
[-3.34429392  2.44727817 -2.11084642 -0.11653103  0.55388707 -2.77010897
  3.06858498  2.18960144  1.18551998 -1.06648309  0.68622881  1.14432445
  1.7889791  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 21:25:14 2024]  Iteration number: 0 with current cost as 0.31123135553699066 and parameters 
[-3.33407814  2.42545831 -2.11166345 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.67805487  1.14432447
  1.78118207 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Training complete taking 1798.1022481918335 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 21:30:48 2024]  Iteration number: 0 with current cost as 0.3246559745236599 and parameters 
[-3.29181247  2.4906329  -2.10793542 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.69764308  1.14432445
  1.7223314  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 21:36:46 2024]  Iteration number: 0 with current cost as 0.3000099586663876 and parameters 
[-3.34804442  2.39267832 -2.11737085 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551998 -1.06648308  0.66975628  1.14432446
  1.80311613 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 21:42:39 2024]  Iteration number: 0 with current cost as 0.30330284663373347 and parameters 
[-3.33246689  2.44212326 -2.1083687  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6772108   1.14432445
  1.77615035 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 21:49:04 2024]  Iteration number: 0 with current cost as 0.3010396322898269 and parameters 
[-3.34429392  2.44727817 -2.11084642 -0.11653103  0.55388707 -2.77010897
  3.06858498  2.18960144  1.18551998 -1.06648309  0.68622881  1.14432445
  1.7889791  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 21:55:14 2024]  Iteration number: 0 with current cost as 0.31123135553699066 and parameters 
[-3.33407814  2.42545831 -2.11166345 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.67805487  1.14432447
  1.78118207 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Training complete taking 1810.8885707855225 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 22:01:00 2024]  Iteration number: 0 with current cost as 0.3246559745236599 and parameters 
[-3.29181247  2.4906329  -2.10793542 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.69764308  1.14432445
  1.7223314  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 22:06:45 2024]  Iteration number: 0 with current cost as 0.3000099586663876 and parameters 
[-3.34804442  2.39267832 -2.11737085 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551998 -1.06648308  0.66975628  1.14432446
  1.80311613 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 22:12:37 2024]  Iteration number: 0 with current cost as 0.30330284663373347 and parameters 
[-3.33246689  2.44212326 -2.1083687  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6772108   1.14432445
  1.77615035 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 22:18:56 2024]  Iteration number: 0 with current cost as 0.3010396322898269 and parameters 
[-3.34429392  2.44727817 -2.11084642 -0.11653103  0.55388707 -2.77010897
  3.06858498  2.18960144  1.18551998 -1.06648309  0.68622881  1.14432445
  1.7889791  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 22:24:56 2024]  Iteration number: 0 with current cost as 0.31123135553699066 and parameters 
[-3.33407814  2.42545831 -2.11166345 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.67805487  1.14432447
  1.78118207 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Training complete taking 1774.8651206493378 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 22:30:35 2024]  Iteration number: 0 with current cost as 0.3246559745236599 and parameters 
[-3.29181247  2.4906329  -2.10793542 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.69764308  1.14432445
  1.7223314  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 22:36:20 2024]  Iteration number: 0 with current cost as 0.3000099586663876 and parameters 
[-3.34804442  2.39267832 -2.11737085 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551998 -1.06648308  0.66975628  1.14432446
  1.80311613 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 22:42:12 2024]  Iteration number: 0 with current cost as 0.30330284663373347 and parameters 
[-3.33246689  2.44212326 -2.1083687  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6772108   1.14432445
  1.77615035 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 22:48:49 2024]  Iteration number: 0 with current cost as 0.3010396322898269 and parameters 
[-3.34429392  2.44727817 -2.11084642 -0.11653103  0.55388707 -2.77010897
  3.06858498  2.18960144  1.18551998 -1.06648309  0.68622881  1.14432445
  1.7889791  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 22:54:50 2024]  Iteration number: 0 with current cost as 0.31123135553699066 and parameters 
[-3.33407814  2.42545831 -2.11166345 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.67805487  1.14432447
  1.78118207 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Training complete taking 1789.2683100700378 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 23:00:23 2024]  Iteration number: 0 with current cost as 0.3246559745236599 and parameters 
[-3.29181247  2.4906329  -2.10793542 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.69764308  1.14432445
  1.7223314  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 23:06:10 2024]  Iteration number: 0 with current cost as 0.3000099586663876 and parameters 
[-3.34804442  2.39267832 -2.11737085 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551998 -1.06648308  0.66975628  1.14432446
  1.80311613 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 23:12:07 2024]  Iteration number: 0 with current cost as 0.30330284663373347 and parameters 
[-3.33246689  2.44212326 -2.1083687  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6772108   1.14432445
  1.77615035 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 23:18:40 2024]  Iteration number: 0 with current cost as 0.3010396322898269 and parameters 
[-3.34429392  2.44727817 -2.11084642 -0.11653103  0.55388707 -2.77010897
  3.06858498  2.18960144  1.18551998 -1.06648309  0.68622881  1.14432445
  1.7889791  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 23:24:45 2024]  Iteration number: 0 with current cost as 0.31123135553699066 and parameters 
[-3.33407814  2.42545831 -2.11166345 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.67805487  1.14432447
  1.78118207 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Training complete taking 1795.8086552619934 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 23:30:23 2024]  Iteration number: 0 with current cost as 0.3246559745236599 and parameters 
[-3.29181247  2.4906329  -2.10793542 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.69764308  1.14432445
  1.7223314  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 23:36:21 2024]  Iteration number: 0 with current cost as 0.3000099586663876 and parameters 
[-3.34804442  2.39267832 -2.11737085 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551998 -1.06648308  0.66975628  1.14432446
  1.80311613 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 23:42:14 2024]  Iteration number: 0 with current cost as 0.30330284663373347 and parameters 
[-3.33246689  2.44212326 -2.1083687  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6772108   1.14432445
  1.77615035 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 23:48:54 2024]  Iteration number: 0 with current cost as 0.3010396322898269 and parameters 
[-3.34429392  2.44727817 -2.11084642 -0.11653103  0.55388707 -2.77010897
  3.06858498  2.18960144  1.18551998 -1.06648309  0.68622881  1.14432445
  1.7889791  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 23:54:59 2024]  Iteration number: 0 with current cost as 0.31123135553699066 and parameters 
[-3.33407814  2.42545831 -2.11166345 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.67805487  1.14432447
  1.78118207 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Training complete taking 1814.477808713913 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 00:00:35 2024]  Iteration number: 0 with current cost as 0.3246559745236599 and parameters 
[-3.29181247  2.4906329  -2.10793542 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.69764308  1.14432445
  1.7223314  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 00:06:37 2024]  Iteration number: 0 with current cost as 0.3000099586663876 and parameters 
[-3.34804442  2.39267832 -2.11737085 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551998 -1.06648308  0.66975628  1.14432446
  1.80311613 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 00:12:28 2024]  Iteration number: 0 with current cost as 0.30330284663373347 and parameters 
[-3.33246689  2.44212326 -2.1083687  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6772108   1.14432445
  1.77615035 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 00:18:52 2024]  Iteration number: 0 with current cost as 0.3010396322898269 and parameters 
[-3.34429392  2.44727817 -2.11084642 -0.11653103  0.55388707 -2.77010897
  3.06858498  2.18960144  1.18551998 -1.06648309  0.68622881  1.14432445
  1.7889791  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 00:25:20 2024]  Iteration number: 0 with current cost as 0.31123135553699066 and parameters 
[-3.33407814  2.42545831 -2.11166345 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.67805487  1.14432447
  1.78118207 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Training complete taking 1818.0196480751038 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 00:30:53 2024]  Iteration number: 0 with current cost as 0.3246559745236599 and parameters 
[-3.29181247  2.4906329  -2.10793542 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.69764308  1.14432445
  1.7223314  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 00:36:35 2024]  Iteration number: 0 with current cost as 0.3000099586663876 and parameters 
[-3.34804442  2.39267832 -2.11737085 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551998 -1.06648308  0.66975628  1.14432446
  1.80311613 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 00:42:26 2024]  Iteration number: 0 with current cost as 0.30330284663373347 and parameters 
[-3.33246689  2.44212326 -2.1083687  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6772108   1.14432445
  1.77615035 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 00:48:44 2024]  Iteration number: 0 with current cost as 0.3010396322898269 and parameters 
[-3.34429392  2.44727817 -2.11084642 -0.11653103  0.55388707 -2.77010897
  3.06858498  2.18960144  1.18551998 -1.06648309  0.68622881  1.14432445
  1.7889791  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 00:54:46 2024]  Iteration number: 0 with current cost as 0.31123135553699066 and parameters 
[-3.33407814  2.42545831 -2.11166345 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.67805487  1.14432447
  1.78118207 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Training complete taking 1766.8784487247467 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 01:00:20 2024]  Iteration number: 0 with current cost as 0.3246559745236599 and parameters 
[-3.29181247  2.4906329  -2.10793542 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.69764308  1.14432445
  1.7223314  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 01:06:02 2024]  Iteration number: 0 with current cost as 0.3000099586663876 and parameters 
[-3.34804442  2.39267832 -2.11737085 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551998 -1.06648308  0.66975628  1.14432446
  1.80311613 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 01:12:01 2024]  Iteration number: 0 with current cost as 0.30330284663373347 and parameters 
[-3.33246689  2.44212326 -2.1083687  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6772108   1.14432445
  1.77615035 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 01:18:22 2024]  Iteration number: 0 with current cost as 0.3010396322898269 and parameters 
[-3.34429392  2.44727817 -2.11084642 -0.11653103  0.55388707 -2.77010897
  3.06858498  2.18960144  1.18551998 -1.06648309  0.68622881  1.14432445
  1.7889791  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 01:25:04 2024]  Iteration number: 0 with current cost as 0.31123135553699066 and parameters 
[-3.33407814  2.42545831 -2.11166345 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.67805487  1.14432447
  1.78118207 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Training complete taking 1818.590922832489 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 01:30:37 2024]  Iteration number: 0 with current cost as 0.3246559745236599 and parameters 
[-3.29181247  2.4906329  -2.10793542 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.69764308  1.14432445
  1.7223314  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 01:36:19 2024]  Iteration number: 0 with current cost as 0.3000099586663876 and parameters 
[-3.34804442  2.39267832 -2.11737085 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551998 -1.06648308  0.66975628  1.14432446
  1.80311613 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 01:42:10 2024]  Iteration number: 0 with current cost as 0.30330284663373347 and parameters 
[-3.33246689  2.44212326 -2.1083687  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6772108   1.14432445
  1.77615035 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 01:48:46 2024]  Iteration number: 0 with current cost as 0.3010396322898269 and parameters 
[-3.34429392  2.44727817 -2.11084642 -0.11653103  0.55388707 -2.77010897
  3.06858498  2.18960144  1.18551998 -1.06648309  0.68622881  1.14432445
  1.7889791  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 01:54:49 2024]  Iteration number: 0 with current cost as 0.31123135553699066 and parameters 
[-3.33407814  2.42545831 -2.11166345 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.67805487  1.14432447
  1.78118207 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Training complete taking 1785.7559268474579 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 02:00:24 2024]  Iteration number: 0 with current cost as 0.3246559745236599 and parameters 
[-3.29181247  2.4906329  -2.10793542 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.69764308  1.14432445
  1.7223314  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 02:06:06 2024]  Iteration number: 0 with current cost as 0.3000099586663876 and parameters 
[-3.34804442  2.39267832 -2.11737085 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551998 -1.06648308  0.66975628  1.14432446
  1.80311613 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 02:11:58 2024]  Iteration number: 0 with current cost as 0.30330284663373347 and parameters 
[-3.33246689  2.44212326 -2.1083687  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6772108   1.14432445
  1.77615035 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 02:18:18 2024]  Iteration number: 0 with current cost as 0.3010396322898269 and parameters 
[-3.34429392  2.44727817 -2.11084642 -0.11653103  0.55388707 -2.77010897
  3.06858498  2.18960144  1.18551998 -1.06648309  0.68622881  1.14432445
  1.7889791  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 02:24:19 2024]  Iteration number: 0 with current cost as 0.31123135553699066 and parameters 
[-3.33407814  2.42545831 -2.11166345 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.67805487  1.14432447
  1.78118207 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Training complete taking 1768.2148361206055 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 02:29:52 2024]  Iteration number: 0 with current cost as 0.3246559745236599 and parameters 
[-3.29181247  2.4906329  -2.10793542 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.69764308  1.14432445
  1.7223314  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 02:35:36 2024]  Iteration number: 0 with current cost as 0.3000099586663876 and parameters 
[-3.34804442  2.39267832 -2.11737085 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551998 -1.06648308  0.66975628  1.14432446
  1.80311613 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 02:41:32 2024]  Iteration number: 0 with current cost as 0.30330284663373347 and parameters 
[-3.33246689  2.44212326 -2.1083687  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6772108   1.14432445
  1.77615035 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 02:47:57 2024]  Iteration number: 0 with current cost as 0.3010396322898269 and parameters 
[-3.34429392  2.44727817 -2.11084642 -0.11653103  0.55388707 -2.77010897
  3.06858498  2.18960144  1.18551998 -1.06648309  0.68622881  1.14432445
  1.7889791  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 02:54:00 2024]  Iteration number: 0 with current cost as 0.31123135553699066 and parameters 
[-3.33407814  2.42545831 -2.11166345 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.67805487  1.14432447
  1.78118207 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Training complete taking 1783.0637555122375 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 02:59:34 2024]  Iteration number: 0 with current cost as 0.3246559745236599 and parameters 
[-3.29181247  2.4906329  -2.10793542 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.69764308  1.14432445
  1.7223314  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 03:05:18 2024]  Iteration number: 0 with current cost as 0.3000099586663876 and parameters 
[-3.34804442  2.39267832 -2.11737085 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551998 -1.06648308  0.66975628  1.14432446
  1.80311613 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 03:11:08 2024]  Iteration number: 0 with current cost as 0.30330284663373347 and parameters 
[-3.33246689  2.44212326 -2.1083687  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6772108   1.14432445
  1.77615035 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 03:17:28 2024]  Iteration number: 0 with current cost as 0.3010396322898269 and parameters 
[-3.34429392  2.44727817 -2.11084642 -0.11653103  0.55388707 -2.77010897
  3.06858498  2.18960144  1.18551998 -1.06648309  0.68622881  1.14432445
  1.7889791  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 03:23:34 2024]  Iteration number: 0 with current cost as 0.31123135553699066 and parameters 
[-3.33407814  2.42545831 -2.11166345 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.67805487  1.14432447
  1.78118207 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Training complete taking 1773.8195765018463 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 03:29:08 2024]  Iteration number: 0 with current cost as 0.3246559745236599 and parameters 
[-3.29181247  2.4906329  -2.10793542 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.69764308  1.14432445
  1.7223314  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 03:34:53 2024]  Iteration number: 0 with current cost as 0.3000099586663876 and parameters 
[-3.34804442  2.39267832 -2.11737085 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551998 -1.06648308  0.66975628  1.14432446
  1.80311613 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 03:40:51 2024]  Iteration number: 0 with current cost as 0.30330284663373347 and parameters 
[-3.33246689  2.44212326 -2.1083687  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6772108   1.14432445
  1.77615035 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 03:47:30 2024]  Iteration number: 0 with current cost as 0.3010396322898269 and parameters 
[-3.34429392  2.44727817 -2.11084642 -0.11653103  0.55388707 -2.77010897
  3.06858498  2.18960144  1.18551998 -1.06648309  0.68622881  1.14432445
  1.7889791  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 03:53:49 2024]  Iteration number: 0 with current cost as 0.31123135553699066 and parameters 
[-3.33407814  2.42545831 -2.11166345 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.67805487  1.14432447
  1.78118207 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Training complete taking 1822.8904175758362 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 03:59:32 2024]  Iteration number: 0 with current cost as 0.3246559745236599 and parameters 
[-3.29181247  2.4906329  -2.10793542 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.69764308  1.14432445
  1.7223314  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 04:05:16 2024]  Iteration number: 0 with current cost as 0.3000099586663876 and parameters 
[-3.34804442  2.39267832 -2.11737085 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551998 -1.06648308  0.66975628  1.14432446
  1.80311613 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 04:11:12 2024]  Iteration number: 0 with current cost as 0.30330284663373347 and parameters 
[-3.33246689  2.44212326 -2.1083687  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6772108   1.14432445
  1.77615035 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 04:17:47 2024]  Iteration number: 0 with current cost as 0.3010396322898269 and parameters 
[-3.34429392  2.44727817 -2.11084642 -0.11653103  0.55388707 -2.77010897
  3.06858498  2.18960144  1.18551998 -1.06648309  0.68622881  1.14432445
  1.7889791  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 04:24:17 2024]  Iteration number: 0 with current cost as 0.31123135553699066 and parameters 
[-3.33407814  2.42545831 -2.11166345 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.67805487  1.14432447
  1.78118207 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Training complete taking 1832.4370801448822 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 04:30:04 2024]  Iteration number: 0 with current cost as 0.3246559745236599 and parameters 
[-3.29181247  2.4906329  -2.10793542 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.69764308  1.14432445
  1.7223314  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 04:35:49 2024]  Iteration number: 0 with current cost as 0.3000099586663876 and parameters 
[-3.34804442  2.39267832 -2.11737085 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551998 -1.06648308  0.66975628  1.14432446
  1.80311613 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 04:41:55 2024]  Iteration number: 0 with current cost as 0.30330284663373347 and parameters 
[-3.33246689  2.44212326 -2.1083687  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6772108   1.14432445
  1.77615035 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 04:48:25 2024]  Iteration number: 0 with current cost as 0.3010396322898269 and parameters 
[-3.34429392  2.44727817 -2.11084642 -0.11653103  0.55388707 -2.77010897
  3.06858498  2.18960144  1.18551998 -1.06648309  0.68622881  1.14432445
  1.7889791  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 04:54:39 2024]  Iteration number: 0 with current cost as 0.31123135553699066 and parameters 
[-3.33407814  2.42545831 -2.11166345 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.67805487  1.14432447
  1.78118207 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Training complete taking 1811.5417308807373 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 05:00:15 2024]  Iteration number: 0 with current cost as 0.3246559745236599 and parameters 
[-3.29181247  2.4906329  -2.10793542 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.69764308  1.14432445
  1.7223314  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 05:05:59 2024]  Iteration number: 0 with current cost as 0.3000099586663876 and parameters 
[-3.34804442  2.39267832 -2.11737085 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551998 -1.06648308  0.66975628  1.14432446
  1.80311613 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 05:11:51 2024]  Iteration number: 0 with current cost as 0.30330284663373347 and parameters 
[-3.33246689  2.44212326 -2.1083687  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6772108   1.14432445
  1.77615035 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 05:18:16 2024]  Iteration number: 0 with current cost as 0.3010396322898269 and parameters 
[-3.34429392  2.44727817 -2.11084642 -0.11653103  0.55388707 -2.77010897
  3.06858498  2.18960144  1.18551998 -1.06648309  0.68622881  1.14432445
  1.7889791  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 05:24:20 2024]  Iteration number: 0 with current cost as 0.31123135553699066 and parameters 
[-3.33407814  2.42545831 -2.11166345 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.67805487  1.14432447
  1.78118207 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Training complete taking 1779.8917434215546 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 05:29:55 2024]  Iteration number: 0 with current cost as 0.3246559745236599 and parameters 
[-3.29181247  2.4906329  -2.10793542 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.69764308  1.14432445
  1.7223314  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 05:35:37 2024]  Iteration number: 0 with current cost as 0.3000099586663876 and parameters 
[-3.34804442  2.39267832 -2.11737085 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551998 -1.06648308  0.66975628  1.14432446
  1.80311613 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 05:41:32 2024]  Iteration number: 0 with current cost as 0.30330284663373347 and parameters 
[-3.33246689  2.44212326 -2.1083687  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6772108   1.14432445
  1.77615035 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 05:47:51 2024]  Iteration number: 0 with current cost as 0.3010396322898269 and parameters 
[-3.34429392  2.44727817 -2.11084642 -0.11653103  0.55388707 -2.77010897
  3.06858498  2.18960144  1.18551998 -1.06648309  0.68622881  1.14432445
  1.7889791  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 05:53:53 2024]  Iteration number: 0 with current cost as 0.31123135553699066 and parameters 
[-3.33407814  2.42545831 -2.11166345 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.67805487  1.14432447
  1.78118207 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Training complete taking 1773.8020741939545 seconds. 
Discarding model... 

Training complete taking 44958.0766146183 total seconds. 
Now scoring model... 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Scoring complete taking 0.9419088363647461 seconds. 
Saved predicted values as M-A1-CNOT_Full-Pauli-CRZ_predicted_values.csv
Model scores: {'MSE_train': (89.74705502836767,), 'R2_train': 0.56520563892786, 'MAE_train': 8.481528342937583, 'MSE_test': 207.79839811226148, 'R2_test': -0.2525700746251711, 'MAE_test': 10.734429761373152}. 
Saved model results as M-A1-CNOT_Full-Pauli-CRZ_results.json. 
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/circuits/Encoders.py:12: RuntimeWarning: invalid value encountered in arcsin
  if np.isnan(np.arcsin(features[feature_idx])) or np.isnan(np.arccos(features[feature_idx])):
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Thu Apr  4 11:28:41 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 11:29:00 2024]  Iteration number: 0 with current cost as 0.3246559745236599 and parameters 
[-3.29181247  2.4906329  -2.10793542 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.69764308  1.14432445
  1.7223314  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 11:34:48 2024]  Iteration number: 0 with current cost as 0.3000099586663876 and parameters 
[-3.34804442  2.39267832 -2.11737085 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551998 -1.06648308  0.66975628  1.14432446
  1.80311613 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 11:40:44 2024]  Iteration number: 0 with current cost as 0.30330284663373347 and parameters 
[-3.33246689  2.44212326 -2.1083687  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6772108   1.14432445
  1.77615035 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 11:47:08 2024]  Iteration number: 0 with current cost as 0.3010396322898269 and parameters 
[-3.34429392  2.44727817 -2.11084642 -0.11653103  0.55388707 -2.77010897
  3.06858498  2.18960144  1.18551998 -1.06648309  0.68622881  1.14432445
  1.7889791  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 11:53:11 2024]  Iteration number: 0 with current cost as 0.31123135553699066 and parameters 
[-3.33407814  2.42545831 -2.11166345 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.67805487  1.14432447
  1.78118207 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Training complete taking 1791.625928401947 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 11:58:50 2024]  Iteration number: 0 with current cost as 0.3246559745236599 and parameters 
[-3.29181247  2.4906329  -2.10793542 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.69764308  1.14432445
  1.7223314  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 12:04:43 2024]  Iteration number: 0 with current cost as 0.3000099586663876 and parameters 
[-3.34804442  2.39267832 -2.11737085 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551998 -1.06648308  0.66975628  1.14432446
  1.80311613 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 12:10:45 2024]  Iteration number: 0 with current cost as 0.30330284663373347 and parameters 
[-3.33246689  2.44212326 -2.1083687  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6772108   1.14432445
  1.77615035 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 12:17:23 2024]  Iteration number: 0 with current cost as 0.3010396322898269 and parameters 
[-3.34429392  2.44727817 -2.11084642 -0.11653103  0.55388707 -2.77010897
  3.06858498  2.18960144  1.18551998 -1.06648309  0.68622881  1.14432445
  1.7889791  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 12:23:35 2024]  Iteration number: 0 with current cost as 0.31123135553699066 and parameters 
[-3.33407814  2.42545831 -2.11166345 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.67805487  1.14432447
  1.78118207 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Training complete taking 1821.4669289588928 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 12:29:14 2024]  Iteration number: 0 with current cost as 0.3246559745236599 and parameters 
[-3.29181247  2.4906329  -2.10793542 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.69764308  1.14432445
  1.7223314  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 12:35:00 2024]  Iteration number: 0 with current cost as 0.3000099586663876 and parameters 
[-3.34804442  2.39267832 -2.11737085 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551998 -1.06648308  0.66975628  1.14432446
  1.80311613 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 12:40:55 2024]  Iteration number: 0 with current cost as 0.30330284663373347 and parameters 
[-3.33246689  2.44212326 -2.1083687  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6772108   1.14432445
  1.77615035 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 12:47:20 2024]  Iteration number: 0 with current cost as 0.3010396322898269 and parameters 
[-3.34429392  2.44727817 -2.11084642 -0.11653103  0.55388707 -2.77010897
  3.06858498  2.18960144  1.18551998 -1.06648309  0.68622881  1.14432445
  1.7889791  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 12:53:23 2024]  Iteration number: 0 with current cost as 0.31123135553699066 and parameters 
[-3.33407814  2.42545831 -2.11166345 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.67805487  1.14432447
  1.78118207 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Training complete taking 1787.086356639862 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 12:59:00 2024]  Iteration number: 0 with current cost as 0.3246559745236599 and parameters 
[-3.29181247  2.4906329  -2.10793542 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.69764308  1.14432445
  1.7223314  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 13:04:47 2024]  Iteration number: 0 with current cost as 0.3000099586663876 and parameters 
[-3.34804442  2.39267832 -2.11737085 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551998 -1.06648308  0.66975628  1.14432446
  1.80311613 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 13:11:02 2024]  Iteration number: 0 with current cost as 0.30330284663373347 and parameters 
[-3.33246689  2.44212326 -2.1083687  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6772108   1.14432445
  1.77615035 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 13:17:25 2024]  Iteration number: 0 with current cost as 0.3010396322898269 and parameters 
[-3.34429392  2.44727817 -2.11084642 -0.11653103  0.55388707 -2.77010897
  3.06858498  2.18960144  1.18551998 -1.06648309  0.68622881  1.14432445
  1.7889791  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 13:23:38 2024]  Iteration number: 0 with current cost as 0.31123135553699066 and parameters 
[-3.33407814  2.42545831 -2.11166345 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.67805487  1.14432447
  1.78118207 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Training complete taking 1817.6215007305145 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 13:29:17 2024]  Iteration number: 0 with current cost as 0.3246559745236599 and parameters 
[-3.29181247  2.4906329  -2.10793542 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.69764308  1.14432445
  1.7223314  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 13:35:15 2024]  Iteration number: 0 with current cost as 0.3000099586663876 and parameters 
[-3.34804442  2.39267832 -2.11737085 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551998 -1.06648308  0.66975628  1.14432446
  1.80311613 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 13:41:13 2024]  Iteration number: 0 with current cost as 0.30330284663373347 and parameters 
[-3.33246689  2.44212326 -2.1083687  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6772108   1.14432445
  1.77615035 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 13:47:34 2024]  Iteration number: 0 with current cost as 0.3010396322898269 and parameters 
[-3.34429392  2.44727817 -2.11084642 -0.11653103  0.55388707 -2.77010897
  3.06858498  2.18960144  1.18551998 -1.06648309  0.68622881  1.14432445
  1.7889791  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 13:53:35 2024]  Iteration number: 0 with current cost as 0.31123135553699066 and parameters 
[-3.33407814  2.42545831 -2.11166345 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.67805487  1.14432447
  1.78118207 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Training complete taking 1834.389478445053 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 13:59:51 2024]  Iteration number: 0 with current cost as 0.3246559745236599 and parameters 
[-3.29181247  2.4906329  -2.10793542 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.69764308  1.14432445
  1.7223314  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 14:05:48 2024]  Iteration number: 0 with current cost as 0.3000099586663876 and parameters 
[-3.34804442  2.39267832 -2.11737085 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551998 -1.06648308  0.66975628  1.14432446
  1.80311613 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 14:11:47 2024]  Iteration number: 0 with current cost as 0.30330284663373347 and parameters 
[-3.33246689  2.44212326 -2.1083687  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6772108   1.14432445
  1.77615035 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 14:18:08 2024]  Iteration number: 0 with current cost as 0.3010396322898269 and parameters 
[-3.34429392  2.44727817 -2.11084642 -0.11653103  0.55388707 -2.77010897
  3.06858498  2.18960144  1.18551998 -1.06648309  0.68622881  1.14432445
  1.7889791  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 14:24:11 2024]  Iteration number: 0 with current cost as 0.31123135553699066 and parameters 
[-3.33407814  2.42545831 -2.11166345 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.67805487  1.14432447
  1.78118207 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Training complete taking 1792.2994229793549 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 14:29:45 2024]  Iteration number: 0 with current cost as 0.3246559745236599 and parameters 
[-3.29181247  2.4906329  -2.10793542 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.69764308  1.14432445
  1.7223314  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 14:35:28 2024]  Iteration number: 0 with current cost as 0.3000099586663876 and parameters 
[-3.34804442  2.39267832 -2.11737085 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551998 -1.06648308  0.66975628  1.14432446
  1.80311613 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 14:41:22 2024]  Iteration number: 0 with current cost as 0.30330284663373347 and parameters 
[-3.33246689  2.44212326 -2.1083687  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6772108   1.14432445
  1.77615035 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 14:47:45 2024]  Iteration number: 0 with current cost as 0.3010396322898269 and parameters 
[-3.34429392  2.44727817 -2.11084642 -0.11653103  0.55388707 -2.77010897
  3.06858498  2.18960144  1.18551998 -1.06648309  0.68622881  1.14432445
  1.7889791  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 14:53:52 2024]  Iteration number: 0 with current cost as 0.31123135553699066 and parameters 
[-3.33407814  2.42545831 -2.11166345 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.67805487  1.14432447
  1.78118207 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Training complete taking 1797.9555237293243 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 14:59:42 2024]  Iteration number: 0 with current cost as 0.3246559745236599 and parameters 
[-3.29181247  2.4906329  -2.10793542 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.69764308  1.14432445
  1.7223314  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 15:05:52 2024]  Iteration number: 0 with current cost as 0.3000099586663876 and parameters 
[-3.34804442  2.39267832 -2.11737085 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551998 -1.06648308  0.66975628  1.14432446
  1.80311613 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 15:11:54 2024]  Iteration number: 0 with current cost as 0.30330284663373347 and parameters 
[-3.33246689  2.44212326 -2.1083687  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6772108   1.14432445
  1.77615035 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 15:18:18 2024]  Iteration number: 0 with current cost as 0.3010396322898269 and parameters 
[-3.34429392  2.44727817 -2.11084642 -0.11653103  0.55388707 -2.77010897
  3.06858498  2.18960144  1.18551998 -1.06648309  0.68622881  1.14432445
  1.7889791  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 15:24:21 2024]  Iteration number: 0 with current cost as 0.31123135553699066 and parameters 
[-3.33407814  2.42545831 -2.11166345 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.67805487  1.14432447
  1.78118207 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Training complete taking 1812.1678895950317 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 15:29:55 2024]  Iteration number: 0 with current cost as 0.3246559745236599 and parameters 
[-3.29181247  2.4906329  -2.10793542 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.69764308  1.14432445
  1.7223314  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 15:35:52 2024]  Iteration number: 0 with current cost as 0.3000099586663876 and parameters 
[-3.34804442  2.39267832 -2.11737085 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551998 -1.06648308  0.66975628  1.14432446
  1.80311613 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 15:41:58 2024]  Iteration number: 0 with current cost as 0.30330284663373347 and parameters 
[-3.33246689  2.44212326 -2.1083687  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6772108   1.14432445
  1.77615035 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 15:48:21 2024]  Iteration number: 0 with current cost as 0.3010396322898269 and parameters 
[-3.34429392  2.44727817 -2.11084642 -0.11653103  0.55388707 -2.77010897
  3.06858498  2.18960144  1.18551998 -1.06648309  0.68622881  1.14432445
  1.7889791  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 15:54:33 2024]  Iteration number: 0 with current cost as 0.31123135553699066 and parameters 
[-3.33407814  2.42545831 -2.11166345 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.67805487  1.14432447
  1.78118207 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Training complete taking 1812.9826874732971 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 16:00:06 2024]  Iteration number: 0 with current cost as 0.3246559745236599 and parameters 
[-3.29181247  2.4906329  -2.10793542 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.69764308  1.14432445
  1.7223314  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 16:05:51 2024]  Iteration number: 0 with current cost as 0.3000099586663876 and parameters 
[-3.34804442  2.39267832 -2.11737085 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551998 -1.06648308  0.66975628  1.14432446
  1.80311613 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 16:11:46 2024]  Iteration number: 0 with current cost as 0.30330284663373347 and parameters 
[-3.33246689  2.44212326 -2.1083687  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6772108   1.14432445
  1.77615035 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 16:18:07 2024]  Iteration number: 0 with current cost as 0.3010396322898269 and parameters 
[-3.34429392  2.44727817 -2.11084642 -0.11653103  0.55388707 -2.77010897
  3.06858498  2.18960144  1.18551998 -1.06648309  0.68622881  1.14432445
  1.7889791  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 16:24:13 2024]  Iteration number: 0 with current cost as 0.31123135553699066 and parameters 
[-3.33407814  2.42545831 -2.11166345 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.67805487  1.14432447
  1.78118207 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Training complete taking 1792.6687459945679 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 16:29:59 2024]  Iteration number: 0 with current cost as 0.3246559745236599 and parameters 
[-3.29181247  2.4906329  -2.10793542 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.69764308  1.14432445
  1.7223314  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 16:35:53 2024]  Iteration number: 0 with current cost as 0.3000099586663876 and parameters 
[-3.34804442  2.39267832 -2.11737085 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551998 -1.06648308  0.66975628  1.14432446
  1.80311613 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 16:41:47 2024]  Iteration number: 0 with current cost as 0.30330284663373347 and parameters 
[-3.33246689  2.44212326 -2.1083687  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6772108   1.14432445
  1.77615035 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 16:48:09 2024]  Iteration number: 0 with current cost as 0.3010396322898269 and parameters 
[-3.34429392  2.44727817 -2.11084642 -0.11653103  0.55388707 -2.77010897
  3.06858498  2.18960144  1.18551998 -1.06648309  0.68622881  1.14432445
  1.7889791  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 16:54:14 2024]  Iteration number: 0 with current cost as 0.31123135553699066 and parameters 
[-3.33407814  2.42545831 -2.11166345 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.67805487  1.14432447
  1.78118207 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Training complete taking 1786.7752130031586 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 16:59:47 2024]  Iteration number: 0 with current cost as 0.3246559745236599 and parameters 
[-3.29181247  2.4906329  -2.10793542 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.69764308  1.14432445
  1.7223314  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 17:05:34 2024]  Iteration number: 0 with current cost as 0.3000099586663876 and parameters 
[-3.34804442  2.39267832 -2.11737085 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551998 -1.06648308  0.66975628  1.14432446
  1.80311613 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 17:11:28 2024]  Iteration number: 0 with current cost as 0.30330284663373347 and parameters 
[-3.33246689  2.44212326 -2.1083687  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6772108   1.14432445
  1.77615035 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 17:17:51 2024]  Iteration number: 0 with current cost as 0.3010396322898269 and parameters 
[-3.34429392  2.44727817 -2.11084642 -0.11653103  0.55388707 -2.77010897
  3.06858498  2.18960144  1.18551998 -1.06648309  0.68622881  1.14432445
  1.7889791  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 17:23:55 2024]  Iteration number: 0 with current cost as 0.31123135553699066 and parameters 
[-3.33407814  2.42545831 -2.11166345 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.67805487  1.14432447
  1.78118207 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Training complete taking 1781.4839446544647 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 17:29:29 2024]  Iteration number: 0 with current cost as 0.3246559745236599 and parameters 
[-3.29181247  2.4906329  -2.10793542 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.69764308  1.14432445
  1.7223314  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 17:35:12 2024]  Iteration number: 0 with current cost as 0.3000099586663876 and parameters 
[-3.34804442  2.39267832 -2.11737085 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551998 -1.06648308  0.66975628  1.14432446
  1.80311613 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 17:41:05 2024]  Iteration number: 0 with current cost as 0.30330284663373347 and parameters 
[-3.33246689  2.44212326 -2.1083687  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6772108   1.14432445
  1.77615035 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 17:47:25 2024]  Iteration number: 0 with current cost as 0.3010396322898269 and parameters 
[-3.34429392  2.44727817 -2.11084642 -0.11653103  0.55388707 -2.77010897
  3.06858498  2.18960144  1.18551998 -1.06648309  0.68622881  1.14432445
  1.7889791  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 17:53:27 2024]  Iteration number: 0 with current cost as 0.31123135553699066 and parameters 
[-3.33407814  2.42545831 -2.11166345 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.67805487  1.14432447
  1.78118207 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Training complete taking 1772.9308061599731 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 17:59:01 2024]  Iteration number: 0 with current cost as 0.3246559745236599 and parameters 
[-3.29181247  2.4906329  -2.10793542 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.69764308  1.14432445
  1.7223314  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 18:04:47 2024]  Iteration number: 0 with current cost as 0.3000099586663876 and parameters 
[-3.34804442  2.39267832 -2.11737085 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551998 -1.06648308  0.66975628  1.14432446
  1.80311613 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 18:10:39 2024]  Iteration number: 0 with current cost as 0.30330284663373347 and parameters 
[-3.33246689  2.44212326 -2.1083687  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6772108   1.14432445
  1.77615035 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 18:17:03 2024]  Iteration number: 0 with current cost as 0.3010396322898269 and parameters 
[-3.34429392  2.44727817 -2.11084642 -0.11653103  0.55388707 -2.77010897
  3.06858498  2.18960144  1.18551998 -1.06648309  0.68622881  1.14432445
  1.7889791  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 18:23:09 2024]  Iteration number: 0 with current cost as 0.31123135553699066 and parameters 
[-3.33407814  2.42545831 -2.11166345 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.67805487  1.14432447
  1.78118207 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Training complete taking 1787.23686170578 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 18:28:49 2024]  Iteration number: 0 with current cost as 0.3246559745236599 and parameters 
[-3.29181247  2.4906329  -2.10793542 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.69764308  1.14432445
  1.7223314  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 18:34:53 2024]  Iteration number: 0 with current cost as 0.3000099586663876 and parameters 
[-3.34804442  2.39267832 -2.11737085 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551998 -1.06648308  0.66975628  1.14432446
  1.80311613 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 18:40:45 2024]  Iteration number: 0 with current cost as 0.30330284663373347 and parameters 
[-3.33246689  2.44212326 -2.1083687  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6772108   1.14432445
  1.77615035 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 18:47:10 2024]  Iteration number: 0 with current cost as 0.3010396322898269 and parameters 
[-3.34429392  2.44727817 -2.11084642 -0.11653103  0.55388707 -2.77010897
  3.06858498  2.18960144  1.18551998 -1.06648309  0.68622881  1.14432445
  1.7889791  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 18:53:14 2024]  Iteration number: 0 with current cost as 0.31123135553699066 and parameters 
[-3.33407814  2.42545831 -2.11166345 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.67805487  1.14432447
  1.78118207 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Training complete taking 1801.7355134487152 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 18:58:50 2024]  Iteration number: 0 with current cost as 0.3246559745236599 and parameters 
[-3.29181247  2.4906329  -2.10793542 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.69764308  1.14432445
  1.7223314  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 19:04:35 2024]  Iteration number: 0 with current cost as 0.3000099586663876 and parameters 
[-3.34804442  2.39267832 -2.11737085 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551998 -1.06648308  0.66975628  1.14432446
  1.80311613 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 19:10:29 2024]  Iteration number: 0 with current cost as 0.30330284663373347 and parameters 
[-3.33246689  2.44212326 -2.1083687  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6772108   1.14432445
  1.77615035 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 19:17:00 2024]  Iteration number: 0 with current cost as 0.3010396322898269 and parameters 
[-3.34429392  2.44727817 -2.11084642 -0.11653103  0.55388707 -2.77010897
  3.06858498  2.18960144  1.18551998 -1.06648309  0.68622881  1.14432445
  1.7889791  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 19:23:48 2024]  Iteration number: 0 with current cost as 0.31123135553699066 and parameters 
[-3.33407814  2.42545831 -2.11166345 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.67805487  1.14432447
  1.78118207 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Training complete taking 1844.10152053833 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 19:29:35 2024]  Iteration number: 0 with current cost as 0.3246559745236599 and parameters 
[-3.29181247  2.4906329  -2.10793542 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.69764308  1.14432445
  1.7223314  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 19:35:19 2024]  Iteration number: 0 with current cost as 0.3000099586663876 and parameters 
[-3.34804442  2.39267832 -2.11737085 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551998 -1.06648308  0.66975628  1.14432446
  1.80311613 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 19:41:24 2024]  Iteration number: 0 with current cost as 0.30330284663373347 and parameters 
[-3.33246689  2.44212326 -2.1083687  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6772108   1.14432445
  1.77615035 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 19:47:46 2024]  Iteration number: 0 with current cost as 0.3010396322898269 and parameters 
[-3.34429392  2.44727817 -2.11084642 -0.11653103  0.55388707 -2.77010897
  3.06858498  2.18960144  1.18551998 -1.06648309  0.68622881  1.14432445
  1.7889791  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 19:53:50 2024]  Iteration number: 0 with current cost as 0.31123135553699066 and parameters 
[-3.33407814  2.42545831 -2.11166345 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.67805487  1.14432447
  1.78118207 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Training complete taking 1791.5473070144653 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 19:59:26 2024]  Iteration number: 0 with current cost as 0.3246559745236599 and parameters 
[-3.29181247  2.4906329  -2.10793542 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.69764308  1.14432445
  1.7223314  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 20:05:11 2024]  Iteration number: 0 with current cost as 0.3000099586663876 and parameters 
[-3.34804442  2.39267832 -2.11737085 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551998 -1.06648308  0.66975628  1.14432446
  1.80311613 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 20:11:05 2024]  Iteration number: 0 with current cost as 0.30330284663373347 and parameters 
[-3.33246689  2.44212326 -2.1083687  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6772108   1.14432445
  1.77615035 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 20:17:28 2024]  Iteration number: 0 with current cost as 0.3010396322898269 and parameters 
[-3.34429392  2.44727817 -2.11084642 -0.11653103  0.55388707 -2.77010897
  3.06858498  2.18960144  1.18551998 -1.06648309  0.68622881  1.14432445
  1.7889791  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 20:23:33 2024]  Iteration number: 0 with current cost as 0.31123135553699066 and parameters 
[-3.33407814  2.42545831 -2.11166345 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.67805487  1.14432447
  1.78118207 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Training complete taking 1781.1476957798004 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 20:29:07 2024]  Iteration number: 0 with current cost as 0.3246559745236599 and parameters 
[-3.29181247  2.4906329  -2.10793542 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.69764308  1.14432445
  1.7223314  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 20:34:53 2024]  Iteration number: 0 with current cost as 0.3000099586663876 and parameters 
[-3.34804442  2.39267832 -2.11737085 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551998 -1.06648308  0.66975628  1.14432446
  1.80311613 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 20:40:48 2024]  Iteration number: 0 with current cost as 0.30330284663373347 and parameters 
[-3.33246689  2.44212326 -2.1083687  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6772108   1.14432445
  1.77615035 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 20:47:11 2024]  Iteration number: 0 with current cost as 0.3010396322898269 and parameters 
[-3.34429392  2.44727817 -2.11084642 -0.11653103  0.55388707 -2.77010897
  3.06858498  2.18960144  1.18551998 -1.06648309  0.68622881  1.14432445
  1.7889791  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 20:53:15 2024]  Iteration number: 0 with current cost as 0.31123135553699066 and parameters 
[-3.33407814  2.42545831 -2.11166345 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.67805487  1.14432447
  1.78118207 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Training complete taking 1783.2829217910767 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 20:58:49 2024]  Iteration number: 0 with current cost as 0.3246559745236599 and parameters 
[-3.29181247  2.4906329  -2.10793542 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.69764308  1.14432445
  1.7223314  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 21:04:56 2024]  Iteration number: 0 with current cost as 0.3000099586663876 and parameters 
[-3.34804442  2.39267832 -2.11737085 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551998 -1.06648308  0.66975628  1.14432446
  1.80311613 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 21:10:52 2024]  Iteration number: 0 with current cost as 0.30330284663373347 and parameters 
[-3.33246689  2.44212326 -2.1083687  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6772108   1.14432445
  1.77615035 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 21:17:32 2024]  Iteration number: 0 with current cost as 0.3010396322898269 and parameters 
[-3.34429392  2.44727817 -2.11084642 -0.11653103  0.55388707 -2.77010897
  3.06858498  2.18960144  1.18551998 -1.06648309  0.68622881  1.14432445
  1.7889791  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 21:23:56 2024]  Iteration number: 0 with current cost as 0.31123135553699066 and parameters 
[-3.33407814  2.42545831 -2.11166345 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.67805487  1.14432447
  1.78118207 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Training complete taking 1850.845743894577 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 21:29:41 2024]  Iteration number: 0 with current cost as 0.3246559745236599 and parameters 
[-3.29181247  2.4906329  -2.10793542 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.69764308  1.14432445
  1.7223314  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 21:35:26 2024]  Iteration number: 0 with current cost as 0.3000099586663876 and parameters 
[-3.34804442  2.39267832 -2.11737085 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551998 -1.06648308  0.66975628  1.14432446
  1.80311613 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 21:41:25 2024]  Iteration number: 0 with current cost as 0.30330284663373347 and parameters 
[-3.33246689  2.44212326 -2.1083687  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6772108   1.14432445
  1.77615035 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 21:47:49 2024]  Iteration number: 0 with current cost as 0.3010396322898269 and parameters 
[-3.34429392  2.44727817 -2.11084642 -0.11653103  0.55388707 -2.77010897
  3.06858498  2.18960144  1.18551998 -1.06648309  0.68622881  1.14432445
  1.7889791  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 21:53:53 2024]  Iteration number: 0 with current cost as 0.31123135553699066 and parameters 
[-3.33407814  2.42545831 -2.11166345 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.67805487  1.14432447
  1.78118207 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Training complete taking 1788.4524056911469 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 21:59:28 2024]  Iteration number: 0 with current cost as 0.3246559745236599 and parameters 
[-3.29181247  2.4906329  -2.10793542 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.69764308  1.14432445
  1.7223314  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 22:05:15 2024]  Iteration number: 0 with current cost as 0.3000099586663876 and parameters 
[-3.34804442  2.39267832 -2.11737085 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551998 -1.06648308  0.66975628  1.14432446
  1.80311613 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 22:11:10 2024]  Iteration number: 0 with current cost as 0.30330284663373347 and parameters 
[-3.33246689  2.44212326 -2.1083687  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6772108   1.14432445
  1.77615035 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 22:17:30 2024]  Iteration number: 0 with current cost as 0.3010396322898269 and parameters 
[-3.34429392  2.44727817 -2.11084642 -0.11653103  0.55388707 -2.77010897
  3.06858498  2.18960144  1.18551998 -1.06648309  0.68622881  1.14432445
  1.7889791  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 22:23:40 2024]  Iteration number: 0 with current cost as 0.31123135553699066 and parameters 
[-3.33407814  2.42545831 -2.11166345 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.67805487  1.14432447
  1.78118207 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Training complete taking 1783.8722622394562 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 22:29:14 2024]  Iteration number: 0 with current cost as 0.3246559745236599 and parameters 
[-3.29181247  2.4906329  -2.10793542 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.69764308  1.14432445
  1.7223314  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 22:34:57 2024]  Iteration number: 0 with current cost as 0.3000099586663876 and parameters 
[-3.34804442  2.39267832 -2.11737085 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551998 -1.06648308  0.66975628  1.14432446
  1.80311613 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 22:40:51 2024]  Iteration number: 0 with current cost as 0.30330284663373347 and parameters 
[-3.33246689  2.44212326 -2.1083687  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6772108   1.14432445
  1.77615035 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 22:47:14 2024]  Iteration number: 0 with current cost as 0.3010396322898269 and parameters 
[-3.34429392  2.44727817 -2.11084642 -0.11653103  0.55388707 -2.77010897
  3.06858498  2.18960144  1.18551998 -1.06648309  0.68622881  1.14432445
  1.7889791  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 22:53:17 2024]  Iteration number: 0 with current cost as 0.31123135553699066 and parameters 
[-3.33407814  2.42545831 -2.11166345 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.67805487  1.14432447
  1.78118207 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Training complete taking 1778.680191040039 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 22:58:52 2024]  Iteration number: 0 with current cost as 0.3246559745236599 and parameters 
[-3.29181247  2.4906329  -2.10793542 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.69764308  1.14432445
  1.7223314  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 23:04:38 2024]  Iteration number: 0 with current cost as 0.3000099586663876 and parameters 
[-3.34804442  2.39267832 -2.11737085 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551998 -1.06648308  0.66975628  1.14432446
  1.80311613 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 23:10:30 2024]  Iteration number: 0 with current cost as 0.30330284663373347 and parameters 
[-3.33246689  2.44212326 -2.1083687  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6772108   1.14432445
  1.77615035 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 23:16:52 2024]  Iteration number: 0 with current cost as 0.3010396322898269 and parameters 
[-3.34429392  2.44727817 -2.11084642 -0.11653103  0.55388707 -2.77010897
  3.06858498  2.18960144  1.18551998 -1.06648309  0.68622881  1.14432445
  1.7889791  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 23:22:54 2024]  Iteration number: 0 with current cost as 0.31123135553699066 and parameters 
[-3.33407814  2.42545831 -2.11166345 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.67805487  1.14432447
  1.78118207 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Training complete taking 1776.8888854980469 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 23:28:28 2024]  Iteration number: 0 with current cost as 0.3246559745236599 and parameters 
[-3.29181247  2.4906329  -2.10793542 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.69764308  1.14432445
  1.7223314  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 23:34:13 2024]  Iteration number: 0 with current cost as 0.3000099586663876 and parameters 
[-3.34804442  2.39267832 -2.11737085 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551998 -1.06648308  0.66975628  1.14432446
  1.80311613 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 23:40:20 2024]  Iteration number: 0 with current cost as 0.30330284663373347 and parameters 
[-3.33246689  2.44212326 -2.1083687  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6772108   1.14432445
  1.77615035 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 23:46:46 2024]  Iteration number: 0 with current cost as 0.3010396322898269 and parameters 
[-3.34429392  2.44727817 -2.11084642 -0.11653103  0.55388707 -2.77010897
  3.06858498  2.18960144  1.18551998 -1.06648309  0.68622881  1.14432445
  1.7889791  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 23:52:50 2024]  Iteration number: 0 with current cost as 0.31123135553699066 and parameters 
[-3.33407814  2.42545831 -2.11166345 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.67805487  1.14432447
  1.78118207 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Training complete taking 1798.2821063995361 seconds. 
Discarding model... 

Training complete taking 44967.52945256233 total seconds. 
Now scoring model... 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Scoring complete taking 1.3318967819213867 seconds. 
Saved predicted values as M-A1-CNOT_Full-Pauli-CRZ_predicted_values.csv
Model scores: {'MSE_train': (89.74705502836767,), 'R2_train': 0.56520563892786, 'MAE_train': 8.481528342937583, 'MSE_test': 207.79839811226148, 'R2_test': -0.2525700746251711, 'MAE_test': 10.734429761373152}. 
Saved model results as M-A1-CNOT_Full-Pauli-CRZ_results.json. 
