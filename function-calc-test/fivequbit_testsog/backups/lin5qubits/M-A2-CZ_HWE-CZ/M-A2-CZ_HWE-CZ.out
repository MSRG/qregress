/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/circuits/Encoders.py:12: RuntimeWarning: invalid value encountered in arcsin
  if np.isnan(np.arcsin(features[feature_idx])) or np.isnan(np.arccos(features[feature_idx])):
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Sun Mar 24 17:30:11 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 17:30:33 2024]  Iteration number: 0 with current cost as 0.42134316832544183 and parameters 
[-3.19064797  1.19644309 -0.84184908 -0.11653099  0.55388712 -2.77010897
  3.06858498  2.18960149  1.18552002 -1.06648312  0.60271514  1.14432449
  1.31029902 -1.87354676  0.72965077]. 
Working on 0.4 fold... 
[Sun Mar 24 17:32:29 2024]  Iteration number: 0 with current cost as 0.4843524321368117 and parameters 
[-3.33981057  1.31680264 -0.69514514 -0.11653099  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  0.60271514  1.14432449
  1.31029902 -1.87354673  0.72965077]. 
Working on 0.6 fold... 
[Sun Mar 24 17:35:31 2024]  Iteration number: 0 with current cost as 0.4453203292085197 and parameters 
[-3.23834126  1.23115132 -0.79197665 -0.11653095  0.55388715 -2.77010894
  3.06858502  2.18960153  1.1855201  -1.06648308  0.60271514  1.14432449
  1.31029902 -1.87354669  0.7296508 ]. 
Working on 0.8 fold... 
[Sun Mar 24 17:37:31 2024]  Iteration number: 0 with current cost as 0.44645188646368383 and parameters 
[-3.21750199  1.28621372 -0.86892148 -0.11653095  0.55388708 -2.77010894
  3.06858498  2.18960149  1.18552002 -1.06648308  0.6027151   1.14432449
  1.31029902 -1.87354673  0.72965077]. 
Working on 1.0 fold... 
[Sun Mar 24 17:39:00 2024]  Iteration number: 0 with current cost as 0.4728526464528593 and parameters 
[-3.32287439  1.25059764 -0.67054068 -0.11653095  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  0.60271514  1.14432449
  1.31029902 -1.87354673  0.7296508 ]. 
Training complete taking 689.6021549701691 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 17:42:02 2024]  Iteration number: 0 with current cost as 0.42134316832544183 and parameters 
[-3.19064797  1.19644309 -0.84184908 -0.11653099  0.55388712 -2.77010897
  3.06858498  2.18960149  1.18552002 -1.06648312  0.60271514  1.14432449
  1.31029902 -1.87354676  0.72965077]. 
Working on 0.4 fold... 
[Sun Mar 24 17:43:41 2024]  Iteration number: 0 with current cost as 0.4843524321368117 and parameters 
[-3.33981057  1.31680264 -0.69514514 -0.11653099  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  0.60271514  1.14432449
  1.31029902 -1.87354673  0.72965077]. 
Working on 0.6 fold... 
[Sun Mar 24 17:46:42 2024]  Iteration number: 0 with current cost as 0.4453203292085197 and parameters 
[-3.23834126  1.23115132 -0.79197665 -0.11653095  0.55388715 -2.77010894
  3.06858502  2.18960153  1.1855201  -1.06648308  0.60271514  1.14432449
  1.31029902 -1.87354669  0.7296508 ]. 
Working on 0.8 fold... 
[Sun Mar 24 17:48:42 2024]  Iteration number: 0 with current cost as 0.44645188646368383 and parameters 
[-3.21750199  1.28621372 -0.86892148 -0.11653095  0.55388708 -2.77010894
  3.06858498  2.18960149  1.18552002 -1.06648308  0.6027151   1.14432449
  1.31029902 -1.87354673  0.72965077]. 
Working on 1.0 fold... 
[Sun Mar 24 17:50:14 2024]  Iteration number: 0 with current cost as 0.4728526464528593 and parameters 
[-3.32287439  1.25059764 -0.67054068 -0.11653095  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  0.60271514  1.14432449
  1.31029902 -1.87354673  0.7296508 ]. 
Training complete taking 673.123204946518 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 17:53:16 2024]  Iteration number: 0 with current cost as 0.42134316832544183 and parameters 
[-3.19064797  1.19644309 -0.84184908 -0.11653099  0.55388712 -2.77010897
  3.06858498  2.18960149  1.18552002 -1.06648312  0.60271514  1.14432449
  1.31029902 -1.87354676  0.72965077]. 
Working on 0.4 fold... 
[Sun Mar 24 17:54:55 2024]  Iteration number: 0 with current cost as 0.4843524321368117 and parameters 
[-3.33981057  1.31680264 -0.69514514 -0.11653099  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  0.60271514  1.14432449
  1.31029902 -1.87354673  0.72965077]. 
Working on 0.6 fold... 
[Sun Mar 24 17:57:55 2024]  Iteration number: 0 with current cost as 0.4453203292085197 and parameters 
[-3.23834126  1.23115132 -0.79197665 -0.11653095  0.55388715 -2.77010894
  3.06858502  2.18960153  1.1855201  -1.06648308  0.60271514  1.14432449
  1.31029902 -1.87354669  0.7296508 ]. 
Working on 0.8 fold... 
[Sun Mar 24 17:59:54 2024]  Iteration number: 0 with current cost as 0.44645188646368383 and parameters 
[-3.21750199  1.28621372 -0.86892148 -0.11653095  0.55388708 -2.77010894
  3.06858498  2.18960149  1.18552002 -1.06648308  0.6027151   1.14432449
  1.31029902 -1.87354673  0.72965077]. 
Working on 1.0 fold... 
[Sun Mar 24 18:01:25 2024]  Iteration number: 0 with current cost as 0.4728526464528593 and parameters 
[-3.32287439  1.25059764 -0.67054068 -0.11653095  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  0.60271514  1.14432449
  1.31029902 -1.87354673  0.7296508 ]. 
Training complete taking 668.3420302867889 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 18:04:25 2024]  Iteration number: 0 with current cost as 0.42134316832544183 and parameters 
[-3.19064797  1.19644309 -0.84184908 -0.11653099  0.55388712 -2.77010897
  3.06858498  2.18960149  1.18552002 -1.06648312  0.60271514  1.14432449
  1.31029902 -1.87354676  0.72965077]. 
Working on 0.4 fold... 
[Sun Mar 24 18:06:03 2024]  Iteration number: 0 with current cost as 0.4843524321368117 and parameters 
[-3.33981057  1.31680264 -0.69514514 -0.11653099  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  0.60271514  1.14432449
  1.31029902 -1.87354673  0.72965077]. 
Working on 0.6 fold... 
[Sun Mar 24 18:09:02 2024]  Iteration number: 0 with current cost as 0.4453203292085197 and parameters 
[-3.23834126  1.23115132 -0.79197665 -0.11653095  0.55388715 -2.77010894
  3.06858502  2.18960153  1.1855201  -1.06648308  0.60271514  1.14432449
  1.31029902 -1.87354669  0.7296508 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:11:03 2024]  Iteration number: 0 with current cost as 0.44645188646368383 and parameters 
[-3.21750199  1.28621372 -0.86892148 -0.11653095  0.55388708 -2.77010894
  3.06858498  2.18960149  1.18552002 -1.06648308  0.6027151   1.14432449
  1.31029902 -1.87354673  0.72965077]. 
Working on 1.0 fold... 
[Sun Mar 24 18:12:33 2024]  Iteration number: 0 with current cost as 0.4728526464528593 and parameters 
[-3.32287439  1.25059764 -0.67054068 -0.11653095  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  0.60271514  1.14432449
  1.31029902 -1.87354673  0.7296508 ]. 
Training complete taking 669.1270086765289 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 18:15:33 2024]  Iteration number: 0 with current cost as 0.42134316832544183 and parameters 
[-3.19064797  1.19644309 -0.84184908 -0.11653099  0.55388712 -2.77010897
  3.06858498  2.18960149  1.18552002 -1.06648312  0.60271514  1.14432449
  1.31029902 -1.87354676  0.72965077]. 
Working on 0.4 fold... 
[Sun Mar 24 18:17:12 2024]  Iteration number: 0 with current cost as 0.4843524321368117 and parameters 
[-3.33981057  1.31680264 -0.69514514 -0.11653099  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  0.60271514  1.14432449
  1.31029902 -1.87354673  0.72965077]. 
Working on 0.6 fold... 
[Sun Mar 24 18:20:13 2024]  Iteration number: 0 with current cost as 0.4453203292085197 and parameters 
[-3.23834126  1.23115132 -0.79197665 -0.11653095  0.55388715 -2.77010894
  3.06858502  2.18960153  1.1855201  -1.06648308  0.60271514  1.14432449
  1.31029902 -1.87354669  0.7296508 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:22:13 2024]  Iteration number: 0 with current cost as 0.44645188646368383 and parameters 
[-3.21750199  1.28621372 -0.86892148 -0.11653095  0.55388708 -2.77010894
  3.06858498  2.18960149  1.18552002 -1.06648308  0.6027151   1.14432449
  1.31029902 -1.87354673  0.72965077]. 
Working on 1.0 fold... 
[Sun Mar 24 18:23:44 2024]  Iteration number: 0 with current cost as 0.4728526464528593 and parameters 
[-3.32287439  1.25059764 -0.67054068 -0.11653095  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  0.60271514  1.14432449
  1.31029902 -1.87354673  0.7296508 ]. 
Training complete taking 670.7651748657227 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 18:26:45 2024]  Iteration number: 0 with current cost as 0.42134316832544183 and parameters 
[-3.19064797  1.19644309 -0.84184908 -0.11653099  0.55388712 -2.77010897
  3.06858498  2.18960149  1.18552002 -1.06648312  0.60271514  1.14432449
  1.31029902 -1.87354676  0.72965077]. 
Working on 0.4 fold... 
[Sun Mar 24 18:28:22 2024]  Iteration number: 0 with current cost as 0.4843524321368117 and parameters 
[-3.33981057  1.31680264 -0.69514514 -0.11653099  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  0.60271514  1.14432449
  1.31029902 -1.87354673  0.72965077]. 
Working on 0.6 fold... 
[Sun Mar 24 18:31:24 2024]  Iteration number: 0 with current cost as 0.4453203292085197 and parameters 
[-3.23834126  1.23115132 -0.79197665 -0.11653095  0.55388715 -2.77010894
  3.06858502  2.18960153  1.1855201  -1.06648308  0.60271514  1.14432449
  1.31029902 -1.87354669  0.7296508 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:33:26 2024]  Iteration number: 0 with current cost as 0.44645188646368383 and parameters 
[-3.21750199  1.28621372 -0.86892148 -0.11653095  0.55388708 -2.77010894
  3.06858498  2.18960149  1.18552002 -1.06648308  0.6027151   1.14432449
  1.31029902 -1.87354673  0.72965077]. 
Working on 1.0 fold... 
[Sun Mar 24 18:34:57 2024]  Iteration number: 0 with current cost as 0.4728526464528593 and parameters 
[-3.32287439  1.25059764 -0.67054068 -0.11653095  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  0.60271514  1.14432449
  1.31029902 -1.87354673  0.7296508 ]. 
Training complete taking 672.7937135696411 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 18:37:57 2024]  Iteration number: 0 with current cost as 0.42134316832544183 and parameters 
[-3.19064797  1.19644309 -0.84184908 -0.11653099  0.55388712 -2.77010897
  3.06858498  2.18960149  1.18552002 -1.06648312  0.60271514  1.14432449
  1.31029902 -1.87354676  0.72965077]. 
Working on 0.4 fold... 
[Sun Mar 24 18:39:53 2024]  Iteration number: 0 with current cost as 0.4843524321368117 and parameters 
[-3.33981057  1.31680264 -0.69514514 -0.11653099  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  0.60271514  1.14432449
  1.31029902 -1.87354673  0.72965077]. 
Working on 0.6 fold... 
[Sun Mar 24 18:42:59 2024]  Iteration number: 0 with current cost as 0.4453203292085197 and parameters 
[-3.23834126  1.23115132 -0.79197665 -0.11653095  0.55388715 -2.77010894
  3.06858502  2.18960153  1.1855201  -1.06648308  0.60271514  1.14432449
  1.31029902 -1.87354669  0.7296508 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:45:00 2024]  Iteration number: 0 with current cost as 0.44645188646368383 and parameters 
[-3.21750199  1.28621372 -0.86892148 -0.11653095  0.55388708 -2.77010894
  3.06858498  2.18960149  1.18552002 -1.06648308  0.6027151   1.14432449
  1.31029902 -1.87354673  0.72965077]. 
Working on 1.0 fold... 
[Sun Mar 24 18:46:30 2024]  Iteration number: 0 with current cost as 0.4728526464528593 and parameters 
[-3.32287439  1.25059764 -0.67054068 -0.11653095  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  0.60271514  1.14432449
  1.31029902 -1.87354673  0.7296508 ]. 
Training complete taking 692.4921219348907 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 18:49:30 2024]  Iteration number: 0 with current cost as 0.42134316832544183 and parameters 
[-3.19064797  1.19644309 -0.84184908 -0.11653099  0.55388712 -2.77010897
  3.06858498  2.18960149  1.18552002 -1.06648312  0.60271514  1.14432449
  1.31029902 -1.87354676  0.72965077]. 
Working on 0.4 fold... 
[Sun Mar 24 18:51:11 2024]  Iteration number: 0 with current cost as 0.4843524321368117 and parameters 
[-3.33981057  1.31680264 -0.69514514 -0.11653099  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  0.60271514  1.14432449
  1.31029902 -1.87354673  0.72965077]. 
Working on 0.6 fold... 
[Sun Mar 24 18:54:15 2024]  Iteration number: 0 with current cost as 0.4453203292085197 and parameters 
[-3.23834126  1.23115132 -0.79197665 -0.11653095  0.55388715 -2.77010894
  3.06858502  2.18960153  1.1855201  -1.06648308  0.60271514  1.14432449
  1.31029902 -1.87354669  0.7296508 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:56:15 2024]  Iteration number: 0 with current cost as 0.44645188646368383 and parameters 
[-3.21750199  1.28621372 -0.86892148 -0.11653095  0.55388708 -2.77010894
  3.06858498  2.18960149  1.18552002 -1.06648308  0.6027151   1.14432449
  1.31029902 -1.87354673  0.72965077]. 
Working on 1.0 fold... 
[Sun Mar 24 18:57:45 2024]  Iteration number: 0 with current cost as 0.4728526464528593 and parameters 
[-3.32287439  1.25059764 -0.67054068 -0.11653095  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  0.60271514  1.14432449
  1.31029902 -1.87354673  0.7296508 ]. 
Training complete taking 674.8996934890747 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 19:00:45 2024]  Iteration number: 0 with current cost as 0.42134316832544183 and parameters 
[-3.19064797  1.19644309 -0.84184908 -0.11653099  0.55388712 -2.77010897
  3.06858498  2.18960149  1.18552002 -1.06648312  0.60271514  1.14432449
  1.31029902 -1.87354676  0.72965077]. 
Working on 0.4 fold... 
[Sun Mar 24 19:02:23 2024]  Iteration number: 0 with current cost as 0.4843524321368117 and parameters 
[-3.33981057  1.31680264 -0.69514514 -0.11653099  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  0.60271514  1.14432449
  1.31029902 -1.87354673  0.72965077]. 
Working on 0.6 fold... 
[Sun Mar 24 19:05:23 2024]  Iteration number: 0 with current cost as 0.4453203292085197 and parameters 
[-3.23834126  1.23115132 -0.79197665 -0.11653095  0.55388715 -2.77010894
  3.06858502  2.18960153  1.1855201  -1.06648308  0.60271514  1.14432449
  1.31029902 -1.87354669  0.7296508 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:07:32 2024]  Iteration number: 0 with current cost as 0.44645188646368383 and parameters 
[-3.21750199  1.28621372 -0.86892148 -0.11653095  0.55388708 -2.77010894
  3.06858498  2.18960149  1.18552002 -1.06648308  0.6027151   1.14432449
  1.31029902 -1.87354673  0.72965077]. 
Working on 1.0 fold... 
[Sun Mar 24 19:09:25 2024]  Iteration number: 0 with current cost as 0.4728526464528593 and parameters 
[-3.32287439  1.25059764 -0.67054068 -0.11653095  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  0.60271514  1.14432449
  1.31029902 -1.87354673  0.7296508 ]. 
Training complete taking 700.641982793808 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 19:12:25 2024]  Iteration number: 0 with current cost as 0.42134316832544183 and parameters 
[-3.19064797  1.19644309 -0.84184908 -0.11653099  0.55388712 -2.77010897
  3.06858498  2.18960149  1.18552002 -1.06648312  0.60271514  1.14432449
  1.31029902 -1.87354676  0.72965077]. 
Working on 0.4 fold... 
[Sun Mar 24 19:14:03 2024]  Iteration number: 0 with current cost as 0.4843524321368117 and parameters 
[-3.33981057  1.31680264 -0.69514514 -0.11653099  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  0.60271514  1.14432449
  1.31029902 -1.87354673  0.72965077]. 
Working on 0.6 fold... 
[Sun Mar 24 19:17:04 2024]  Iteration number: 0 with current cost as 0.4453203292085197 and parameters 
[-3.23834126  1.23115132 -0.79197665 -0.11653095  0.55388715 -2.77010894
  3.06858502  2.18960153  1.1855201  -1.06648308  0.60271514  1.14432449
  1.31029902 -1.87354669  0.7296508 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:19:04 2024]  Iteration number: 0 with current cost as 0.44645188646368383 and parameters 
[-3.21750199  1.28621372 -0.86892148 -0.11653095  0.55388708 -2.77010894
  3.06858498  2.18960149  1.18552002 -1.06648308  0.6027151   1.14432449
  1.31029902 -1.87354673  0.72965077]. 
Working on 1.0 fold... 
[Sun Mar 24 19:20:34 2024]  Iteration number: 0 with current cost as 0.4728526464528593 and parameters 
[-3.32287439  1.25059764 -0.67054068 -0.11653095  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  0.60271514  1.14432449
  1.31029902 -1.87354673  0.7296508 ]. 
Training complete taking 671.433183670044 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 19:23:36 2024]  Iteration number: 0 with current cost as 0.42134316832544183 and parameters 
[-3.19064797  1.19644309 -0.84184908 -0.11653099  0.55388712 -2.77010897
  3.06858498  2.18960149  1.18552002 -1.06648312  0.60271514  1.14432449
  1.31029902 -1.87354676  0.72965077]. 
Working on 0.4 fold... 
[Sun Mar 24 19:25:15 2024]  Iteration number: 0 with current cost as 0.4843524321368117 and parameters 
[-3.33981057  1.31680264 -0.69514514 -0.11653099  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  0.60271514  1.14432449
  1.31029902 -1.87354673  0.72965077]. 
Working on 0.6 fold... 
[Sun Mar 24 19:28:16 2024]  Iteration number: 0 with current cost as 0.4453203292085197 and parameters 
[-3.23834126  1.23115132 -0.79197665 -0.11653095  0.55388715 -2.77010894
  3.06858502  2.18960153  1.1855201  -1.06648308  0.60271514  1.14432449
  1.31029902 -1.87354669  0.7296508 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:30:16 2024]  Iteration number: 0 with current cost as 0.44645188646368383 and parameters 
[-3.21750199  1.28621372 -0.86892148 -0.11653095  0.55388708 -2.77010894
  3.06858498  2.18960149  1.18552002 -1.06648308  0.6027151   1.14432449
  1.31029902 -1.87354673  0.72965077]. 
Working on 1.0 fold... 
[Sun Mar 24 19:31:46 2024]  Iteration number: 0 with current cost as 0.4728526464528593 and parameters 
[-3.32287439  1.25059764 -0.67054068 -0.11653095  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  0.60271514  1.14432449
  1.31029902 -1.87354673  0.7296508 ]. 
Training complete taking 669.7931287288666 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 19:34:47 2024]  Iteration number: 0 with current cost as 0.42134316832544183 and parameters 
[-3.19064797  1.19644309 -0.84184908 -0.11653099  0.55388712 -2.77010897
  3.06858498  2.18960149  1.18552002 -1.06648312  0.60271514  1.14432449
  1.31029902 -1.87354676  0.72965077]. 
Working on 0.4 fold... 
[Sun Mar 24 19:36:24 2024]  Iteration number: 0 with current cost as 0.4843524321368117 and parameters 
[-3.33981057  1.31680264 -0.69514514 -0.11653099  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  0.60271514  1.14432449
  1.31029902 -1.87354673  0.72965077]. 
Working on 0.6 fold... 
[Sun Mar 24 19:39:24 2024]  Iteration number: 0 with current cost as 0.4453203292085197 and parameters 
[-3.23834126  1.23115132 -0.79197665 -0.11653095  0.55388715 -2.77010894
  3.06858502  2.18960153  1.1855201  -1.06648308  0.60271514  1.14432449
  1.31029902 -1.87354669  0.7296508 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:41:24 2024]  Iteration number: 0 with current cost as 0.44645188646368383 and parameters 
[-3.21750199  1.28621372 -0.86892148 -0.11653095  0.55388708 -2.77010894
  3.06858498  2.18960149  1.18552002 -1.06648308  0.6027151   1.14432449
  1.31029902 -1.87354673  0.72965077]. 
Working on 1.0 fold... 
[Sun Mar 24 19:42:54 2024]  Iteration number: 0 with current cost as 0.4728526464528593 and parameters 
[-3.32287439  1.25059764 -0.67054068 -0.11653095  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  0.60271514  1.14432449
  1.31029902 -1.87354673  0.7296508 ]. 
Training complete taking 666.4256949424744 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 19:45:53 2024]  Iteration number: 0 with current cost as 0.42134316832544183 and parameters 
[-3.19064797  1.19644309 -0.84184908 -0.11653099  0.55388712 -2.77010897
  3.06858498  2.18960149  1.18552002 -1.06648312  0.60271514  1.14432449
  1.31029902 -1.87354676  0.72965077]. 
Working on 0.4 fold... 
[Sun Mar 24 19:47:29 2024]  Iteration number: 0 with current cost as 0.4843524321368117 and parameters 
[-3.33981057  1.31680264 -0.69514514 -0.11653099  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  0.60271514  1.14432449
  1.31029902 -1.87354673  0.72965077]. 
Working on 0.6 fold... 
[Sun Mar 24 19:50:33 2024]  Iteration number: 0 with current cost as 0.4453203292085197 and parameters 
[-3.23834126  1.23115132 -0.79197665 -0.11653095  0.55388715 -2.77010894
  3.06858502  2.18960153  1.1855201  -1.06648308  0.60271514  1.14432449
  1.31029902 -1.87354669  0.7296508 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:52:47 2024]  Iteration number: 0 with current cost as 0.44645188646368383 and parameters 
[-3.21750199  1.28621372 -0.86892148 -0.11653095  0.55388708 -2.77010894
  3.06858498  2.18960149  1.18552002 -1.06648308  0.6027151   1.14432449
  1.31029902 -1.87354673  0.72965077]. 
Working on 1.0 fold... 
[Sun Mar 24 19:54:17 2024]  Iteration number: 0 with current cost as 0.4728526464528593 and parameters 
[-3.32287439  1.25059764 -0.67054068 -0.11653095  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  0.60271514  1.14432449
  1.31029902 -1.87354673  0.7296508 ]. 
Training complete taking 681.9892485141754 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 19:57:14 2024]  Iteration number: 0 with current cost as 0.42134316832544183 and parameters 
[-3.19064797  1.19644309 -0.84184908 -0.11653099  0.55388712 -2.77010897
  3.06858498  2.18960149  1.18552002 -1.06648312  0.60271514  1.14432449
  1.31029902 -1.87354676  0.72965077]. 
Working on 0.4 fold... 
[Sun Mar 24 19:58:59 2024]  Iteration number: 0 with current cost as 0.4843524321368117 and parameters 
[-3.33981057  1.31680264 -0.69514514 -0.11653099  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  0.60271514  1.14432449
  1.31029902 -1.87354673  0.72965077]. 
Working on 0.6 fold... 
[Sun Mar 24 20:02:02 2024]  Iteration number: 0 with current cost as 0.4453203292085197 and parameters 
[-3.23834126  1.23115132 -0.79197665 -0.11653095  0.55388715 -2.77010894
  3.06858502  2.18960153  1.1855201  -1.06648308  0.60271514  1.14432449
  1.31029902 -1.87354669  0.7296508 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:04:01 2024]  Iteration number: 0 with current cost as 0.44645188646368383 and parameters 
[-3.21750199  1.28621372 -0.86892148 -0.11653095  0.55388708 -2.77010894
  3.06858498  2.18960149  1.18552002 -1.06648308  0.6027151   1.14432449
  1.31029902 -1.87354673  0.72965077]. 
Working on 1.0 fold... 
[Sun Mar 24 20:05:31 2024]  Iteration number: 0 with current cost as 0.4728526464528593 and parameters 
[-3.32287439  1.25059764 -0.67054068 -0.11653095  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  0.60271514  1.14432449
  1.31029902 -1.87354673  0.7296508 ]. 
Training complete taking 673.8758866786957 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 20:08:29 2024]  Iteration number: 0 with current cost as 0.42134316832544183 and parameters 
[-3.19064797  1.19644309 -0.84184908 -0.11653099  0.55388712 -2.77010897
  3.06858498  2.18960149  1.18552002 -1.06648312  0.60271514  1.14432449
  1.31029902 -1.87354676  0.72965077]. 
Working on 0.4 fold... 
[Sun Mar 24 20:10:07 2024]  Iteration number: 0 with current cost as 0.4843524321368117 and parameters 
[-3.33981057  1.31680264 -0.69514514 -0.11653099  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  0.60271514  1.14432449
  1.31029902 -1.87354673  0.72965077]. 
Working on 0.6 fold... 
[Sun Mar 24 20:13:07 2024]  Iteration number: 0 with current cost as 0.4453203292085197 and parameters 
[-3.23834126  1.23115132 -0.79197665 -0.11653095  0.55388715 -2.77010894
  3.06858502  2.18960153  1.1855201  -1.06648308  0.60271514  1.14432449
  1.31029902 -1.87354669  0.7296508 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:15:10 2024]  Iteration number: 0 with current cost as 0.44645188646368383 and parameters 
[-3.21750199  1.28621372 -0.86892148 -0.11653095  0.55388708 -2.77010894
  3.06858498  2.18960149  1.18552002 -1.06648308  0.6027151   1.14432449
  1.31029902 -1.87354673  0.72965077]. 
Working on 1.0 fold... 
[Sun Mar 24 20:16:39 2024]  Iteration number: 0 with current cost as 0.4728526464528593 and parameters 
[-3.32287439  1.25059764 -0.67054068 -0.11653095  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  0.60271514  1.14432449
  1.31029902 -1.87354673  0.7296508 ]. 
Training complete taking 674.3328311443329 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 20:19:42 2024]  Iteration number: 0 with current cost as 0.42134316832544183 and parameters 
[-3.19064797  1.19644309 -0.84184908 -0.11653099  0.55388712 -2.77010897
  3.06858498  2.18960149  1.18552002 -1.06648312  0.60271514  1.14432449
  1.31029902 -1.87354676  0.72965077]. 
Working on 0.4 fold... 
[Sun Mar 24 20:21:20 2024]  Iteration number: 0 with current cost as 0.4843524321368117 and parameters 
[-3.33981057  1.31680264 -0.69514514 -0.11653099  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  0.60271514  1.14432449
  1.31029902 -1.87354673  0.72965077]. 
Working on 0.6 fold... 
[Sun Mar 24 20:24:21 2024]  Iteration number: 0 with current cost as 0.4453203292085197 and parameters 
[-3.23834126  1.23115132 -0.79197665 -0.11653095  0.55388715 -2.77010894
  3.06858502  2.18960153  1.1855201  -1.06648308  0.60271514  1.14432449
  1.31029902 -1.87354669  0.7296508 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:26:20 2024]  Iteration number: 0 with current cost as 0.44645188646368383 and parameters 
[-3.21750199  1.28621372 -0.86892148 -0.11653095  0.55388708 -2.77010894
  3.06858498  2.18960149  1.18552002 -1.06648308  0.6027151   1.14432449
  1.31029902 -1.87354673  0.72965077]. 
Working on 1.0 fold... 
[Sun Mar 24 20:27:49 2024]  Iteration number: 0 with current cost as 0.4728526464528593 and parameters 
[-3.32287439  1.25059764 -0.67054068 -0.11653095  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  0.60271514  1.14432449
  1.31029902 -1.87354673  0.7296508 ]. 
Training complete taking 691.319890499115 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 20:31:13 2024]  Iteration number: 0 with current cost as 0.42134316832544183 and parameters 
[-3.19064797  1.19644309 -0.84184908 -0.11653099  0.55388712 -2.77010897
  3.06858498  2.18960149  1.18552002 -1.06648312  0.60271514  1.14432449
  1.31029902 -1.87354676  0.72965077]. 
Working on 0.4 fold... 
[Sun Mar 24 20:32:52 2024]  Iteration number: 0 with current cost as 0.4843524321368117 and parameters 
[-3.33981057  1.31680264 -0.69514514 -0.11653099  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  0.60271514  1.14432449
  1.31029902 -1.87354673  0.72965077]. 
Working on 0.6 fold... 
[Sun Mar 24 20:35:56 2024]  Iteration number: 0 with current cost as 0.4453203292085197 and parameters 
[-3.23834126  1.23115132 -0.79197665 -0.11653095  0.55388715 -2.77010894
  3.06858502  2.18960153  1.1855201  -1.06648308  0.60271514  1.14432449
  1.31029902 -1.87354669  0.7296508 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:37:55 2024]  Iteration number: 0 with current cost as 0.44645188646368383 and parameters 
[-3.21750199  1.28621372 -0.86892148 -0.11653095  0.55388708 -2.77010894
  3.06858498  2.18960149  1.18552002 -1.06648308  0.6027151   1.14432449
  1.31029902 -1.87354673  0.72965077]. 
Working on 1.0 fold... 
[Sun Mar 24 20:39:24 2024]  Iteration number: 0 with current cost as 0.4728526464528593 and parameters 
[-3.32287439  1.25059764 -0.67054068 -0.11653095  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  0.60271514  1.14432449
  1.31029902 -1.87354673  0.7296508 ]. 
Training complete taking 674.9576089382172 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 20:42:29 2024]  Iteration number: 0 with current cost as 0.42134316832544183 and parameters 
[-3.19064797  1.19644309 -0.84184908 -0.11653099  0.55388712 -2.77010897
  3.06858498  2.18960149  1.18552002 -1.06648312  0.60271514  1.14432449
  1.31029902 -1.87354676  0.72965077]. 
Working on 0.4 fold... 
[Sun Mar 24 20:44:06 2024]  Iteration number: 0 with current cost as 0.4843524321368117 and parameters 
[-3.33981057  1.31680264 -0.69514514 -0.11653099  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  0.60271514  1.14432449
  1.31029902 -1.87354673  0.72965077]. 
Working on 0.6 fold... 
[Sun Mar 24 20:47:06 2024]  Iteration number: 0 with current cost as 0.4453203292085197 and parameters 
[-3.23834126  1.23115132 -0.79197665 -0.11653095  0.55388715 -2.77010894
  3.06858502  2.18960153  1.1855201  -1.06648308  0.60271514  1.14432449
  1.31029902 -1.87354669  0.7296508 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:49:05 2024]  Iteration number: 0 with current cost as 0.44645188646368383 and parameters 
[-3.21750199  1.28621372 -0.86892148 -0.11653095  0.55388708 -2.77010894
  3.06858498  2.18960149  1.18552002 -1.06648308  0.6027151   1.14432449
  1.31029902 -1.87354673  0.72965077]. 
Working on 1.0 fold... 
[Sun Mar 24 20:50:34 2024]  Iteration number: 0 with current cost as 0.4728526464528593 and parameters 
[-3.32287439  1.25059764 -0.67054068 -0.11653095  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  0.60271514  1.14432449
  1.31029902 -1.87354673  0.7296508 ]. 
Training complete taking 666.1197509765625 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 20:53:35 2024]  Iteration number: 0 with current cost as 0.42134316832544183 and parameters 
[-3.19064797  1.19644309 -0.84184908 -0.11653099  0.55388712 -2.77010897
  3.06858498  2.18960149  1.18552002 -1.06648312  0.60271514  1.14432449
  1.31029902 -1.87354676  0.72965077]. 
Working on 0.4 fold... 
[Sun Mar 24 20:55:15 2024]  Iteration number: 0 with current cost as 0.4843524321368117 and parameters 
[-3.33981057  1.31680264 -0.69514514 -0.11653099  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  0.60271514  1.14432449
  1.31029902 -1.87354673  0.72965077]. 
Working on 0.6 fold... 
[Sun Mar 24 20:58:14 2024]  Iteration number: 0 with current cost as 0.4453203292085197 and parameters 
[-3.23834126  1.23115132 -0.79197665 -0.11653095  0.55388715 -2.77010894
  3.06858502  2.18960153  1.1855201  -1.06648308  0.60271514  1.14432449
  1.31029902 -1.87354669  0.7296508 ]. 
Working on 0.8 fold... 
[Sun Mar 24 21:00:14 2024]  Iteration number: 0 with current cost as 0.44645188646368383 and parameters 
[-3.21750199  1.28621372 -0.86892148 -0.11653095  0.55388708 -2.77010894
  3.06858498  2.18960149  1.18552002 -1.06648308  0.6027151   1.14432449
  1.31029902 -1.87354673  0.72965077]. 
Working on 1.0 fold... 
[Sun Mar 24 21:01:44 2024]  Iteration number: 0 with current cost as 0.4728526464528593 and parameters 
[-3.32287439  1.25059764 -0.67054068 -0.11653095  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  0.60271514  1.14432449
  1.31029902 -1.87354673  0.7296508 ]. 
Training complete taking 670.3180921077728 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 21:04:45 2024]  Iteration number: 0 with current cost as 0.42134316832544183 and parameters 
[-3.19064797  1.19644309 -0.84184908 -0.11653099  0.55388712 -2.77010897
  3.06858498  2.18960149  1.18552002 -1.06648312  0.60271514  1.14432449
  1.31029902 -1.87354676  0.72965077]. 
Working on 0.4 fold... 
[Sun Mar 24 21:06:25 2024]  Iteration number: 0 with current cost as 0.4843524321368117 and parameters 
[-3.33981057  1.31680264 -0.69514514 -0.11653099  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  0.60271514  1.14432449
  1.31029902 -1.87354673  0.72965077]. 
Working on 0.6 fold... 
[Sun Mar 24 21:09:24 2024]  Iteration number: 0 with current cost as 0.4453203292085197 and parameters 
[-3.23834126  1.23115132 -0.79197665 -0.11653095  0.55388715 -2.77010894
  3.06858502  2.18960153  1.1855201  -1.06648308  0.60271514  1.14432449
  1.31029902 -1.87354669  0.7296508 ]. 
Working on 0.8 fold... 
[Sun Mar 24 21:11:27 2024]  Iteration number: 0 with current cost as 0.44645188646368383 and parameters 
[-3.21750199  1.28621372 -0.86892148 -0.11653095  0.55388708 -2.77010894
  3.06858498  2.18960149  1.18552002 -1.06648308  0.6027151   1.14432449
  1.31029902 -1.87354673  0.72965077]. 
Working on 1.0 fold... 
[Sun Mar 24 21:12:57 2024]  Iteration number: 0 with current cost as 0.4728526464528593 and parameters 
[-3.32287439  1.25059764 -0.67054068 -0.11653095  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  0.60271514  1.14432449
  1.31029902 -1.87354673  0.7296508 ]. 
Training complete taking 670.2972958087921 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 21:15:56 2024]  Iteration number: 0 with current cost as 0.42134316832544183 and parameters 
[-3.19064797  1.19644309 -0.84184908 -0.11653099  0.55388712 -2.77010897
  3.06858498  2.18960149  1.18552002 -1.06648312  0.60271514  1.14432449
  1.31029902 -1.87354676  0.72965077]. 
Working on 0.4 fold... 
[Sun Mar 24 21:17:33 2024]  Iteration number: 0 with current cost as 0.4843524321368117 and parameters 
[-3.33981057  1.31680264 -0.69514514 -0.11653099  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  0.60271514  1.14432449
  1.31029902 -1.87354673  0.72965077]. 
Working on 0.6 fold... 
[Sun Mar 24 21:20:32 2024]  Iteration number: 0 with current cost as 0.4453203292085197 and parameters 
[-3.23834126  1.23115132 -0.79197665 -0.11653095  0.55388715 -2.77010894
  3.06858502  2.18960153  1.1855201  -1.06648308  0.60271514  1.14432449
  1.31029902 -1.87354669  0.7296508 ]. 
Working on 0.8 fold... 
[Sun Mar 24 21:22:32 2024]  Iteration number: 0 with current cost as 0.44645188646368383 and parameters 
[-3.21750199  1.28621372 -0.86892148 -0.11653095  0.55388708 -2.77010894
  3.06858498  2.18960149  1.18552002 -1.06648308  0.6027151   1.14432449
  1.31029902 -1.87354673  0.72965077]. 
Working on 1.0 fold... 
[Sun Mar 24 21:24:02 2024]  Iteration number: 0 with current cost as 0.4728526464528593 and parameters 
[-3.32287439  1.25059764 -0.67054068 -0.11653095  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  0.60271514  1.14432449
  1.31029902 -1.87354673  0.7296508 ]. 
Training complete taking 664.6779079437256 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 21:27:01 2024]  Iteration number: 0 with current cost as 0.42134316832544183 and parameters 
[-3.19064797  1.19644309 -0.84184908 -0.11653099  0.55388712 -2.77010897
  3.06858498  2.18960149  1.18552002 -1.06648312  0.60271514  1.14432449
  1.31029902 -1.87354676  0.72965077]. 
Working on 0.4 fold... 
[Sun Mar 24 21:28:37 2024]  Iteration number: 0 with current cost as 0.4843524321368117 and parameters 
[-3.33981057  1.31680264 -0.69514514 -0.11653099  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  0.60271514  1.14432449
  1.31029902 -1.87354673  0.72965077]. 
Working on 0.6 fold... 
[Sun Mar 24 21:31:37 2024]  Iteration number: 0 with current cost as 0.4453203292085197 and parameters 
[-3.23834126  1.23115132 -0.79197665 -0.11653095  0.55388715 -2.77010894
  3.06858502  2.18960153  1.1855201  -1.06648308  0.60271514  1.14432449
  1.31029902 -1.87354669  0.7296508 ]. 
Working on 0.8 fold... 
[Sun Mar 24 21:33:36 2024]  Iteration number: 0 with current cost as 0.44645188646368383 and parameters 
[-3.21750199  1.28621372 -0.86892148 -0.11653095  0.55388708 -2.77010894
  3.06858498  2.18960149  1.18552002 -1.06648308  0.6027151   1.14432449
  1.31029902 -1.87354673  0.72965077]. 
Working on 1.0 fold... 
[Sun Mar 24 21:35:06 2024]  Iteration number: 0 with current cost as 0.4728526464528593 and parameters 
[-3.32287439  1.25059764 -0.67054068 -0.11653095  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  0.60271514  1.14432449
  1.31029902 -1.87354673  0.7296508 ]. 
Training complete taking 663.6199023723602 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 21:38:04 2024]  Iteration number: 0 with current cost as 0.42134316832544183 and parameters 
[-3.19064797  1.19644309 -0.84184908 -0.11653099  0.55388712 -2.77010897
  3.06858498  2.18960149  1.18552002 -1.06648312  0.60271514  1.14432449
  1.31029902 -1.87354676  0.72965077]. 
Working on 0.4 fold... 
[Sun Mar 24 21:39:41 2024]  Iteration number: 0 with current cost as 0.4843524321368117 and parameters 
[-3.33981057  1.31680264 -0.69514514 -0.11653099  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  0.60271514  1.14432449
  1.31029902 -1.87354673  0.72965077]. 
Working on 0.6 fold... 
[Sun Mar 24 21:42:43 2024]  Iteration number: 0 with current cost as 0.4453203292085197 and parameters 
[-3.23834126  1.23115132 -0.79197665 -0.11653095  0.55388715 -2.77010894
  3.06858502  2.18960153  1.1855201  -1.06648308  0.60271514  1.14432449
  1.31029902 -1.87354669  0.7296508 ]. 
Working on 0.8 fold... 
[Sun Mar 24 21:45:04 2024]  Iteration number: 0 with current cost as 0.44645188646368383 and parameters 
[-3.21750199  1.28621372 -0.86892148 -0.11653095  0.55388708 -2.77010894
  3.06858498  2.18960149  1.18552002 -1.06648308  0.6027151   1.14432449
  1.31029902 -1.87354673  0.72965077]. 
Working on 1.0 fold... 
[Sun Mar 24 21:46:36 2024]  Iteration number: 0 with current cost as 0.4728526464528593 and parameters 
[-3.32287439  1.25059764 -0.67054068 -0.11653095  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  0.60271514  1.14432449
  1.31029902 -1.87354673  0.7296508 ]. 
Training complete taking 692.6955425739288 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 21:49:37 2024]  Iteration number: 0 with current cost as 0.42134316832544183 and parameters 
[-3.19064797  1.19644309 -0.84184908 -0.11653099  0.55388712 -2.77010897
  3.06858498  2.18960149  1.18552002 -1.06648312  0.60271514  1.14432449
  1.31029902 -1.87354676  0.72965077]. 
Working on 0.4 fold... 
[Sun Mar 24 21:51:14 2024]  Iteration number: 0 with current cost as 0.4843524321368117 and parameters 
[-3.33981057  1.31680264 -0.69514514 -0.11653099  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  0.60271514  1.14432449
  1.31029902 -1.87354673  0.72965077]. 
Working on 0.6 fold... 
[Sun Mar 24 21:54:14 2024]  Iteration number: 0 with current cost as 0.4453203292085197 and parameters 
[-3.23834126  1.23115132 -0.79197665 -0.11653095  0.55388715 -2.77010894
  3.06858502  2.18960153  1.1855201  -1.06648308  0.60271514  1.14432449
  1.31029902 -1.87354669  0.7296508 ]. 
Working on 0.8 fold... 
[Sun Mar 24 21:56:13 2024]  Iteration number: 0 with current cost as 0.44645188646368383 and parameters 
[-3.21750199  1.28621372 -0.86892148 -0.11653095  0.55388708 -2.77010894
  3.06858498  2.18960149  1.18552002 -1.06648308  0.6027151   1.14432449
  1.31029902 -1.87354673  0.72965077]. 
Working on 1.0 fold... 
[Sun Mar 24 21:57:43 2024]  Iteration number: 0 with current cost as 0.4728526464528593 and parameters 
[-3.32287439  1.25059764 -0.67054068 -0.11653095  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  0.60271514  1.14432449
  1.31029902 -1.87354673  0.7296508 ]. 
Training complete taking 664.2949981689453 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 22:00:42 2024]  Iteration number: 0 with current cost as 0.42134316832544183 and parameters 
[-3.19064797  1.19644309 -0.84184908 -0.11653099  0.55388712 -2.77010897
  3.06858498  2.18960149  1.18552002 -1.06648312  0.60271514  1.14432449
  1.31029902 -1.87354676  0.72965077]. 
Working on 0.4 fold... 
[Sun Mar 24 22:02:21 2024]  Iteration number: 0 with current cost as 0.4843524321368117 and parameters 
[-3.33981057  1.31680264 -0.69514514 -0.11653099  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  0.60271514  1.14432449
  1.31029902 -1.87354673  0.72965077]. 
Working on 0.6 fold... 
[Sun Mar 24 22:05:19 2024]  Iteration number: 0 with current cost as 0.4453203292085197 and parameters 
[-3.23834126  1.23115132 -0.79197665 -0.11653095  0.55388715 -2.77010894
  3.06858502  2.18960153  1.1855201  -1.06648308  0.60271514  1.14432449
  1.31029902 -1.87354669  0.7296508 ]. 
Working on 0.8 fold... 
[Sun Mar 24 22:07:24 2024]  Iteration number: 0 with current cost as 0.44645188646368383 and parameters 
[-3.21750199  1.28621372 -0.86892148 -0.11653095  0.55388708 -2.77010894
  3.06858498  2.18960149  1.18552002 -1.06648308  0.6027151   1.14432449
  1.31029902 -1.87354673  0.72965077]. 
Working on 1.0 fold... 
[Sun Mar 24 22:08:52 2024]  Iteration number: 0 with current cost as 0.4728526464528593 and parameters 
[-3.32287439  1.25059764 -0.67054068 -0.11653095  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  0.60271514  1.14432449
  1.31029902 -1.87354673  0.7296508 ]. 
Training complete taking 674.3142850399017 seconds. 
Discarding model... 

Training complete taking 16882.25378537178 total seconds. 
Now scoring model... 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Scoring complete taking 1.0674595832824707 seconds. 
Saved predicted values as M-A2-CZ_HWE-CZ_predicted_values.csv
Model scores: {'MSE_train': (85.16597978862421,), 'R2_train': 0.587399410982643, 'MAE_train': 6.411244666454105, 'MSE_test': 80.10125925463224, 'R2_test': 0.5171645152531876, 'MAE_test': 5.925280135556335}. 
Saved model results as M-A2-CZ_HWE-CZ_results.json. 
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/circuits/Encoders.py:12: RuntimeWarning: invalid value encountered in arcsin
  if np.isnan(np.arcsin(features[feature_idx])) or np.isnan(np.arccos(features[feature_idx])):
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Thu Apr  4 11:28:51 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 11:29:13 2024]  Iteration number: 0 with current cost as 0.42134316832544183 and parameters 
[-3.19064797  1.19644309 -0.84184908 -0.11653099  0.55388712 -2.77010897
  3.06858498  2.18960149  1.18552002 -1.06648312  0.60271514  1.14432449
  1.31029902 -1.87354676  0.72965077]. 
Working on 0.4 fold... 
[Thu Apr  4 11:30:57 2024]  Iteration number: 0 with current cost as 0.4843524321368117 and parameters 
[-3.33981057  1.31680264 -0.69514514 -0.11653099  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  0.60271514  1.14432449
  1.31029902 -1.87354673  0.72965077]. 
Working on 0.6 fold... 
[Thu Apr  4 11:34:00 2024]  Iteration number: 0 with current cost as 0.4453203292085197 and parameters 
[-3.23834126  1.23115132 -0.79197665 -0.11653095  0.55388715 -2.77010894
  3.06858502  2.18960153  1.1855201  -1.06648308  0.60271514  1.14432449
  1.31029902 -1.87354669  0.7296508 ]. 
Working on 0.8 fold... 
[Thu Apr  4 11:36:00 2024]  Iteration number: 0 with current cost as 0.44645188646368383 and parameters 
[-3.21750199  1.28621372 -0.86892148 -0.11653095  0.55388708 -2.77010894
  3.06858498  2.18960149  1.18552002 -1.06648308  0.6027151   1.14432449
  1.31029902 -1.87354673  0.72965077]. 
Working on 1.0 fold... 
[Thu Apr  4 11:37:30 2024]  Iteration number: 0 with current cost as 0.4728526464528593 and parameters 
[-3.32287439  1.25059764 -0.67054068 -0.11653095  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  0.60271514  1.14432449
  1.31029902 -1.87354673  0.7296508 ]. 
Training complete taking 676.6579821109772 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 11:40:31 2024]  Iteration number: 0 with current cost as 0.42134316832544183 and parameters 
[-3.19064797  1.19644309 -0.84184908 -0.11653099  0.55388712 -2.77010897
  3.06858498  2.18960149  1.18552002 -1.06648312  0.60271514  1.14432449
  1.31029902 -1.87354676  0.72965077]. 
Working on 0.4 fold... 
[Thu Apr  4 11:42:08 2024]  Iteration number: 0 with current cost as 0.4843524321368117 and parameters 
[-3.33981057  1.31680264 -0.69514514 -0.11653099  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  0.60271514  1.14432449
  1.31029902 -1.87354673  0.72965077]. 
Working on 0.6 fold... 
[Thu Apr  4 11:45:08 2024]  Iteration number: 0 with current cost as 0.4453203292085197 and parameters 
[-3.23834126  1.23115132 -0.79197665 -0.11653095  0.55388715 -2.77010894
  3.06858502  2.18960153  1.1855201  -1.06648308  0.60271514  1.14432449
  1.31029902 -1.87354669  0.7296508 ]. 
Working on 0.8 fold... 
[Thu Apr  4 11:47:09 2024]  Iteration number: 0 with current cost as 0.44645188646368383 and parameters 
[-3.21750199  1.28621372 -0.86892148 -0.11653095  0.55388708 -2.77010894
  3.06858498  2.18960149  1.18552002 -1.06648308  0.6027151   1.14432449
  1.31029902 -1.87354673  0.72965077]. 
Working on 1.0 fold... 
[Thu Apr  4 11:48:39 2024]  Iteration number: 0 with current cost as 0.4728526464528593 and parameters 
[-3.32287439  1.25059764 -0.67054068 -0.11653095  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  0.60271514  1.14432449
  1.31029902 -1.87354673  0.7296508 ]. 
Training complete taking 669.6111664772034 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 11:51:40 2024]  Iteration number: 0 with current cost as 0.42134316832544183 and parameters 
[-3.19064797  1.19644309 -0.84184908 -0.11653099  0.55388712 -2.77010897
  3.06858498  2.18960149  1.18552002 -1.06648312  0.60271514  1.14432449
  1.31029902 -1.87354676  0.72965077]. 
Working on 0.4 fold... 
[Thu Apr  4 11:53:20 2024]  Iteration number: 0 with current cost as 0.4843524321368117 and parameters 
[-3.33981057  1.31680264 -0.69514514 -0.11653099  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  0.60271514  1.14432449
  1.31029902 -1.87354673  0.72965077]. 
Working on 0.6 fold... 
[Thu Apr  4 11:56:21 2024]  Iteration number: 0 with current cost as 0.4453203292085197 and parameters 
[-3.23834126  1.23115132 -0.79197665 -0.11653095  0.55388715 -2.77010894
  3.06858502  2.18960153  1.1855201  -1.06648308  0.60271514  1.14432449
  1.31029902 -1.87354669  0.7296508 ]. 
Working on 0.8 fold... 
[Thu Apr  4 11:58:21 2024]  Iteration number: 0 with current cost as 0.44645188646368383 and parameters 
[-3.21750199  1.28621372 -0.86892148 -0.11653095  0.55388708 -2.77010894
  3.06858498  2.18960149  1.18552002 -1.06648308  0.6027151   1.14432449
  1.31029902 -1.87354673  0.72965077]. 
Working on 1.0 fold... 
[Thu Apr  4 11:59:51 2024]  Iteration number: 0 with current cost as 0.4728526464528593 and parameters 
[-3.32287439  1.25059764 -0.67054068 -0.11653095  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  0.60271514  1.14432449
  1.31029902 -1.87354673  0.7296508 ]. 
Training complete taking 671.4840197563171 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 12:02:52 2024]  Iteration number: 0 with current cost as 0.42134316832544183 and parameters 
[-3.19064797  1.19644309 -0.84184908 -0.11653099  0.55388712 -2.77010897
  3.06858498  2.18960149  1.18552002 -1.06648312  0.60271514  1.14432449
  1.31029902 -1.87354676  0.72965077]. 
Working on 0.4 fold... 
[Thu Apr  4 12:04:30 2024]  Iteration number: 0 with current cost as 0.4843524321368117 and parameters 
[-3.33981057  1.31680264 -0.69514514 -0.11653099  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  0.60271514  1.14432449
  1.31029902 -1.87354673  0.72965077]. 
Working on 0.6 fold... 
[Thu Apr  4 12:07:31 2024]  Iteration number: 0 with current cost as 0.4453203292085197 and parameters 
[-3.23834126  1.23115132 -0.79197665 -0.11653095  0.55388715 -2.77010894
  3.06858502  2.18960153  1.1855201  -1.06648308  0.60271514  1.14432449
  1.31029902 -1.87354669  0.7296508 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:09:46 2024]  Iteration number: 0 with current cost as 0.44645188646368383 and parameters 
[-3.21750199  1.28621372 -0.86892148 -0.11653095  0.55388708 -2.77010894
  3.06858498  2.18960149  1.18552002 -1.06648308  0.6027151   1.14432449
  1.31029902 -1.87354673  0.72965077]. 
Working on 1.0 fold... 
[Thu Apr  4 12:11:16 2024]  Iteration number: 0 with current cost as 0.4728526464528593 and parameters 
[-3.32287439  1.25059764 -0.67054068 -0.11653095  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  0.60271514  1.14432449
  1.31029902 -1.87354673  0.7296508 ]. 
Training complete taking 691.7564861774445 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 12:14:24 2024]  Iteration number: 0 with current cost as 0.42134316832544183 and parameters 
[-3.19064797  1.19644309 -0.84184908 -0.11653099  0.55388712 -2.77010897
  3.06858498  2.18960149  1.18552002 -1.06648312  0.60271514  1.14432449
  1.31029902 -1.87354676  0.72965077]. 
Working on 0.4 fold... 
[Thu Apr  4 12:16:02 2024]  Iteration number: 0 with current cost as 0.4843524321368117 and parameters 
[-3.33981057  1.31680264 -0.69514514 -0.11653099  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  0.60271514  1.14432449
  1.31029902 -1.87354673  0.72965077]. 
Working on 0.6 fold... 
[Thu Apr  4 12:19:03 2024]  Iteration number: 0 with current cost as 0.4453203292085197 and parameters 
[-3.23834126  1.23115132 -0.79197665 -0.11653095  0.55388715 -2.77010894
  3.06858502  2.18960153  1.1855201  -1.06648308  0.60271514  1.14432449
  1.31029902 -1.87354669  0.7296508 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:21:03 2024]  Iteration number: 0 with current cost as 0.44645188646368383 and parameters 
[-3.21750199  1.28621372 -0.86892148 -0.11653095  0.55388708 -2.77010894
  3.06858498  2.18960149  1.18552002 -1.06648308  0.6027151   1.14432449
  1.31029902 -1.87354673  0.72965077]. 
Working on 1.0 fold... 
[Thu Apr  4 12:22:35 2024]  Iteration number: 0 with current cost as 0.4728526464528593 and parameters 
[-3.32287439  1.25059764 -0.67054068 -0.11653095  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  0.60271514  1.14432449
  1.31029902 -1.87354673  0.7296508 ]. 
Training complete taking 675.1843490600586 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 12:25:39 2024]  Iteration number: 0 with current cost as 0.42134316832544183 and parameters 
[-3.19064797  1.19644309 -0.84184908 -0.11653099  0.55388712 -2.77010897
  3.06858498  2.18960149  1.18552002 -1.06648312  0.60271514  1.14432449
  1.31029902 -1.87354676  0.72965077]. 
Working on 0.4 fold... 
[Thu Apr  4 12:27:16 2024]  Iteration number: 0 with current cost as 0.4843524321368117 and parameters 
[-3.33981057  1.31680264 -0.69514514 -0.11653099  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  0.60271514  1.14432449
  1.31029902 -1.87354673  0.72965077]. 
Working on 0.6 fold... 
[Thu Apr  4 12:30:27 2024]  Iteration number: 0 with current cost as 0.4453203292085197 and parameters 
[-3.23834126  1.23115132 -0.79197665 -0.11653095  0.55388715 -2.77010894
  3.06858502  2.18960153  1.1855201  -1.06648308  0.60271514  1.14432449
  1.31029902 -1.87354669  0.7296508 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:32:28 2024]  Iteration number: 0 with current cost as 0.44645188646368383 and parameters 
[-3.21750199  1.28621372 -0.86892148 -0.11653095  0.55388708 -2.77010894
  3.06858498  2.18960149  1.18552002 -1.06648308  0.6027151   1.14432449
  1.31029902 -1.87354673  0.72965077]. 
Working on 1.0 fold... 
[Thu Apr  4 12:33:59 2024]  Iteration number: 0 with current cost as 0.4728526464528593 and parameters 
[-3.32287439  1.25059764 -0.67054068 -0.11653095  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  0.60271514  1.14432449
  1.31029902 -1.87354673  0.7296508 ]. 
Training complete taking 680.5324964523315 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 12:36:59 2024]  Iteration number: 0 with current cost as 0.42134316832544183 and parameters 
[-3.19064797  1.19644309 -0.84184908 -0.11653099  0.55388712 -2.77010897
  3.06858498  2.18960149  1.18552002 -1.06648312  0.60271514  1.14432449
  1.31029902 -1.87354676  0.72965077]. 
Working on 0.4 fold... 
[Thu Apr  4 12:38:38 2024]  Iteration number: 0 with current cost as 0.4843524321368117 and parameters 
[-3.33981057  1.31680264 -0.69514514 -0.11653099  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  0.60271514  1.14432449
  1.31029902 -1.87354673  0.72965077]. 
Working on 0.6 fold... 
[Thu Apr  4 12:41:41 2024]  Iteration number: 0 with current cost as 0.4453203292085197 and parameters 
[-3.23834126  1.23115132 -0.79197665 -0.11653095  0.55388715 -2.77010894
  3.06858502  2.18960153  1.1855201  -1.06648308  0.60271514  1.14432449
  1.31029902 -1.87354669  0.7296508 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:43:41 2024]  Iteration number: 0 with current cost as 0.44645188646368383 and parameters 
[-3.21750199  1.28621372 -0.86892148 -0.11653095  0.55388708 -2.77010894
  3.06858498  2.18960149  1.18552002 -1.06648308  0.6027151   1.14432449
  1.31029902 -1.87354673  0.72965077]. 
Working on 1.0 fold... 
[Thu Apr  4 12:45:11 2024]  Iteration number: 0 with current cost as 0.4728526464528593 and parameters 
[-3.32287439  1.25059764 -0.67054068 -0.11653095  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  0.60271514  1.14432449
  1.31029902 -1.87354673  0.7296508 ]. 
Training complete taking 672.1079411506653 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 12:48:12 2024]  Iteration number: 0 with current cost as 0.42134316832544183 and parameters 
[-3.19064797  1.19644309 -0.84184908 -0.11653099  0.55388712 -2.77010897
  3.06858498  2.18960149  1.18552002 -1.06648312  0.60271514  1.14432449
  1.31029902 -1.87354676  0.72965077]. 
Working on 0.4 fold... 
[Thu Apr  4 12:49:49 2024]  Iteration number: 0 with current cost as 0.4843524321368117 and parameters 
[-3.33981057  1.31680264 -0.69514514 -0.11653099  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  0.60271514  1.14432449
  1.31029902 -1.87354673  0.72965077]. 
Working on 0.6 fold... 
[Thu Apr  4 12:52:56 2024]  Iteration number: 0 with current cost as 0.4453203292085197 and parameters 
[-3.23834126  1.23115132 -0.79197665 -0.11653095  0.55388715 -2.77010894
  3.06858502  2.18960153  1.1855201  -1.06648308  0.60271514  1.14432449
  1.31029902 -1.87354669  0.7296508 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:54:57 2024]  Iteration number: 0 with current cost as 0.44645188646368383 and parameters 
[-3.21750199  1.28621372 -0.86892148 -0.11653095  0.55388708 -2.77010894
  3.06858498  2.18960149  1.18552002 -1.06648308  0.6027151   1.14432449
  1.31029902 -1.87354673  0.72965077]. 
Working on 1.0 fold... 
[Thu Apr  4 12:56:31 2024]  Iteration number: 0 with current cost as 0.4728526464528593 and parameters 
[-3.32287439  1.25059764 -0.67054068 -0.11653095  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  0.60271514  1.14432449
  1.31029902 -1.87354673  0.7296508 ]. 
Training complete taking 680.8816108703613 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 12:59:32 2024]  Iteration number: 0 with current cost as 0.42134316832544183 and parameters 
[-3.19064797  1.19644309 -0.84184908 -0.11653099  0.55388712 -2.77010897
  3.06858498  2.18960149  1.18552002 -1.06648312  0.60271514  1.14432449
  1.31029902 -1.87354676  0.72965077]. 
Working on 0.4 fold... 
[Thu Apr  4 13:01:10 2024]  Iteration number: 0 with current cost as 0.4843524321368117 and parameters 
[-3.33981057  1.31680264 -0.69514514 -0.11653099  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  0.60271514  1.14432449
  1.31029902 -1.87354673  0.72965077]. 
Working on 0.6 fold... 
[Thu Apr  4 13:04:14 2024]  Iteration number: 0 with current cost as 0.4453203292085197 and parameters 
[-3.23834126  1.23115132 -0.79197665 -0.11653095  0.55388715 -2.77010894
  3.06858502  2.18960153  1.1855201  -1.06648308  0.60271514  1.14432449
  1.31029902 -1.87354669  0.7296508 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:06:26 2024]  Iteration number: 0 with current cost as 0.44645188646368383 and parameters 
[-3.21750199  1.28621372 -0.86892148 -0.11653095  0.55388708 -2.77010894
  3.06858498  2.18960149  1.18552002 -1.06648308  0.6027151   1.14432449
  1.31029902 -1.87354673  0.72965077]. 
Working on 1.0 fold... 
[Thu Apr  4 13:07:55 2024]  Iteration number: 0 with current cost as 0.4728526464528593 and parameters 
[-3.32287439  1.25059764 -0.67054068 -0.11653095  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  0.60271514  1.14432449
  1.31029902 -1.87354673  0.7296508 ]. 
Training complete taking 685.8411238193512 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 13:10:58 2024]  Iteration number: 0 with current cost as 0.42134316832544183 and parameters 
[-3.19064797  1.19644309 -0.84184908 -0.11653099  0.55388712 -2.77010897
  3.06858498  2.18960149  1.18552002 -1.06648312  0.60271514  1.14432449
  1.31029902 -1.87354676  0.72965077]. 
Working on 0.4 fold... 
[Thu Apr  4 13:12:36 2024]  Iteration number: 0 with current cost as 0.4843524321368117 and parameters 
[-3.33981057  1.31680264 -0.69514514 -0.11653099  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  0.60271514  1.14432449
  1.31029902 -1.87354673  0.72965077]. 
Working on 0.6 fold... 
[Thu Apr  4 13:15:35 2024]  Iteration number: 0 with current cost as 0.4453203292085197 and parameters 
[-3.23834126  1.23115132 -0.79197665 -0.11653095  0.55388715 -2.77010894
  3.06858502  2.18960153  1.1855201  -1.06648308  0.60271514  1.14432449
  1.31029902 -1.87354669  0.7296508 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:17:35 2024]  Iteration number: 0 with current cost as 0.44645188646368383 and parameters 
[-3.21750199  1.28621372 -0.86892148 -0.11653095  0.55388708 -2.77010894
  3.06858498  2.18960149  1.18552002 -1.06648308  0.6027151   1.14432449
  1.31029902 -1.87354673  0.72965077]. 
Working on 1.0 fold... 
[Thu Apr  4 13:19:06 2024]  Iteration number: 0 with current cost as 0.4728526464528593 and parameters 
[-3.32287439  1.25059764 -0.67054068 -0.11653095  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  0.60271514  1.14432449
  1.31029902 -1.87354673  0.7296508 ]. 
Training complete taking 666.6788439750671 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 13:22:05 2024]  Iteration number: 0 with current cost as 0.42134316832544183 and parameters 
[-3.19064797  1.19644309 -0.84184908 -0.11653099  0.55388712 -2.77010897
  3.06858498  2.18960149  1.18552002 -1.06648312  0.60271514  1.14432449
  1.31029902 -1.87354676  0.72965077]. 
Working on 0.4 fold... 
[Thu Apr  4 13:23:43 2024]  Iteration number: 0 with current cost as 0.4843524321368117 and parameters 
[-3.33981057  1.31680264 -0.69514514 -0.11653099  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  0.60271514  1.14432449
  1.31029902 -1.87354673  0.72965077]. 
Working on 0.6 fold... 
[Thu Apr  4 13:26:55 2024]  Iteration number: 0 with current cost as 0.4453203292085197 and parameters 
[-3.23834126  1.23115132 -0.79197665 -0.11653095  0.55388715 -2.77010894
  3.06858502  2.18960153  1.1855201  -1.06648308  0.60271514  1.14432449
  1.31029902 -1.87354669  0.7296508 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:28:52 2024]  Iteration number: 0 with current cost as 0.44645188646368383 and parameters 
[-3.21750199  1.28621372 -0.86892148 -0.11653095  0.55388708 -2.77010894
  3.06858498  2.18960149  1.18552002 -1.06648308  0.6027151   1.14432449
  1.31029902 -1.87354673  0.72965077]. 
Working on 1.0 fold... 
[Thu Apr  4 13:30:24 2024]  Iteration number: 0 with current cost as 0.4728526464528593 and parameters 
[-3.32287439  1.25059764 -0.67054068 -0.11653095  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  0.60271514  1.14432449
  1.31029902 -1.87354673  0.7296508 ]. 
Training complete taking 683.2009143829346 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 13:33:28 2024]  Iteration number: 0 with current cost as 0.42134316832544183 and parameters 
[-3.19064797  1.19644309 -0.84184908 -0.11653099  0.55388712 -2.77010897
  3.06858498  2.18960149  1.18552002 -1.06648312  0.60271514  1.14432449
  1.31029902 -1.87354676  0.72965077]. 
Working on 0.4 fold... 
[Thu Apr  4 13:35:06 2024]  Iteration number: 0 with current cost as 0.4843524321368117 and parameters 
[-3.33981057  1.31680264 -0.69514514 -0.11653099  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  0.60271514  1.14432449
  1.31029902 -1.87354673  0.72965077]. 
Working on 0.6 fold... 
[Thu Apr  4 13:38:07 2024]  Iteration number: 0 with current cost as 0.4453203292085197 and parameters 
[-3.23834126  1.23115132 -0.79197665 -0.11653095  0.55388715 -2.77010894
  3.06858502  2.18960153  1.1855201  -1.06648308  0.60271514  1.14432449
  1.31029902 -1.87354669  0.7296508 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:40:08 2024]  Iteration number: 0 with current cost as 0.44645188646368383 and parameters 
[-3.21750199  1.28621372 -0.86892148 -0.11653095  0.55388708 -2.77010894
  3.06858498  2.18960149  1.18552002 -1.06648308  0.6027151   1.14432449
  1.31029902 -1.87354673  0.72965077]. 
Working on 1.0 fold... 
[Thu Apr  4 13:41:39 2024]  Iteration number: 0 with current cost as 0.4728526464528593 and parameters 
[-3.32287439  1.25059764 -0.67054068 -0.11653095  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  0.60271514  1.14432449
  1.31029902 -1.87354673  0.7296508 ]. 
Training complete taking 670.6240386962891 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 13:44:39 2024]  Iteration number: 0 with current cost as 0.42134316832544183 and parameters 
[-3.19064797  1.19644309 -0.84184908 -0.11653099  0.55388712 -2.77010897
  3.06858498  2.18960149  1.18552002 -1.06648312  0.60271514  1.14432449
  1.31029902 -1.87354676  0.72965077]. 
Working on 0.4 fold... 
[Thu Apr  4 13:46:20 2024]  Iteration number: 0 with current cost as 0.4843524321368117 and parameters 
[-3.33981057  1.31680264 -0.69514514 -0.11653099  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  0.60271514  1.14432449
  1.31029902 -1.87354673  0.72965077]. 
Working on 0.6 fold... 
[Thu Apr  4 13:49:32 2024]  Iteration number: 0 with current cost as 0.4453203292085197 and parameters 
[-3.23834126  1.23115132 -0.79197665 -0.11653095  0.55388715 -2.77010894
  3.06858502  2.18960153  1.1855201  -1.06648308  0.60271514  1.14432449
  1.31029902 -1.87354669  0.7296508 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:51:40 2024]  Iteration number: 0 with current cost as 0.44645188646368383 and parameters 
[-3.21750199  1.28621372 -0.86892148 -0.11653095  0.55388708 -2.77010894
  3.06858498  2.18960149  1.18552002 -1.06648308  0.6027151   1.14432449
  1.31029902 -1.87354673  0.72965077]. 
Working on 1.0 fold... 
[Thu Apr  4 13:53:11 2024]  Iteration number: 0 with current cost as 0.4728526464528593 and parameters 
[-3.32287439  1.25059764 -0.67054068 -0.11653095  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  0.60271514  1.14432449
  1.31029902 -1.87354673  0.7296508 ]. 
Training complete taking 694.6446177959442 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 13:56:13 2024]  Iteration number: 0 with current cost as 0.42134316832544183 and parameters 
[-3.19064797  1.19644309 -0.84184908 -0.11653099  0.55388712 -2.77010897
  3.06858498  2.18960149  1.18552002 -1.06648312  0.60271514  1.14432449
  1.31029902 -1.87354676  0.72965077]. 
Working on 0.4 fold... 
[Thu Apr  4 13:57:49 2024]  Iteration number: 0 with current cost as 0.4843524321368117 and parameters 
[-3.33981057  1.31680264 -0.69514514 -0.11653099  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  0.60271514  1.14432449
  1.31029902 -1.87354673  0.72965077]. 
Working on 0.6 fold... 
[Thu Apr  4 14:00:51 2024]  Iteration number: 0 with current cost as 0.4453203292085197 and parameters 
[-3.23834126  1.23115132 -0.79197665 -0.11653095  0.55388715 -2.77010894
  3.06858502  2.18960153  1.1855201  -1.06648308  0.60271514  1.14432449
  1.31029902 -1.87354669  0.7296508 ]. 
Working on 0.8 fold... 
[Thu Apr  4 14:02:52 2024]  Iteration number: 0 with current cost as 0.44645188646368383 and parameters 
[-3.21750199  1.28621372 -0.86892148 -0.11653095  0.55388708 -2.77010894
  3.06858498  2.18960149  1.18552002 -1.06648308  0.6027151   1.14432449
  1.31029902 -1.87354673  0.72965077]. 
Working on 1.0 fold... 
[Thu Apr  4 14:04:23 2024]  Iteration number: 0 with current cost as 0.4728526464528593 and parameters 
[-3.32287439  1.25059764 -0.67054068 -0.11653095  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  0.60271514  1.14432449
  1.31029902 -1.87354673  0.7296508 ]. 
Training complete taking 669.7779593467712 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 14:07:22 2024]  Iteration number: 0 with current cost as 0.42134316832544183 and parameters 
[-3.19064797  1.19644309 -0.84184908 -0.11653099  0.55388712 -2.77010897
  3.06858498  2.18960149  1.18552002 -1.06648312  0.60271514  1.14432449
  1.31029902 -1.87354676  0.72965077]. 
Working on 0.4 fold... 
[Thu Apr  4 14:09:04 2024]  Iteration number: 0 with current cost as 0.4843524321368117 and parameters 
[-3.33981057  1.31680264 -0.69514514 -0.11653099  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  0.60271514  1.14432449
  1.31029902 -1.87354673  0.72965077]. 
Working on 0.6 fold... 
[Thu Apr  4 14:12:05 2024]  Iteration number: 0 with current cost as 0.4453203292085197 and parameters 
[-3.23834126  1.23115132 -0.79197665 -0.11653095  0.55388715 -2.77010894
  3.06858502  2.18960153  1.1855201  -1.06648308  0.60271514  1.14432449
  1.31029902 -1.87354669  0.7296508 ]. 
Working on 0.8 fold... 
[Thu Apr  4 14:14:16 2024]  Iteration number: 0 with current cost as 0.44645188646368383 and parameters 
[-3.21750199  1.28621372 -0.86892148 -0.11653095  0.55388708 -2.77010894
  3.06858498  2.18960149  1.18552002 -1.06648308  0.6027151   1.14432449
  1.31029902 -1.87354673  0.72965077]. 
Working on 1.0 fold... 
[Thu Apr  4 14:15:47 2024]  Iteration number: 0 with current cost as 0.4728526464528593 and parameters 
[-3.32287439  1.25059764 -0.67054068 -0.11653095  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  0.60271514  1.14432449
  1.31029902 -1.87354673  0.7296508 ]. 
Training complete taking 681.9049577713013 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 14:18:45 2024]  Iteration number: 0 with current cost as 0.42134316832544183 and parameters 
[-3.19064797  1.19644309 -0.84184908 -0.11653099  0.55388712 -2.77010897
  3.06858498  2.18960149  1.18552002 -1.06648312  0.60271514  1.14432449
  1.31029902 -1.87354676  0.72965077]. 
Working on 0.4 fold... 
[Thu Apr  4 14:20:21 2024]  Iteration number: 0 with current cost as 0.4843524321368117 and parameters 
[-3.33981057  1.31680264 -0.69514514 -0.11653099  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  0.60271514  1.14432449
  1.31029902 -1.87354673  0.72965077]. 
Working on 0.6 fold... 
[Thu Apr  4 14:23:27 2024]  Iteration number: 0 with current cost as 0.4453203292085197 and parameters 
[-3.23834126  1.23115132 -0.79197665 -0.11653095  0.55388715 -2.77010894
  3.06858502  2.18960153  1.1855201  -1.06648308  0.60271514  1.14432449
  1.31029902 -1.87354669  0.7296508 ]. 
Working on 0.8 fold... 
[Thu Apr  4 14:25:25 2024]  Iteration number: 0 with current cost as 0.44645188646368383 and parameters 
[-3.21750199  1.28621372 -0.86892148 -0.11653095  0.55388708 -2.77010894
  3.06858498  2.18960149  1.18552002 -1.06648308  0.6027151   1.14432449
  1.31029902 -1.87354673  0.72965077]. 
Working on 1.0 fold... 
[Thu Apr  4 14:26:55 2024]  Iteration number: 0 with current cost as 0.4728526464528593 and parameters 
[-3.32287439  1.25059764 -0.67054068 -0.11653095  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  0.60271514  1.14432449
  1.31029902 -1.87354673  0.7296508 ]. 
Training complete taking 684.1865050792694 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 14:30:19 2024]  Iteration number: 0 with current cost as 0.42134316832544183 and parameters 
[-3.19064797  1.19644309 -0.84184908 -0.11653099  0.55388712 -2.77010897
  3.06858498  2.18960149  1.18552002 -1.06648312  0.60271514  1.14432449
  1.31029902 -1.87354676  0.72965077]. 
Working on 0.4 fold... 
[Thu Apr  4 14:32:00 2024]  Iteration number: 0 with current cost as 0.4843524321368117 and parameters 
[-3.33981057  1.31680264 -0.69514514 -0.11653099  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  0.60271514  1.14432449
  1.31029902 -1.87354673  0.72965077]. 
Working on 0.6 fold... 
[Thu Apr  4 14:35:00 2024]  Iteration number: 0 with current cost as 0.4453203292085197 and parameters 
[-3.23834126  1.23115132 -0.79197665 -0.11653095  0.55388715 -2.77010894
  3.06858502  2.18960153  1.1855201  -1.06648308  0.60271514  1.14432449
  1.31029902 -1.87354669  0.7296508 ]. 
Working on 0.8 fold... 
[Thu Apr  4 14:36:58 2024]  Iteration number: 0 with current cost as 0.44645188646368383 and parameters 
[-3.21750199  1.28621372 -0.86892148 -0.11653095  0.55388708 -2.77010894
  3.06858498  2.18960149  1.18552002 -1.06648308  0.6027151   1.14432449
  1.31029902 -1.87354673  0.72965077]. 
Working on 1.0 fold... 
[Thu Apr  4 14:38:27 2024]  Iteration number: 0 with current cost as 0.4728526464528593 and parameters 
[-3.32287439  1.25059764 -0.67054068 -0.11653095  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  0.60271514  1.14432449
  1.31029902 -1.87354673  0.7296508 ]. 
Training complete taking 683.9431874752045 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 14:41:33 2024]  Iteration number: 0 with current cost as 0.42134316832544183 and parameters 
[-3.19064797  1.19644309 -0.84184908 -0.11653099  0.55388712 -2.77010897
  3.06858498  2.18960149  1.18552002 -1.06648312  0.60271514  1.14432449
  1.31029902 -1.87354676  0.72965077]. 
Working on 0.4 fold... 
[Thu Apr  4 14:43:10 2024]  Iteration number: 0 with current cost as 0.4843524321368117 and parameters 
[-3.33981057  1.31680264 -0.69514514 -0.11653099  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  0.60271514  1.14432449
  1.31029902 -1.87354673  0.72965077]. 
Working on 0.6 fold... 
[Thu Apr  4 14:46:09 2024]  Iteration number: 0 with current cost as 0.4453203292085197 and parameters 
[-3.23834126  1.23115132 -0.79197665 -0.11653095  0.55388715 -2.77010894
  3.06858502  2.18960153  1.1855201  -1.06648308  0.60271514  1.14432449
  1.31029902 -1.87354669  0.7296508 ]. 
Working on 0.8 fold... 
[Thu Apr  4 14:48:34 2024]  Iteration number: 0 with current cost as 0.44645188646368383 and parameters 
[-3.21750199  1.28621372 -0.86892148 -0.11653095  0.55388708 -2.77010894
  3.06858498  2.18960149  1.18552002 -1.06648308  0.6027151   1.14432449
  1.31029902 -1.87354673  0.72965077]. 
Working on 1.0 fold... 
[Thu Apr  4 14:50:10 2024]  Iteration number: 0 with current cost as 0.4728526464528593 and parameters 
[-3.32287439  1.25059764 -0.67054068 -0.11653095  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  0.60271514  1.14432449
  1.31029902 -1.87354673  0.7296508 ]. 
Training complete taking 695.9627621173859 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 14:53:09 2024]  Iteration number: 0 with current cost as 0.42134316832544183 and parameters 
[-3.19064797  1.19644309 -0.84184908 -0.11653099  0.55388712 -2.77010897
  3.06858498  2.18960149  1.18552002 -1.06648312  0.60271514  1.14432449
  1.31029902 -1.87354676  0.72965077]. 
Working on 0.4 fold... 
[Thu Apr  4 14:54:46 2024]  Iteration number: 0 with current cost as 0.4843524321368117 and parameters 
[-3.33981057  1.31680264 -0.69514514 -0.11653099  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  0.60271514  1.14432449
  1.31029902 -1.87354673  0.72965077]. 
Working on 0.6 fold... 
[Thu Apr  4 14:57:55 2024]  Iteration number: 0 with current cost as 0.4453203292085197 and parameters 
[-3.23834126  1.23115132 -0.79197665 -0.11653095  0.55388715 -2.77010894
  3.06858502  2.18960153  1.1855201  -1.06648308  0.60271514  1.14432449
  1.31029902 -1.87354669  0.7296508 ]. 
Working on 0.8 fold... 
[Thu Apr  4 14:59:54 2024]  Iteration number: 0 with current cost as 0.44645188646368383 and parameters 
[-3.21750199  1.28621372 -0.86892148 -0.11653095  0.55388708 -2.77010894
  3.06858498  2.18960149  1.18552002 -1.06648308  0.6027151   1.14432449
  1.31029902 -1.87354673  0.72965077]. 
Working on 1.0 fold... 
[Thu Apr  4 15:01:25 2024]  Iteration number: 0 with current cost as 0.4728526464528593 and parameters 
[-3.32287439  1.25059764 -0.67054068 -0.11653095  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  0.60271514  1.14432449
  1.31029902 -1.87354673  0.7296508 ]. 
Training complete taking 678.7055385112762 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 15:04:26 2024]  Iteration number: 0 with current cost as 0.42134316832544183 and parameters 
[-3.19064797  1.19644309 -0.84184908 -0.11653099  0.55388712 -2.77010897
  3.06858498  2.18960149  1.18552002 -1.06648312  0.60271514  1.14432449
  1.31029902 -1.87354676  0.72965077]. 
Working on 0.4 fold... 
[Thu Apr  4 15:06:05 2024]  Iteration number: 0 with current cost as 0.4843524321368117 and parameters 
[-3.33981057  1.31680264 -0.69514514 -0.11653099  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  0.60271514  1.14432449
  1.31029902 -1.87354673  0.72965077]. 
Working on 0.6 fold... 
[Thu Apr  4 15:09:08 2024]  Iteration number: 0 with current cost as 0.4453203292085197 and parameters 
[-3.23834126  1.23115132 -0.79197665 -0.11653095  0.55388715 -2.77010894
  3.06858502  2.18960153  1.1855201  -1.06648308  0.60271514  1.14432449
  1.31029902 -1.87354669  0.7296508 ]. 
Working on 0.8 fold... 
[Thu Apr  4 15:11:07 2024]  Iteration number: 0 with current cost as 0.44645188646368383 and parameters 
[-3.21750199  1.28621372 -0.86892148 -0.11653095  0.55388708 -2.77010894
  3.06858498  2.18960149  1.18552002 -1.06648308  0.6027151   1.14432449
  1.31029902 -1.87354673  0.72965077]. 
Working on 1.0 fold... 
[Thu Apr  4 15:12:38 2024]  Iteration number: 0 with current cost as 0.4728526464528593 and parameters 
[-3.32287439  1.25059764 -0.67054068 -0.11653095  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  0.60271514  1.14432449
  1.31029902 -1.87354673  0.7296508 ]. 
Training complete taking 669.7667014598846 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 15:15:37 2024]  Iteration number: 0 with current cost as 0.42134316832544183 and parameters 
[-3.19064797  1.19644309 -0.84184908 -0.11653099  0.55388712 -2.77010897
  3.06858498  2.18960149  1.18552002 -1.06648312  0.60271514  1.14432449
  1.31029902 -1.87354676  0.72965077]. 
Working on 0.4 fold... 
[Thu Apr  4 15:17:17 2024]  Iteration number: 0 with current cost as 0.4843524321368117 and parameters 
[-3.33981057  1.31680264 -0.69514514 -0.11653099  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  0.60271514  1.14432449
  1.31029902 -1.87354673  0.72965077]. 
Working on 0.6 fold... 
[Thu Apr  4 15:20:16 2024]  Iteration number: 0 with current cost as 0.4453203292085197 and parameters 
[-3.23834126  1.23115132 -0.79197665 -0.11653095  0.55388715 -2.77010894
  3.06858502  2.18960153  1.1855201  -1.06648308  0.60271514  1.14432449
  1.31029902 -1.87354669  0.7296508 ]. 
Working on 0.8 fold... 
[Thu Apr  4 15:22:16 2024]  Iteration number: 0 with current cost as 0.44645188646368383 and parameters 
[-3.21750199  1.28621372 -0.86892148 -0.11653095  0.55388708 -2.77010894
  3.06858498  2.18960149  1.18552002 -1.06648308  0.6027151   1.14432449
  1.31029902 -1.87354673  0.72965077]. 
Working on 1.0 fold... 
[Thu Apr  4 15:23:47 2024]  Iteration number: 0 with current cost as 0.4728526464528593 and parameters 
[-3.32287439  1.25059764 -0.67054068 -0.11653095  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  0.60271514  1.14432449
  1.31029902 -1.87354673  0.7296508 ]. 
Training complete taking 668.3590762615204 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 15:26:46 2024]  Iteration number: 0 with current cost as 0.42134316832544183 and parameters 
[-3.19064797  1.19644309 -0.84184908 -0.11653099  0.55388712 -2.77010897
  3.06858498  2.18960149  1.18552002 -1.06648312  0.60271514  1.14432449
  1.31029902 -1.87354676  0.72965077]. 
Working on 0.4 fold... 
[Thu Apr  4 15:28:24 2024]  Iteration number: 0 with current cost as 0.4843524321368117 and parameters 
[-3.33981057  1.31680264 -0.69514514 -0.11653099  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  0.60271514  1.14432449
  1.31029902 -1.87354673  0.72965077]. 
Working on 0.6 fold... 
[Thu Apr  4 15:31:23 2024]  Iteration number: 0 with current cost as 0.4453203292085197 and parameters 
[-3.23834126  1.23115132 -0.79197665 -0.11653095  0.55388715 -2.77010894
  3.06858502  2.18960153  1.1855201  -1.06648308  0.60271514  1.14432449
  1.31029902 -1.87354669  0.7296508 ]. 
Working on 0.8 fold... 
[Thu Apr  4 15:33:24 2024]  Iteration number: 0 with current cost as 0.44645188646368383 and parameters 
[-3.21750199  1.28621372 -0.86892148 -0.11653095  0.55388708 -2.77010894
  3.06858498  2.18960149  1.18552002 -1.06648308  0.6027151   1.14432449
  1.31029902 -1.87354673  0.72965077]. 
Working on 1.0 fold... 
[Thu Apr  4 15:34:57 2024]  Iteration number: 0 with current cost as 0.4728526464528593 and parameters 
[-3.32287439  1.25059764 -0.67054068 -0.11653095  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  0.60271514  1.14432449
  1.31029902 -1.87354673  0.7296508 ]. 
Training complete taking 679.3510239124298 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 15:38:05 2024]  Iteration number: 0 with current cost as 0.42134316832544183 and parameters 
[-3.19064797  1.19644309 -0.84184908 -0.11653099  0.55388712 -2.77010897
  3.06858498  2.18960149  1.18552002 -1.06648312  0.60271514  1.14432449
  1.31029902 -1.87354676  0.72965077]. 
Working on 0.4 fold... 
[Thu Apr  4 15:39:41 2024]  Iteration number: 0 with current cost as 0.4843524321368117 and parameters 
[-3.33981057  1.31680264 -0.69514514 -0.11653099  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  0.60271514  1.14432449
  1.31029902 -1.87354673  0.72965077]. 
Working on 0.6 fold... 
[Thu Apr  4 15:42:41 2024]  Iteration number: 0 with current cost as 0.4453203292085197 and parameters 
[-3.23834126  1.23115132 -0.79197665 -0.11653095  0.55388715 -2.77010894
  3.06858502  2.18960153  1.1855201  -1.06648308  0.60271514  1.14432449
  1.31029902 -1.87354669  0.7296508 ]. 
Working on 0.8 fold... 
[Thu Apr  4 15:44:41 2024]  Iteration number: 0 with current cost as 0.44645188646368383 and parameters 
[-3.21750199  1.28621372 -0.86892148 -0.11653095  0.55388708 -2.77010894
  3.06858498  2.18960149  1.18552002 -1.06648308  0.6027151   1.14432449
  1.31029902 -1.87354673  0.72965077]. 
Working on 1.0 fold... 
[Thu Apr  4 15:46:10 2024]  Iteration number: 0 with current cost as 0.4728526464528593 and parameters 
[-3.32287439  1.25059764 -0.67054068 -0.11653095  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  0.60271514  1.14432449
  1.31029902 -1.87354673  0.7296508 ]. 
Training complete taking 663.2714264392853 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 15:49:09 2024]  Iteration number: 0 with current cost as 0.42134316832544183 and parameters 
[-3.19064797  1.19644309 -0.84184908 -0.11653099  0.55388712 -2.77010897
  3.06858498  2.18960149  1.18552002 -1.06648312  0.60271514  1.14432449
  1.31029902 -1.87354676  0.72965077]. 
Working on 0.4 fold... 
[Thu Apr  4 15:50:45 2024]  Iteration number: 0 with current cost as 0.4843524321368117 and parameters 
[-3.33981057  1.31680264 -0.69514514 -0.11653099  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  0.60271514  1.14432449
  1.31029902 -1.87354673  0.72965077]. 
Working on 0.6 fold... 
[Thu Apr  4 15:53:45 2024]  Iteration number: 0 with current cost as 0.4453203292085197 and parameters 
[-3.23834126  1.23115132 -0.79197665 -0.11653095  0.55388715 -2.77010894
  3.06858502  2.18960153  1.1855201  -1.06648308  0.60271514  1.14432449
  1.31029902 -1.87354669  0.7296508 ]. 
Working on 0.8 fold... 
[Thu Apr  4 15:55:44 2024]  Iteration number: 0 with current cost as 0.44645188646368383 and parameters 
[-3.21750199  1.28621372 -0.86892148 -0.11653095  0.55388708 -2.77010894
  3.06858498  2.18960149  1.18552002 -1.06648308  0.6027151   1.14432449
  1.31029902 -1.87354673  0.72965077]. 
Working on 1.0 fold... 
[Thu Apr  4 15:57:14 2024]  Iteration number: 0 with current cost as 0.4728526464528593 and parameters 
[-3.32287439  1.25059764 -0.67054068 -0.11653095  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  0.60271514  1.14432449
  1.31029902 -1.87354673  0.7296508 ]. 
Training complete taking 663.981048822403 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 16:00:12 2024]  Iteration number: 0 with current cost as 0.42134316832544183 and parameters 
[-3.19064797  1.19644309 -0.84184908 -0.11653099  0.55388712 -2.77010897
  3.06858498  2.18960149  1.18552002 -1.06648312  0.60271514  1.14432449
  1.31029902 -1.87354676  0.72965077]. 
Working on 0.4 fold... 
[Thu Apr  4 16:01:49 2024]  Iteration number: 0 with current cost as 0.4843524321368117 and parameters 
[-3.33981057  1.31680264 -0.69514514 -0.11653099  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  0.60271514  1.14432449
  1.31029902 -1.87354673  0.72965077]. 
Working on 0.6 fold... 
[Thu Apr  4 16:04:48 2024]  Iteration number: 0 with current cost as 0.4453203292085197 and parameters 
[-3.23834126  1.23115132 -0.79197665 -0.11653095  0.55388715 -2.77010894
  3.06858502  2.18960153  1.1855201  -1.06648308  0.60271514  1.14432449
  1.31029902 -1.87354669  0.7296508 ]. 
Working on 0.8 fold... 
[Thu Apr  4 16:06:48 2024]  Iteration number: 0 with current cost as 0.44645188646368383 and parameters 
[-3.21750199  1.28621372 -0.86892148 -0.11653095  0.55388708 -2.77010894
  3.06858498  2.18960149  1.18552002 -1.06648308  0.6027151   1.14432449
  1.31029902 -1.87354673  0.72965077]. 
Working on 1.0 fold... 
[Thu Apr  4 16:08:17 2024]  Iteration number: 0 with current cost as 0.4728526464528593 and parameters 
[-3.32287439  1.25059764 -0.67054068 -0.11653095  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  0.60271514  1.14432449
  1.31029902 -1.87354673  0.7296508 ]. 
Training complete taking 664.0565044879913 seconds. 
Discarding model... 

Training complete taking 16922.47413110733 total seconds. 
Now scoring model... 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Scoring complete taking 1.1042745113372803 seconds. 
Saved predicted values as M-A2-CZ_HWE-CZ_predicted_values.csv
Model scores: {'MSE_train': (85.16597978862421,), 'R2_train': 0.587399410982643, 'MAE_train': 6.411244666454105, 'MSE_test': 80.10125925463224, 'R2_test': 0.5171645152531876, 'MAE_test': 5.925280135556335}. 
Saved model results as M-A2-CZ_HWE-CZ_results.json. 
