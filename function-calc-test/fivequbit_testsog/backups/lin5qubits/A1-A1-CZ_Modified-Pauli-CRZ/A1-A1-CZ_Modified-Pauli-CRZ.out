/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Sun Mar 24 17:30:13 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 17:30:52 2024]  Iteration number: 0 with current cost as 0.3700387255470995 and parameters 
[-1.82176683  2.23743458 -2.12427953 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960151  1.18552004 -1.06648308  0.6027151   1.14432445
  1.31029904 -1.87354675  0.72965075  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 17:32:13 2024]  Iteration number: 0 with current cost as 0.3122912106762491 and parameters 
[-1.54439865  2.23743459 -2.12427959 -0.11653103  0.55388713 -2.77010902
  3.06858493  2.18960145  1.18552003 -1.06648313  0.60271515  1.1443244
  1.31029903 -1.87354675  0.72965075  2.88578424 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 17:33:10 2024]  Iteration number: 0 with current cost as 0.3315072595772406 and parameters 
[-1.43602935  2.23743458 -2.12427952 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.87354674  0.72965069  2.88578419 -0.54534341 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 17:34:21 2024]  Iteration number: 0 with current cost as 0.33672967186451364 and parameters 
[-1.41170488  2.23743464 -2.12427952 -0.11653097  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271516  1.14432445
  1.31029904 -1.87354674  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 17:35:32 2024]  Iteration number: 0 with current cost as 0.33971857161021435 and parameters 
[-1.40907062  2.23743464 -2.12427958 -0.11653103  0.55388702 -2.77010909
  3.06858498  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432439
  1.31029899 -1.87354674  0.72965074  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 352.72865104675293 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 17:36:45 2024]  Iteration number: 0 with current cost as 0.3700387255470995 and parameters 
[-1.82176683  2.23743458 -2.12427953 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960151  1.18552004 -1.06648308  0.6027151   1.14432445
  1.31029904 -1.87354675  0.72965075  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 17:38:10 2024]  Iteration number: 0 with current cost as 0.3122912106762491 and parameters 
[-1.54439865  2.23743459 -2.12427959 -0.11653103  0.55388713 -2.77010902
  3.06858493  2.18960145  1.18552003 -1.06648313  0.60271515  1.1443244
  1.31029903 -1.87354675  0.72965075  2.88578424 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 17:39:06 2024]  Iteration number: 0 with current cost as 0.3315072595772406 and parameters 
[-1.43602935  2.23743458 -2.12427952 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.87354674  0.72965069  2.88578419 -0.54534341 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 17:40:17 2024]  Iteration number: 0 with current cost as 0.33672967186451364 and parameters 
[-1.41170488  2.23743464 -2.12427952 -0.11653097  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271516  1.14432445
  1.31029904 -1.87354674  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 17:41:28 2024]  Iteration number: 0 with current cost as 0.33971857161021435 and parameters 
[-1.40907062  2.23743464 -2.12427958 -0.11653103  0.55388702 -2.77010909
  3.06858498  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432439
  1.31029899 -1.87354674  0.72965074  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 354.7460153102875 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 17:42:41 2024]  Iteration number: 0 with current cost as 0.3700387255470995 and parameters 
[-1.82176683  2.23743458 -2.12427953 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960151  1.18552004 -1.06648308  0.6027151   1.14432445
  1.31029904 -1.87354675  0.72965075  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 17:44:00 2024]  Iteration number: 0 with current cost as 0.3122912106762491 and parameters 
[-1.54439865  2.23743459 -2.12427959 -0.11653103  0.55388713 -2.77010902
  3.06858493  2.18960145  1.18552003 -1.06648313  0.60271515  1.1443244
  1.31029903 -1.87354675  0.72965075  2.88578424 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 17:44:56 2024]  Iteration number: 0 with current cost as 0.3315072595772406 and parameters 
[-1.43602935  2.23743458 -2.12427952 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.87354674  0.72965069  2.88578419 -0.54534341 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 17:46:09 2024]  Iteration number: 0 with current cost as 0.33672967186451364 and parameters 
[-1.41170488  2.23743464 -2.12427952 -0.11653097  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271516  1.14432445
  1.31029904 -1.87354674  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 17:47:20 2024]  Iteration number: 0 with current cost as 0.33971857161021435 and parameters 
[-1.40907062  2.23743464 -2.12427958 -0.11653103  0.55388702 -2.77010909
  3.06858498  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432439
  1.31029899 -1.87354674  0.72965074  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 351.9039659500122 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 17:48:32 2024]  Iteration number: 0 with current cost as 0.3700387255470995 and parameters 
[-1.82176683  2.23743458 -2.12427953 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960151  1.18552004 -1.06648308  0.6027151   1.14432445
  1.31029904 -1.87354675  0.72965075  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 17:49:52 2024]  Iteration number: 0 with current cost as 0.3122912106762491 and parameters 
[-1.54439865  2.23743459 -2.12427959 -0.11653103  0.55388713 -2.77010902
  3.06858493  2.18960145  1.18552003 -1.06648313  0.60271515  1.1443244
  1.31029903 -1.87354675  0.72965075  2.88578424 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 17:50:48 2024]  Iteration number: 0 with current cost as 0.3315072595772406 and parameters 
[-1.43602935  2.23743458 -2.12427952 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.87354674  0.72965069  2.88578419 -0.54534341 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 17:52:02 2024]  Iteration number: 0 with current cost as 0.33672967186451364 and parameters 
[-1.41170488  2.23743464 -2.12427952 -0.11653097  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271516  1.14432445
  1.31029904 -1.87354674  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 17:53:15 2024]  Iteration number: 0 with current cost as 0.33971857161021435 and parameters 
[-1.40907062  2.23743464 -2.12427958 -0.11653103  0.55388702 -2.77010909
  3.06858498  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432439
  1.31029899 -1.87354674  0.72965074  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 354.7590534687042 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 17:54:26 2024]  Iteration number: 0 with current cost as 0.3700387255470995 and parameters 
[-1.82176683  2.23743458 -2.12427953 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960151  1.18552004 -1.06648308  0.6027151   1.14432445
  1.31029904 -1.87354675  0.72965075  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 17:55:46 2024]  Iteration number: 0 with current cost as 0.3122912106762491 and parameters 
[-1.54439865  2.23743459 -2.12427959 -0.11653103  0.55388713 -2.77010902
  3.06858493  2.18960145  1.18552003 -1.06648313  0.60271515  1.1443244
  1.31029903 -1.87354675  0.72965075  2.88578424 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 17:56:43 2024]  Iteration number: 0 with current cost as 0.3315072595772406 and parameters 
[-1.43602935  2.23743458 -2.12427952 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.87354674  0.72965069  2.88578419 -0.54534341 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 17:57:56 2024]  Iteration number: 0 with current cost as 0.33672967186451364 and parameters 
[-1.41170488  2.23743464 -2.12427952 -0.11653097  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271516  1.14432445
  1.31029904 -1.87354674  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 17:59:09 2024]  Iteration number: 0 with current cost as 0.33971857161021435 and parameters 
[-1.40907062  2.23743464 -2.12427958 -0.11653103  0.55388702 -2.77010909
  3.06858498  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432439
  1.31029899 -1.87354674  0.72965074  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 354.3056342601776 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 18:00:20 2024]  Iteration number: 0 with current cost as 0.3700387255470995 and parameters 
[-1.82176683  2.23743458 -2.12427953 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960151  1.18552004 -1.06648308  0.6027151   1.14432445
  1.31029904 -1.87354675  0.72965075  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:01:41 2024]  Iteration number: 0 with current cost as 0.3122912106762491 and parameters 
[-1.54439865  2.23743459 -2.12427959 -0.11653103  0.55388713 -2.77010902
  3.06858493  2.18960145  1.18552003 -1.06648313  0.60271515  1.1443244
  1.31029903 -1.87354675  0.72965075  2.88578424 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:02:38 2024]  Iteration number: 0 with current cost as 0.3315072595772406 and parameters 
[-1.43602935  2.23743458 -2.12427952 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.87354674  0.72965069  2.88578419 -0.54534341 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:03:49 2024]  Iteration number: 0 with current cost as 0.33672967186451364 and parameters 
[-1.41170488  2.23743464 -2.12427952 -0.11653097  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271516  1.14432445
  1.31029904 -1.87354674  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:05:01 2024]  Iteration number: 0 with current cost as 0.33971857161021435 and parameters 
[-1.40907062  2.23743464 -2.12427958 -0.11653103  0.55388702 -2.77010909
  3.06858498  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432439
  1.31029899 -1.87354674  0.72965074  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 351.91362380981445 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 18:06:16 2024]  Iteration number: 0 with current cost as 0.3700387255470995 and parameters 
[-1.82176683  2.23743458 -2.12427953 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960151  1.18552004 -1.06648308  0.6027151   1.14432445
  1.31029904 -1.87354675  0.72965075  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:07:36 2024]  Iteration number: 0 with current cost as 0.3122912106762491 and parameters 
[-1.54439865  2.23743459 -2.12427959 -0.11653103  0.55388713 -2.77010902
  3.06858493  2.18960145  1.18552003 -1.06648313  0.60271515  1.1443244
  1.31029903 -1.87354675  0.72965075  2.88578424 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:08:31 2024]  Iteration number: 0 with current cost as 0.3315072595772406 and parameters 
[-1.43602935  2.23743458 -2.12427952 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.87354674  0.72965069  2.88578419 -0.54534341 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:09:45 2024]  Iteration number: 0 with current cost as 0.33672967186451364 and parameters 
[-1.41170488  2.23743464 -2.12427952 -0.11653097  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271516  1.14432445
  1.31029904 -1.87354674  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:10:57 2024]  Iteration number: 0 with current cost as 0.33971857161021435 and parameters 
[-1.40907062  2.23743464 -2.12427958 -0.11653103  0.55388702 -2.77010909
  3.06858498  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432439
  1.31029899 -1.87354674  0.72965074  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 355.85923767089844 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 18:12:09 2024]  Iteration number: 0 with current cost as 0.3700387255470995 and parameters 
[-1.82176683  2.23743458 -2.12427953 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960151  1.18552004 -1.06648308  0.6027151   1.14432445
  1.31029904 -1.87354675  0.72965075  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:13:30 2024]  Iteration number: 0 with current cost as 0.3122912106762491 and parameters 
[-1.54439865  2.23743459 -2.12427959 -0.11653103  0.55388713 -2.77010902
  3.06858493  2.18960145  1.18552003 -1.06648313  0.60271515  1.1443244
  1.31029903 -1.87354675  0.72965075  2.88578424 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:14:26 2024]  Iteration number: 0 with current cost as 0.3315072595772406 and parameters 
[-1.43602935  2.23743458 -2.12427952 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.87354674  0.72965069  2.88578419 -0.54534341 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:15:39 2024]  Iteration number: 0 with current cost as 0.33672967186451364 and parameters 
[-1.41170488  2.23743464 -2.12427952 -0.11653097  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271516  1.14432445
  1.31029904 -1.87354674  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:16:51 2024]  Iteration number: 0 with current cost as 0.33971857161021435 and parameters 
[-1.40907062  2.23743464 -2.12427958 -0.11653103  0.55388702 -2.77010909
  3.06858498  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432439
  1.31029899 -1.87354674  0.72965074  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 354.5564181804657 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 18:18:03 2024]  Iteration number: 0 with current cost as 0.3700387255470995 and parameters 
[-1.82176683  2.23743458 -2.12427953 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960151  1.18552004 -1.06648308  0.6027151   1.14432445
  1.31029904 -1.87354675  0.72965075  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:19:28 2024]  Iteration number: 0 with current cost as 0.3122912106762491 and parameters 
[-1.54439865  2.23743459 -2.12427959 -0.11653103  0.55388713 -2.77010902
  3.06858493  2.18960145  1.18552003 -1.06648313  0.60271515  1.1443244
  1.31029903 -1.87354675  0.72965075  2.88578424 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:20:23 2024]  Iteration number: 0 with current cost as 0.3315072595772406 and parameters 
[-1.43602935  2.23743458 -2.12427952 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.87354674  0.72965069  2.88578419 -0.54534341 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:21:39 2024]  Iteration number: 0 with current cost as 0.33672967186451364 and parameters 
[-1.41170488  2.23743464 -2.12427952 -0.11653097  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271516  1.14432445
  1.31029904 -1.87354674  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:22:52 2024]  Iteration number: 0 with current cost as 0.33971857161021435 and parameters 
[-1.40907062  2.23743464 -2.12427958 -0.11653103  0.55388702 -2.77010909
  3.06858498  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432439
  1.31029899 -1.87354674  0.72965074  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 360.90182614326477 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 18:24:05 2024]  Iteration number: 0 with current cost as 0.3700387255470995 and parameters 
[-1.82176683  2.23743458 -2.12427953 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960151  1.18552004 -1.06648308  0.6027151   1.14432445
  1.31029904 -1.87354675  0.72965075  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:25:25 2024]  Iteration number: 0 with current cost as 0.3122912106762491 and parameters 
[-1.54439865  2.23743459 -2.12427959 -0.11653103  0.55388713 -2.77010902
  3.06858493  2.18960145  1.18552003 -1.06648313  0.60271515  1.1443244
  1.31029903 -1.87354675  0.72965075  2.88578424 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:26:22 2024]  Iteration number: 0 with current cost as 0.3315072595772406 and parameters 
[-1.43602935  2.23743458 -2.12427952 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.87354674  0.72965069  2.88578419 -0.54534341 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:27:34 2024]  Iteration number: 0 with current cost as 0.33672967186451364 and parameters 
[-1.41170488  2.23743464 -2.12427952 -0.11653097  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271516  1.14432445
  1.31029904 -1.87354674  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:28:46 2024]  Iteration number: 0 with current cost as 0.33971857161021435 and parameters 
[-1.40907062  2.23743464 -2.12427958 -0.11653103  0.55388702 -2.77010909
  3.06858498  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432439
  1.31029899 -1.87354674  0.72965074  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 354.03939867019653 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 18:29:58 2024]  Iteration number: 0 with current cost as 0.3700387255470995 and parameters 
[-1.82176683  2.23743458 -2.12427953 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960151  1.18552004 -1.06648308  0.6027151   1.14432445
  1.31029904 -1.87354675  0.72965075  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:31:19 2024]  Iteration number: 0 with current cost as 0.3122912106762491 and parameters 
[-1.54439865  2.23743459 -2.12427959 -0.11653103  0.55388713 -2.77010902
  3.06858493  2.18960145  1.18552003 -1.06648313  0.60271515  1.1443244
  1.31029903 -1.87354675  0.72965075  2.88578424 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:32:15 2024]  Iteration number: 0 with current cost as 0.3315072595772406 and parameters 
[-1.43602935  2.23743458 -2.12427952 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.87354674  0.72965069  2.88578419 -0.54534341 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:33:26 2024]  Iteration number: 0 with current cost as 0.33672967186451364 and parameters 
[-1.41170488  2.23743464 -2.12427952 -0.11653097  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271516  1.14432445
  1.31029904 -1.87354674  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:34:41 2024]  Iteration number: 0 with current cost as 0.33971857161021435 and parameters 
[-1.40907062  2.23743464 -2.12427958 -0.11653103  0.55388702 -2.77010909
  3.06858498  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432439
  1.31029899 -1.87354674  0.72965074  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 354.5845470428467 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 18:35:52 2024]  Iteration number: 0 with current cost as 0.3700387255470995 and parameters 
[-1.82176683  2.23743458 -2.12427953 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960151  1.18552004 -1.06648308  0.6027151   1.14432445
  1.31029904 -1.87354675  0.72965075  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:37:13 2024]  Iteration number: 0 with current cost as 0.3122912106762491 and parameters 
[-1.54439865  2.23743459 -2.12427959 -0.11653103  0.55388713 -2.77010902
  3.06858493  2.18960145  1.18552003 -1.06648313  0.60271515  1.1443244
  1.31029903 -1.87354675  0.72965075  2.88578424 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:38:11 2024]  Iteration number: 0 with current cost as 0.3315072595772406 and parameters 
[-1.43602935  2.23743458 -2.12427952 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.87354674  0.72965069  2.88578419 -0.54534341 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:39:23 2024]  Iteration number: 0 with current cost as 0.33672967186451364 and parameters 
[-1.41170488  2.23743464 -2.12427952 -0.11653097  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271516  1.14432445
  1.31029904 -1.87354674  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:40:34 2024]  Iteration number: 0 with current cost as 0.33971857161021435 and parameters 
[-1.40907062  2.23743464 -2.12427958 -0.11653103  0.55388702 -2.77010909
  3.06858498  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432439
  1.31029899 -1.87354674  0.72965074  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 352.7283058166504 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 18:41:47 2024]  Iteration number: 0 with current cost as 0.3700387255470995 and parameters 
[-1.82176683  2.23743458 -2.12427953 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960151  1.18552004 -1.06648308  0.6027151   1.14432445
  1.31029904 -1.87354675  0.72965075  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:43:07 2024]  Iteration number: 0 with current cost as 0.3122912106762491 and parameters 
[-1.54439865  2.23743459 -2.12427959 -0.11653103  0.55388713 -2.77010902
  3.06858493  2.18960145  1.18552003 -1.06648313  0.60271515  1.1443244
  1.31029903 -1.87354675  0.72965075  2.88578424 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:44:04 2024]  Iteration number: 0 with current cost as 0.3315072595772406 and parameters 
[-1.43602935  2.23743458 -2.12427952 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.87354674  0.72965069  2.88578419 -0.54534341 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:45:19 2024]  Iteration number: 0 with current cost as 0.33672967186451364 and parameters 
[-1.41170488  2.23743464 -2.12427952 -0.11653097  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271516  1.14432445
  1.31029904 -1.87354674  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:46:32 2024]  Iteration number: 0 with current cost as 0.33971857161021435 and parameters 
[-1.40907062  2.23743464 -2.12427958 -0.11653103  0.55388702 -2.77010909
  3.06858498  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432439
  1.31029899 -1.87354674  0.72965074  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 358.0167324542999 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 18:47:45 2024]  Iteration number: 0 with current cost as 0.3700387255470995 and parameters 
[-1.82176683  2.23743458 -2.12427953 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960151  1.18552004 -1.06648308  0.6027151   1.14432445
  1.31029904 -1.87354675  0.72965075  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:49:04 2024]  Iteration number: 0 with current cost as 0.3122912106762491 and parameters 
[-1.54439865  2.23743459 -2.12427959 -0.11653103  0.55388713 -2.77010902
  3.06858493  2.18960145  1.18552003 -1.06648313  0.60271515  1.1443244
  1.31029903 -1.87354675  0.72965075  2.88578424 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:50:02 2024]  Iteration number: 0 with current cost as 0.3315072595772406 and parameters 
[-1.43602935  2.23743458 -2.12427952 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.87354674  0.72965069  2.88578419 -0.54534341 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:51:15 2024]  Iteration number: 0 with current cost as 0.33672967186451364 and parameters 
[-1.41170488  2.23743464 -2.12427952 -0.11653097  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271516  1.14432445
  1.31029904 -1.87354674  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:52:28 2024]  Iteration number: 0 with current cost as 0.33971857161021435 and parameters 
[-1.40907062  2.23743464 -2.12427958 -0.11653103  0.55388702 -2.77010909
  3.06858498  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432439
  1.31029899 -1.87354674  0.72965074  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 355.8863408565521 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 18:53:41 2024]  Iteration number: 0 with current cost as 0.3700387255470995 and parameters 
[-1.82176683  2.23743458 -2.12427953 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960151  1.18552004 -1.06648308  0.6027151   1.14432445
  1.31029904 -1.87354675  0.72965075  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:55:07 2024]  Iteration number: 0 with current cost as 0.3122912106762491 and parameters 
[-1.54439865  2.23743459 -2.12427959 -0.11653103  0.55388713 -2.77010902
  3.06858493  2.18960145  1.18552003 -1.06648313  0.60271515  1.1443244
  1.31029903 -1.87354675  0.72965075  2.88578424 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:56:03 2024]  Iteration number: 0 with current cost as 0.3315072595772406 and parameters 
[-1.43602935  2.23743458 -2.12427952 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.87354674  0.72965069  2.88578419 -0.54534341 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:57:17 2024]  Iteration number: 0 with current cost as 0.33672967186451364 and parameters 
[-1.41170488  2.23743464 -2.12427952 -0.11653097  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271516  1.14432445
  1.31029904 -1.87354674  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:58:29 2024]  Iteration number: 0 with current cost as 0.33971857161021435 and parameters 
[-1.40907062  2.23743464 -2.12427958 -0.11653103  0.55388702 -2.77010909
  3.06858498  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432439
  1.31029899 -1.87354674  0.72965074  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 361.19529914855957 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 18:59:43 2024]  Iteration number: 0 with current cost as 0.3700387255470995 and parameters 
[-1.82176683  2.23743458 -2.12427953 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960151  1.18552004 -1.06648308  0.6027151   1.14432445
  1.31029904 -1.87354675  0.72965075  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:01:02 2024]  Iteration number: 0 with current cost as 0.3122912106762491 and parameters 
[-1.54439865  2.23743459 -2.12427959 -0.11653103  0.55388713 -2.77010902
  3.06858493  2.18960145  1.18552003 -1.06648313  0.60271515  1.1443244
  1.31029903 -1.87354675  0.72965075  2.88578424 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:01:59 2024]  Iteration number: 0 with current cost as 0.3315072595772406 and parameters 
[-1.43602935  2.23743458 -2.12427952 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.87354674  0.72965069  2.88578419 -0.54534341 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:03:13 2024]  Iteration number: 0 with current cost as 0.33672967186451364 and parameters 
[-1.41170488  2.23743464 -2.12427952 -0.11653097  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271516  1.14432445
  1.31029904 -1.87354674  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:04:24 2024]  Iteration number: 0 with current cost as 0.33971857161021435 and parameters 
[-1.40907062  2.23743464 -2.12427958 -0.11653103  0.55388702 -2.77010909
  3.06858498  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432439
  1.31029899 -1.87354674  0.72965074  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 355.6200556755066 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 19:05:36 2024]  Iteration number: 0 with current cost as 0.3700387255470995 and parameters 
[-1.82176683  2.23743458 -2.12427953 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960151  1.18552004 -1.06648308  0.6027151   1.14432445
  1.31029904 -1.87354675  0.72965075  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:06:59 2024]  Iteration number: 0 with current cost as 0.3122912106762491 and parameters 
[-1.54439865  2.23743459 -2.12427959 -0.11653103  0.55388713 -2.77010902
  3.06858493  2.18960145  1.18552003 -1.06648313  0.60271515  1.1443244
  1.31029903 -1.87354675  0.72965075  2.88578424 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:08:05 2024]  Iteration number: 0 with current cost as 0.3315072595772406 and parameters 
[-1.43602935  2.23743458 -2.12427952 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.87354674  0.72965069  2.88578419 -0.54534341 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:09:25 2024]  Iteration number: 0 with current cost as 0.33672967186451364 and parameters 
[-1.41170488  2.23743464 -2.12427952 -0.11653097  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271516  1.14432445
  1.31029904 -1.87354674  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:10:36 2024]  Iteration number: 0 with current cost as 0.33971857161021435 and parameters 
[-1.40907062  2.23743464 -2.12427958 -0.11653103  0.55388702 -2.77010909
  3.06858498  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432439
  1.31029899 -1.87354674  0.72965074  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 373.7437307834625 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 19:11:50 2024]  Iteration number: 0 with current cost as 0.3700387255470995 and parameters 
[-1.82176683  2.23743458 -2.12427953 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960151  1.18552004 -1.06648308  0.6027151   1.14432445
  1.31029904 -1.87354675  0.72965075  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:13:12 2024]  Iteration number: 0 with current cost as 0.3122912106762491 and parameters 
[-1.54439865  2.23743459 -2.12427959 -0.11653103  0.55388713 -2.77010902
  3.06858493  2.18960145  1.18552003 -1.06648313  0.60271515  1.1443244
  1.31029903 -1.87354675  0.72965075  2.88578424 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:14:19 2024]  Iteration number: 0 with current cost as 0.3315072595772406 and parameters 
[-1.43602935  2.23743458 -2.12427952 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.87354674  0.72965069  2.88578419 -0.54534341 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:15:30 2024]  Iteration number: 0 with current cost as 0.33672967186451364 and parameters 
[-1.41170488  2.23743464 -2.12427952 -0.11653097  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271516  1.14432445
  1.31029904 -1.87354674  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:16:48 2024]  Iteration number: 0 with current cost as 0.33971857161021435 and parameters 
[-1.40907062  2.23743464 -2.12427958 -0.11653103  0.55388702 -2.77010909
  3.06858498  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432439
  1.31029899 -1.87354674  0.72965074  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 369.84809160232544 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 19:18:01 2024]  Iteration number: 0 with current cost as 0.3700387255470995 and parameters 
[-1.82176683  2.23743458 -2.12427953 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960151  1.18552004 -1.06648308  0.6027151   1.14432445
  1.31029904 -1.87354675  0.72965075  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:19:32 2024]  Iteration number: 0 with current cost as 0.3122912106762491 and parameters 
[-1.54439865  2.23743459 -2.12427959 -0.11653103  0.55388713 -2.77010902
  3.06858493  2.18960145  1.18552003 -1.06648313  0.60271515  1.1443244
  1.31029903 -1.87354675  0.72965075  2.88578424 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:20:33 2024]  Iteration number: 0 with current cost as 0.3315072595772406 and parameters 
[-1.43602935  2.23743458 -2.12427952 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.87354674  0.72965069  2.88578419 -0.54534341 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:21:48 2024]  Iteration number: 0 with current cost as 0.33672967186451364 and parameters 
[-1.41170488  2.23743464 -2.12427952 -0.11653097  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271516  1.14432445
  1.31029904 -1.87354674  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:23:00 2024]  Iteration number: 0 with current cost as 0.33971857161021435 and parameters 
[-1.40907062  2.23743464 -2.12427958 -0.11653103  0.55388702 -2.77010909
  3.06858498  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432439
  1.31029899 -1.87354674  0.72965074  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 372.81181263923645 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 19:24:26 2024]  Iteration number: 0 with current cost as 0.3700387255470995 and parameters 
[-1.82176683  2.23743458 -2.12427953 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960151  1.18552004 -1.06648308  0.6027151   1.14432445
  1.31029904 -1.87354675  0.72965075  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:25:46 2024]  Iteration number: 0 with current cost as 0.3122912106762491 and parameters 
[-1.54439865  2.23743459 -2.12427959 -0.11653103  0.55388713 -2.77010902
  3.06858493  2.18960145  1.18552003 -1.06648313  0.60271515  1.1443244
  1.31029903 -1.87354675  0.72965075  2.88578424 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:26:42 2024]  Iteration number: 0 with current cost as 0.3315072595772406 and parameters 
[-1.43602935  2.23743458 -2.12427952 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.87354674  0.72965069  2.88578419 -0.54534341 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:27:53 2024]  Iteration number: 0 with current cost as 0.33672967186451364 and parameters 
[-1.41170488  2.23743464 -2.12427952 -0.11653097  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271516  1.14432445
  1.31029904 -1.87354674  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:29:03 2024]  Iteration number: 0 with current cost as 0.33971857161021435 and parameters 
[-1.40907062  2.23743464 -2.12427958 -0.11653103  0.55388702 -2.77010909
  3.06858498  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432439
  1.31029899 -1.87354674  0.72965074  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 361.56711316108704 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 19:30:18 2024]  Iteration number: 0 with current cost as 0.3700387255470995 and parameters 
[-1.82176683  2.23743458 -2.12427953 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960151  1.18552004 -1.06648308  0.6027151   1.14432445
  1.31029904 -1.87354675  0.72965075  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:31:37 2024]  Iteration number: 0 with current cost as 0.3122912106762491 and parameters 
[-1.54439865  2.23743459 -2.12427959 -0.11653103  0.55388713 -2.77010902
  3.06858493  2.18960145  1.18552003 -1.06648313  0.60271515  1.1443244
  1.31029903 -1.87354675  0.72965075  2.88578424 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:32:33 2024]  Iteration number: 0 with current cost as 0.3315072595772406 and parameters 
[-1.43602935  2.23743458 -2.12427952 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.87354674  0.72965069  2.88578419 -0.54534341 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:33:46 2024]  Iteration number: 0 with current cost as 0.33672967186451364 and parameters 
[-1.41170488  2.23743464 -2.12427952 -0.11653097  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271516  1.14432445
  1.31029904 -1.87354674  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:35:01 2024]  Iteration number: 0 with current cost as 0.33971857161021435 and parameters 
[-1.40907062  2.23743464 -2.12427958 -0.11653103  0.55388702 -2.77010909
  3.06858498  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432439
  1.31029899 -1.87354674  0.72965074  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 358.04457330703735 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 19:36:13 2024]  Iteration number: 0 with current cost as 0.3700387255470995 and parameters 
[-1.82176683  2.23743458 -2.12427953 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960151  1.18552004 -1.06648308  0.6027151   1.14432445
  1.31029904 -1.87354675  0.72965075  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:37:32 2024]  Iteration number: 0 with current cost as 0.3122912106762491 and parameters 
[-1.54439865  2.23743459 -2.12427959 -0.11653103  0.55388713 -2.77010902
  3.06858493  2.18960145  1.18552003 -1.06648313  0.60271515  1.1443244
  1.31029903 -1.87354675  0.72965075  2.88578424 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:38:40 2024]  Iteration number: 0 with current cost as 0.3315072595772406 and parameters 
[-1.43602935  2.23743458 -2.12427952 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.87354674  0.72965069  2.88578419 -0.54534341 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:39:52 2024]  Iteration number: 0 with current cost as 0.33672967186451364 and parameters 
[-1.41170488  2.23743464 -2.12427952 -0.11653097  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271516  1.14432445
  1.31029904 -1.87354674  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:41:03 2024]  Iteration number: 0 with current cost as 0.33971857161021435 and parameters 
[-1.40907062  2.23743464 -2.12427958 -0.11653103  0.55388702 -2.77010909
  3.06858498  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432439
  1.31029899 -1.87354674  0.72965074  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 362.3794801235199 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 19:42:15 2024]  Iteration number: 0 with current cost as 0.3700387255470995 and parameters 
[-1.82176683  2.23743458 -2.12427953 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960151  1.18552004 -1.06648308  0.6027151   1.14432445
  1.31029904 -1.87354675  0.72965075  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:43:37 2024]  Iteration number: 0 with current cost as 0.3122912106762491 and parameters 
[-1.54439865  2.23743459 -2.12427959 -0.11653103  0.55388713 -2.77010902
  3.06858493  2.18960145  1.18552003 -1.06648313  0.60271515  1.1443244
  1.31029903 -1.87354675  0.72965075  2.88578424 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:44:35 2024]  Iteration number: 0 with current cost as 0.3315072595772406 and parameters 
[-1.43602935  2.23743458 -2.12427952 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.87354674  0.72965069  2.88578419 -0.54534341 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:45:50 2024]  Iteration number: 0 with current cost as 0.33672967186451364 and parameters 
[-1.41170488  2.23743464 -2.12427952 -0.11653097  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271516  1.14432445
  1.31029904 -1.87354674  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:47:01 2024]  Iteration number: 0 with current cost as 0.33971857161021435 and parameters 
[-1.40907062  2.23743464 -2.12427958 -0.11653103  0.55388702 -2.77010909
  3.06858498  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432439
  1.31029899 -1.87354674  0.72965074  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 357.51040744781494 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 19:48:13 2024]  Iteration number: 0 with current cost as 0.3700387255470995 and parameters 
[-1.82176683  2.23743458 -2.12427953 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960151  1.18552004 -1.06648308  0.6027151   1.14432445
  1.31029904 -1.87354675  0.72965075  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:49:31 2024]  Iteration number: 0 with current cost as 0.3122912106762491 and parameters 
[-1.54439865  2.23743459 -2.12427959 -0.11653103  0.55388713 -2.77010902
  3.06858493  2.18960145  1.18552003 -1.06648313  0.60271515  1.1443244
  1.31029903 -1.87354675  0.72965075  2.88578424 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:50:36 2024]  Iteration number: 0 with current cost as 0.3315072595772406 and parameters 
[-1.43602935  2.23743458 -2.12427952 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.87354674  0.72965069  2.88578419 -0.54534341 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:51:49 2024]  Iteration number: 0 with current cost as 0.33672967186451364 and parameters 
[-1.41170488  2.23743464 -2.12427952 -0.11653097  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271516  1.14432445
  1.31029904 -1.87354674  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:53:01 2024]  Iteration number: 0 with current cost as 0.33971857161021435 and parameters 
[-1.40907062  2.23743464 -2.12427958 -0.11653103  0.55388702 -2.77010909
  3.06858498  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432439
  1.31029899 -1.87354674  0.72965074  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 360.63054275512695 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 19:54:14 2024]  Iteration number: 0 with current cost as 0.3700387255470995 and parameters 
[-1.82176683  2.23743458 -2.12427953 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960151  1.18552004 -1.06648308  0.6027151   1.14432445
  1.31029904 -1.87354675  0.72965075  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:55:54 2024]  Iteration number: 0 with current cost as 0.3122912106762491 and parameters 
[-1.54439865  2.23743459 -2.12427959 -0.11653103  0.55388713 -2.77010902
  3.06858493  2.18960145  1.18552003 -1.06648313  0.60271515  1.1443244
  1.31029903 -1.87354675  0.72965075  2.88578424 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:56:53 2024]  Iteration number: 0 with current cost as 0.3315072595772406 and parameters 
[-1.43602935  2.23743458 -2.12427952 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.87354674  0.72965069  2.88578419 -0.54534341 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:58:05 2024]  Iteration number: 0 with current cost as 0.33672967186451364 and parameters 
[-1.41170488  2.23743464 -2.12427952 -0.11653097  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271516  1.14432445
  1.31029904 -1.87354674  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:59:17 2024]  Iteration number: 0 with current cost as 0.33971857161021435 and parameters 
[-1.40907062  2.23743464 -2.12427958 -0.11653103  0.55388702 -2.77010909
  3.06858498  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432439
  1.31029899 -1.87354674  0.72965074  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 376.08386969566345 seconds. 
Discarding model... 

Training complete taking 8976.365704536438 total seconds. 
Now scoring model... 
Scoring complete taking 0.9617588520050049 seconds. 
Saved predicted values as A1-A1-CZ_Modified-Pauli-CRZ_predicted_values.csv
Model scores: {'MSE_train': (206.93879274676516,), 'R2_train': -0.0025489989050868633, 'MAE_train': 12.57727151849314, 'MSE_test': 191.37228842545193, 'R2_test': -0.15355654216717363, 'MAE_test': 12.01322042722634}. 
Saved model results as A1-A1-CZ_Modified-Pauli-CRZ_results.json. 
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Thu Apr  4 11:36:38 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 11:37:17 2024]  Iteration number: 0 with current cost as 0.3700387255470995 and parameters 
[-1.82176683  2.23743458 -2.12427953 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960151  1.18552004 -1.06648308  0.6027151   1.14432445
  1.31029904 -1.87354675  0.72965075  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 11:38:38 2024]  Iteration number: 0 with current cost as 0.3122912106762491 and parameters 
[-1.54439865  2.23743459 -2.12427959 -0.11653103  0.55388713 -2.77010902
  3.06858493  2.18960145  1.18552003 -1.06648313  0.60271515  1.1443244
  1.31029903 -1.87354675  0.72965075  2.88578424 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 11:39:34 2024]  Iteration number: 0 with current cost as 0.3315072595772406 and parameters 
[-1.43602935  2.23743458 -2.12427952 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.87354674  0.72965069  2.88578419 -0.54534341 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 11:40:46 2024]  Iteration number: 0 with current cost as 0.33672967186451364 and parameters 
[-1.41170488  2.23743464 -2.12427952 -0.11653097  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271516  1.14432445
  1.31029904 -1.87354674  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 11:41:57 2024]  Iteration number: 0 with current cost as 0.33971857161021435 and parameters 
[-1.40907062  2.23743464 -2.12427958 -0.11653103  0.55388702 -2.77010909
  3.06858498  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432439
  1.31029899 -1.87354674  0.72965074  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 351.93089723587036 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 11:43:10 2024]  Iteration number: 0 with current cost as 0.3700387255470995 and parameters 
[-1.82176683  2.23743458 -2.12427953 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960151  1.18552004 -1.06648308  0.6027151   1.14432445
  1.31029904 -1.87354675  0.72965075  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 11:44:41 2024]  Iteration number: 0 with current cost as 0.3122912106762491 and parameters 
[-1.54439865  2.23743459 -2.12427959 -0.11653103  0.55388713 -2.77010902
  3.06858493  2.18960145  1.18552003 -1.06648313  0.60271515  1.1443244
  1.31029903 -1.87354675  0.72965075  2.88578424 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 11:45:37 2024]  Iteration number: 0 with current cost as 0.3315072595772406 and parameters 
[-1.43602935  2.23743458 -2.12427952 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.87354674  0.72965069  2.88578419 -0.54534341 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 11:46:48 2024]  Iteration number: 0 with current cost as 0.33672967186451364 and parameters 
[-1.41170488  2.23743464 -2.12427952 -0.11653097  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271516  1.14432445
  1.31029904 -1.87354674  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 11:47:59 2024]  Iteration number: 0 with current cost as 0.33971857161021435 and parameters 
[-1.40907062  2.23743464 -2.12427958 -0.11653103  0.55388702 -2.77010909
  3.06858498  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432439
  1.31029899 -1.87354674  0.72965074  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 361.06307911872864 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 11:49:11 2024]  Iteration number: 0 with current cost as 0.3700387255470995 and parameters 
[-1.82176683  2.23743458 -2.12427953 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960151  1.18552004 -1.06648308  0.6027151   1.14432445
  1.31029904 -1.87354675  0.72965075  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 11:50:31 2024]  Iteration number: 0 with current cost as 0.3122912106762491 and parameters 
[-1.54439865  2.23743459 -2.12427959 -0.11653103  0.55388713 -2.77010902
  3.06858493  2.18960145  1.18552003 -1.06648313  0.60271515  1.1443244
  1.31029903 -1.87354675  0.72965075  2.88578424 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 11:51:28 2024]  Iteration number: 0 with current cost as 0.3315072595772406 and parameters 
[-1.43602935  2.23743458 -2.12427952 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.87354674  0.72965069  2.88578419 -0.54534341 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 11:52:42 2024]  Iteration number: 0 with current cost as 0.33672967186451364 and parameters 
[-1.41170488  2.23743464 -2.12427952 -0.11653097  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271516  1.14432445
  1.31029904 -1.87354674  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 11:53:54 2024]  Iteration number: 0 with current cost as 0.33971857161021435 and parameters 
[-1.40907062  2.23743464 -2.12427958 -0.11653103  0.55388702 -2.77010909
  3.06858498  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432439
  1.31029899 -1.87354674  0.72965074  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 354.46199107170105 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 11:55:06 2024]  Iteration number: 0 with current cost as 0.3700387255470995 and parameters 
[-1.82176683  2.23743458 -2.12427953 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960151  1.18552004 -1.06648308  0.6027151   1.14432445
  1.31029904 -1.87354675  0.72965075  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 11:56:26 2024]  Iteration number: 0 with current cost as 0.3122912106762491 and parameters 
[-1.54439865  2.23743459 -2.12427959 -0.11653103  0.55388713 -2.77010902
  3.06858493  2.18960145  1.18552003 -1.06648313  0.60271515  1.1443244
  1.31029903 -1.87354675  0.72965075  2.88578424 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 11:57:23 2024]  Iteration number: 0 with current cost as 0.3315072595772406 and parameters 
[-1.43602935  2.23743458 -2.12427952 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.87354674  0.72965069  2.88578419 -0.54534341 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 11:58:40 2024]  Iteration number: 0 with current cost as 0.33672967186451364 and parameters 
[-1.41170488  2.23743464 -2.12427952 -0.11653097  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271516  1.14432445
  1.31029904 -1.87354674  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 11:59:51 2024]  Iteration number: 0 with current cost as 0.33971857161021435 and parameters 
[-1.40907062  2.23743464 -2.12427958 -0.11653103  0.55388702 -2.77010909
  3.06858498  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432439
  1.31029899 -1.87354674  0.72965074  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 358.10140776634216 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 12:01:04 2024]  Iteration number: 0 with current cost as 0.3700387255470995 and parameters 
[-1.82176683  2.23743458 -2.12427953 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960151  1.18552004 -1.06648308  0.6027151   1.14432445
  1.31029904 -1.87354675  0.72965075  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 12:02:23 2024]  Iteration number: 0 with current cost as 0.3122912106762491 and parameters 
[-1.54439865  2.23743459 -2.12427959 -0.11653103  0.55388713 -2.77010902
  3.06858493  2.18960145  1.18552003 -1.06648313  0.60271515  1.1443244
  1.31029903 -1.87354675  0.72965075  2.88578424 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 12:03:19 2024]  Iteration number: 0 with current cost as 0.3315072595772406 and parameters 
[-1.43602935  2.23743458 -2.12427952 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.87354674  0.72965069  2.88578419 -0.54534341 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:04:31 2024]  Iteration number: 0 with current cost as 0.33672967186451364 and parameters 
[-1.41170488  2.23743464 -2.12427952 -0.11653097  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271516  1.14432445
  1.31029904 -1.87354674  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:05:44 2024]  Iteration number: 0 with current cost as 0.33971857161021435 and parameters 
[-1.40907062  2.23743464 -2.12427958 -0.11653103  0.55388702 -2.77010909
  3.06858498  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432439
  1.31029899 -1.87354674  0.72965074  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 353.2103817462921 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 12:06:56 2024]  Iteration number: 0 with current cost as 0.3700387255470995 and parameters 
[-1.82176683  2.23743458 -2.12427953 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960151  1.18552004 -1.06648308  0.6027151   1.14432445
  1.31029904 -1.87354675  0.72965075  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 12:08:16 2024]  Iteration number: 0 with current cost as 0.3122912106762491 and parameters 
[-1.54439865  2.23743459 -2.12427959 -0.11653103  0.55388713 -2.77010902
  3.06858493  2.18960145  1.18552003 -1.06648313  0.60271515  1.1443244
  1.31029903 -1.87354675  0.72965075  2.88578424 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 12:09:14 2024]  Iteration number: 0 with current cost as 0.3315072595772406 and parameters 
[-1.43602935  2.23743458 -2.12427952 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.87354674  0.72965069  2.88578419 -0.54534341 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:10:26 2024]  Iteration number: 0 with current cost as 0.33672967186451364 and parameters 
[-1.41170488  2.23743464 -2.12427952 -0.11653097  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271516  1.14432445
  1.31029904 -1.87354674  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:11:39 2024]  Iteration number: 0 with current cost as 0.33971857161021435 and parameters 
[-1.40907062  2.23743464 -2.12427958 -0.11653103  0.55388702 -2.77010909
  3.06858498  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432439
  1.31029899 -1.87354674  0.72965074  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 354.8437292575836 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 12:12:51 2024]  Iteration number: 0 with current cost as 0.3700387255470995 and parameters 
[-1.82176683  2.23743458 -2.12427953 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960151  1.18552004 -1.06648308  0.6027151   1.14432445
  1.31029904 -1.87354675  0.72965075  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 12:14:13 2024]  Iteration number: 0 with current cost as 0.3122912106762491 and parameters 
[-1.54439865  2.23743459 -2.12427959 -0.11653103  0.55388713 -2.77010902
  3.06858493  2.18960145  1.18552003 -1.06648313  0.60271515  1.1443244
  1.31029903 -1.87354675  0.72965075  2.88578424 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 12:15:10 2024]  Iteration number: 0 with current cost as 0.3315072595772406 and parameters 
[-1.43602935  2.23743458 -2.12427952 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.87354674  0.72965069  2.88578419 -0.54534341 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:16:22 2024]  Iteration number: 0 with current cost as 0.33672967186451364 and parameters 
[-1.41170488  2.23743464 -2.12427952 -0.11653097  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271516  1.14432445
  1.31029904 -1.87354674  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:17:34 2024]  Iteration number: 0 with current cost as 0.33971857161021435 and parameters 
[-1.40907062  2.23743464 -2.12427958 -0.11653103  0.55388702 -2.77010909
  3.06858498  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432439
  1.31029899 -1.87354674  0.72965074  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 355.0053377151489 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 12:18:46 2024]  Iteration number: 0 with current cost as 0.3700387255470995 and parameters 
[-1.82176683  2.23743458 -2.12427953 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960151  1.18552004 -1.06648308  0.6027151   1.14432445
  1.31029904 -1.87354675  0.72965075  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 12:20:07 2024]  Iteration number: 0 with current cost as 0.3122912106762491 and parameters 
[-1.54439865  2.23743459 -2.12427959 -0.11653103  0.55388713 -2.77010902
  3.06858493  2.18960145  1.18552003 -1.06648313  0.60271515  1.1443244
  1.31029903 -1.87354675  0.72965075  2.88578424 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 12:21:02 2024]  Iteration number: 0 with current cost as 0.3315072595772406 and parameters 
[-1.43602935  2.23743458 -2.12427952 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.87354674  0.72965069  2.88578419 -0.54534341 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:22:17 2024]  Iteration number: 0 with current cost as 0.33672967186451364 and parameters 
[-1.41170488  2.23743464 -2.12427952 -0.11653097  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271516  1.14432445
  1.31029904 -1.87354674  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:23:29 2024]  Iteration number: 0 with current cost as 0.33971857161021435 and parameters 
[-1.40907062  2.23743464 -2.12427958 -0.11653103  0.55388702 -2.77010909
  3.06858498  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432439
  1.31029899 -1.87354674  0.72965074  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 354.6369926929474 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 12:24:42 2024]  Iteration number: 0 with current cost as 0.3700387255470995 and parameters 
[-1.82176683  2.23743458 -2.12427953 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960151  1.18552004 -1.06648308  0.6027151   1.14432445
  1.31029904 -1.87354675  0.72965075  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 12:26:02 2024]  Iteration number: 0 with current cost as 0.3122912106762491 and parameters 
[-1.54439865  2.23743459 -2.12427959 -0.11653103  0.55388713 -2.77010902
  3.06858493  2.18960145  1.18552003 -1.06648313  0.60271515  1.1443244
  1.31029903 -1.87354675  0.72965075  2.88578424 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 12:26:59 2024]  Iteration number: 0 with current cost as 0.3315072595772406 and parameters 
[-1.43602935  2.23743458 -2.12427952 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.87354674  0.72965069  2.88578419 -0.54534341 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:28:12 2024]  Iteration number: 0 with current cost as 0.33672967186451364 and parameters 
[-1.41170488  2.23743464 -2.12427952 -0.11653097  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271516  1.14432445
  1.31029904 -1.87354674  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:29:24 2024]  Iteration number: 0 with current cost as 0.33971857161021435 and parameters 
[-1.40907062  2.23743464 -2.12427958 -0.11653103  0.55388702 -2.77010909
  3.06858498  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432439
  1.31029899 -1.87354674  0.72965074  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 361.4490144252777 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 12:30:44 2024]  Iteration number: 0 with current cost as 0.3700387255470995 and parameters 
[-1.82176683  2.23743458 -2.12427953 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960151  1.18552004 -1.06648308  0.6027151   1.14432445
  1.31029904 -1.87354675  0.72965075  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 12:32:06 2024]  Iteration number: 0 with current cost as 0.3122912106762491 and parameters 
[-1.54439865  2.23743459 -2.12427959 -0.11653103  0.55388713 -2.77010902
  3.06858493  2.18960145  1.18552003 -1.06648313  0.60271515  1.1443244
  1.31029903 -1.87354675  0.72965075  2.88578424 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 12:33:01 2024]  Iteration number: 0 with current cost as 0.3315072595772406 and parameters 
[-1.43602935  2.23743458 -2.12427952 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.87354674  0.72965069  2.88578419 -0.54534341 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:34:15 2024]  Iteration number: 0 with current cost as 0.33672967186451364 and parameters 
[-1.41170488  2.23743464 -2.12427952 -0.11653097  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271516  1.14432445
  1.31029904 -1.87354674  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:35:27 2024]  Iteration number: 0 with current cost as 0.33971857161021435 and parameters 
[-1.40907062  2.23743464 -2.12427958 -0.11653103  0.55388702 -2.77010909
  3.06858498  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432439
  1.31029899 -1.87354674  0.72965074  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 356.2212154865265 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 12:36:40 2024]  Iteration number: 0 with current cost as 0.3700387255470995 and parameters 
[-1.82176683  2.23743458 -2.12427953 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960151  1.18552004 -1.06648308  0.6027151   1.14432445
  1.31029904 -1.87354675  0.72965075  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 12:37:58 2024]  Iteration number: 0 with current cost as 0.3122912106762491 and parameters 
[-1.54439865  2.23743459 -2.12427959 -0.11653103  0.55388713 -2.77010902
  3.06858493  2.18960145  1.18552003 -1.06648313  0.60271515  1.1443244
  1.31029903 -1.87354675  0.72965075  2.88578424 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 12:38:54 2024]  Iteration number: 0 with current cost as 0.3315072595772406 and parameters 
[-1.43602935  2.23743458 -2.12427952 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.87354674  0.72965069  2.88578419 -0.54534341 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:40:07 2024]  Iteration number: 0 with current cost as 0.33672967186451364 and parameters 
[-1.41170488  2.23743464 -2.12427952 -0.11653097  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271516  1.14432445
  1.31029904 -1.87354674  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:41:19 2024]  Iteration number: 0 with current cost as 0.33971857161021435 and parameters 
[-1.40907062  2.23743464 -2.12427958 -0.11653103  0.55388702 -2.77010909
  3.06858498  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432439
  1.31029899 -1.87354674  0.72965074  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 357.1674110889435 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 12:42:37 2024]  Iteration number: 0 with current cost as 0.3700387255470995 and parameters 
[-1.82176683  2.23743458 -2.12427953 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960151  1.18552004 -1.06648308  0.6027151   1.14432445
  1.31029904 -1.87354675  0.72965075  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 12:43:57 2024]  Iteration number: 0 with current cost as 0.3122912106762491 and parameters 
[-1.54439865  2.23743459 -2.12427959 -0.11653103  0.55388713 -2.77010902
  3.06858493  2.18960145  1.18552003 -1.06648313  0.60271515  1.1443244
  1.31029903 -1.87354675  0.72965075  2.88578424 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 12:44:53 2024]  Iteration number: 0 with current cost as 0.3315072595772406 and parameters 
[-1.43602935  2.23743458 -2.12427952 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.87354674  0.72965069  2.88578419 -0.54534341 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:46:06 2024]  Iteration number: 0 with current cost as 0.33672967186451364 and parameters 
[-1.41170488  2.23743464 -2.12427952 -0.11653097  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271516  1.14432445
  1.31029904 -1.87354674  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:47:17 2024]  Iteration number: 0 with current cost as 0.33971857161021435 and parameters 
[-1.40907062  2.23743464 -2.12427958 -0.11653103  0.55388702 -2.77010909
  3.06858498  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432439
  1.31029899 -1.87354674  0.72965074  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 354.1325252056122 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 12:48:30 2024]  Iteration number: 0 with current cost as 0.3700387255470995 and parameters 
[-1.82176683  2.23743458 -2.12427953 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960151  1.18552004 -1.06648308  0.6027151   1.14432445
  1.31029904 -1.87354675  0.72965075  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 12:50:14 2024]  Iteration number: 0 with current cost as 0.3122912106762491 and parameters 
[-1.54439865  2.23743459 -2.12427959 -0.11653103  0.55388713 -2.77010902
  3.06858493  2.18960145  1.18552003 -1.06648313  0.60271515  1.1443244
  1.31029903 -1.87354675  0.72965075  2.88578424 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 12:51:12 2024]  Iteration number: 0 with current cost as 0.3315072595772406 and parameters 
[-1.43602935  2.23743458 -2.12427952 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.87354674  0.72965069  2.88578419 -0.54534341 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:52:27 2024]  Iteration number: 0 with current cost as 0.33672967186451364 and parameters 
[-1.41170488  2.23743464 -2.12427952 -0.11653097  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271516  1.14432445
  1.31029904 -1.87354674  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:53:40 2024]  Iteration number: 0 with current cost as 0.33971857161021435 and parameters 
[-1.40907062  2.23743464 -2.12427958 -0.11653103  0.55388702 -2.77010909
  3.06858498  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432439
  1.31029899 -1.87354674  0.72965074  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 382.5619990825653 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 12:54:52 2024]  Iteration number: 0 with current cost as 0.3700387255470995 and parameters 
[-1.82176683  2.23743458 -2.12427953 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960151  1.18552004 -1.06648308  0.6027151   1.14432445
  1.31029904 -1.87354675  0.72965075  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 12:56:13 2024]  Iteration number: 0 with current cost as 0.3122912106762491 and parameters 
[-1.54439865  2.23743459 -2.12427959 -0.11653103  0.55388713 -2.77010902
  3.06858493  2.18960145  1.18552003 -1.06648313  0.60271515  1.1443244
  1.31029903 -1.87354675  0.72965075  2.88578424 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 12:57:12 2024]  Iteration number: 0 with current cost as 0.3315072595772406 and parameters 
[-1.43602935  2.23743458 -2.12427952 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.87354674  0.72965069  2.88578419 -0.54534341 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:58:23 2024]  Iteration number: 0 with current cost as 0.33672967186451364 and parameters 
[-1.41170488  2.23743464 -2.12427952 -0.11653097  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271516  1.14432445
  1.31029904 -1.87354674  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:59:38 2024]  Iteration number: 0 with current cost as 0.33971857161021435 and parameters 
[-1.40907062  2.23743464 -2.12427958 -0.11653103  0.55388702 -2.77010909
  3.06858498  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432439
  1.31029899 -1.87354674  0.72965074  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 357.45372676849365 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 13:00:50 2024]  Iteration number: 0 with current cost as 0.3700387255470995 and parameters 
[-1.82176683  2.23743458 -2.12427953 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960151  1.18552004 -1.06648308  0.6027151   1.14432445
  1.31029904 -1.87354675  0.72965075  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 13:02:11 2024]  Iteration number: 0 with current cost as 0.3122912106762491 and parameters 
[-1.54439865  2.23743459 -2.12427959 -0.11653103  0.55388713 -2.77010902
  3.06858493  2.18960145  1.18552003 -1.06648313  0.60271515  1.1443244
  1.31029903 -1.87354675  0.72965075  2.88578424 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 13:03:06 2024]  Iteration number: 0 with current cost as 0.3315072595772406 and parameters 
[-1.43602935  2.23743458 -2.12427952 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.87354674  0.72965069  2.88578419 -0.54534341 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:04:20 2024]  Iteration number: 0 with current cost as 0.33672967186451364 and parameters 
[-1.41170488  2.23743464 -2.12427952 -0.11653097  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271516  1.14432445
  1.31029904 -1.87354674  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:05:33 2024]  Iteration number: 0 with current cost as 0.33971857161021435 and parameters 
[-1.40907062  2.23743464 -2.12427958 -0.11653103  0.55388702 -2.77010909
  3.06858498  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432439
  1.31029899 -1.87354674  0.72965074  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 354.6864094734192 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 13:06:45 2024]  Iteration number: 0 with current cost as 0.3700387255470995 and parameters 
[-1.82176683  2.23743458 -2.12427953 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960151  1.18552004 -1.06648308  0.6027151   1.14432445
  1.31029904 -1.87354675  0.72965075  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 13:08:05 2024]  Iteration number: 0 with current cost as 0.3122912106762491 and parameters 
[-1.54439865  2.23743459 -2.12427959 -0.11653103  0.55388713 -2.77010902
  3.06858493  2.18960145  1.18552003 -1.06648313  0.60271515  1.1443244
  1.31029903 -1.87354675  0.72965075  2.88578424 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 13:09:02 2024]  Iteration number: 0 with current cost as 0.3315072595772406 and parameters 
[-1.43602935  2.23743458 -2.12427952 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.87354674  0.72965069  2.88578419 -0.54534341 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:10:15 2024]  Iteration number: 0 with current cost as 0.33672967186451364 and parameters 
[-1.41170488  2.23743464 -2.12427952 -0.11653097  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271516  1.14432445
  1.31029904 -1.87354674  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:11:27 2024]  Iteration number: 0 with current cost as 0.33971857161021435 and parameters 
[-1.40907062  2.23743464 -2.12427958 -0.11653103  0.55388702 -2.77010909
  3.06858498  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432439
  1.31029899 -1.87354674  0.72965074  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 353.9736108779907 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 13:12:40 2024]  Iteration number: 0 with current cost as 0.3700387255470995 and parameters 
[-1.82176683  2.23743458 -2.12427953 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960151  1.18552004 -1.06648308  0.6027151   1.14432445
  1.31029904 -1.87354675  0.72965075  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 13:13:59 2024]  Iteration number: 0 with current cost as 0.3122912106762491 and parameters 
[-1.54439865  2.23743459 -2.12427959 -0.11653103  0.55388713 -2.77010902
  3.06858493  2.18960145  1.18552003 -1.06648313  0.60271515  1.1443244
  1.31029903 -1.87354675  0.72965075  2.88578424 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 13:14:58 2024]  Iteration number: 0 with current cost as 0.3315072595772406 and parameters 
[-1.43602935  2.23743458 -2.12427952 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.87354674  0.72965069  2.88578419 -0.54534341 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:16:12 2024]  Iteration number: 0 with current cost as 0.33672967186451364 and parameters 
[-1.41170488  2.23743464 -2.12427952 -0.11653097  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271516  1.14432445
  1.31029904 -1.87354674  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:17:23 2024]  Iteration number: 0 with current cost as 0.33971857161021435 and parameters 
[-1.40907062  2.23743464 -2.12427958 -0.11653103  0.55388702 -2.77010909
  3.06858498  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432439
  1.31029899 -1.87354674  0.72965074  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 356.5865709781647 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 13:18:35 2024]  Iteration number: 0 with current cost as 0.3700387255470995 and parameters 
[-1.82176683  2.23743458 -2.12427953 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960151  1.18552004 -1.06648308  0.6027151   1.14432445
  1.31029904 -1.87354675  0.72965075  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 13:19:56 2024]  Iteration number: 0 with current cost as 0.3122912106762491 and parameters 
[-1.54439865  2.23743459 -2.12427959 -0.11653103  0.55388713 -2.77010902
  3.06858493  2.18960145  1.18552003 -1.06648313  0.60271515  1.1443244
  1.31029903 -1.87354675  0.72965075  2.88578424 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 13:20:52 2024]  Iteration number: 0 with current cost as 0.3315072595772406 and parameters 
[-1.43602935  2.23743458 -2.12427952 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.87354674  0.72965069  2.88578419 -0.54534341 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:22:05 2024]  Iteration number: 0 with current cost as 0.33672967186451364 and parameters 
[-1.41170488  2.23743464 -2.12427952 -0.11653097  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271516  1.14432445
  1.31029904 -1.87354674  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:23:19 2024]  Iteration number: 0 with current cost as 0.33971857161021435 and parameters 
[-1.40907062  2.23743464 -2.12427958 -0.11653103  0.55388702 -2.77010909
  3.06858498  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432439
  1.31029899 -1.87354674  0.72965074  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 355.9618785381317 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 13:24:31 2024]  Iteration number: 0 with current cost as 0.3700387255470995 and parameters 
[-1.82176683  2.23743458 -2.12427953 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960151  1.18552004 -1.06648308  0.6027151   1.14432445
  1.31029904 -1.87354675  0.72965075  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 13:25:52 2024]  Iteration number: 0 with current cost as 0.3122912106762491 and parameters 
[-1.54439865  2.23743459 -2.12427959 -0.11653103  0.55388713 -2.77010902
  3.06858493  2.18960145  1.18552003 -1.06648313  0.60271515  1.1443244
  1.31029903 -1.87354675  0.72965075  2.88578424 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 13:26:48 2024]  Iteration number: 0 with current cost as 0.3315072595772406 and parameters 
[-1.43602935  2.23743458 -2.12427952 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.87354674  0.72965069  2.88578419 -0.54534341 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:28:00 2024]  Iteration number: 0 with current cost as 0.33672967186451364 and parameters 
[-1.41170488  2.23743464 -2.12427952 -0.11653097  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271516  1.14432445
  1.31029904 -1.87354674  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:29:13 2024]  Iteration number: 0 with current cost as 0.33971857161021435 and parameters 
[-1.40907062  2.23743464 -2.12427958 -0.11653103  0.55388702 -2.77010909
  3.06858498  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432439
  1.31029899 -1.87354674  0.72965074  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 353.96903562545776 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 13:30:25 2024]  Iteration number: 0 with current cost as 0.3700387255470995 and parameters 
[-1.82176683  2.23743458 -2.12427953 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960151  1.18552004 -1.06648308  0.6027151   1.14432445
  1.31029904 -1.87354675  0.72965075  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 13:31:46 2024]  Iteration number: 0 with current cost as 0.3122912106762491 and parameters 
[-1.54439865  2.23743459 -2.12427959 -0.11653103  0.55388713 -2.77010902
  3.06858493  2.18960145  1.18552003 -1.06648313  0.60271515  1.1443244
  1.31029903 -1.87354675  0.72965075  2.88578424 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 13:32:43 2024]  Iteration number: 0 with current cost as 0.3315072595772406 and parameters 
[-1.43602935  2.23743458 -2.12427952 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.87354674  0.72965069  2.88578419 -0.54534341 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:33:56 2024]  Iteration number: 0 with current cost as 0.33672967186451364 and parameters 
[-1.41170488  2.23743464 -2.12427952 -0.11653097  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271516  1.14432445
  1.31029904 -1.87354674  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:35:22 2024]  Iteration number: 0 with current cost as 0.33971857161021435 and parameters 
[-1.40907062  2.23743464 -2.12427958 -0.11653103  0.55388702 -2.77010909
  3.06858498  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432439
  1.31029899 -1.87354674  0.72965074  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 369.05691027641296 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 13:36:34 2024]  Iteration number: 0 with current cost as 0.3700387255470995 and parameters 
[-1.82176683  2.23743458 -2.12427953 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960151  1.18552004 -1.06648308  0.6027151   1.14432445
  1.31029904 -1.87354675  0.72965075  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 13:37:56 2024]  Iteration number: 0 with current cost as 0.3122912106762491 and parameters 
[-1.54439865  2.23743459 -2.12427959 -0.11653103  0.55388713 -2.77010902
  3.06858493  2.18960145  1.18552003 -1.06648313  0.60271515  1.1443244
  1.31029903 -1.87354675  0.72965075  2.88578424 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 13:38:52 2024]  Iteration number: 0 with current cost as 0.3315072595772406 and parameters 
[-1.43602935  2.23743458 -2.12427952 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.87354674  0.72965069  2.88578419 -0.54534341 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:40:04 2024]  Iteration number: 0 with current cost as 0.33672967186451364 and parameters 
[-1.41170488  2.23743464 -2.12427952 -0.11653097  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271516  1.14432445
  1.31029904 -1.87354674  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:41:17 2024]  Iteration number: 0 with current cost as 0.33971857161021435 and parameters 
[-1.40907062  2.23743464 -2.12427958 -0.11653103  0.55388702 -2.77010909
  3.06858498  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432439
  1.31029899 -1.87354674  0.72965074  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 355.2253768444061 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 13:42:29 2024]  Iteration number: 0 with current cost as 0.3700387255470995 and parameters 
[-1.82176683  2.23743458 -2.12427953 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960151  1.18552004 -1.06648308  0.6027151   1.14432445
  1.31029904 -1.87354675  0.72965075  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 13:43:51 2024]  Iteration number: 0 with current cost as 0.3122912106762491 and parameters 
[-1.54439865  2.23743459 -2.12427959 -0.11653103  0.55388713 -2.77010902
  3.06858493  2.18960145  1.18552003 -1.06648313  0.60271515  1.1443244
  1.31029903 -1.87354675  0.72965075  2.88578424 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 13:44:59 2024]  Iteration number: 0 with current cost as 0.3315072595772406 and parameters 
[-1.43602935  2.23743458 -2.12427952 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.87354674  0.72965069  2.88578419 -0.54534341 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:46:13 2024]  Iteration number: 0 with current cost as 0.33672967186451364 and parameters 
[-1.41170488  2.23743464 -2.12427952 -0.11653097  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271516  1.14432445
  1.31029904 -1.87354674  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:47:29 2024]  Iteration number: 0 with current cost as 0.33971857161021435 and parameters 
[-1.40907062  2.23743464 -2.12427958 -0.11653103  0.55388702 -2.77010909
  3.06858498  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432439
  1.31029899 -1.87354674  0.72965074  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 371.8754107952118 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 13:48:42 2024]  Iteration number: 0 with current cost as 0.3700387255470995 and parameters 
[-1.82176683  2.23743458 -2.12427953 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960151  1.18552004 -1.06648308  0.6027151   1.14432445
  1.31029904 -1.87354675  0.72965075  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 13:50:18 2024]  Iteration number: 0 with current cost as 0.3122912106762491 and parameters 
[-1.54439865  2.23743459 -2.12427959 -0.11653103  0.55388713 -2.77010902
  3.06858493  2.18960145  1.18552003 -1.06648313  0.60271515  1.1443244
  1.31029903 -1.87354675  0.72965075  2.88578424 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 13:51:14 2024]  Iteration number: 0 with current cost as 0.3315072595772406 and parameters 
[-1.43602935  2.23743458 -2.12427952 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.87354674  0.72965069  2.88578419 -0.54534341 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:52:26 2024]  Iteration number: 0 with current cost as 0.33672967186451364 and parameters 
[-1.41170488  2.23743464 -2.12427952 -0.11653097  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271516  1.14432445
  1.31029904 -1.87354674  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:53:38 2024]  Iteration number: 0 with current cost as 0.33971857161021435 and parameters 
[-1.40907062  2.23743464 -2.12427958 -0.11653103  0.55388702 -2.77010909
  3.06858498  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432439
  1.31029899 -1.87354674  0.72965074  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 368.0467267036438 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 13:54:50 2024]  Iteration number: 0 with current cost as 0.3700387255470995 and parameters 
[-1.82176683  2.23743458 -2.12427953 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960151  1.18552004 -1.06648308  0.6027151   1.14432445
  1.31029904 -1.87354675  0.72965075  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 13:56:08 2024]  Iteration number: 0 with current cost as 0.3122912106762491 and parameters 
[-1.54439865  2.23743459 -2.12427959 -0.11653103  0.55388713 -2.77010902
  3.06858493  2.18960145  1.18552003 -1.06648313  0.60271515  1.1443244
  1.31029903 -1.87354675  0.72965075  2.88578424 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 13:57:04 2024]  Iteration number: 0 with current cost as 0.3315072595772406 and parameters 
[-1.43602935  2.23743458 -2.12427952 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.87354674  0.72965069  2.88578419 -0.54534341 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:58:17 2024]  Iteration number: 0 with current cost as 0.33672967186451364 and parameters 
[-1.41170488  2.23743464 -2.12427952 -0.11653097  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271516  1.14432445
  1.31029904 -1.87354674  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:59:28 2024]  Iteration number: 0 with current cost as 0.33971857161021435 and parameters 
[-1.40907062  2.23743464 -2.12427958 -0.11653103  0.55388702 -2.77010909
  3.06858498  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432439
  1.31029899 -1.87354674  0.72965074  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 350.3298830986023 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 14:00:39 2024]  Iteration number: 0 with current cost as 0.3700387255470995 and parameters 
[-1.82176683  2.23743458 -2.12427953 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960151  1.18552004 -1.06648308  0.6027151   1.14432445
  1.31029904 -1.87354675  0.72965075  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 14:02:00 2024]  Iteration number: 0 with current cost as 0.3122912106762491 and parameters 
[-1.54439865  2.23743459 -2.12427959 -0.11653103  0.55388713 -2.77010902
  3.06858493  2.18960145  1.18552003 -1.06648313  0.60271515  1.1443244
  1.31029903 -1.87354675  0.72965075  2.88578424 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 14:02:56 2024]  Iteration number: 0 with current cost as 0.3315072595772406 and parameters 
[-1.43602935  2.23743458 -2.12427952 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.87354674  0.72965069  2.88578419 -0.54534341 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 14:04:07 2024]  Iteration number: 0 with current cost as 0.33672967186451364 and parameters 
[-1.41170488  2.23743464 -2.12427952 -0.11653097  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271516  1.14432445
  1.31029904 -1.87354674  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 14:05:19 2024]  Iteration number: 0 with current cost as 0.33971857161021435 and parameters 
[-1.40907062  2.23743464 -2.12427958 -0.11653103  0.55388702 -2.77010909
  3.06858498  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432439
  1.31029899 -1.87354674  0.72965074  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 351.16189432144165 seconds. 
Discarding model... 

Training complete taking 8953.114292860031 total seconds. 
Now scoring model... 
Scoring complete taking 0.9577467441558838 seconds. 
Saved predicted values as A1-A1-CZ_Modified-Pauli-CRZ_predicted_values.csv
Model scores: {'MSE_train': (206.93879274676516,), 'R2_train': -0.0025489989050868633, 'MAE_train': 12.57727151849314, 'MSE_test': 191.37228842545193, 'R2_test': -0.15355654216717363, 'MAE_test': 12.01322042722634}. 
Saved model results as A1-A1-CZ_Modified-Pauli-CRZ_results.json. 
