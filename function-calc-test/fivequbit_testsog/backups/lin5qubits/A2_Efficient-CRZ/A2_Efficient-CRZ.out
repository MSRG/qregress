/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Sun Mar 24 17:30:08 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 17:31:39 2024]  Iteration number: 0 with current cost as 0.4549975158283851 and parameters 
[-4.26569987  2.23743474 -2.12427953 -0.11653092  0.55388708 -2.77010897
  3.06858488  2.18960156  1.1855202  -1.0664833   0.60271521  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 17:33:27 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743401 -2.12427964 -0.11653103  0.55388677 -2.7701096
  3.06858436  2.18960145  1.18551998 -1.06648371  0.60271448  1.14432445
  1.31029867 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 17:35:11 2024]  Iteration number: 0 with current cost as 0.4011745314291728 and parameters 
[-4.37377097  2.23743432 -2.12427964 -0.11653103  0.55388677 -2.7701096
  3.06858436  2.18960145  1.18551967 -1.06648371  0.60271479  1.14432414
  1.31029867 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 17:38:53 2024]  Iteration number: 0 with current cost as 0.3470058589954329 and parameters 
[-20.14851494   2.23743464  -2.12426934  -0.11652588   0.55388708
  -2.77010897   3.06858498   2.1896066    1.18551998  -1.06648308
   0.6027151    1.14432445   1.31029899  -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 17:40:38 2024]  Iteration number: 0 with current cost as 0.4019619828723463 and parameters 
[-4.43042718  2.23743432 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 674.0332844257355 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 17:42:51 2024]  Iteration number: 0 with current cost as 0.4549975158283851 and parameters 
[-4.26569987  2.23743474 -2.12427953 -0.11653092  0.55388708 -2.77010897
  3.06858488  2.18960156  1.1855202  -1.0664833   0.60271521  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 17:44:33 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743401 -2.12427964 -0.11653103  0.55388677 -2.7701096
  3.06858436  2.18960145  1.18551998 -1.06648371  0.60271448  1.14432445
  1.31029867 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 17:46:18 2024]  Iteration number: 0 with current cost as 0.4011745314291728 and parameters 
[-4.37377097  2.23743432 -2.12427964 -0.11653103  0.55388677 -2.7701096
  3.06858436  2.18960145  1.18551967 -1.06648371  0.60271479  1.14432414
  1.31029867 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 17:50:05 2024]  Iteration number: 0 with current cost as 0.3470058589954329 and parameters 
[-20.14851494   2.23743464  -2.12426934  -0.11652588   0.55388708
  -2.77010897   3.06858498   2.1896066    1.18551998  -1.06648308
   0.6027151    1.14432445   1.31029899  -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 17:51:52 2024]  Iteration number: 0 with current cost as 0.4019619828723463 and parameters 
[-4.43042718  2.23743432 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 675.5715565681458 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 17:54:08 2024]  Iteration number: 0 with current cost as 0.4549975158283851 and parameters 
[-4.26569987  2.23743474 -2.12427953 -0.11653092  0.55388708 -2.77010897
  3.06858488  2.18960156  1.1855202  -1.0664833   0.60271521  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 17:55:56 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743401 -2.12427964 -0.11653103  0.55388677 -2.7701096
  3.06858436  2.18960145  1.18551998 -1.06648371  0.60271448  1.14432445
  1.31029867 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 17:57:45 2024]  Iteration number: 0 with current cost as 0.4011745314291728 and parameters 
[-4.37377097  2.23743432 -2.12427964 -0.11653103  0.55388677 -2.7701096
  3.06858436  2.18960145  1.18551967 -1.06648371  0.60271479  1.14432414
  1.31029867 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:01:33 2024]  Iteration number: 0 with current cost as 0.3470058589954329 and parameters 
[-20.14851494   2.23743464  -2.12426934  -0.11652588   0.55388708
  -2.77010897   3.06858498   2.1896066    1.18551998  -1.06648308
   0.6027151    1.14432445   1.31029899  -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:03:19 2024]  Iteration number: 0 with current cost as 0.4019619828723463 and parameters 
[-4.43042718  2.23743432 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 687.421220779419 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 18:05:34 2024]  Iteration number: 0 with current cost as 0.4549975158283851 and parameters 
[-4.26569987  2.23743474 -2.12427953 -0.11653092  0.55388708 -2.77010897
  3.06858488  2.18960156  1.1855202  -1.0664833   0.60271521  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:07:18 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743401 -2.12427964 -0.11653103  0.55388677 -2.7701096
  3.06858436  2.18960145  1.18551998 -1.06648371  0.60271448  1.14432445
  1.31029867 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:09:00 2024]  Iteration number: 0 with current cost as 0.4011745314291728 and parameters 
[-4.37377097  2.23743432 -2.12427964 -0.11653103  0.55388677 -2.7701096
  3.06858436  2.18960145  1.18551967 -1.06648371  0.60271479  1.14432414
  1.31029867 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:12:44 2024]  Iteration number: 0 with current cost as 0.3470058589954329 and parameters 
[-20.14851494   2.23743464  -2.12426934  -0.11652588   0.55388708
  -2.77010897   3.06858498   2.1896066    1.18551998  -1.06648308
   0.6027151    1.14432445   1.31029899  -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:14:30 2024]  Iteration number: 0 with current cost as 0.4019619828723463 and parameters 
[-4.43042718  2.23743432 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 670.8102321624756 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 18:16:48 2024]  Iteration number: 0 with current cost as 0.4549975158283851 and parameters 
[-4.26569987  2.23743474 -2.12427953 -0.11653092  0.55388708 -2.77010897
  3.06858488  2.18960156  1.1855202  -1.0664833   0.60271521  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:18:33 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743401 -2.12427964 -0.11653103  0.55388677 -2.7701096
  3.06858436  2.18960145  1.18551998 -1.06648371  0.60271448  1.14432445
  1.31029867 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:20:18 2024]  Iteration number: 0 with current cost as 0.4011745314291728 and parameters 
[-4.37377097  2.23743432 -2.12427964 -0.11653103  0.55388677 -2.7701096
  3.06858436  2.18960145  1.18551967 -1.06648371  0.60271479  1.14432414
  1.31029867 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:24:01 2024]  Iteration number: 0 with current cost as 0.3470058589954329 and parameters 
[-20.14851494   2.23743464  -2.12426934  -0.11652588   0.55388708
  -2.77010897   3.06858498   2.1896066    1.18551998  -1.06648308
   0.6027151    1.14432445   1.31029899  -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:25:48 2024]  Iteration number: 0 with current cost as 0.4019619828723463 and parameters 
[-4.43042718  2.23743432 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 676.5006477832794 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 18:28:03 2024]  Iteration number: 0 with current cost as 0.4549975158283851 and parameters 
[-4.26569987  2.23743474 -2.12427953 -0.11653092  0.55388708 -2.77010897
  3.06858488  2.18960156  1.1855202  -1.0664833   0.60271521  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:29:50 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743401 -2.12427964 -0.11653103  0.55388677 -2.7701096
  3.06858436  2.18960145  1.18551998 -1.06648371  0.60271448  1.14432445
  1.31029867 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:31:35 2024]  Iteration number: 0 with current cost as 0.4011745314291728 and parameters 
[-4.37377097  2.23743432 -2.12427964 -0.11653103  0.55388677 -2.7701096
  3.06858436  2.18960145  1.18551967 -1.06648371  0.60271479  1.14432414
  1.31029867 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:35:23 2024]  Iteration number: 0 with current cost as 0.3470058589954329 and parameters 
[-20.14851494   2.23743464  -2.12426934  -0.11652588   0.55388708
  -2.77010897   3.06858498   2.1896066    1.18551998  -1.06648308
   0.6027151    1.14432445   1.31029899  -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:37:06 2024]  Iteration number: 0 with current cost as 0.4019619828723463 and parameters 
[-4.43042718  2.23743432 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 678.4353711605072 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 18:39:23 2024]  Iteration number: 0 with current cost as 0.4549975158283851 and parameters 
[-4.26569987  2.23743474 -2.12427953 -0.11653092  0.55388708 -2.77010897
  3.06858488  2.18960156  1.1855202  -1.0664833   0.60271521  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:41:07 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743401 -2.12427964 -0.11653103  0.55388677 -2.7701096
  3.06858436  2.18960145  1.18551998 -1.06648371  0.60271448  1.14432445
  1.31029867 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:42:50 2024]  Iteration number: 0 with current cost as 0.4011745314291728 and parameters 
[-4.37377097  2.23743432 -2.12427964 -0.11653103  0.55388677 -2.7701096
  3.06858436  2.18960145  1.18551967 -1.06648371  0.60271479  1.14432414
  1.31029867 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:46:31 2024]  Iteration number: 0 with current cost as 0.3470058589954329 and parameters 
[-20.14851494   2.23743464  -2.12426934  -0.11652588   0.55388708
  -2.77010897   3.06858498   2.1896066    1.18551998  -1.06648308
   0.6027151    1.14432445   1.31029899  -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:48:25 2024]  Iteration number: 0 with current cost as 0.4019619828723463 and parameters 
[-4.43042718  2.23743432 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 677.4659843444824 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 18:50:37 2024]  Iteration number: 0 with current cost as 0.4549975158283851 and parameters 
[-4.26569987  2.23743474 -2.12427953 -0.11653092  0.55388708 -2.77010897
  3.06858488  2.18960156  1.1855202  -1.0664833   0.60271521  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:52:25 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743401 -2.12427964 -0.11653103  0.55388677 -2.7701096
  3.06858436  2.18960145  1.18551998 -1.06648371  0.60271448  1.14432445
  1.31029867 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:54:07 2024]  Iteration number: 0 with current cost as 0.4011745314291728 and parameters 
[-4.37377097  2.23743432 -2.12427964 -0.11653103  0.55388677 -2.7701096
  3.06858436  2.18960145  1.18551967 -1.06648371  0.60271479  1.14432414
  1.31029867 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:57:52 2024]  Iteration number: 0 with current cost as 0.3470058589954329 and parameters 
[-20.14851494   2.23743464  -2.12426934  -0.11652588   0.55388708
  -2.77010897   3.06858498   2.1896066    1.18551998  -1.06648308
   0.6027151    1.14432445   1.31029899  -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:59:40 2024]  Iteration number: 0 with current cost as 0.4019619828723463 and parameters 
[-4.43042718  2.23743432 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 677.7489786148071 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 19:01:55 2024]  Iteration number: 0 with current cost as 0.4549975158283851 and parameters 
[-4.26569987  2.23743474 -2.12427953 -0.11653092  0.55388708 -2.77010897
  3.06858488  2.18960156  1.1855202  -1.0664833   0.60271521  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:03:38 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743401 -2.12427964 -0.11653103  0.55388677 -2.7701096
  3.06858436  2.18960145  1.18551998 -1.06648371  0.60271448  1.14432445
  1.31029867 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:05:36 2024]  Iteration number: 0 with current cost as 0.4011745314291728 and parameters 
[-4.37377097  2.23743432 -2.12427964 -0.11653103  0.55388677 -2.7701096
  3.06858436  2.18960145  1.18551967 -1.06648371  0.60271479  1.14432414
  1.31029867 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:09:34 2024]  Iteration number: 0 with current cost as 0.3470058589954329 and parameters 
[-20.14851494   2.23743464  -2.12426934  -0.11652588   0.55388708
  -2.77010897   3.06858498   2.1896066    1.18551998  -1.06648308
   0.6027151    1.14432445   1.31029899  -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:11:21 2024]  Iteration number: 0 with current cost as 0.4019619828723463 and parameters 
[-4.43042718  2.23743432 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 698.1053950786591 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 19:13:32 2024]  Iteration number: 0 with current cost as 0.4549975158283851 and parameters 
[-4.26569987  2.23743474 -2.12427953 -0.11653092  0.55388708 -2.77010897
  3.06858488  2.18960156  1.1855202  -1.0664833   0.60271521  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:15:14 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743401 -2.12427964 -0.11653103  0.55388677 -2.7701096
  3.06858436  2.18960145  1.18551998 -1.06648371  0.60271448  1.14432445
  1.31029867 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:17:01 2024]  Iteration number: 0 with current cost as 0.4011745314291728 and parameters 
[-4.37377097  2.23743432 -2.12427964 -0.11653103  0.55388677 -2.7701096
  3.06858436  2.18960145  1.18551967 -1.06648371  0.60271479  1.14432414
  1.31029867 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:20:44 2024]  Iteration number: 0 with current cost as 0.3470058589954329 and parameters 
[-20.14851494   2.23743464  -2.12426934  -0.11652588   0.55388708
  -2.77010897   3.06858498   2.1896066    1.18551998  -1.06648308
   0.6027151    1.14432445   1.31029899  -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:22:37 2024]  Iteration number: 0 with current cost as 0.4019619828723463 and parameters 
[-4.43042718  2.23743432 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 677.9655153751373 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 19:24:51 2024]  Iteration number: 0 with current cost as 0.4549975158283851 and parameters 
[-4.26569987  2.23743474 -2.12427953 -0.11653092  0.55388708 -2.77010897
  3.06858488  2.18960156  1.1855202  -1.0664833   0.60271521  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:26:38 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743401 -2.12427964 -0.11653103  0.55388677 -2.7701096
  3.06858436  2.18960145  1.18551998 -1.06648371  0.60271448  1.14432445
  1.31029867 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:28:20 2024]  Iteration number: 0 with current cost as 0.4011745314291728 and parameters 
[-4.37377097  2.23743432 -2.12427964 -0.11653103  0.55388677 -2.7701096
  3.06858436  2.18960145  1.18551967 -1.06648371  0.60271479  1.14432414
  1.31029867 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:32:16 2024]  Iteration number: 0 with current cost as 0.3470058589954329 and parameters 
[-20.14851494   2.23743464  -2.12426934  -0.11652588   0.55388708
  -2.77010897   3.06858498   2.1896066    1.18551998  -1.06648308
   0.6027151    1.14432445   1.31029899  -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:34:03 2024]  Iteration number: 0 with current cost as 0.4019619828723463 and parameters 
[-4.43042718  2.23743432 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 686.9182057380676 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 19:36:16 2024]  Iteration number: 0 with current cost as 0.4549975158283851 and parameters 
[-4.26569987  2.23743474 -2.12427953 -0.11653092  0.55388708 -2.77010897
  3.06858488  2.18960156  1.1855202  -1.0664833   0.60271521  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:37:58 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743401 -2.12427964 -0.11653103  0.55388677 -2.7701096
  3.06858436  2.18960145  1.18551998 -1.06648371  0.60271448  1.14432445
  1.31029867 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:39:42 2024]  Iteration number: 0 with current cost as 0.4011745314291728 and parameters 
[-4.37377097  2.23743432 -2.12427964 -0.11653103  0.55388677 -2.7701096
  3.06858436  2.18960145  1.18551967 -1.06648371  0.60271479  1.14432414
  1.31029867 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:43:27 2024]  Iteration number: 0 with current cost as 0.3470058589954329 and parameters 
[-20.14851494   2.23743464  -2.12426934  -0.11652588   0.55388708
  -2.77010897   3.06858498   2.1896066    1.18551998  -1.06648308
   0.6027151    1.14432445   1.31029899  -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:45:14 2024]  Iteration number: 0 with current cost as 0.4019619828723463 and parameters 
[-4.43042718  2.23743432 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 669.8152618408203 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 19:47:30 2024]  Iteration number: 0 with current cost as 0.4549975158283851 and parameters 
[-4.26569987  2.23743474 -2.12427953 -0.11653092  0.55388708 -2.77010897
  3.06858488  2.18960156  1.1855202  -1.0664833   0.60271521  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:49:12 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743401 -2.12427964 -0.11653103  0.55388677 -2.7701096
  3.06858436  2.18960145  1.18551998 -1.06648371  0.60271448  1.14432445
  1.31029867 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:50:54 2024]  Iteration number: 0 with current cost as 0.4011745314291728 and parameters 
[-4.37377097  2.23743432 -2.12427964 -0.11653103  0.55388677 -2.7701096
  3.06858436  2.18960145  1.18551967 -1.06648371  0.60271479  1.14432414
  1.31029867 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:54:47 2024]  Iteration number: 0 with current cost as 0.3470058589954329 and parameters 
[-20.14851494   2.23743464  -2.12426934  -0.11652588   0.55388708
  -2.77010897   3.06858498   2.1896066    1.18551998  -1.06648308
   0.6027151    1.14432445   1.31029899  -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:56:35 2024]  Iteration number: 0 with current cost as 0.4019619828723463 and parameters 
[-4.43042718  2.23743432 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 680.439624786377 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 19:58:47 2024]  Iteration number: 0 with current cost as 0.4549975158283851 and parameters 
[-4.26569987  2.23743474 -2.12427953 -0.11653092  0.55388708 -2.77010897
  3.06858488  2.18960156  1.1855202  -1.0664833   0.60271521  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 20:00:43 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743401 -2.12427964 -0.11653103  0.55388677 -2.7701096
  3.06858436  2.18960145  1.18551998 -1.06648371  0.60271448  1.14432445
  1.31029867 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 20:02:28 2024]  Iteration number: 0 with current cost as 0.4011745314291728 and parameters 
[-4.37377097  2.23743432 -2.12427964 -0.11653103  0.55388677 -2.7701096
  3.06858436  2.18960145  1.18551967 -1.06648371  0.60271479  1.14432414
  1.31029867 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:06:06 2024]  Iteration number: 0 with current cost as 0.3470058589954329 and parameters 
[-20.14851494   2.23743464  -2.12426934  -0.11652588   0.55388708
  -2.77010897   3.06858498   2.1896066    1.18551998  -1.06648308
   0.6027151    1.14432445   1.31029899  -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 20:07:50 2024]  Iteration number: 0 with current cost as 0.4019619828723463 and parameters 
[-4.43042718  2.23743432 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 674.1979682445526 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 20:10:00 2024]  Iteration number: 0 with current cost as 0.4549975158283851 and parameters 
[-4.26569987  2.23743474 -2.12427953 -0.11653092  0.55388708 -2.77010897
  3.06858488  2.18960156  1.1855202  -1.0664833   0.60271521  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 20:11:49 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743401 -2.12427964 -0.11653103  0.55388677 -2.7701096
  3.06858436  2.18960145  1.18551998 -1.06648371  0.60271448  1.14432445
  1.31029867 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 20:13:35 2024]  Iteration number: 0 with current cost as 0.4011745314291728 and parameters 
[-4.37377097  2.23743432 -2.12427964 -0.11653103  0.55388677 -2.7701096
  3.06858436  2.18960145  1.18551967 -1.06648371  0.60271479  1.14432414
  1.31029867 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:17:19 2024]  Iteration number: 0 with current cost as 0.3470058589954329 and parameters 
[-20.14851494   2.23743464  -2.12426934  -0.11652588   0.55388708
  -2.77010897   3.06858498   2.1896066    1.18551998  -1.06648308
   0.6027151    1.14432445   1.31029899  -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 20:19:01 2024]  Iteration number: 0 with current cost as 0.4019619828723463 and parameters 
[-4.43042718  2.23743432 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 671.6759462356567 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 20:21:13 2024]  Iteration number: 0 with current cost as 0.4549975158283851 and parameters 
[-4.26569987  2.23743474 -2.12427953 -0.11653092  0.55388708 -2.77010897
  3.06858488  2.18960156  1.1855202  -1.0664833   0.60271521  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 20:23:01 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743401 -2.12427964 -0.11653103  0.55388677 -2.7701096
  3.06858436  2.18960145  1.18551998 -1.06648371  0.60271448  1.14432445
  1.31029867 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 20:24:44 2024]  Iteration number: 0 with current cost as 0.4011745314291728 and parameters 
[-4.37377097  2.23743432 -2.12427964 -0.11653103  0.55388677 -2.7701096
  3.06858436  2.18960145  1.18551967 -1.06648371  0.60271479  1.14432414
  1.31029867 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:28:30 2024]  Iteration number: 0 with current cost as 0.3470058589954329 and parameters 
[-20.14851494   2.23743464  -2.12426934  -0.11652588   0.55388708
  -2.77010897   3.06858498   2.1896066    1.18551998  -1.06648308
   0.6027151    1.14432445   1.31029899  -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 20:30:11 2024]  Iteration number: 0 with current cost as 0.4019619828723463 and parameters 
[-4.43042718  2.23743432 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 674.0359387397766 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 20:32:32 2024]  Iteration number: 0 with current cost as 0.4549975158283851 and parameters 
[-4.26569987  2.23743474 -2.12427953 -0.11653092  0.55388708 -2.77010897
  3.06858488  2.18960156  1.1855202  -1.0664833   0.60271521  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 20:34:17 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743401 -2.12427964 -0.11653103  0.55388677 -2.7701096
  3.06858436  2.18960145  1.18551998 -1.06648371  0.60271448  1.14432445
  1.31029867 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 20:35:59 2024]  Iteration number: 0 with current cost as 0.4011745314291728 and parameters 
[-4.37377097  2.23743432 -2.12427964 -0.11653103  0.55388677 -2.7701096
  3.06858436  2.18960145  1.18551967 -1.06648371  0.60271479  1.14432414
  1.31029867 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:39:40 2024]  Iteration number: 0 with current cost as 0.3470058589954329 and parameters 
[-20.14851494   2.23743464  -2.12426934  -0.11652588   0.55388708
  -2.77010897   3.06858498   2.1896066    1.18551998  -1.06648308
   0.6027151    1.14432445   1.31029899  -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 20:41:22 2024]  Iteration number: 0 with current cost as 0.4019619828723463 and parameters 
[-4.43042718  2.23743432 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 676.9536244869232 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 20:43:47 2024]  Iteration number: 0 with current cost as 0.4549975158283851 and parameters 
[-4.26569987  2.23743474 -2.12427953 -0.11653092  0.55388708 -2.77010897
  3.06858488  2.18960156  1.1855202  -1.0664833   0.60271521  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 20:45:28 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743401 -2.12427964 -0.11653103  0.55388677 -2.7701096
  3.06858436  2.18960145  1.18551998 -1.06648371  0.60271448  1.14432445
  1.31029867 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 20:47:10 2024]  Iteration number: 0 with current cost as 0.4011745314291728 and parameters 
[-4.37377097  2.23743432 -2.12427964 -0.11653103  0.55388677 -2.7701096
  3.06858436  2.18960145  1.18551967 -1.06648371  0.60271479  1.14432414
  1.31029867 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:50:58 2024]  Iteration number: 0 with current cost as 0.3470058589954329 and parameters 
[-20.14851494   2.23743464  -2.12426934  -0.11652588   0.55388708
  -2.77010897   3.06858498   2.1896066    1.18551998  -1.06648308
   0.6027151    1.14432445   1.31029899  -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 20:52:43 2024]  Iteration number: 0 with current cost as 0.4019619828723463 and parameters 
[-4.43042718  2.23743432 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 670.2868762016296 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 20:54:54 2024]  Iteration number: 0 with current cost as 0.4549975158283851 and parameters 
[-4.26569987  2.23743474 -2.12427953 -0.11653092  0.55388708 -2.77010897
  3.06858488  2.18960156  1.1855202  -1.0664833   0.60271521  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 20:56:45 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743401 -2.12427964 -0.11653103  0.55388677 -2.7701096
  3.06858436  2.18960145  1.18551998 -1.06648371  0.60271448  1.14432445
  1.31029867 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 20:58:30 2024]  Iteration number: 0 with current cost as 0.4011745314291728 and parameters 
[-4.37377097  2.23743432 -2.12427964 -0.11653103  0.55388677 -2.7701096
  3.06858436  2.18960145  1.18551967 -1.06648371  0.60271479  1.14432414
  1.31029867 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 21:02:13 2024]  Iteration number: 0 with current cost as 0.3470058589954329 and parameters 
[-20.14851494   2.23743464  -2.12426934  -0.11652588   0.55388708
  -2.77010897   3.06858498   2.1896066    1.18551998  -1.06648308
   0.6027151    1.14432445   1.31029899  -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 21:03:59 2024]  Iteration number: 0 with current cost as 0.4019619828723463 and parameters 
[-4.43042718  2.23743432 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 679.0481917858124 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 21:06:15 2024]  Iteration number: 0 with current cost as 0.4549975158283851 and parameters 
[-4.26569987  2.23743474 -2.12427953 -0.11653092  0.55388708 -2.77010897
  3.06858488  2.18960156  1.1855202  -1.0664833   0.60271521  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 21:08:04 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743401 -2.12427964 -0.11653103  0.55388677 -2.7701096
  3.06858436  2.18960145  1.18551998 -1.06648371  0.60271448  1.14432445
  1.31029867 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 21:09:50 2024]  Iteration number: 0 with current cost as 0.4011745314291728 and parameters 
[-4.37377097  2.23743432 -2.12427964 -0.11653103  0.55388677 -2.7701096
  3.06858436  2.18960145  1.18551967 -1.06648371  0.60271479  1.14432414
  1.31029867 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 21:13:41 2024]  Iteration number: 0 with current cost as 0.3470058589954329 and parameters 
[-20.14851494   2.23743464  -2.12426934  -0.11652588   0.55388708
  -2.77010897   3.06858498   2.1896066    1.18551998  -1.06648308
   0.6027151    1.14432445   1.31029899  -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 21:15:29 2024]  Iteration number: 0 with current cost as 0.4019619828723463 and parameters 
[-4.43042718  2.23743432 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 687.1013638973236 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 21:17:39 2024]  Iteration number: 0 with current cost as 0.4549975158283851 and parameters 
[-4.26569987  2.23743474 -2.12427953 -0.11653092  0.55388708 -2.77010897
  3.06858488  2.18960156  1.1855202  -1.0664833   0.60271521  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 21:19:20 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743401 -2.12427964 -0.11653103  0.55388677 -2.7701096
  3.06858436  2.18960145  1.18551998 -1.06648371  0.60271448  1.14432445
  1.31029867 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 21:21:03 2024]  Iteration number: 0 with current cost as 0.4011745314291728 and parameters 
[-4.37377097  2.23743432 -2.12427964 -0.11653103  0.55388677 -2.7701096
  3.06858436  2.18960145  1.18551967 -1.06648371  0.60271479  1.14432414
  1.31029867 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 21:24:49 2024]  Iteration number: 0 with current cost as 0.3470058589954329 and parameters 
[-20.14851494   2.23743464  -2.12426934  -0.11652588   0.55388708
  -2.77010897   3.06858498   2.1896066    1.18551998  -1.06648308
   0.6027151    1.14432445   1.31029899  -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 21:26:31 2024]  Iteration number: 0 with current cost as 0.4019619828723463 and parameters 
[-4.43042718  2.23743432 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 661.6371629238129 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 21:28:41 2024]  Iteration number: 0 with current cost as 0.4549975158283851 and parameters 
[-4.26569987  2.23743474 -2.12427953 -0.11653092  0.55388708 -2.77010897
  3.06858488  2.18960156  1.1855202  -1.0664833   0.60271521  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 21:30:31 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743401 -2.12427964 -0.11653103  0.55388677 -2.7701096
  3.06858436  2.18960145  1.18551998 -1.06648371  0.60271448  1.14432445
  1.31029867 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 21:32:14 2024]  Iteration number: 0 with current cost as 0.4011745314291728 and parameters 
[-4.37377097  2.23743432 -2.12427964 -0.11653103  0.55388677 -2.7701096
  3.06858436  2.18960145  1.18551967 -1.06648371  0.60271479  1.14432414
  1.31029867 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 21:35:59 2024]  Iteration number: 0 with current cost as 0.3470058589954329 and parameters 
[-20.14851494   2.23743464  -2.12426934  -0.11652588   0.55388708
  -2.77010897   3.06858498   2.1896066    1.18551998  -1.06648308
   0.6027151    1.14432445   1.31029899  -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 21:37:45 2024]  Iteration number: 0 with current cost as 0.4019619828723463 and parameters 
[-4.43042718  2.23743432 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 673.8309330940247 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 21:39:54 2024]  Iteration number: 0 with current cost as 0.4549975158283851 and parameters 
[-4.26569987  2.23743474 -2.12427953 -0.11653092  0.55388708 -2.77010897
  3.06858488  2.18960156  1.1855202  -1.0664833   0.60271521  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 21:41:37 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743401 -2.12427964 -0.11653103  0.55388677 -2.7701096
  3.06858436  2.18960145  1.18551998 -1.06648371  0.60271448  1.14432445
  1.31029867 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 21:43:25 2024]  Iteration number: 0 with current cost as 0.4011745314291728 and parameters 
[-4.37377097  2.23743432 -2.12427964 -0.11653103  0.55388677 -2.7701096
  3.06858436  2.18960145  1.18551967 -1.06648371  0.60271479  1.14432414
  1.31029867 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 21:47:09 2024]  Iteration number: 0 with current cost as 0.3470058589954329 and parameters 
[-20.14851494   2.23743464  -2.12426934  -0.11652588   0.55388708
  -2.77010897   3.06858498   2.1896066    1.18551998  -1.06648308
   0.6027151    1.14432445   1.31029899  -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 21:48:56 2024]  Iteration number: 0 with current cost as 0.4019619828723463 and parameters 
[-4.43042718  2.23743432 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 678.4324760437012 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 21:51:20 2024]  Iteration number: 0 with current cost as 0.4549975158283851 and parameters 
[-4.26569987  2.23743474 -2.12427953 -0.11653092  0.55388708 -2.77010897
  3.06858488  2.18960156  1.1855202  -1.0664833   0.60271521  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 21:53:10 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743401 -2.12427964 -0.11653103  0.55388677 -2.7701096
  3.06858436  2.18960145  1.18551998 -1.06648371  0.60271448  1.14432445
  1.31029867 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 21:54:51 2024]  Iteration number: 0 with current cost as 0.4011745314291728 and parameters 
[-4.37377097  2.23743432 -2.12427964 -0.11653103  0.55388677 -2.7701096
  3.06858436  2.18960145  1.18551967 -1.06648371  0.60271479  1.14432414
  1.31029867 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 21:58:42 2024]  Iteration number: 0 with current cost as 0.3470058589954329 and parameters 
[-20.14851494   2.23743464  -2.12426934  -0.11652588   0.55388708
  -2.77010897   3.06858498   2.1896066    1.18551998  -1.06648308
   0.6027151    1.14432445   1.31029899  -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 22:00:25 2024]  Iteration number: 0 with current cost as 0.4019619828723463 and parameters 
[-4.43042718  2.23743432 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 682.6323835849762 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 22:02:38 2024]  Iteration number: 0 with current cost as 0.4549975158283851 and parameters 
[-4.26569987  2.23743474 -2.12427953 -0.11653092  0.55388708 -2.77010897
  3.06858488  2.18960156  1.1855202  -1.0664833   0.60271521  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 22:04:21 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743401 -2.12427964 -0.11653103  0.55388677 -2.7701096
  3.06858436  2.18960145  1.18551998 -1.06648371  0.60271448  1.14432445
  1.31029867 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 22:06:06 2024]  Iteration number: 0 with current cost as 0.4011745314291728 and parameters 
[-4.37377097  2.23743432 -2.12427964 -0.11653103  0.55388677 -2.7701096
  3.06858436  2.18960145  1.18551967 -1.06648371  0.60271479  1.14432414
  1.31029867 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 22:09:46 2024]  Iteration number: 0 with current cost as 0.3470058589954329 and parameters 
[-20.14851494   2.23743464  -2.12426934  -0.11652588   0.55388708
  -2.77010897   3.06858498   2.1896066    1.18551998  -1.06648308
   0.6027151    1.14432445   1.31029899  -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 22:11:30 2024]  Iteration number: 0 with current cost as 0.4019619828723463 and parameters 
[-4.43042718  2.23743432 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 664.9343750476837 seconds. 
Discarding model... 

Training complete taking 16926.00032734871 total seconds. 
Now scoring model... 
Scoring complete taking 1.7460517883300781 seconds. 
Saved predicted values as A2_Efficient-CRZ_predicted_values.csv
Model scores: {'MSE_train': (260.3130301213822,), 'R2_train': -0.2611292657414088, 'MAE_train': 13.78007493740001, 'MSE_test': 122.8497457252412, 'R2_test': 0.25948459387248346, 'MAE_test': 10.149093192959475}. 
Saved model results as A2_Efficient-CRZ_results.json. 
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Thu Apr  4 11:36:38 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 11:38:09 2024]  Iteration number: 0 with current cost as 0.4549975158283851 and parameters 
[-4.26569987  2.23743474 -2.12427953 -0.11653092  0.55388708 -2.77010897
  3.06858488  2.18960156  1.1855202  -1.0664833   0.60271521  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Apr  4 11:39:57 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743401 -2.12427964 -0.11653103  0.55388677 -2.7701096
  3.06858436  2.18960145  1.18551998 -1.06648371  0.60271448  1.14432445
  1.31029867 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 11:41:45 2024]  Iteration number: 0 with current cost as 0.4011745314291728 and parameters 
[-4.37377097  2.23743432 -2.12427964 -0.11653103  0.55388677 -2.7701096
  3.06858436  2.18960145  1.18551967 -1.06648371  0.60271479  1.14432414
  1.31029867 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 11:45:32 2024]  Iteration number: 0 with current cost as 0.3470058589954329 and parameters 
[-20.14851494   2.23743464  -2.12426934  -0.11652588   0.55388708
  -2.77010897   3.06858498   2.1896066    1.18551998  -1.06648308
   0.6027151    1.14432445   1.31029899  -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 11:47:17 2024]  Iteration number: 0 with current cost as 0.4019619828723463 and parameters 
[-4.43042718  2.23743432 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 686.1232824325562 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 11:49:36 2024]  Iteration number: 0 with current cost as 0.4549975158283851 and parameters 
[-4.26569987  2.23743474 -2.12427953 -0.11653092  0.55388708 -2.77010897
  3.06858488  2.18960156  1.1855202  -1.0664833   0.60271521  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Apr  4 11:51:22 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743401 -2.12427964 -0.11653103  0.55388677 -2.7701096
  3.06858436  2.18960145  1.18551998 -1.06648371  0.60271448  1.14432445
  1.31029867 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 11:53:08 2024]  Iteration number: 0 with current cost as 0.4011745314291728 and parameters 
[-4.37377097  2.23743432 -2.12427964 -0.11653103  0.55388677 -2.7701096
  3.06858436  2.18960145  1.18551967 -1.06648371  0.60271479  1.14432414
  1.31029867 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 11:57:02 2024]  Iteration number: 0 with current cost as 0.3470058589954329 and parameters 
[-20.14851494   2.23743464  -2.12426934  -0.11652588   0.55388708
  -2.77010897   3.06858498   2.1896066    1.18551998  -1.06648308
   0.6027151    1.14432445   1.31029899  -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 11:58:49 2024]  Iteration number: 0 with current cost as 0.4019619828723463 and parameters 
[-4.43042718  2.23743432 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 692.399254322052 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 12:01:12 2024]  Iteration number: 0 with current cost as 0.4549975158283851 and parameters 
[-4.26569987  2.23743474 -2.12427953 -0.11653092  0.55388708 -2.77010897
  3.06858488  2.18960156  1.1855202  -1.0664833   0.60271521  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Apr  4 12:03:03 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743401 -2.12427964 -0.11653103  0.55388677 -2.7701096
  3.06858436  2.18960145  1.18551998 -1.06648371  0.60271448  1.14432445
  1.31029867 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 12:04:53 2024]  Iteration number: 0 with current cost as 0.4011745314291728 and parameters 
[-4.37377097  2.23743432 -2.12427964 -0.11653103  0.55388677 -2.7701096
  3.06858436  2.18960145  1.18551967 -1.06648371  0.60271479  1.14432414
  1.31029867 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:08:43 2024]  Iteration number: 0 with current cost as 0.3470058589954329 and parameters 
[-20.14851494   2.23743464  -2.12426934  -0.11652588   0.55388708
  -2.77010897   3.06858498   2.1896066    1.18551998  -1.06648308
   0.6027151    1.14432445   1.31029899  -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:10:30 2024]  Iteration number: 0 with current cost as 0.4019619828723463 and parameters 
[-4.43042718  2.23743432 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 697.2934200763702 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 12:12:46 2024]  Iteration number: 0 with current cost as 0.4549975158283851 and parameters 
[-4.26569987  2.23743474 -2.12427953 -0.11653092  0.55388708 -2.77010897
  3.06858488  2.18960156  1.1855202  -1.0664833   0.60271521  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Apr  4 12:14:35 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743401 -2.12427964 -0.11653103  0.55388677 -2.7701096
  3.06858436  2.18960145  1.18551998 -1.06648371  0.60271448  1.14432445
  1.31029867 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 12:16:23 2024]  Iteration number: 0 with current cost as 0.4011745314291728 and parameters 
[-4.37377097  2.23743432 -2.12427964 -0.11653103  0.55388677 -2.7701096
  3.06858436  2.18960145  1.18551967 -1.06648371  0.60271479  1.14432414
  1.31029867 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:20:12 2024]  Iteration number: 0 with current cost as 0.3470058589954329 and parameters 
[-20.14851494   2.23743464  -2.12426934  -0.11652588   0.55388708
  -2.77010897   3.06858498   2.1896066    1.18551998  -1.06648308
   0.6027151    1.14432445   1.31029899  -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:22:02 2024]  Iteration number: 0 with current cost as 0.4019619828723463 and parameters 
[-4.43042718  2.23743432 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 694.2188215255737 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 12:24:20 2024]  Iteration number: 0 with current cost as 0.4549975158283851 and parameters 
[-4.26569987  2.23743474 -2.12427953 -0.11653092  0.55388708 -2.77010897
  3.06858488  2.18960156  1.1855202  -1.0664833   0.60271521  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Apr  4 12:26:08 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743401 -2.12427964 -0.11653103  0.55388677 -2.7701096
  3.06858436  2.18960145  1.18551998 -1.06648371  0.60271448  1.14432445
  1.31029867 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 12:27:54 2024]  Iteration number: 0 with current cost as 0.4011745314291728 and parameters 
[-4.37377097  2.23743432 -2.12427964 -0.11653103  0.55388677 -2.7701096
  3.06858436  2.18960145  1.18551967 -1.06648371  0.60271479  1.14432414
  1.31029867 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:31:45 2024]  Iteration number: 0 with current cost as 0.3470058589954329 and parameters 
[-20.14851494   2.23743464  -2.12426934  -0.11652588   0.55388708
  -2.77010897   3.06858498   2.1896066    1.18551998  -1.06648308
   0.6027151    1.14432445   1.31029899  -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:33:34 2024]  Iteration number: 0 with current cost as 0.4019619828723463 and parameters 
[-4.43042718  2.23743432 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 692.5966606140137 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 12:35:52 2024]  Iteration number: 0 with current cost as 0.4549975158283851 and parameters 
[-4.26569987  2.23743474 -2.12427953 -0.11653092  0.55388708 -2.77010897
  3.06858488  2.18960156  1.1855202  -1.0664833   0.60271521  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Apr  4 12:37:43 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743401 -2.12427964 -0.11653103  0.55388677 -2.7701096
  3.06858436  2.18960145  1.18551998 -1.06648371  0.60271448  1.14432445
  1.31029867 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 12:39:33 2024]  Iteration number: 0 with current cost as 0.4011745314291728 and parameters 
[-4.37377097  2.23743432 -2.12427964 -0.11653103  0.55388677 -2.7701096
  3.06858436  2.18960145  1.18551967 -1.06648371  0.60271479  1.14432414
  1.31029867 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:43:22 2024]  Iteration number: 0 with current cost as 0.3470058589954329 and parameters 
[-20.14851494   2.23743464  -2.12426934  -0.11652588   0.55388708
  -2.77010897   3.06858498   2.1896066    1.18551998  -1.06648308
   0.6027151    1.14432445   1.31029899  -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:45:12 2024]  Iteration number: 0 with current cost as 0.4019619828723463 and parameters 
[-4.43042718  2.23743432 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 699.3612563610077 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 12:47:32 2024]  Iteration number: 0 with current cost as 0.4549975158283851 and parameters 
[-4.26569987  2.23743474 -2.12427953 -0.11653092  0.55388708 -2.77010897
  3.06858488  2.18960156  1.1855202  -1.0664833   0.60271521  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Apr  4 12:49:21 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743401 -2.12427964 -0.11653103  0.55388677 -2.7701096
  3.06858436  2.18960145  1.18551998 -1.06648371  0.60271448  1.14432445
  1.31029867 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 12:51:08 2024]  Iteration number: 0 with current cost as 0.4011745314291728 and parameters 
[-4.37377097  2.23743432 -2.12427964 -0.11653103  0.55388677 -2.7701096
  3.06858436  2.18960145  1.18551967 -1.06648371  0.60271479  1.14432414
  1.31029867 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:54:59 2024]  Iteration number: 0 with current cost as 0.3470058589954329 and parameters 
[-20.14851494   2.23743464  -2.12426934  -0.11652588   0.55388708
  -2.77010897   3.06858498   2.1896066    1.18551998  -1.06648308
   0.6027151    1.14432445   1.31029899  -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:56:47 2024]  Iteration number: 0 with current cost as 0.4019619828723463 and parameters 
[-4.43042718  2.23743432 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 693.824845790863 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 12:59:05 2024]  Iteration number: 0 with current cost as 0.4549975158283851 and parameters 
[-4.26569987  2.23743474 -2.12427953 -0.11653092  0.55388708 -2.77010897
  3.06858488  2.18960156  1.1855202  -1.0664833   0.60271521  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Apr  4 13:00:51 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743401 -2.12427964 -0.11653103  0.55388677 -2.7701096
  3.06858436  2.18960145  1.18551998 -1.06648371  0.60271448  1.14432445
  1.31029867 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 13:02:39 2024]  Iteration number: 0 with current cost as 0.4011745314291728 and parameters 
[-4.37377097  2.23743432 -2.12427964 -0.11653103  0.55388677 -2.7701096
  3.06858436  2.18960145  1.18551967 -1.06648371  0.60271479  1.14432414
  1.31029867 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:06:29 2024]  Iteration number: 0 with current cost as 0.3470058589954329 and parameters 
[-20.14851494   2.23743464  -2.12426934  -0.11652588   0.55388708
  -2.77010897   3.06858498   2.1896066    1.18551998  -1.06648308
   0.6027151    1.14432445   1.31029899  -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:08:17 2024]  Iteration number: 0 with current cost as 0.4019619828723463 and parameters 
[-4.43042718  2.23743432 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 690.3877067565918 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 13:10:38 2024]  Iteration number: 0 with current cost as 0.4549975158283851 and parameters 
[-4.26569987  2.23743474 -2.12427953 -0.11653092  0.55388708 -2.77010897
  3.06858488  2.18960156  1.1855202  -1.0664833   0.60271521  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Apr  4 13:12:26 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743401 -2.12427964 -0.11653103  0.55388677 -2.7701096
  3.06858436  2.18960145  1.18551998 -1.06648371  0.60271448  1.14432445
  1.31029867 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 13:14:14 2024]  Iteration number: 0 with current cost as 0.4011745314291728 and parameters 
[-4.37377097  2.23743432 -2.12427964 -0.11653103  0.55388677 -2.7701096
  3.06858436  2.18960145  1.18551967 -1.06648371  0.60271479  1.14432414
  1.31029867 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:18:02 2024]  Iteration number: 0 with current cost as 0.3470058589954329 and parameters 
[-20.14851494   2.23743464  -2.12426934  -0.11652588   0.55388708
  -2.77010897   3.06858498   2.1896066    1.18551998  -1.06648308
   0.6027151    1.14432445   1.31029899  -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:19:49 2024]  Iteration number: 0 with current cost as 0.4019619828723463 and parameters 
[-4.43042718  2.23743432 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 691.5185832977295 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 13:22:07 2024]  Iteration number: 0 with current cost as 0.4549975158283851 and parameters 
[-4.26569987  2.23743474 -2.12427953 -0.11653092  0.55388708 -2.77010897
  3.06858488  2.18960156  1.1855202  -1.0664833   0.60271521  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Apr  4 13:23:54 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743401 -2.12427964 -0.11653103  0.55388677 -2.7701096
  3.06858436  2.18960145  1.18551998 -1.06648371  0.60271448  1.14432445
  1.31029867 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 13:25:40 2024]  Iteration number: 0 with current cost as 0.4011745314291728 and parameters 
[-4.37377097  2.23743432 -2.12427964 -0.11653103  0.55388677 -2.7701096
  3.06858436  2.18960145  1.18551967 -1.06648371  0.60271479  1.14432414
  1.31029867 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:29:28 2024]  Iteration number: 0 with current cost as 0.3470058589954329 and parameters 
[-20.14851494   2.23743464  -2.12426934  -0.11652588   0.55388708
  -2.77010897   3.06858498   2.1896066    1.18551998  -1.06648308
   0.6027151    1.14432445   1.31029899  -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:31:17 2024]  Iteration number: 0 with current cost as 0.4019619828723463 and parameters 
[-4.43042718  2.23743432 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 687.3848316669464 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 13:33:38 2024]  Iteration number: 0 with current cost as 0.4549975158283851 and parameters 
[-4.26569987  2.23743474 -2.12427953 -0.11653092  0.55388708 -2.77010897
  3.06858488  2.18960156  1.1855202  -1.0664833   0.60271521  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Apr  4 13:35:24 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743401 -2.12427964 -0.11653103  0.55388677 -2.7701096
  3.06858436  2.18960145  1.18551998 -1.06648371  0.60271448  1.14432445
  1.31029867 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 13:37:11 2024]  Iteration number: 0 with current cost as 0.4011745314291728 and parameters 
[-4.37377097  2.23743432 -2.12427964 -0.11653103  0.55388677 -2.7701096
  3.06858436  2.18960145  1.18551967 -1.06648371  0.60271479  1.14432414
  1.31029867 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:41:02 2024]  Iteration number: 0 with current cost as 0.3470058589954329 and parameters 
[-20.14851494   2.23743464  -2.12426934  -0.11652588   0.55388708
  -2.77010897   3.06858498   2.1896066    1.18551998  -1.06648308
   0.6027151    1.14432445   1.31029899  -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:42:46 2024]  Iteration number: 0 with current cost as 0.4019619828723463 and parameters 
[-4.43042718  2.23743432 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 689.5991277694702 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 13:45:03 2024]  Iteration number: 0 with current cost as 0.4549975158283851 and parameters 
[-4.26569987  2.23743474 -2.12427953 -0.11653092  0.55388708 -2.77010897
  3.06858488  2.18960156  1.1855202  -1.0664833   0.60271521  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Apr  4 13:46:50 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743401 -2.12427964 -0.11653103  0.55388677 -2.7701096
  3.06858436  2.18960145  1.18551998 -1.06648371  0.60271448  1.14432445
  1.31029867 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 13:48:36 2024]  Iteration number: 0 with current cost as 0.4011745314291728 and parameters 
[-4.37377097  2.23743432 -2.12427964 -0.11653103  0.55388677 -2.7701096
  3.06858436  2.18960145  1.18551967 -1.06648371  0.60271479  1.14432414
  1.31029867 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:52:26 2024]  Iteration number: 0 with current cost as 0.3470058589954329 and parameters 
[-20.14851494   2.23743464  -2.12426934  -0.11652588   0.55388708
  -2.77010897   3.06858498   2.1896066    1.18551998  -1.06648308
   0.6027151    1.14432445   1.31029899  -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:54:15 2024]  Iteration number: 0 with current cost as 0.4019619828723463 and parameters 
[-4.43042718  2.23743432 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 689.3026905059814 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 13:56:33 2024]  Iteration number: 0 with current cost as 0.4549975158283851 and parameters 
[-4.26569987  2.23743474 -2.12427953 -0.11653092  0.55388708 -2.77010897
  3.06858488  2.18960156  1.1855202  -1.0664833   0.60271521  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Apr  4 13:58:20 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743401 -2.12427964 -0.11653103  0.55388677 -2.7701096
  3.06858436  2.18960145  1.18551998 -1.06648371  0.60271448  1.14432445
  1.31029867 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 14:00:08 2024]  Iteration number: 0 with current cost as 0.4011745314291728 and parameters 
[-4.37377097  2.23743432 -2.12427964 -0.11653103  0.55388677 -2.7701096
  3.06858436  2.18960145  1.18551967 -1.06648371  0.60271479  1.14432414
  1.31029867 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 14:04:00 2024]  Iteration number: 0 with current cost as 0.3470058589954329 and parameters 
[-20.14851494   2.23743464  -2.12426934  -0.11652588   0.55388708
  -2.77010897   3.06858498   2.1896066    1.18551998  -1.06648308
   0.6027151    1.14432445   1.31029899  -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 14:05:49 2024]  Iteration number: 0 with current cost as 0.4019619828723463 and parameters 
[-4.43042718  2.23743432 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 693.2947051525116 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 14:08:05 2024]  Iteration number: 0 with current cost as 0.4549975158283851 and parameters 
[-4.26569987  2.23743474 -2.12427953 -0.11653092  0.55388708 -2.77010897
  3.06858488  2.18960156  1.1855202  -1.0664833   0.60271521  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Apr  4 14:09:55 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743401 -2.12427964 -0.11653103  0.55388677 -2.7701096
  3.06858436  2.18960145  1.18551998 -1.06648371  0.60271448  1.14432445
  1.31029867 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 14:11:41 2024]  Iteration number: 0 with current cost as 0.4011745314291728 and parameters 
[-4.37377097  2.23743432 -2.12427964 -0.11653103  0.55388677 -2.7701096
  3.06858436  2.18960145  1.18551967 -1.06648371  0.60271479  1.14432414
  1.31029867 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 14:15:36 2024]  Iteration number: 0 with current cost as 0.3470058589954329 and parameters 
[-20.14851494   2.23743464  -2.12426934  -0.11652588   0.55388708
  -2.77010897   3.06858498   2.1896066    1.18551998  -1.06648308
   0.6027151    1.14432445   1.31029899  -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 14:17:24 2024]  Iteration number: 0 with current cost as 0.4019619828723463 and parameters 
[-4.43042718  2.23743432 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 696.3995125293732 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 14:19:44 2024]  Iteration number: 0 with current cost as 0.4549975158283851 and parameters 
[-4.26569987  2.23743474 -2.12427953 -0.11653092  0.55388708 -2.77010897
  3.06858488  2.18960156  1.1855202  -1.0664833   0.60271521  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Apr  4 14:21:33 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743401 -2.12427964 -0.11653103  0.55388677 -2.7701096
  3.06858436  2.18960145  1.18551998 -1.06648371  0.60271448  1.14432445
  1.31029867 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 14:23:22 2024]  Iteration number: 0 with current cost as 0.4011745314291728 and parameters 
[-4.37377097  2.23743432 -2.12427964 -0.11653103  0.55388677 -2.7701096
  3.06858436  2.18960145  1.18551967 -1.06648371  0.60271479  1.14432414
  1.31029867 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 14:27:11 2024]  Iteration number: 0 with current cost as 0.3470058589954329 and parameters 
[-20.14851494   2.23743464  -2.12426934  -0.11652588   0.55388708
  -2.77010897   3.06858498   2.1896066    1.18551998  -1.06648308
   0.6027151    1.14432445   1.31029899  -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 14:28:59 2024]  Iteration number: 0 with current cost as 0.4019619828723463 and parameters 
[-4.43042718  2.23743432 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 694.6535985469818 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 14:31:20 2024]  Iteration number: 0 with current cost as 0.4549975158283851 and parameters 
[-4.26569987  2.23743474 -2.12427953 -0.11653092  0.55388708 -2.77010897
  3.06858488  2.18960156  1.1855202  -1.0664833   0.60271521  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Apr  4 14:33:07 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743401 -2.12427964 -0.11653103  0.55388677 -2.7701096
  3.06858436  2.18960145  1.18551998 -1.06648371  0.60271448  1.14432445
  1.31029867 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 14:34:54 2024]  Iteration number: 0 with current cost as 0.4011745314291728 and parameters 
[-4.37377097  2.23743432 -2.12427964 -0.11653103  0.55388677 -2.7701096
  3.06858436  2.18960145  1.18551967 -1.06648371  0.60271479  1.14432414
  1.31029867 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 14:38:42 2024]  Iteration number: 0 with current cost as 0.3470058589954329 and parameters 
[-20.14851494   2.23743464  -2.12426934  -0.11652588   0.55388708
  -2.77010897   3.06858498   2.1896066    1.18551998  -1.06648308
   0.6027151    1.14432445   1.31029899  -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 14:40:29 2024]  Iteration number: 0 with current cost as 0.4019619828723463 and parameters 
[-4.43042718  2.23743432 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 689.8806161880493 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 14:42:49 2024]  Iteration number: 0 with current cost as 0.4549975158283851 and parameters 
[-4.26569987  2.23743474 -2.12427953 -0.11653092  0.55388708 -2.77010897
  3.06858488  2.18960156  1.1855202  -1.0664833   0.60271521  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Apr  4 14:44:37 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743401 -2.12427964 -0.11653103  0.55388677 -2.7701096
  3.06858436  2.18960145  1.18551998 -1.06648371  0.60271448  1.14432445
  1.31029867 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 14:46:23 2024]  Iteration number: 0 with current cost as 0.4011745314291728 and parameters 
[-4.37377097  2.23743432 -2.12427964 -0.11653103  0.55388677 -2.7701096
  3.06858436  2.18960145  1.18551967 -1.06648371  0.60271479  1.14432414
  1.31029867 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 14:50:12 2024]  Iteration number: 0 with current cost as 0.3470058589954329 and parameters 
[-20.14851494   2.23743464  -2.12426934  -0.11652588   0.55388708
  -2.77010897   3.06858498   2.1896066    1.18551998  -1.06648308
   0.6027151    1.14432445   1.31029899  -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 14:52:03 2024]  Iteration number: 0 with current cost as 0.4019619828723463 and parameters 
[-4.43042718  2.23743432 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 693.8045456409454 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 14:54:20 2024]  Iteration number: 0 with current cost as 0.4549975158283851 and parameters 
[-4.26569987  2.23743474 -2.12427953 -0.11653092  0.55388708 -2.77010897
  3.06858488  2.18960156  1.1855202  -1.0664833   0.60271521  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Apr  4 14:56:08 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743401 -2.12427964 -0.11653103  0.55388677 -2.7701096
  3.06858436  2.18960145  1.18551998 -1.06648371  0.60271448  1.14432445
  1.31029867 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 14:57:56 2024]  Iteration number: 0 with current cost as 0.4011745314291728 and parameters 
[-4.37377097  2.23743432 -2.12427964 -0.11653103  0.55388677 -2.7701096
  3.06858436  2.18960145  1.18551967 -1.06648371  0.60271479  1.14432414
  1.31029867 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 15:01:47 2024]  Iteration number: 0 with current cost as 0.3470058589954329 and parameters 
[-20.14851494   2.23743464  -2.12426934  -0.11652588   0.55388708
  -2.77010897   3.06858498   2.1896066    1.18551998  -1.06648308
   0.6027151    1.14432445   1.31029899  -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 15:03:36 2024]  Iteration number: 0 with current cost as 0.4019619828723463 and parameters 
[-4.43042718  2.23743432 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 693.0506205558777 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 15:05:54 2024]  Iteration number: 0 with current cost as 0.4549975158283851 and parameters 
[-4.26569987  2.23743474 -2.12427953 -0.11653092  0.55388708 -2.77010897
  3.06858488  2.18960156  1.1855202  -1.0664833   0.60271521  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Apr  4 15:07:44 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743401 -2.12427964 -0.11653103  0.55388677 -2.7701096
  3.06858436  2.18960145  1.18551998 -1.06648371  0.60271448  1.14432445
  1.31029867 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 15:09:32 2024]  Iteration number: 0 with current cost as 0.4011745314291728 and parameters 
[-4.37377097  2.23743432 -2.12427964 -0.11653103  0.55388677 -2.7701096
  3.06858436  2.18960145  1.18551967 -1.06648371  0.60271479  1.14432414
  1.31029867 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 15:13:29 2024]  Iteration number: 0 with current cost as 0.3470058589954329 and parameters 
[-20.14851494   2.23743464  -2.12426934  -0.11652588   0.55388708
  -2.77010897   3.06858498   2.1896066    1.18551998  -1.06648308
   0.6027151    1.14432445   1.31029899  -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 15:15:16 2024]  Iteration number: 0 with current cost as 0.4019619828723463 and parameters 
[-4.43042718  2.23743432 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 698.1110517978668 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 15:17:33 2024]  Iteration number: 0 with current cost as 0.4549975158283851 and parameters 
[-4.26569987  2.23743474 -2.12427953 -0.11653092  0.55388708 -2.77010897
  3.06858488  2.18960156  1.1855202  -1.0664833   0.60271521  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Apr  4 15:19:27 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743401 -2.12427964 -0.11653103  0.55388677 -2.7701096
  3.06858436  2.18960145  1.18551998 -1.06648371  0.60271448  1.14432445
  1.31029867 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 15:21:16 2024]  Iteration number: 0 with current cost as 0.4011745314291728 and parameters 
[-4.37377097  2.23743432 -2.12427964 -0.11653103  0.55388677 -2.7701096
  3.06858436  2.18960145  1.18551967 -1.06648371  0.60271479  1.14432414
  1.31029867 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 15:25:09 2024]  Iteration number: 0 with current cost as 0.3470058589954329 and parameters 
[-20.14851494   2.23743464  -2.12426934  -0.11652588   0.55388708
  -2.77010897   3.06858498   2.1896066    1.18551998  -1.06648308
   0.6027151    1.14432445   1.31029899  -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 15:27:00 2024]  Iteration number: 0 with current cost as 0.4019619828723463 and parameters 
[-4.43042718  2.23743432 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 705.1930840015411 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 15:29:16 2024]  Iteration number: 0 with current cost as 0.4549975158283851 and parameters 
[-4.26569987  2.23743474 -2.12427953 -0.11653092  0.55388708 -2.77010897
  3.06858488  2.18960156  1.1855202  -1.0664833   0.60271521  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Apr  4 15:31:05 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743401 -2.12427964 -0.11653103  0.55388677 -2.7701096
  3.06858436  2.18960145  1.18551998 -1.06648371  0.60271448  1.14432445
  1.31029867 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 15:32:56 2024]  Iteration number: 0 with current cost as 0.4011745314291728 and parameters 
[-4.37377097  2.23743432 -2.12427964 -0.11653103  0.55388677 -2.7701096
  3.06858436  2.18960145  1.18551967 -1.06648371  0.60271479  1.14432414
  1.31029867 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 15:36:42 2024]  Iteration number: 0 with current cost as 0.3470058589954329 and parameters 
[-20.14851494   2.23743464  -2.12426934  -0.11652588   0.55388708
  -2.77010897   3.06858498   2.1896066    1.18551998  -1.06648308
   0.6027151    1.14432445   1.31029899  -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 15:38:28 2024]  Iteration number: 0 with current cost as 0.4019619828723463 and parameters 
[-4.43042718  2.23743432 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 686.4432127475739 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 15:40:42 2024]  Iteration number: 0 with current cost as 0.4549975158283851 and parameters 
[-4.26569987  2.23743474 -2.12427953 -0.11653092  0.55388708 -2.77010897
  3.06858488  2.18960156  1.1855202  -1.0664833   0.60271521  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Apr  4 15:42:25 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743401 -2.12427964 -0.11653103  0.55388677 -2.7701096
  3.06858436  2.18960145  1.18551998 -1.06648371  0.60271448  1.14432445
  1.31029867 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 15:44:12 2024]  Iteration number: 0 with current cost as 0.4011745314291728 and parameters 
[-4.37377097  2.23743432 -2.12427964 -0.11653103  0.55388677 -2.7701096
  3.06858436  2.18960145  1.18551967 -1.06648371  0.60271479  1.14432414
  1.31029867 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 15:47:55 2024]  Iteration number: 0 with current cost as 0.3470058589954329 and parameters 
[-20.14851494   2.23743464  -2.12426934  -0.11652588   0.55388708
  -2.77010897   3.06858498   2.1896066    1.18551998  -1.06648308
   0.6027151    1.14432445   1.31029899  -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 15:49:41 2024]  Iteration number: 0 with current cost as 0.4019619828723463 and parameters 
[-4.43042718  2.23743432 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 673.7900240421295 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 15:51:56 2024]  Iteration number: 0 with current cost as 0.4549975158283851 and parameters 
[-4.26569987  2.23743474 -2.12427953 -0.11653092  0.55388708 -2.77010897
  3.06858488  2.18960156  1.1855202  -1.0664833   0.60271521  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Apr  4 15:53:42 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743401 -2.12427964 -0.11653103  0.55388677 -2.7701096
  3.06858436  2.18960145  1.18551998 -1.06648371  0.60271448  1.14432445
  1.31029867 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 15:55:25 2024]  Iteration number: 0 with current cost as 0.4011745314291728 and parameters 
[-4.37377097  2.23743432 -2.12427964 -0.11653103  0.55388677 -2.7701096
  3.06858436  2.18960145  1.18551967 -1.06648371  0.60271479  1.14432414
  1.31029867 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 15:59:08 2024]  Iteration number: 0 with current cost as 0.3470058589954329 and parameters 
[-20.14851494   2.23743464  -2.12426934  -0.11652588   0.55388708
  -2.77010897   3.06858498   2.1896066    1.18551998  -1.06648308
   0.6027151    1.14432445   1.31029899  -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 16:00:52 2024]  Iteration number: 0 with current cost as 0.4019619828723463 and parameters 
[-4.43042718  2.23743432 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 672.7857730388641 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 16:03:09 2024]  Iteration number: 0 with current cost as 0.4549975158283851 and parameters 
[-4.26569987  2.23743474 -2.12427953 -0.11653092  0.55388708 -2.77010897
  3.06858488  2.18960156  1.1855202  -1.0664833   0.60271521  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Apr  4 16:04:54 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743401 -2.12427964 -0.11653103  0.55388677 -2.7701096
  3.06858436  2.18960145  1.18551998 -1.06648371  0.60271448  1.14432445
  1.31029867 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 16:06:39 2024]  Iteration number: 0 with current cost as 0.4011745314291728 and parameters 
[-4.37377097  2.23743432 -2.12427964 -0.11653103  0.55388677 -2.7701096
  3.06858436  2.18960145  1.18551967 -1.06648371  0.60271479  1.14432414
  1.31029867 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 16:10:22 2024]  Iteration number: 0 with current cost as 0.3470058589954329 and parameters 
[-20.14851494   2.23743464  -2.12426934  -0.11652588   0.55388708
  -2.77010897   3.06858498   2.1896066    1.18551998  -1.06648308
   0.6027151    1.14432445   1.31029899  -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 16:12:07 2024]  Iteration number: 0 with current cost as 0.4019619828723463 and parameters 
[-4.43042718  2.23743432 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 674.5543751716614 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 16:14:23 2024]  Iteration number: 0 with current cost as 0.4549975158283851 and parameters 
[-4.26569987  2.23743474 -2.12427953 -0.11653092  0.55388708 -2.77010897
  3.06858488  2.18960156  1.1855202  -1.0664833   0.60271521  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Apr  4 16:16:08 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743401 -2.12427964 -0.11653103  0.55388677 -2.7701096
  3.06858436  2.18960145  1.18551998 -1.06648371  0.60271448  1.14432445
  1.31029867 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 16:17:52 2024]  Iteration number: 0 with current cost as 0.4011745314291728 and parameters 
[-4.37377097  2.23743432 -2.12427964 -0.11653103  0.55388677 -2.7701096
  3.06858436  2.18960145  1.18551967 -1.06648371  0.60271479  1.14432414
  1.31029867 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 16:21:39 2024]  Iteration number: 0 with current cost as 0.3470058589954329 and parameters 
[-20.14851494   2.23743464  -2.12426934  -0.11652588   0.55388708
  -2.77010897   3.06858498   2.1896066    1.18551998  -1.06648308
   0.6027151    1.14432445   1.31029899  -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 16:23:26 2024]  Iteration number: 0 with current cost as 0.4019619828723463 and parameters 
[-4.43042718  2.23743432 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 679.2208304405212 seconds. 
Discarding model... 

Training complete taking 17255.19445872307 total seconds. 
Now scoring model... 
Scoring complete taking 1.825944185256958 seconds. 
Saved predicted values as A2_Efficient-CRZ_predicted_values.csv
Model scores: {'MSE_train': (260.3130301213822,), 'R2_train': -0.2611292657414088, 'MAE_train': 13.78007493740001, 'MSE_test': 122.8497457252412, 'R2_test': 0.25948459387248346, 'MAE_test': 10.149093192959475}. 
Saved model results as A2_Efficient-CRZ_results.json. 
