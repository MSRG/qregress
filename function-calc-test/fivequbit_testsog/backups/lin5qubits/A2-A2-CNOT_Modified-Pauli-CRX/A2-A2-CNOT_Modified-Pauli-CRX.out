/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Sun Mar 24 17:56:50 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 17:57:22 2024]  Iteration number: 0 with current cost as 0.3946272296631361 and parameters 
[-2.82278494  2.0256901  -2.40223636 -0.11653103  0.55388708 -2.77010901
  3.06858495  2.18960141  1.18551995 -1.06648312  1.67589967  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:02:02 2024]  Iteration number: 0 with current cost as 0.3329604614022771 and parameters 
[-2.99138249  1.96502368 -2.3306498  -0.11653103  0.55388704 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648316  2.00049886  1.14432445
  1.31029899 -1.87354673  0.72965073  2.88578416 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:08:05 2024]  Iteration number: 0 with current cost as 0.3345007694083931 and parameters 
[-2.95591232  1.98219276 -2.37226945 -0.11653103  0.55388712 -2.77010897
  3.06858502  2.18960149  1.18552002 -1.06648312  1.91767049  1.14432449
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:14:01 2024]  Iteration number: 0 with current cost as 0.3321590371089672 and parameters 
[-2.95929422  1.989361   -2.3831704  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648312  1.9106503   1.14432445
  1.31029906 -1.87354673  0.72965077  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:19:32 2024]  Iteration number: 0 with current cost as 0.34649450653514635 and parameters 
[-2.94362125  1.98017033 -2.35602868 -0.11653095  0.55388708 -2.77010894
  3.06858502  2.18960149  1.18552006 -1.06648308  1.90922735  1.14432449
  1.31029906 -1.87354673  0.7296508   2.88578419 -0.5453432  -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1656.0055372714996 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 18:24:57 2024]  Iteration number: 0 with current cost as 0.3946272296631361 and parameters 
[-2.82278494  2.0256901  -2.40223636 -0.11653103  0.55388708 -2.77010901
  3.06858495  2.18960141  1.18551995 -1.06648312  1.67589967  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:29:39 2024]  Iteration number: 0 with current cost as 0.3329604614022771 and parameters 
[-2.99138249  1.96502368 -2.3306498  -0.11653103  0.55388704 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648316  2.00049886  1.14432445
  1.31029899 -1.87354673  0.72965073  2.88578416 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:35:46 2024]  Iteration number: 0 with current cost as 0.3345007694083931 and parameters 
[-2.95591232  1.98219276 -2.37226945 -0.11653103  0.55388712 -2.77010897
  3.06858502  2.18960149  1.18552002 -1.06648312  1.91767049  1.14432449
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:41:33 2024]  Iteration number: 0 with current cost as 0.3321590371089672 and parameters 
[-2.95929422  1.989361   -2.3831704  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648312  1.9106503   1.14432445
  1.31029906 -1.87354673  0.72965077  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:47:03 2024]  Iteration number: 0 with current cost as 0.34649450653514635 and parameters 
[-2.94362125  1.98017033 -2.35602868 -0.11653095  0.55388708 -2.77010894
  3.06858502  2.18960149  1.18552006 -1.06648308  1.90922735  1.14432449
  1.31029906 -1.87354673  0.7296508   2.88578419 -0.5453432  -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1645.1677997112274 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 18:52:22 2024]  Iteration number: 0 with current cost as 0.3946272296631361 and parameters 
[-2.82278494  2.0256901  -2.40223636 -0.11653103  0.55388708 -2.77010901
  3.06858495  2.18960141  1.18551995 -1.06648312  1.67589967  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:57:00 2024]  Iteration number: 0 with current cost as 0.3329604614022771 and parameters 
[-2.99138249  1.96502368 -2.3306498  -0.11653103  0.55388704 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648316  2.00049886  1.14432445
  1.31029899 -1.87354673  0.72965073  2.88578416 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:03:03 2024]  Iteration number: 0 with current cost as 0.3345007694083931 and parameters 
[-2.95591232  1.98219276 -2.37226945 -0.11653103  0.55388712 -2.77010897
  3.06858502  2.18960149  1.18552002 -1.06648312  1.91767049  1.14432449
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:08:54 2024]  Iteration number: 0 with current cost as 0.3321590371089672 and parameters 
[-2.95929422  1.989361   -2.3831704  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648312  1.9106503   1.14432445
  1.31029906 -1.87354673  0.72965077  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:14:26 2024]  Iteration number: 0 with current cost as 0.34649450653514635 and parameters 
[-2.94362125  1.98017033 -2.35602868 -0.11653095  0.55388708 -2.77010894
  3.06858502  2.18960149  1.18552006 -1.06648308  1.90922735  1.14432449
  1.31029906 -1.87354673  0.7296508   2.88578419 -0.5453432  -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1670.922236442566 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 19:20:13 2024]  Iteration number: 0 with current cost as 0.3946272296631361 and parameters 
[-2.82278494  2.0256901  -2.40223636 -0.11653103  0.55388708 -2.77010901
  3.06858495  2.18960141  1.18551995 -1.06648312  1.67589967  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:24:50 2024]  Iteration number: 0 with current cost as 0.3329604614022771 and parameters 
[-2.99138249  1.96502368 -2.3306498  -0.11653103  0.55388704 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648316  2.00049886  1.14432445
  1.31029899 -1.87354673  0.72965073  2.88578416 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:30:59 2024]  Iteration number: 0 with current cost as 0.3345007694083931 and parameters 
[-2.95591232  1.98219276 -2.37226945 -0.11653103  0.55388712 -2.77010897
  3.06858502  2.18960149  1.18552002 -1.06648312  1.91767049  1.14432449
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:37:04 2024]  Iteration number: 0 with current cost as 0.3321590371089672 and parameters 
[-2.95929422  1.989361   -2.3831704  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648312  1.9106503   1.14432445
  1.31029906 -1.87354673  0.72965077  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:42:38 2024]  Iteration number: 0 with current cost as 0.34649450653514635 and parameters 
[-2.94362125  1.98017033 -2.35602868 -0.11653095  0.55388708 -2.77010894
  3.06858502  2.18960149  1.18552006 -1.06648308  1.90922735  1.14432449
  1.31029906 -1.87354673  0.7296508   2.88578419 -0.5453432  -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1672.8482103347778 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 19:48:06 2024]  Iteration number: 0 with current cost as 0.3946272296631361 and parameters 
[-2.82278494  2.0256901  -2.40223636 -0.11653103  0.55388708 -2.77010901
  3.06858495  2.18960141  1.18551995 -1.06648312  1.67589967  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:52:41 2024]  Iteration number: 0 with current cost as 0.3329604614022771 and parameters 
[-2.99138249  1.96502368 -2.3306498  -0.11653103  0.55388704 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648316  2.00049886  1.14432445
  1.31029899 -1.87354673  0.72965073  2.88578416 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:58:38 2024]  Iteration number: 0 with current cost as 0.3345007694083931 and parameters 
[-2.95591232  1.98219276 -2.37226945 -0.11653103  0.55388712 -2.77010897
  3.06858502  2.18960149  1.18552002 -1.06648312  1.91767049  1.14432449
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:04:28 2024]  Iteration number: 0 with current cost as 0.3321590371089672 and parameters 
[-2.95929422  1.989361   -2.3831704  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648312  1.9106503   1.14432445
  1.31029906 -1.87354673  0.72965077  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 20:10:08 2024]  Iteration number: 0 with current cost as 0.34649450653514635 and parameters 
[-2.94362125  1.98017033 -2.35602868 -0.11653095  0.55388708 -2.77010894
  3.06858502  2.18960149  1.18552006 -1.06648308  1.90922735  1.14432449
  1.31029906 -1.87354673  0.7296508   2.88578419 -0.5453432  -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1643.215057849884 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 20:15:29 2024]  Iteration number: 0 with current cost as 0.3946272296631361 and parameters 
[-2.82278494  2.0256901  -2.40223636 -0.11653103  0.55388708 -2.77010901
  3.06858495  2.18960141  1.18551995 -1.06648312  1.67589967  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 20:20:06 2024]  Iteration number: 0 with current cost as 0.3329604614022771 and parameters 
[-2.99138249  1.96502368 -2.3306498  -0.11653103  0.55388704 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648316  2.00049886  1.14432445
  1.31029899 -1.87354673  0.72965073  2.88578416 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 20:26:03 2024]  Iteration number: 0 with current cost as 0.3345007694083931 and parameters 
[-2.95591232  1.98219276 -2.37226945 -0.11653103  0.55388712 -2.77010897
  3.06858502  2.18960149  1.18552002 -1.06648312  1.91767049  1.14432449
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:31:51 2024]  Iteration number: 0 with current cost as 0.3321590371089672 and parameters 
[-2.95929422  1.989361   -2.3831704  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648312  1.9106503   1.14432445
  1.31029906 -1.87354673  0.72965077  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 20:37:17 2024]  Iteration number: 0 with current cost as 0.34649450653514635 and parameters 
[-2.94362125  1.98017033 -2.35602868 -0.11653095  0.55388708 -2.77010894
  3.06858502  2.18960149  1.18552006 -1.06648308  1.90922735  1.14432449
  1.31029906 -1.87354673  0.7296508   2.88578419 -0.5453432  -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1623.7770097255707 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 20:42:33 2024]  Iteration number: 0 with current cost as 0.3946272296631361 and parameters 
[-2.82278494  2.0256901  -2.40223636 -0.11653103  0.55388708 -2.77010901
  3.06858495  2.18960141  1.18551995 -1.06648312  1.67589967  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 20:47:06 2024]  Iteration number: 0 with current cost as 0.3329604614022771 and parameters 
[-2.99138249  1.96502368 -2.3306498  -0.11653103  0.55388704 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648316  2.00049886  1.14432445
  1.31029899 -1.87354673  0.72965073  2.88578416 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 20:53:03 2024]  Iteration number: 0 with current cost as 0.3345007694083931 and parameters 
[-2.95591232  1.98219276 -2.37226945 -0.11653103  0.55388712 -2.77010897
  3.06858502  2.18960149  1.18552002 -1.06648312  1.91767049  1.14432449
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:59:14 2024]  Iteration number: 0 with current cost as 0.3321590371089672 and parameters 
[-2.95929422  1.989361   -2.3831704  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648312  1.9106503   1.14432445
  1.31029906 -1.87354673  0.72965077  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 21:04:41 2024]  Iteration number: 0 with current cost as 0.34649450653514635 and parameters 
[-2.94362125  1.98017033 -2.35602868 -0.11653095  0.55388708 -2.77010894
  3.06858502  2.18960149  1.18552006 -1.06648308  1.90922735  1.14432449
  1.31029906 -1.87354673  0.7296508   2.88578419 -0.5453432  -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1645.6704127788544 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 21:09:59 2024]  Iteration number: 0 with current cost as 0.3946272296631361 and parameters 
[-2.82278494  2.0256901  -2.40223636 -0.11653103  0.55388708 -2.77010901
  3.06858495  2.18960141  1.18551995 -1.06648312  1.67589967  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 21:14:35 2024]  Iteration number: 0 with current cost as 0.3329604614022771 and parameters 
[-2.99138249  1.96502368 -2.3306498  -0.11653103  0.55388704 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648316  2.00049886  1.14432445
  1.31029899 -1.87354673  0.72965073  2.88578416 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 21:20:33 2024]  Iteration number: 0 with current cost as 0.3345007694083931 and parameters 
[-2.95591232  1.98219276 -2.37226945 -0.11653103  0.55388712 -2.77010897
  3.06858502  2.18960149  1.18552002 -1.06648312  1.91767049  1.14432449
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 21:26:20 2024]  Iteration number: 0 with current cost as 0.3321590371089672 and parameters 
[-2.95929422  1.989361   -2.3831704  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648312  1.9106503   1.14432445
  1.31029906 -1.87354673  0.72965077  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 21:31:43 2024]  Iteration number: 0 with current cost as 0.34649450653514635 and parameters 
[-2.94362125  1.98017033 -2.35602868 -0.11653095  0.55388708 -2.77010894
  3.06858502  2.18960149  1.18552006 -1.06648308  1.90922735  1.14432449
  1.31029906 -1.87354673  0.7296508   2.88578419 -0.5453432  -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1620.622328042984 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 21:36:59 2024]  Iteration number: 0 with current cost as 0.3946272296631361 and parameters 
[-2.82278494  2.0256901  -2.40223636 -0.11653103  0.55388708 -2.77010901
  3.06858495  2.18960141  1.18551995 -1.06648312  1.67589967  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 21:41:38 2024]  Iteration number: 0 with current cost as 0.3329604614022771 and parameters 
[-2.99138249  1.96502368 -2.3306498  -0.11653103  0.55388704 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648316  2.00049886  1.14432445
  1.31029899 -1.87354673  0.72965073  2.88578416 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 21:47:40 2024]  Iteration number: 0 with current cost as 0.3345007694083931 and parameters 
[-2.95591232  1.98219276 -2.37226945 -0.11653103  0.55388712 -2.77010897
  3.06858502  2.18960149  1.18552002 -1.06648312  1.91767049  1.14432449
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 21:53:29 2024]  Iteration number: 0 with current cost as 0.3321590371089672 and parameters 
[-2.95929422  1.989361   -2.3831704  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648312  1.9106503   1.14432445
  1.31029906 -1.87354673  0.72965077  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 21:58:57 2024]  Iteration number: 0 with current cost as 0.34649450653514635 and parameters 
[-2.94362125  1.98017033 -2.35602868 -0.11653095  0.55388708 -2.77010894
  3.06858502  2.18960149  1.18552006 -1.06648308  1.90922735  1.14432449
  1.31029906 -1.87354673  0.7296508   2.88578419 -0.5453432  -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1633.164823770523 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 22:04:12 2024]  Iteration number: 0 with current cost as 0.3946272296631361 and parameters 
[-2.82278494  2.0256901  -2.40223636 -0.11653103  0.55388708 -2.77010901
  3.06858495  2.18960141  1.18551995 -1.06648312  1.67589967  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 22:08:48 2024]  Iteration number: 0 with current cost as 0.3329604614022771 and parameters 
[-2.99138249  1.96502368 -2.3306498  -0.11653103  0.55388704 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648316  2.00049886  1.14432445
  1.31029899 -1.87354673  0.72965073  2.88578416 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 22:14:45 2024]  Iteration number: 0 with current cost as 0.3345007694083931 and parameters 
[-2.95591232  1.98219276 -2.37226945 -0.11653103  0.55388712 -2.77010897
  3.06858502  2.18960149  1.18552002 -1.06648312  1.91767049  1.14432449
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 22:20:32 2024]  Iteration number: 0 with current cost as 0.3321590371089672 and parameters 
[-2.95929422  1.989361   -2.3831704  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648312  1.9106503   1.14432445
  1.31029906 -1.87354673  0.72965077  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 22:26:01 2024]  Iteration number: 0 with current cost as 0.34649450653514635 and parameters 
[-2.94362125  1.98017033 -2.35602868 -0.11653095  0.55388708 -2.77010894
  3.06858502  2.18960149  1.18552006 -1.06648308  1.90922735  1.14432449
  1.31029906 -1.87354673  0.7296508   2.88578419 -0.5453432  -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1625.2819592952728 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 22:31:17 2024]  Iteration number: 0 with current cost as 0.3946272296631361 and parameters 
[-2.82278494  2.0256901  -2.40223636 -0.11653103  0.55388708 -2.77010901
  3.06858495  2.18960141  1.18551995 -1.06648312  1.67589967  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 22:35:55 2024]  Iteration number: 0 with current cost as 0.3329604614022771 and parameters 
[-2.99138249  1.96502368 -2.3306498  -0.11653103  0.55388704 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648316  2.00049886  1.14432445
  1.31029899 -1.87354673  0.72965073  2.88578416 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 22:41:53 2024]  Iteration number: 0 with current cost as 0.3345007694083931 and parameters 
[-2.95591232  1.98219276 -2.37226945 -0.11653103  0.55388712 -2.77010897
  3.06858502  2.18960149  1.18552002 -1.06648312  1.91767049  1.14432449
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 22:47:40 2024]  Iteration number: 0 with current cost as 0.3321590371089672 and parameters 
[-2.95929422  1.989361   -2.3831704  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648312  1.9106503   1.14432445
  1.31029906 -1.87354673  0.72965077  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 22:53:06 2024]  Iteration number: 0 with current cost as 0.34649450653514635 and parameters 
[-2.94362125  1.98017033 -2.35602868 -0.11653095  0.55388708 -2.77010894
  3.06858502  2.18960149  1.18552006 -1.06648308  1.90922735  1.14432449
  1.31029906 -1.87354673  0.7296508   2.88578419 -0.5453432  -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1622.4361248016357 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 22:58:20 2024]  Iteration number: 0 with current cost as 0.3946272296631361 and parameters 
[-2.82278494  2.0256901  -2.40223636 -0.11653103  0.55388708 -2.77010901
  3.06858495  2.18960141  1.18551995 -1.06648312  1.67589967  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 23:03:00 2024]  Iteration number: 0 with current cost as 0.3329604614022771 and parameters 
[-2.99138249  1.96502368 -2.3306498  -0.11653103  0.55388704 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648316  2.00049886  1.14432445
  1.31029899 -1.87354673  0.72965073  2.88578416 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 23:08:58 2024]  Iteration number: 0 with current cost as 0.3345007694083931 and parameters 
[-2.95591232  1.98219276 -2.37226945 -0.11653103  0.55388712 -2.77010897
  3.06858502  2.18960149  1.18552002 -1.06648312  1.91767049  1.14432449
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 23:14:46 2024]  Iteration number: 0 with current cost as 0.3321590371089672 and parameters 
[-2.95929422  1.989361   -2.3831704  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648312  1.9106503   1.14432445
  1.31029906 -1.87354673  0.72965077  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 23:20:13 2024]  Iteration number: 0 with current cost as 0.34649450653514635 and parameters 
[-2.94362125  1.98017033 -2.35602868 -0.11653095  0.55388708 -2.77010894
  3.06858502  2.18960149  1.18552006 -1.06648308  1.90922735  1.14432449
  1.31029906 -1.87354673  0.7296508   2.88578419 -0.5453432  -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1629.775845527649 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 23:25:30 2024]  Iteration number: 0 with current cost as 0.3946272296631361 and parameters 
[-2.82278494  2.0256901  -2.40223636 -0.11653103  0.55388708 -2.77010901
  3.06858495  2.18960141  1.18551995 -1.06648312  1.67589967  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 23:30:06 2024]  Iteration number: 0 with current cost as 0.3329604614022771 and parameters 
[-2.99138249  1.96502368 -2.3306498  -0.11653103  0.55388704 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648316  2.00049886  1.14432445
  1.31029899 -1.87354673  0.72965073  2.88578416 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 23:36:02 2024]  Iteration number: 0 with current cost as 0.3345007694083931 and parameters 
[-2.95591232  1.98219276 -2.37226945 -0.11653103  0.55388712 -2.77010897
  3.06858502  2.18960149  1.18552002 -1.06648312  1.91767049  1.14432449
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 23:41:47 2024]  Iteration number: 0 with current cost as 0.3321590371089672 and parameters 
[-2.95929422  1.989361   -2.3831704  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648312  1.9106503   1.14432445
  1.31029906 -1.87354673  0.72965077  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 23:47:14 2024]  Iteration number: 0 with current cost as 0.34649450653514635 and parameters 
[-2.94362125  1.98017033 -2.35602868 -0.11653095  0.55388708 -2.77010894
  3.06858502  2.18960149  1.18552006 -1.06648308  1.90922735  1.14432449
  1.31029906 -1.87354673  0.7296508   2.88578419 -0.5453432  -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1621.2111387252808 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 23:52:31 2024]  Iteration number: 0 with current cost as 0.3946272296631361 and parameters 
[-2.82278494  2.0256901  -2.40223636 -0.11653103  0.55388708 -2.77010901
  3.06858495  2.18960141  1.18551995 -1.06648312  1.67589967  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 23:57:08 2024]  Iteration number: 0 with current cost as 0.3329604614022771 and parameters 
[-2.99138249  1.96502368 -2.3306498  -0.11653103  0.55388704 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648316  2.00049886  1.14432445
  1.31029899 -1.87354673  0.72965073  2.88578416 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 00:03:06 2024]  Iteration number: 0 with current cost as 0.3345007694083931 and parameters 
[-2.95591232  1.98219276 -2.37226945 -0.11653103  0.55388712 -2.77010897
  3.06858502  2.18960149  1.18552002 -1.06648312  1.91767049  1.14432449
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 00:08:52 2024]  Iteration number: 0 with current cost as 0.3321590371089672 and parameters 
[-2.95929422  1.989361   -2.3831704  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648312  1.9106503   1.14432445
  1.31029906 -1.87354673  0.72965077  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 00:14:22 2024]  Iteration number: 0 with current cost as 0.34649450653514635 and parameters 
[-2.94362125  1.98017033 -2.35602868 -0.11653095  0.55388708 -2.77010894
  3.06858502  2.18960149  1.18552006 -1.06648308  1.90922735  1.14432449
  1.31029906 -1.87354673  0.7296508   2.88578419 -0.5453432  -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1628.3709259033203 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 00:19:39 2024]  Iteration number: 0 with current cost as 0.3946272296631361 and parameters 
[-2.82278494  2.0256901  -2.40223636 -0.11653103  0.55388708 -2.77010901
  3.06858495  2.18960141  1.18551995 -1.06648312  1.67589967  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 00:24:15 2024]  Iteration number: 0 with current cost as 0.3329604614022771 and parameters 
[-2.99138249  1.96502368 -2.3306498  -0.11653103  0.55388704 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648316  2.00049886  1.14432445
  1.31029899 -1.87354673  0.72965073  2.88578416 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 00:30:11 2024]  Iteration number: 0 with current cost as 0.3345007694083931 and parameters 
[-2.95591232  1.98219276 -2.37226945 -0.11653103  0.55388712 -2.77010897
  3.06858502  2.18960149  1.18552002 -1.06648312  1.91767049  1.14432449
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 00:36:01 2024]  Iteration number: 0 with current cost as 0.3321590371089672 and parameters 
[-2.95929422  1.989361   -2.3831704  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648312  1.9106503   1.14432445
  1.31029906 -1.87354673  0.72965077  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 00:41:30 2024]  Iteration number: 0 with current cost as 0.34649450653514635 and parameters 
[-2.94362125  1.98017033 -2.35602868 -0.11653095  0.55388708 -2.77010894
  3.06858502  2.18960149  1.18552006 -1.06648308  1.90922735  1.14432449
  1.31029906 -1.87354673  0.7296508   2.88578419 -0.5453432  -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1628.1339452266693 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 00:46:47 2024]  Iteration number: 0 with current cost as 0.3946272296631361 and parameters 
[-2.82278494  2.0256901  -2.40223636 -0.11653103  0.55388708 -2.77010901
  3.06858495  2.18960141  1.18551995 -1.06648312  1.67589967  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 00:51:28 2024]  Iteration number: 0 with current cost as 0.3329604614022771 and parameters 
[-2.99138249  1.96502368 -2.3306498  -0.11653103  0.55388704 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648316  2.00049886  1.14432445
  1.31029899 -1.87354673  0.72965073  2.88578416 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 00:57:29 2024]  Iteration number: 0 with current cost as 0.3345007694083931 and parameters 
[-2.95591232  1.98219276 -2.37226945 -0.11653103  0.55388712 -2.77010897
  3.06858502  2.18960149  1.18552002 -1.06648312  1.91767049  1.14432449
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 01:03:17 2024]  Iteration number: 0 with current cost as 0.3321590371089672 and parameters 
[-2.95929422  1.989361   -2.3831704  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648312  1.9106503   1.14432445
  1.31029906 -1.87354673  0.72965077  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 01:08:44 2024]  Iteration number: 0 with current cost as 0.34649450653514635 and parameters 
[-2.94362125  1.98017033 -2.35602868 -0.11653095  0.55388708 -2.77010894
  3.06858502  2.18960149  1.18552006 -1.06648308  1.90922735  1.14432449
  1.31029906 -1.87354673  0.7296508   2.88578419 -0.5453432  -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1633.367655992508 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 01:14:02 2024]  Iteration number: 0 with current cost as 0.3946272296631361 and parameters 
[-2.82278494  2.0256901  -2.40223636 -0.11653103  0.55388708 -2.77010901
  3.06858495  2.18960141  1.18551995 -1.06648312  1.67589967  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 01:18:38 2024]  Iteration number: 0 with current cost as 0.3329604614022771 and parameters 
[-2.99138249  1.96502368 -2.3306498  -0.11653103  0.55388704 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648316  2.00049886  1.14432445
  1.31029899 -1.87354673  0.72965073  2.88578416 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 01:24:36 2024]  Iteration number: 0 with current cost as 0.3345007694083931 and parameters 
[-2.95591232  1.98219276 -2.37226945 -0.11653103  0.55388712 -2.77010897
  3.06858502  2.18960149  1.18552002 -1.06648312  1.91767049  1.14432449
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 01:30:24 2024]  Iteration number: 0 with current cost as 0.3321590371089672 and parameters 
[-2.95929422  1.989361   -2.3831704  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648312  1.9106503   1.14432445
  1.31029906 -1.87354673  0.72965077  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 01:35:51 2024]  Iteration number: 0 with current cost as 0.34649450653514635 and parameters 
[-2.94362125  1.98017033 -2.35602868 -0.11653095  0.55388708 -2.77010894
  3.06858502  2.18960149  1.18552006 -1.06648308  1.90922735  1.14432449
  1.31029906 -1.87354673  0.7296508   2.88578419 -0.5453432  -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1640.6694412231445 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 01:41:22 2024]  Iteration number: 0 with current cost as 0.3946272296631361 and parameters 
[-2.82278494  2.0256901  -2.40223636 -0.11653103  0.55388708 -2.77010901
  3.06858495  2.18960141  1.18551995 -1.06648312  1.67589967  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 01:45:58 2024]  Iteration number: 0 with current cost as 0.3329604614022771 and parameters 
[-2.99138249  1.96502368 -2.3306498  -0.11653103  0.55388704 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648316  2.00049886  1.14432445
  1.31029899 -1.87354673  0.72965073  2.88578416 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 01:51:56 2024]  Iteration number: 0 with current cost as 0.3345007694083931 and parameters 
[-2.95591232  1.98219276 -2.37226945 -0.11653103  0.55388712 -2.77010897
  3.06858502  2.18960149  1.18552002 -1.06648312  1.91767049  1.14432449
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 01:57:45 2024]  Iteration number: 0 with current cost as 0.3321590371089672 and parameters 
[-2.95929422  1.989361   -2.3831704  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648312  1.9106503   1.14432445
  1.31029906 -1.87354673  0.72965077  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 02:03:16 2024]  Iteration number: 0 with current cost as 0.34649450653514635 and parameters 
[-2.94362125  1.98017033 -2.35602868 -0.11653095  0.55388708 -2.77010894
  3.06858502  2.18960149  1.18552006 -1.06648308  1.90922735  1.14432449
  1.31029906 -1.87354673  0.7296508   2.88578419 -0.5453432  -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1642.9276044368744 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 02:08:45 2024]  Iteration number: 0 with current cost as 0.3946272296631361 and parameters 
[-2.82278494  2.0256901  -2.40223636 -0.11653103  0.55388708 -2.77010901
  3.06858495  2.18960141  1.18551995 -1.06648312  1.67589967  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 02:13:21 2024]  Iteration number: 0 with current cost as 0.3329604614022771 and parameters 
[-2.99138249  1.96502368 -2.3306498  -0.11653103  0.55388704 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648316  2.00049886  1.14432445
  1.31029899 -1.87354673  0.72965073  2.88578416 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 02:19:19 2024]  Iteration number: 0 with current cost as 0.3345007694083931 and parameters 
[-2.95591232  1.98219276 -2.37226945 -0.11653103  0.55388712 -2.77010897
  3.06858502  2.18960149  1.18552002 -1.06648312  1.91767049  1.14432449
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 02:25:08 2024]  Iteration number: 0 with current cost as 0.3321590371089672 and parameters 
[-2.95929422  1.989361   -2.3831704  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648312  1.9106503   1.14432445
  1.31029906 -1.87354673  0.72965077  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 02:30:33 2024]  Iteration number: 0 with current cost as 0.34649450653514635 and parameters 
[-2.94362125  1.98017033 -2.35602868 -0.11653095  0.55388708 -2.77010894
  3.06858502  2.18960149  1.18552006 -1.06648308  1.90922735  1.14432449
  1.31029906 -1.87354673  0.7296508   2.88578419 -0.5453432  -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1624.1512372493744 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 02:35:49 2024]  Iteration number: 0 with current cost as 0.3946272296631361 and parameters 
[-2.82278494  2.0256901  -2.40223636 -0.11653103  0.55388708 -2.77010901
  3.06858495  2.18960141  1.18551995 -1.06648312  1.67589967  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 02:40:25 2024]  Iteration number: 0 with current cost as 0.3329604614022771 and parameters 
[-2.99138249  1.96502368 -2.3306498  -0.11653103  0.55388704 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648316  2.00049886  1.14432445
  1.31029899 -1.87354673  0.72965073  2.88578416 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 02:46:22 2024]  Iteration number: 0 with current cost as 0.3345007694083931 and parameters 
[-2.95591232  1.98219276 -2.37226945 -0.11653103  0.55388712 -2.77010897
  3.06858502  2.18960149  1.18552002 -1.06648312  1.91767049  1.14432449
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 02:52:11 2024]  Iteration number: 0 with current cost as 0.3321590371089672 and parameters 
[-2.95929422  1.989361   -2.3831704  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648312  1.9106503   1.14432445
  1.31029906 -1.87354673  0.72965077  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 02:57:40 2024]  Iteration number: 0 with current cost as 0.34649450653514635 and parameters 
[-2.94362125  1.98017033 -2.35602868 -0.11653095  0.55388708 -2.77010894
  3.06858502  2.18960149  1.18552006 -1.06648308  1.90922735  1.14432449
  1.31029906 -1.87354673  0.7296508   2.88578419 -0.5453432  -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1627.2440996170044 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 03:03:00 2024]  Iteration number: 0 with current cost as 0.3946272296631361 and parameters 
[-2.82278494  2.0256901  -2.40223636 -0.11653103  0.55388708 -2.77010901
  3.06858495  2.18960141  1.18551995 -1.06648312  1.67589967  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 03:07:59 2024]  Iteration number: 0 with current cost as 0.3329604614022771 and parameters 
[-2.99138249  1.96502368 -2.3306498  -0.11653103  0.55388704 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648316  2.00049886  1.14432445
  1.31029899 -1.87354673  0.72965073  2.88578416 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 03:14:07 2024]  Iteration number: 0 with current cost as 0.3345007694083931 and parameters 
[-2.95591232  1.98219276 -2.37226945 -0.11653103  0.55388712 -2.77010897
  3.06858502  2.18960149  1.18552002 -1.06648312  1.91767049  1.14432449
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 03:19:53 2024]  Iteration number: 0 with current cost as 0.3321590371089672 and parameters 
[-2.95929422  1.989361   -2.3831704  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648312  1.9106503   1.14432445
  1.31029906 -1.87354673  0.72965077  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 03:25:19 2024]  Iteration number: 0 with current cost as 0.34649450653514635 and parameters 
[-2.94362125  1.98017033 -2.35602868 -0.11653095  0.55388708 -2.77010894
  3.06858502  2.18960149  1.18552006 -1.06648308  1.90922735  1.14432449
  1.31029906 -1.87354673  0.7296508   2.88578419 -0.5453432  -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1660.0869250297546 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 03:30:36 2024]  Iteration number: 0 with current cost as 0.3946272296631361 and parameters 
[-2.82278494  2.0256901  -2.40223636 -0.11653103  0.55388708 -2.77010901
  3.06858495  2.18960141  1.18551995 -1.06648312  1.67589967  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 03:35:13 2024]  Iteration number: 0 with current cost as 0.3329604614022771 and parameters 
[-2.99138249  1.96502368 -2.3306498  -0.11653103  0.55388704 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648316  2.00049886  1.14432445
  1.31029899 -1.87354673  0.72965073  2.88578416 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 03:41:12 2024]  Iteration number: 0 with current cost as 0.3345007694083931 and parameters 
[-2.95591232  1.98219276 -2.37226945 -0.11653103  0.55388712 -2.77010897
  3.06858502  2.18960149  1.18552002 -1.06648312  1.91767049  1.14432449
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 03:46:59 2024]  Iteration number: 0 with current cost as 0.3321590371089672 and parameters 
[-2.95929422  1.989361   -2.3831704  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648312  1.9106503   1.14432445
  1.31029906 -1.87354673  0.72965077  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 03:52:27 2024]  Iteration number: 0 with current cost as 0.34649450653514635 and parameters 
[-2.94362125  1.98017033 -2.35602868 -0.11653095  0.55388708 -2.77010894
  3.06858502  2.18960149  1.18552006 -1.06648308  1.90922735  1.14432449
  1.31029906 -1.87354673  0.7296508   2.88578419 -0.5453432  -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1631.993748664856 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 03:57:48 2024]  Iteration number: 0 with current cost as 0.3946272296631361 and parameters 
[-2.82278494  2.0256901  -2.40223636 -0.11653103  0.55388708 -2.77010901
  3.06858495  2.18960141  1.18551995 -1.06648312  1.67589967  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 04:02:22 2024]  Iteration number: 0 with current cost as 0.3329604614022771 and parameters 
[-2.99138249  1.96502368 -2.3306498  -0.11653103  0.55388704 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648316  2.00049886  1.14432445
  1.31029899 -1.87354673  0.72965073  2.88578416 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 04:08:19 2024]  Iteration number: 0 with current cost as 0.3345007694083931 and parameters 
[-2.95591232  1.98219276 -2.37226945 -0.11653103  0.55388712 -2.77010897
  3.06858502  2.18960149  1.18552002 -1.06648312  1.91767049  1.14432449
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 04:14:10 2024]  Iteration number: 0 with current cost as 0.3321590371089672 and parameters 
[-2.95929422  1.989361   -2.3831704  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648312  1.9106503   1.14432445
  1.31029906 -1.87354673  0.72965077  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 04:19:37 2024]  Iteration number: 0 with current cost as 0.34649450653514635 and parameters 
[-2.94362125  1.98017033 -2.35602868 -0.11653095  0.55388708 -2.77010894
  3.06858502  2.18960149  1.18552006 -1.06648308  1.90922735  1.14432449
  1.31029906 -1.87354673  0.7296508   2.88578419 -0.5453432  -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1626.9824573993683 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 04:24:55 2024]  Iteration number: 0 with current cost as 0.3946272296631361 and parameters 
[-2.82278494  2.0256901  -2.40223636 -0.11653103  0.55388708 -2.77010901
  3.06858495  2.18960141  1.18551995 -1.06648312  1.67589967  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 04:29:31 2024]  Iteration number: 0 with current cost as 0.3329604614022771 and parameters 
[-2.99138249  1.96502368 -2.3306498  -0.11653103  0.55388704 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648316  2.00049886  1.14432445
  1.31029899 -1.87354673  0.72965073  2.88578416 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 04:35:29 2024]  Iteration number: 0 with current cost as 0.3345007694083931 and parameters 
[-2.95591232  1.98219276 -2.37226945 -0.11653103  0.55388712 -2.77010897
  3.06858502  2.18960149  1.18552002 -1.06648312  1.91767049  1.14432449
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 04:41:16 2024]  Iteration number: 0 with current cost as 0.3321590371089672 and parameters 
[-2.95929422  1.989361   -2.3831704  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648312  1.9106503   1.14432445
  1.31029906 -1.87354673  0.72965077  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 04:46:40 2024]  Iteration number: 0 with current cost as 0.34649450653514635 and parameters 
[-2.94362125  1.98017033 -2.35602868 -0.11653095  0.55388708 -2.77010894
  3.06858502  2.18960149  1.18552006 -1.06648308  1.90922735  1.14432449
  1.31029906 -1.87354673  0.7296508   2.88578419 -0.5453432  -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1621.6662969589233 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 04:51:57 2024]  Iteration number: 0 with current cost as 0.3946272296631361 and parameters 
[-2.82278494  2.0256901  -2.40223636 -0.11653103  0.55388708 -2.77010901
  3.06858495  2.18960141  1.18551995 -1.06648312  1.67589967  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 04:56:32 2024]  Iteration number: 0 with current cost as 0.3329604614022771 and parameters 
[-2.99138249  1.96502368 -2.3306498  -0.11653103  0.55388704 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648316  2.00049886  1.14432445
  1.31029899 -1.87354673  0.72965073  2.88578416 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 05:02:30 2024]  Iteration number: 0 with current cost as 0.3345007694083931 and parameters 
[-2.95591232  1.98219276 -2.37226945 -0.11653103  0.55388712 -2.77010897
  3.06858502  2.18960149  1.18552002 -1.06648312  1.91767049  1.14432449
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 05:08:20 2024]  Iteration number: 0 with current cost as 0.3321590371089672 and parameters 
[-2.95929422  1.989361   -2.3831704  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648312  1.9106503   1.14432445
  1.31029906 -1.87354673  0.72965077  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 05:13:46 2024]  Iteration number: 0 with current cost as 0.34649450653514635 and parameters 
[-2.94362125  1.98017033 -2.35602868 -0.11653095  0.55388708 -2.77010894
  3.06858502  2.18960149  1.18552006 -1.06648308  1.90922735  1.14432449
  1.31029906 -1.87354673  0.7296508   2.88578419 -0.5453432  -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1627.4993879795074 seconds. 
Discarding model... 

Training complete taking 40903.19436621666 total seconds. 
Now scoring model... 
Scoring complete taking 1.108515977859497 seconds. 
Saved predicted values as A2-A2-CNOT_Modified-Pauli-CRX_predicted_values.csv
Model scores: {'MSE_train': (111.40195566903135,), 'R2_train': 0.4602949130532119, 'MAE_train': 9.491056955666982, 'MSE_test': 128.96323767794826, 'R2_test': 0.22263360204103688, 'MAE_test': 9.769200931316783}. 
Saved model results as A2-A2-CNOT_Modified-Pauli-CRX_results.json. 
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Thu Apr  4 12:38:04 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 12:38:35 2024]  Iteration number: 0 with current cost as 0.3946272296631361 and parameters 
[-2.82278494  2.0256901  -2.40223636 -0.11653103  0.55388708 -2.77010901
  3.06858495  2.18960141  1.18551995 -1.06648312  1.67589967  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 12:43:11 2024]  Iteration number: 0 with current cost as 0.3329604614022771 and parameters 
[-2.99138249  1.96502368 -2.3306498  -0.11653103  0.55388704 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648316  2.00049886  1.14432445
  1.31029899 -1.87354673  0.72965073  2.88578416 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 12:49:09 2024]  Iteration number: 0 with current cost as 0.3345007694083931 and parameters 
[-2.95591232  1.98219276 -2.37226945 -0.11653103  0.55388712 -2.77010897
  3.06858502  2.18960149  1.18552002 -1.06648312  1.91767049  1.14432449
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:54:56 2024]  Iteration number: 0 with current cost as 0.3321590371089672 and parameters 
[-2.95929422  1.989361   -2.3831704  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648312  1.9106503   1.14432445
  1.31029906 -1.87354673  0.72965077  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:00:24 2024]  Iteration number: 0 with current cost as 0.34649450653514635 and parameters 
[-2.94362125  1.98017033 -2.35602868 -0.11653095  0.55388708 -2.77010894
  3.06858502  2.18960149  1.18552006 -1.06648308  1.90922735  1.14432449
  1.31029906 -1.87354673  0.7296508   2.88578419 -0.5453432  -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1628.2913398742676 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 13:05:43 2024]  Iteration number: 0 with current cost as 0.3946272296631361 and parameters 
[-2.82278494  2.0256901  -2.40223636 -0.11653103  0.55388708 -2.77010901
  3.06858495  2.18960141  1.18551995 -1.06648312  1.67589967  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 13:10:20 2024]  Iteration number: 0 with current cost as 0.3329604614022771 and parameters 
[-2.99138249  1.96502368 -2.3306498  -0.11653103  0.55388704 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648316  2.00049886  1.14432445
  1.31029899 -1.87354673  0.72965073  2.88578416 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 13:16:33 2024]  Iteration number: 0 with current cost as 0.3345007694083931 and parameters 
[-2.95591232  1.98219276 -2.37226945 -0.11653103  0.55388712 -2.77010897
  3.06858502  2.18960149  1.18552002 -1.06648312  1.91767049  1.14432449
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:22:23 2024]  Iteration number: 0 with current cost as 0.3321590371089672 and parameters 
[-2.95929422  1.989361   -2.3831704  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648312  1.9106503   1.14432445
  1.31029906 -1.87354673  0.72965077  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:27:56 2024]  Iteration number: 0 with current cost as 0.34649450653514635 and parameters 
[-2.94362125  1.98017033 -2.35602868 -0.11653095  0.55388708 -2.77010894
  3.06858502  2.18960149  1.18552006 -1.06648308  1.90922735  1.14432449
  1.31029906 -1.87354673  0.7296508   2.88578419 -0.5453432  -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1651.62761926651 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 13:33:14 2024]  Iteration number: 0 with current cost as 0.3946272296631361 and parameters 
[-2.82278494  2.0256901  -2.40223636 -0.11653103  0.55388708 -2.77010901
  3.06858495  2.18960141  1.18551995 -1.06648312  1.67589967  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 13:37:51 2024]  Iteration number: 0 with current cost as 0.3329604614022771 and parameters 
[-2.99138249  1.96502368 -2.3306498  -0.11653103  0.55388704 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648316  2.00049886  1.14432445
  1.31029899 -1.87354673  0.72965073  2.88578416 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 13:43:58 2024]  Iteration number: 0 with current cost as 0.3345007694083931 and parameters 
[-2.95591232  1.98219276 -2.37226945 -0.11653103  0.55388712 -2.77010897
  3.06858502  2.18960149  1.18552002 -1.06648312  1.91767049  1.14432449
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:50:02 2024]  Iteration number: 0 with current cost as 0.3321590371089672 and parameters 
[-2.95929422  1.989361   -2.3831704  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648312  1.9106503   1.14432445
  1.31029906 -1.87354673  0.72965077  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:55:27 2024]  Iteration number: 0 with current cost as 0.34649450653514635 and parameters 
[-2.94362125  1.98017033 -2.35602868 -0.11653095  0.55388708 -2.77010894
  3.06858502  2.18960149  1.18552006 -1.06648308  1.90922735  1.14432449
  1.31029906 -1.87354673  0.7296508   2.88578419 -0.5453432  -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1658.4754531383514 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 14:00:52 2024]  Iteration number: 0 with current cost as 0.3946272296631361 and parameters 
[-2.82278494  2.0256901  -2.40223636 -0.11653103  0.55388708 -2.77010901
  3.06858495  2.18960141  1.18551995 -1.06648312  1.67589967  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 14:05:34 2024]  Iteration number: 0 with current cost as 0.3329604614022771 and parameters 
[-2.99138249  1.96502368 -2.3306498  -0.11653103  0.55388704 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648316  2.00049886  1.14432445
  1.31029899 -1.87354673  0.72965073  2.88578416 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 14:11:37 2024]  Iteration number: 0 with current cost as 0.3345007694083931 and parameters 
[-2.95591232  1.98219276 -2.37226945 -0.11653103  0.55388712 -2.77010897
  3.06858502  2.18960149  1.18552002 -1.06648312  1.91767049  1.14432449
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 14:17:49 2024]  Iteration number: 0 with current cost as 0.3321590371089672 and parameters 
[-2.95929422  1.989361   -2.3831704  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648312  1.9106503   1.14432445
  1.31029906 -1.87354673  0.72965077  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 14:23:14 2024]  Iteration number: 0 with current cost as 0.34649450653514635 and parameters 
[-2.94362125  1.98017033 -2.35602868 -0.11653095  0.55388708 -2.77010894
  3.06858502  2.18960149  1.18552006 -1.06648308  1.90922735  1.14432449
  1.31029906 -1.87354673  0.7296508   2.88578419 -0.5453432  -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1657.2237589359283 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 14:28:30 2024]  Iteration number: 0 with current cost as 0.3946272296631361 and parameters 
[-2.82278494  2.0256901  -2.40223636 -0.11653103  0.55388708 -2.77010901
  3.06858495  2.18960141  1.18551995 -1.06648312  1.67589967  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 14:33:03 2024]  Iteration number: 0 with current cost as 0.3329604614022771 and parameters 
[-2.99138249  1.96502368 -2.3306498  -0.11653103  0.55388704 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648316  2.00049886  1.14432445
  1.31029899 -1.87354673  0.72965073  2.88578416 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 14:38:57 2024]  Iteration number: 0 with current cost as 0.3345007694083931 and parameters 
[-2.95591232  1.98219276 -2.37226945 -0.11653103  0.55388712 -2.77010897
  3.06858502  2.18960149  1.18552002 -1.06648312  1.91767049  1.14432449
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 14:44:41 2024]  Iteration number: 0 with current cost as 0.3321590371089672 and parameters 
[-2.95929422  1.989361   -2.3831704  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648312  1.9106503   1.14432445
  1.31029906 -1.87354673  0.72965077  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 14:50:03 2024]  Iteration number: 0 with current cost as 0.34649450653514635 and parameters 
[-2.94362125  1.98017033 -2.35602868 -0.11653095  0.55388708 -2.77010894
  3.06858502  2.18960149  1.18552006 -1.06648308  1.90922735  1.14432449
  1.31029906 -1.87354673  0.7296508   2.88578419 -0.5453432  -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1614.9850430488586 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 14:55:25 2024]  Iteration number: 0 with current cost as 0.3946272296631361 and parameters 
[-2.82278494  2.0256901  -2.40223636 -0.11653103  0.55388708 -2.77010901
  3.06858495  2.18960141  1.18551995 -1.06648312  1.67589967  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 15:00:00 2024]  Iteration number: 0 with current cost as 0.3329604614022771 and parameters 
[-2.99138249  1.96502368 -2.3306498  -0.11653103  0.55388704 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648316  2.00049886  1.14432445
  1.31029899 -1.87354673  0.72965073  2.88578416 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 15:05:56 2024]  Iteration number: 0 with current cost as 0.3345007694083931 and parameters 
[-2.95591232  1.98219276 -2.37226945 -0.11653103  0.55388712 -2.77010897
  3.06858502  2.18960149  1.18552002 -1.06648312  1.91767049  1.14432449
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 15:12:02 2024]  Iteration number: 0 with current cost as 0.3321590371089672 and parameters 
[-2.95929422  1.989361   -2.3831704  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648312  1.9106503   1.14432445
  1.31029906 -1.87354673  0.72965077  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 15:17:28 2024]  Iteration number: 0 with current cost as 0.34649450653514635 and parameters 
[-2.94362125  1.98017033 -2.35602868 -0.11653095  0.55388708 -2.77010894
  3.06858502  2.18960149  1.18552006 -1.06648308  1.90922735  1.14432449
  1.31029906 -1.87354673  0.7296508   2.88578419 -0.5453432  -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1640.0620415210724 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 15:22:45 2024]  Iteration number: 0 with current cost as 0.3946272296631361 and parameters 
[-2.82278494  2.0256901  -2.40223636 -0.11653103  0.55388708 -2.77010901
  3.06858495  2.18960141  1.18551995 -1.06648312  1.67589967  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 15:27:20 2024]  Iteration number: 0 with current cost as 0.3329604614022771 and parameters 
[-2.99138249  1.96502368 -2.3306498  -0.11653103  0.55388704 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648316  2.00049886  1.14432445
  1.31029899 -1.87354673  0.72965073  2.88578416 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 15:33:41 2024]  Iteration number: 0 with current cost as 0.3345007694083931 and parameters 
[-2.95591232  1.98219276 -2.37226945 -0.11653103  0.55388712 -2.77010897
  3.06858502  2.18960149  1.18552002 -1.06648312  1.91767049  1.14432449
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 15:40:03 2024]  Iteration number: 0 with current cost as 0.3321590371089672 and parameters 
[-2.95929422  1.989361   -2.3831704  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648312  1.9106503   1.14432445
  1.31029906 -1.87354673  0.72965077  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 15:45:28 2024]  Iteration number: 0 with current cost as 0.34649450653514635 and parameters 
[-2.94362125  1.98017033 -2.35602868 -0.11653095  0.55388708 -2.77010894
  3.06858502  2.18960149  1.18552006 -1.06648308  1.90922735  1.14432449
  1.31029906 -1.87354673  0.7296508   2.88578419 -0.5453432  -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1688.0488741397858 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 15:50:53 2024]  Iteration number: 0 with current cost as 0.3946272296631361 and parameters 
[-2.82278494  2.0256901  -2.40223636 -0.11653103  0.55388708 -2.77010901
  3.06858495  2.18960141  1.18551995 -1.06648312  1.67589967  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 15:55:30 2024]  Iteration number: 0 with current cost as 0.3329604614022771 and parameters 
[-2.99138249  1.96502368 -2.3306498  -0.11653103  0.55388704 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648316  2.00049886  1.14432445
  1.31029899 -1.87354673  0.72965073  2.88578416 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 16:01:30 2024]  Iteration number: 0 with current cost as 0.3345007694083931 and parameters 
[-2.95591232  1.98219276 -2.37226945 -0.11653103  0.55388712 -2.77010897
  3.06858502  2.18960149  1.18552002 -1.06648312  1.91767049  1.14432449
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 16:07:15 2024]  Iteration number: 0 with current cost as 0.3321590371089672 and parameters 
[-2.95929422  1.989361   -2.3831704  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648312  1.9106503   1.14432445
  1.31029906 -1.87354673  0.72965077  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 16:12:45 2024]  Iteration number: 0 with current cost as 0.34649450653514635 and parameters 
[-2.94362125  1.98017033 -2.35602868 -0.11653095  0.55388708 -2.77010894
  3.06858502  2.18960149  1.18552006 -1.06648308  1.90922735  1.14432449
  1.31029906 -1.87354673  0.7296508   2.88578419 -0.5453432  -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1627.721219778061 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 16:18:01 2024]  Iteration number: 0 with current cost as 0.3946272296631361 and parameters 
[-2.82278494  2.0256901  -2.40223636 -0.11653103  0.55388708 -2.77010901
  3.06858495  2.18960141  1.18551995 -1.06648312  1.67589967  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 16:22:34 2024]  Iteration number: 0 with current cost as 0.3329604614022771 and parameters 
[-2.99138249  1.96502368 -2.3306498  -0.11653103  0.55388704 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648316  2.00049886  1.14432445
  1.31029899 -1.87354673  0.72965073  2.88578416 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 16:29:03 2024]  Iteration number: 0 with current cost as 0.3345007694083931 and parameters 
[-2.95591232  1.98219276 -2.37226945 -0.11653103  0.55388712 -2.77010897
  3.06858502  2.18960149  1.18552002 -1.06648312  1.91767049  1.14432449
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 16:34:48 2024]  Iteration number: 0 with current cost as 0.3321590371089672 and parameters 
[-2.95929422  1.989361   -2.3831704  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648312  1.9106503   1.14432445
  1.31029906 -1.87354673  0.72965077  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 16:40:17 2024]  Iteration number: 0 with current cost as 0.34649450653514635 and parameters 
[-2.94362125  1.98017033 -2.35602868 -0.11653095  0.55388708 -2.77010894
  3.06858502  2.18960149  1.18552006 -1.06648308  1.90922735  1.14432449
  1.31029906 -1.87354673  0.7296508   2.88578419 -0.5453432  -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1654.115903377533 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 16:45:35 2024]  Iteration number: 0 with current cost as 0.3946272296631361 and parameters 
[-2.82278494  2.0256901  -2.40223636 -0.11653103  0.55388708 -2.77010901
  3.06858495  2.18960141  1.18551995 -1.06648312  1.67589967  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 16:50:29 2024]  Iteration number: 0 with current cost as 0.3329604614022771 and parameters 
[-2.99138249  1.96502368 -2.3306498  -0.11653103  0.55388704 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648316  2.00049886  1.14432445
  1.31029899 -1.87354673  0.72965073  2.88578416 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 16:56:25 2024]  Iteration number: 0 with current cost as 0.3345007694083931 and parameters 
[-2.95591232  1.98219276 -2.37226945 -0.11653103  0.55388712 -2.77010897
  3.06858502  2.18960149  1.18552002 -1.06648312  1.91767049  1.14432449
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 17:02:17 2024]  Iteration number: 0 with current cost as 0.3321590371089672 and parameters 
[-2.95929422  1.989361   -2.3831704  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648312  1.9106503   1.14432445
  1.31029906 -1.87354673  0.72965077  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 17:07:42 2024]  Iteration number: 0 with current cost as 0.34649450653514635 and parameters 
[-2.94362125  1.98017033 -2.35602868 -0.11653095  0.55388708 -2.77010894
  3.06858502  2.18960149  1.18552006 -1.06648308  1.90922735  1.14432449
  1.31029906 -1.87354673  0.7296508   2.88578419 -0.5453432  -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1639.964637517929 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 17:12:55 2024]  Iteration number: 0 with current cost as 0.3946272296631361 and parameters 
[-2.82278494  2.0256901  -2.40223636 -0.11653103  0.55388708 -2.77010901
  3.06858495  2.18960141  1.18551995 -1.06648312  1.67589967  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 17:17:30 2024]  Iteration number: 0 with current cost as 0.3329604614022771 and parameters 
[-2.99138249  1.96502368 -2.3306498  -0.11653103  0.55388704 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648316  2.00049886  1.14432445
  1.31029899 -1.87354673  0.72965073  2.88578416 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 17:23:27 2024]  Iteration number: 0 with current cost as 0.3345007694083931 and parameters 
[-2.95591232  1.98219276 -2.37226945 -0.11653103  0.55388712 -2.77010897
  3.06858502  2.18960149  1.18552002 -1.06648312  1.91767049  1.14432449
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 17:29:25 2024]  Iteration number: 0 with current cost as 0.3321590371089672 and parameters 
[-2.95929422  1.989361   -2.3831704  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648312  1.9106503   1.14432445
  1.31029906 -1.87354673  0.72965077  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 17:34:51 2024]  Iteration number: 0 with current cost as 0.34649450653514635 and parameters 
[-2.94362125  1.98017033 -2.35602868 -0.11653095  0.55388708 -2.77010894
  3.06858502  2.18960149  1.18552006 -1.06648308  1.90922735  1.14432449
  1.31029906 -1.87354673  0.7296508   2.88578419 -0.5453432  -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1634.0017983913422 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 17:40:08 2024]  Iteration number: 0 with current cost as 0.3946272296631361 and parameters 
[-2.82278494  2.0256901  -2.40223636 -0.11653103  0.55388708 -2.77010901
  3.06858495  2.18960141  1.18551995 -1.06648312  1.67589967  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 17:44:49 2024]  Iteration number: 0 with current cost as 0.3329604614022771 and parameters 
[-2.99138249  1.96502368 -2.3306498  -0.11653103  0.55388704 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648316  2.00049886  1.14432445
  1.31029899 -1.87354673  0.72965073  2.88578416 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 17:50:43 2024]  Iteration number: 0 with current cost as 0.3345007694083931 and parameters 
[-2.95591232  1.98219276 -2.37226945 -0.11653103  0.55388712 -2.77010897
  3.06858502  2.18960149  1.18552002 -1.06648312  1.91767049  1.14432449
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 17:56:33 2024]  Iteration number: 0 with current cost as 0.3321590371089672 and parameters 
[-2.95929422  1.989361   -2.3831704  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648312  1.9106503   1.14432445
  1.31029906 -1.87354673  0.72965077  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 18:01:57 2024]  Iteration number: 0 with current cost as 0.34649450653514635 and parameters 
[-2.94362125  1.98017033 -2.35602868 -0.11653095  0.55388708 -2.77010894
  3.06858502  2.18960149  1.18552006 -1.06648308  1.90922735  1.14432449
  1.31029906 -1.87354673  0.7296508   2.88578419 -0.5453432  -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1639.181470632553 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 18:07:28 2024]  Iteration number: 0 with current cost as 0.3946272296631361 and parameters 
[-2.82278494  2.0256901  -2.40223636 -0.11653103  0.55388708 -2.77010901
  3.06858495  2.18960141  1.18551995 -1.06648312  1.67589967  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 18:12:06 2024]  Iteration number: 0 with current cost as 0.3329604614022771 and parameters 
[-2.99138249  1.96502368 -2.3306498  -0.11653103  0.55388704 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648316  2.00049886  1.14432445
  1.31029899 -1.87354673  0.72965073  2.88578416 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 18:18:05 2024]  Iteration number: 0 with current cost as 0.3345007694083931 and parameters 
[-2.95591232  1.98219276 -2.37226945 -0.11653103  0.55388712 -2.77010897
  3.06858502  2.18960149  1.18552002 -1.06648312  1.91767049  1.14432449
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 18:23:49 2024]  Iteration number: 0 with current cost as 0.3321590371089672 and parameters 
[-2.95929422  1.989361   -2.3831704  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648312  1.9106503   1.14432445
  1.31029906 -1.87354673  0.72965077  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 18:29:13 2024]  Iteration number: 0 with current cost as 0.34649450653514635 and parameters 
[-2.94362125  1.98017033 -2.35602868 -0.11653095  0.55388708 -2.77010894
  3.06858502  2.18960149  1.18552006 -1.06648308  1.90922735  1.14432449
  1.31029906 -1.87354673  0.7296508   2.88578419 -0.5453432  -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1618.5260393619537 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 18:34:27 2024]  Iteration number: 0 with current cost as 0.3946272296631361 and parameters 
[-2.82278494  2.0256901  -2.40223636 -0.11653103  0.55388708 -2.77010901
  3.06858495  2.18960141  1.18551995 -1.06648312  1.67589967  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 18:39:13 2024]  Iteration number: 0 with current cost as 0.3329604614022771 and parameters 
[-2.99138249  1.96502368 -2.3306498  -0.11653103  0.55388704 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648316  2.00049886  1.14432445
  1.31029899 -1.87354673  0.72965073  2.88578416 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 18:45:17 2024]  Iteration number: 0 with current cost as 0.3345007694083931 and parameters 
[-2.95591232  1.98219276 -2.37226945 -0.11653103  0.55388712 -2.77010897
  3.06858502  2.18960149  1.18552002 -1.06648312  1.91767049  1.14432449
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 18:51:10 2024]  Iteration number: 0 with current cost as 0.3321590371089672 and parameters 
[-2.95929422  1.989361   -2.3831704  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648312  1.9106503   1.14432445
  1.31029906 -1.87354673  0.72965077  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 18:56:44 2024]  Iteration number: 0 with current cost as 0.34649450653514635 and parameters 
[-2.94362125  1.98017033 -2.35602868 -0.11653095  0.55388708 -2.77010894
  3.06858502  2.18960149  1.18552006 -1.06648308  1.90922735  1.14432449
  1.31029906 -1.87354673  0.7296508   2.88578419 -0.5453432  -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1681.4845433235168 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 19:02:28 2024]  Iteration number: 0 with current cost as 0.3946272296631361 and parameters 
[-2.82278494  2.0256901  -2.40223636 -0.11653103  0.55388708 -2.77010901
  3.06858495  2.18960141  1.18551995 -1.06648312  1.67589967  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 19:07:02 2024]  Iteration number: 0 with current cost as 0.3329604614022771 and parameters 
[-2.99138249  1.96502368 -2.3306498  -0.11653103  0.55388704 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648316  2.00049886  1.14432445
  1.31029899 -1.87354673  0.72965073  2.88578416 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 19:12:56 2024]  Iteration number: 0 with current cost as 0.3345007694083931 and parameters 
[-2.95591232  1.98219276 -2.37226945 -0.11653103  0.55388712 -2.77010897
  3.06858502  2.18960149  1.18552002 -1.06648312  1.91767049  1.14432449
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 19:18:42 2024]  Iteration number: 0 with current cost as 0.3321590371089672 and parameters 
[-2.95929422  1.989361   -2.3831704  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648312  1.9106503   1.14432445
  1.31029906 -1.87354673  0.72965077  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 19:24:07 2024]  Iteration number: 0 with current cost as 0.34649450653514635 and parameters 
[-2.94362125  1.98017033 -2.35602868 -0.11653095  0.55388708 -2.77010894
  3.06858502  2.18960149  1.18552006 -1.06648308  1.90922735  1.14432449
  1.31029906 -1.87354673  0.7296508   2.88578419 -0.5453432  -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1622.8830461502075 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 19:29:31 2024]  Iteration number: 0 with current cost as 0.3946272296631361 and parameters 
[-2.82278494  2.0256901  -2.40223636 -0.11653103  0.55388708 -2.77010901
  3.06858495  2.18960141  1.18551995 -1.06648312  1.67589967  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 19:34:25 2024]  Iteration number: 0 with current cost as 0.3329604614022771 and parameters 
[-2.99138249  1.96502368 -2.3306498  -0.11653103  0.55388704 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648316  2.00049886  1.14432445
  1.31029899 -1.87354673  0.72965073  2.88578416 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 19:40:35 2024]  Iteration number: 0 with current cost as 0.3345007694083931 and parameters 
[-2.95591232  1.98219276 -2.37226945 -0.11653103  0.55388712 -2.77010897
  3.06858502  2.18960149  1.18552002 -1.06648312  1.91767049  1.14432449
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 19:46:28 2024]  Iteration number: 0 with current cost as 0.3321590371089672 and parameters 
[-2.95929422  1.989361   -2.3831704  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648312  1.9106503   1.14432445
  1.31029906 -1.87354673  0.72965077  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 19:51:58 2024]  Iteration number: 0 with current cost as 0.34649450653514635 and parameters 
[-2.94362125  1.98017033 -2.35602868 -0.11653095  0.55388708 -2.77010894
  3.06858502  2.18960149  1.18552006 -1.06648308  1.90922735  1.14432449
  1.31029906 -1.87354673  0.7296508   2.88578419 -0.5453432  -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1666.316888332367 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 19:57:17 2024]  Iteration number: 0 with current cost as 0.3946272296631361 and parameters 
[-2.82278494  2.0256901  -2.40223636 -0.11653103  0.55388708 -2.77010901
  3.06858495  2.18960141  1.18551995 -1.06648312  1.67589967  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 20:01:54 2024]  Iteration number: 0 with current cost as 0.3329604614022771 and parameters 
[-2.99138249  1.96502368 -2.3306498  -0.11653103  0.55388704 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648316  2.00049886  1.14432445
  1.31029899 -1.87354673  0.72965073  2.88578416 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 20:07:51 2024]  Iteration number: 0 with current cost as 0.3345007694083931 and parameters 
[-2.95591232  1.98219276 -2.37226945 -0.11653103  0.55388712 -2.77010897
  3.06858502  2.18960149  1.18552002 -1.06648312  1.91767049  1.14432449
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 20:13:35 2024]  Iteration number: 0 with current cost as 0.3321590371089672 and parameters 
[-2.95929422  1.989361   -2.3831704  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648312  1.9106503   1.14432445
  1.31029906 -1.87354673  0.72965077  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 20:19:01 2024]  Iteration number: 0 with current cost as 0.34649450653514635 and parameters 
[-2.94362125  1.98017033 -2.35602868 -0.11653095  0.55388708 -2.77010894
  3.06858502  2.18960149  1.18552006 -1.06648308  1.90922735  1.14432449
  1.31029906 -1.87354673  0.7296508   2.88578419 -0.5453432  -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1619.4560577869415 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 20:24:17 2024]  Iteration number: 0 with current cost as 0.3946272296631361 and parameters 
[-2.82278494  2.0256901  -2.40223636 -0.11653103  0.55388708 -2.77010901
  3.06858495  2.18960141  1.18551995 -1.06648312  1.67589967  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 20:28:51 2024]  Iteration number: 0 with current cost as 0.3329604614022771 and parameters 
[-2.99138249  1.96502368 -2.3306498  -0.11653103  0.55388704 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648316  2.00049886  1.14432445
  1.31029899 -1.87354673  0.72965073  2.88578416 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 20:34:45 2024]  Iteration number: 0 with current cost as 0.3345007694083931 and parameters 
[-2.95591232  1.98219276 -2.37226945 -0.11653103  0.55388712 -2.77010897
  3.06858502  2.18960149  1.18552002 -1.06648312  1.91767049  1.14432449
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 20:40:31 2024]  Iteration number: 0 with current cost as 0.3321590371089672 and parameters 
[-2.95929422  1.989361   -2.3831704  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648312  1.9106503   1.14432445
  1.31029906 -1.87354673  0.72965077  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 20:45:58 2024]  Iteration number: 0 with current cost as 0.34649450653514635 and parameters 
[-2.94362125  1.98017033 -2.35602868 -0.11653095  0.55388708 -2.77010894
  3.06858502  2.18960149  1.18552006 -1.06648308  1.90922735  1.14432449
  1.31029906 -1.87354673  0.7296508   2.88578419 -0.5453432  -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1616.7568595409393 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 20:51:14 2024]  Iteration number: 0 with current cost as 0.3946272296631361 and parameters 
[-2.82278494  2.0256901  -2.40223636 -0.11653103  0.55388708 -2.77010901
  3.06858495  2.18960141  1.18551995 -1.06648312  1.67589967  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 20:55:54 2024]  Iteration number: 0 with current cost as 0.3329604614022771 and parameters 
[-2.99138249  1.96502368 -2.3306498  -0.11653103  0.55388704 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648316  2.00049886  1.14432445
  1.31029899 -1.87354673  0.72965073  2.88578416 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 21:01:52 2024]  Iteration number: 0 with current cost as 0.3345007694083931 and parameters 
[-2.95591232  1.98219276 -2.37226945 -0.11653103  0.55388712 -2.77010897
  3.06858502  2.18960149  1.18552002 -1.06648312  1.91767049  1.14432449
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 21:07:55 2024]  Iteration number: 0 with current cost as 0.3321590371089672 and parameters 
[-2.95929422  1.989361   -2.3831704  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648312  1.9106503   1.14432445
  1.31029906 -1.87354673  0.72965077  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 21:13:21 2024]  Iteration number: 0 with current cost as 0.34649450653514635 and parameters 
[-2.94362125  1.98017033 -2.35602868 -0.11653095  0.55388708 -2.77010894
  3.06858502  2.18960149  1.18552006 -1.06648308  1.90922735  1.14432449
  1.31029906 -1.87354673  0.7296508   2.88578419 -0.5453432  -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1649.171370267868 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 21:18:42 2024]  Iteration number: 0 with current cost as 0.3946272296631361 and parameters 
[-2.82278494  2.0256901  -2.40223636 -0.11653103  0.55388708 -2.77010901
  3.06858495  2.18960141  1.18551995 -1.06648312  1.67589967  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 21:23:40 2024]  Iteration number: 0 with current cost as 0.3329604614022771 and parameters 
[-2.99138249  1.96502368 -2.3306498  -0.11653103  0.55388704 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648316  2.00049886  1.14432445
  1.31029899 -1.87354673  0.72965073  2.88578416 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 21:29:40 2024]  Iteration number: 0 with current cost as 0.3345007694083931 and parameters 
[-2.95591232  1.98219276 -2.37226945 -0.11653103  0.55388712 -2.77010897
  3.06858502  2.18960149  1.18552002 -1.06648312  1.91767049  1.14432449
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 21:35:26 2024]  Iteration number: 0 with current cost as 0.3321590371089672 and parameters 
[-2.95929422  1.989361   -2.3831704  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648312  1.9106503   1.14432445
  1.31029906 -1.87354673  0.72965077  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 21:40:57 2024]  Iteration number: 0 with current cost as 0.34649450653514635 and parameters 
[-2.94362125  1.98017033 -2.35602868 -0.11653095  0.55388708 -2.77010894
  3.06858502  2.18960149  1.18552006 -1.06648308  1.90922735  1.14432449
  1.31029906 -1.87354673  0.7296508   2.88578419 -0.5453432  -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1649.6282331943512 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 21:46:12 2024]  Iteration number: 0 with current cost as 0.3946272296631361 and parameters 
[-2.82278494  2.0256901  -2.40223636 -0.11653103  0.55388708 -2.77010901
  3.06858495  2.18960141  1.18551995 -1.06648312  1.67589967  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 21:50:48 2024]  Iteration number: 0 with current cost as 0.3329604614022771 and parameters 
[-2.99138249  1.96502368 -2.3306498  -0.11653103  0.55388704 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648316  2.00049886  1.14432445
  1.31029899 -1.87354673  0.72965073  2.88578416 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 21:56:44 2024]  Iteration number: 0 with current cost as 0.3345007694083931 and parameters 
[-2.95591232  1.98219276 -2.37226945 -0.11653103  0.55388712 -2.77010897
  3.06858502  2.18960149  1.18552002 -1.06648312  1.91767049  1.14432449
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 22:02:45 2024]  Iteration number: 0 with current cost as 0.3321590371089672 and parameters 
[-2.95929422  1.989361   -2.3831704  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648312  1.9106503   1.14432445
  1.31029906 -1.87354673  0.72965077  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 22:08:10 2024]  Iteration number: 0 with current cost as 0.34649450653514635 and parameters 
[-2.94362125  1.98017033 -2.35602868 -0.11653095  0.55388708 -2.77010894
  3.06858502  2.18960149  1.18552006 -1.06648308  1.90922735  1.14432449
  1.31029906 -1.87354673  0.7296508   2.88578419 -0.5453432  -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1632.9311320781708 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 22:13:25 2024]  Iteration number: 0 with current cost as 0.3946272296631361 and parameters 
[-2.82278494  2.0256901  -2.40223636 -0.11653103  0.55388708 -2.77010901
  3.06858495  2.18960141  1.18551995 -1.06648312  1.67589967  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 22:18:21 2024]  Iteration number: 0 with current cost as 0.3329604614022771 and parameters 
[-2.99138249  1.96502368 -2.3306498  -0.11653103  0.55388704 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648316  2.00049886  1.14432445
  1.31029899 -1.87354673  0.72965073  2.88578416 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 22:24:23 2024]  Iteration number: 0 with current cost as 0.3345007694083931 and parameters 
[-2.95591232  1.98219276 -2.37226945 -0.11653103  0.55388712 -2.77010897
  3.06858502  2.18960149  1.18552002 -1.06648312  1.91767049  1.14432449
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 22:30:10 2024]  Iteration number: 0 with current cost as 0.3321590371089672 and parameters 
[-2.95929422  1.989361   -2.3831704  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648312  1.9106503   1.14432445
  1.31029906 -1.87354673  0.72965077  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 22:35:46 2024]  Iteration number: 0 with current cost as 0.34649450653514635 and parameters 
[-2.94362125  1.98017033 -2.35602868 -0.11653095  0.55388708 -2.77010894
  3.06858502  2.18960149  1.18552006 -1.06648308  1.90922735  1.14432449
  1.31029906 -1.87354673  0.7296508   2.88578419 -0.5453432  -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1665.465743303299 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 22:41:11 2024]  Iteration number: 0 with current cost as 0.3946272296631361 and parameters 
[-2.82278494  2.0256901  -2.40223636 -0.11653103  0.55388708 -2.77010901
  3.06858495  2.18960141  1.18551995 -1.06648312  1.67589967  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 22:46:15 2024]  Iteration number: 0 with current cost as 0.3329604614022771 and parameters 
[-2.99138249  1.96502368 -2.3306498  -0.11653103  0.55388704 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648316  2.00049886  1.14432445
  1.31029899 -1.87354673  0.72965073  2.88578416 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 22:52:34 2024]  Iteration number: 0 with current cost as 0.3345007694083931 and parameters 
[-2.95591232  1.98219276 -2.37226945 -0.11653103  0.55388712 -2.77010897
  3.06858502  2.18960149  1.18552002 -1.06648312  1.91767049  1.14432449
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 22:58:22 2024]  Iteration number: 0 with current cost as 0.3321590371089672 and parameters 
[-2.95929422  1.989361   -2.3831704  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648312  1.9106503   1.14432445
  1.31029906 -1.87354673  0.72965077  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 23:03:49 2024]  Iteration number: 0 with current cost as 0.34649450653514635 and parameters 
[-2.94362125  1.98017033 -2.35602868 -0.11653095  0.55388708 -2.77010894
  3.06858502  2.18960149  1.18552006 -1.06648308  1.90922735  1.14432449
  1.31029906 -1.87354673  0.7296508   2.88578419 -0.5453432  -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1676.2020108699799 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 23:09:07 2024]  Iteration number: 0 with current cost as 0.3946272296631361 and parameters 
[-2.82278494  2.0256901  -2.40223636 -0.11653103  0.55388708 -2.77010901
  3.06858495  2.18960141  1.18551995 -1.06648312  1.67589967  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 23:13:41 2024]  Iteration number: 0 with current cost as 0.3329604614022771 and parameters 
[-2.99138249  1.96502368 -2.3306498  -0.11653103  0.55388704 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648316  2.00049886  1.14432445
  1.31029899 -1.87354673  0.72965073  2.88578416 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 23:19:37 2024]  Iteration number: 0 with current cost as 0.3345007694083931 and parameters 
[-2.95591232  1.98219276 -2.37226945 -0.11653103  0.55388712 -2.77010897
  3.06858502  2.18960149  1.18552002 -1.06648312  1.91767049  1.14432449
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 23:25:23 2024]  Iteration number: 0 with current cost as 0.3321590371089672 and parameters 
[-2.95929422  1.989361   -2.3831704  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648312  1.9106503   1.14432445
  1.31029906 -1.87354673  0.72965077  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 23:30:45 2024]  Iteration number: 0 with current cost as 0.34649450653514635 and parameters 
[-2.94362125  1.98017033 -2.35602868 -0.11653095  0.55388708 -2.77010894
  3.06858502  2.18960149  1.18552006 -1.06648308  1.90922735  1.14432449
  1.31029906 -1.87354673  0.7296508   2.88578419 -0.5453432  -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1612.478500366211 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 23:36:15 2024]  Iteration number: 0 with current cost as 0.3946272296631361 and parameters 
[-2.82278494  2.0256901  -2.40223636 -0.11653103  0.55388708 -2.77010901
  3.06858495  2.18960141  1.18551995 -1.06648312  1.67589967  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 23:40:50 2024]  Iteration number: 0 with current cost as 0.3329604614022771 and parameters 
[-2.99138249  1.96502368 -2.3306498  -0.11653103  0.55388704 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648316  2.00049886  1.14432445
  1.31029899 -1.87354673  0.72965073  2.88578416 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 23:46:59 2024]  Iteration number: 0 with current cost as 0.3345007694083931 and parameters 
[-2.95591232  1.98219276 -2.37226945 -0.11653103  0.55388712 -2.77010897
  3.06858502  2.18960149  1.18552002 -1.06648312  1.91767049  1.14432449
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 23:53:07 2024]  Iteration number: 0 with current cost as 0.3321590371089672 and parameters 
[-2.95929422  1.989361   -2.3831704  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648312  1.9106503   1.14432445
  1.31029906 -1.87354673  0.72965077  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 23:58:41 2024]  Iteration number: 0 with current cost as 0.34649450653514635 and parameters 
[-2.94362125  1.98017033 -2.35602868 -0.11653095  0.55388708 -2.77010894
  3.06858502  2.18960149  1.18552006 -1.06648308  1.90922735  1.14432449
  1.31029906 -1.87354673  0.7296508   2.88578419 -0.5453432  -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1679.1477856636047 seconds. 
Discarding model... 

Training complete taking 41124.14961767197 total seconds. 
Now scoring model... 
Scoring complete taking 1.1092822551727295 seconds. 
Saved predicted values as A2-A2-CNOT_Modified-Pauli-CRX_predicted_values.csv
Model scores: {'MSE_train': (111.40195566903135,), 'R2_train': 0.4602949130532119, 'MAE_train': 9.491056955666982, 'MSE_test': 128.96323767794826, 'R2_test': 0.22263360204103688, 'MAE_test': 9.769200931316783}. 
Saved model results as A2-A2-CNOT_Modified-Pauli-CRX_results.json. 
