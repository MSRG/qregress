/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/circuits/Encoders.py:12: RuntimeWarning: invalid value encountered in arcsin
  if np.isnan(np.arcsin(features[feature_idx])) or np.isnan(np.arccos(features[feature_idx])):
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Sun Mar 24 17:29:58 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 17:31:09 2024]  Iteration number: 0 with current cost as 0.5502979012576306 and parameters 
[-1.74536127  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552044 -1.06648331  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965035  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 17:32:20 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743464 -2.12427964 -0.11653071  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18552014 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.87354664  0.72965049  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 17:35:04 2024]  Iteration number: 0 with current cost as 0.19763134090932857 and parameters 
[64.41734463  2.23744483 -2.12428983 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18554037 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87353661  0.72963042  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 17:37:02 2024]  Iteration number: 0 with current cost as 0.20134480339841582 and parameters 
[ 1.53284567  2.23743464 -2.12427964 -0.11652953  0.55388708 -2.77010897
  3.06858498  2.1896022   1.18552224 -1.06648383  0.60271585  1.14432445
  1.31029899 -1.87354605  0.7296493   2.88578344 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 17:38:59 2024]  Iteration number: 0 with current cost as 0.5086763801217657 and parameters 
[-1.61121048  2.23743464 -2.12427964 -0.11653069  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552032 -1.06648325  0.60271527  1.14432445
  1.31029899 -1.87354663  0.72965047  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 569.8514566421509 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 17:40:36 2024]  Iteration number: 0 with current cost as 0.5502979012576306 and parameters 
[-1.74536127  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552044 -1.06648331  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965035  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 17:41:45 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743464 -2.12427964 -0.11653071  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18552014 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.87354664  0.72965049  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 17:44:29 2024]  Iteration number: 0 with current cost as 0.19763134090932857 and parameters 
[64.41734463  2.23744483 -2.12428983 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18554037 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87353661  0.72963042  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 17:46:26 2024]  Iteration number: 0 with current cost as 0.20134480339841582 and parameters 
[ 1.53284567  2.23743464 -2.12427964 -0.11652953  0.55388708 -2.77010897
  3.06858498  2.1896022   1.18552224 -1.06648383  0.60271585  1.14432445
  1.31029899 -1.87354605  0.7296493   2.88578344 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 17:48:25 2024]  Iteration number: 0 with current cost as 0.5086763801217657 and parameters 
[-1.61121048  2.23743464 -2.12427964 -0.11653069  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552032 -1.06648325  0.60271527  1.14432445
  1.31029899 -1.87354663  0.72965047  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 566.2972173690796 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 17:50:01 2024]  Iteration number: 0 with current cost as 0.5502979012576306 and parameters 
[-1.74536127  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552044 -1.06648331  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965035  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 17:51:12 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743464 -2.12427964 -0.11653071  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18552014 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.87354664  0.72965049  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 17:53:57 2024]  Iteration number: 0 with current cost as 0.19763134090932857 and parameters 
[64.41734463  2.23744483 -2.12428983 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18554037 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87353661  0.72963042  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 17:55:54 2024]  Iteration number: 0 with current cost as 0.20134480339841582 and parameters 
[ 1.53284567  2.23743464 -2.12427964 -0.11652953  0.55388708 -2.77010897
  3.06858498  2.1896022   1.18552224 -1.06648383  0.60271585  1.14432445
  1.31029899 -1.87354605  0.7296493   2.88578344 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 17:57:51 2024]  Iteration number: 0 with current cost as 0.5086763801217657 and parameters 
[-1.61121048  2.23743464 -2.12427964 -0.11653069  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552032 -1.06648325  0.60271527  1.14432445
  1.31029899 -1.87354663  0.72965047  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 565.9429836273193 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 17:59:28 2024]  Iteration number: 0 with current cost as 0.5502979012576306 and parameters 
[-1.74536127  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552044 -1.06648331  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965035  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:00:36 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743464 -2.12427964 -0.11653071  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18552014 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.87354664  0.72965049  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:03:24 2024]  Iteration number: 0 with current cost as 0.19763134090932857 and parameters 
[64.41734463  2.23744483 -2.12428983 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18554037 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87353661  0.72963042  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:05:20 2024]  Iteration number: 0 with current cost as 0.20134480339841582 and parameters 
[ 1.53284567  2.23743464 -2.12427964 -0.11652953  0.55388708 -2.77010897
  3.06858498  2.1896022   1.18552224 -1.06648383  0.60271585  1.14432445
  1.31029899 -1.87354605  0.7296493   2.88578344 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:07:17 2024]  Iteration number: 0 with current cost as 0.5086763801217657 and parameters 
[-1.61121048  2.23743464 -2.12427964 -0.11653069  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552032 -1.06648325  0.60271527  1.14432445
  1.31029899 -1.87354663  0.72965047  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 565.7577078342438 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 18:08:54 2024]  Iteration number: 0 with current cost as 0.5502979012576306 and parameters 
[-1.74536127  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552044 -1.06648331  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965035  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:10:02 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743464 -2.12427964 -0.11653071  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18552014 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.87354664  0.72965049  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:12:49 2024]  Iteration number: 0 with current cost as 0.19763134090932857 and parameters 
[64.41734463  2.23744483 -2.12428983 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18554037 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87353661  0.72963042  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:14:46 2024]  Iteration number: 0 with current cost as 0.20134480339841582 and parameters 
[ 1.53284567  2.23743464 -2.12427964 -0.11652953  0.55388708 -2.77010897
  3.06858498  2.1896022   1.18552224 -1.06648383  0.60271585  1.14432445
  1.31029899 -1.87354605  0.7296493   2.88578344 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:16:42 2024]  Iteration number: 0 with current cost as 0.5086763801217657 and parameters 
[-1.61121048  2.23743464 -2.12427964 -0.11653069  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552032 -1.06648325  0.60271527  1.14432445
  1.31029899 -1.87354663  0.72965047  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 564.8849833011627 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 18:18:19 2024]  Iteration number: 0 with current cost as 0.5502979012576306 and parameters 
[-1.74536127  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552044 -1.06648331  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965035  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:19:27 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743464 -2.12427964 -0.11653071  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18552014 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.87354664  0.72965049  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:22:13 2024]  Iteration number: 0 with current cost as 0.19763134090932857 and parameters 
[64.41734463  2.23744483 -2.12428983 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18554037 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87353661  0.72963042  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:24:17 2024]  Iteration number: 0 with current cost as 0.20134480339841582 and parameters 
[ 1.53284567  2.23743464 -2.12427964 -0.11652953  0.55388708 -2.77010897
  3.06858498  2.1896022   1.18552224 -1.06648383  0.60271585  1.14432445
  1.31029899 -1.87354605  0.7296493   2.88578344 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:26:16 2024]  Iteration number: 0 with current cost as 0.5086763801217657 and parameters 
[-1.61121048  2.23743464 -2.12427964 -0.11653069  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552032 -1.06648325  0.60271527  1.14432445
  1.31029899 -1.87354663  0.72965047  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 574.105190038681 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 18:27:52 2024]  Iteration number: 0 with current cost as 0.5502979012576306 and parameters 
[-1.74536127  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552044 -1.06648331  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965035  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:29:01 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743464 -2.12427964 -0.11653071  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18552014 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.87354664  0.72965049  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:31:52 2024]  Iteration number: 0 with current cost as 0.19763134090932857 and parameters 
[64.41734463  2.23744483 -2.12428983 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18554037 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87353661  0.72963042  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:33:49 2024]  Iteration number: 0 with current cost as 0.20134480339841582 and parameters 
[ 1.53284567  2.23743464 -2.12427964 -0.11652953  0.55388708 -2.77010897
  3.06858498  2.1896022   1.18552224 -1.06648383  0.60271585  1.14432445
  1.31029899 -1.87354605  0.7296493   2.88578344 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:35:46 2024]  Iteration number: 0 with current cost as 0.5086763801217657 and parameters 
[-1.61121048  2.23743464 -2.12427964 -0.11653069  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552032 -1.06648325  0.60271527  1.14432445
  1.31029899 -1.87354663  0.72965047  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 570.5653579235077 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 18:37:24 2024]  Iteration number: 0 with current cost as 0.5502979012576306 and parameters 
[-1.74536127  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552044 -1.06648331  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965035  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:38:31 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743464 -2.12427964 -0.11653071  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18552014 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.87354664  0.72965049  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:41:17 2024]  Iteration number: 0 with current cost as 0.19763134090932857 and parameters 
[64.41734463  2.23744483 -2.12428983 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18554037 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87353661  0.72963042  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:43:15 2024]  Iteration number: 0 with current cost as 0.20134480339841582 and parameters 
[ 1.53284567  2.23743464 -2.12427964 -0.11652953  0.55388708 -2.77010897
  3.06858498  2.1896022   1.18552224 -1.06648383  0.60271585  1.14432445
  1.31029899 -1.87354605  0.7296493   2.88578344 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:45:10 2024]  Iteration number: 0 with current cost as 0.5086763801217657 and parameters 
[-1.61121048  2.23743464 -2.12427964 -0.11653069  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552032 -1.06648325  0.60271527  1.14432445
  1.31029899 -1.87354663  0.72965047  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 564.1418445110321 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 18:46:49 2024]  Iteration number: 0 with current cost as 0.5502979012576306 and parameters 
[-1.74536127  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552044 -1.06648331  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965035  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:47:57 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743464 -2.12427964 -0.11653071  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18552014 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.87354664  0.72965049  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:50:43 2024]  Iteration number: 0 with current cost as 0.19763134090932857 and parameters 
[64.41734463  2.23744483 -2.12428983 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18554037 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87353661  0.72963042  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:52:39 2024]  Iteration number: 0 with current cost as 0.20134480339841582 and parameters 
[ 1.53284567  2.23743464 -2.12427964 -0.11652953  0.55388708 -2.77010897
  3.06858498  2.1896022   1.18552224 -1.06648383  0.60271585  1.14432445
  1.31029899 -1.87354605  0.7296493   2.88578344 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:54:37 2024]  Iteration number: 0 with current cost as 0.5086763801217657 and parameters 
[-1.61121048  2.23743464 -2.12427964 -0.11653069  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552032 -1.06648325  0.60271527  1.14432445
  1.31029899 -1.87354663  0.72965047  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 566.2634656429291 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 18:56:16 2024]  Iteration number: 0 with current cost as 0.5502979012576306 and parameters 
[-1.74536127  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552044 -1.06648331  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965035  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:57:23 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743464 -2.12427964 -0.11653071  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18552014 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.87354664  0.72965049  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:00:09 2024]  Iteration number: 0 with current cost as 0.19763134090932857 and parameters 
[64.41734463  2.23744483 -2.12428983 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18554037 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87353661  0.72963042  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:02:06 2024]  Iteration number: 0 with current cost as 0.20134480339841582 and parameters 
[ 1.53284567  2.23743464 -2.12427964 -0.11652953  0.55388708 -2.77010897
  3.06858498  2.1896022   1.18552224 -1.06648383  0.60271585  1.14432445
  1.31029899 -1.87354605  0.7296493   2.88578344 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:04:03 2024]  Iteration number: 0 with current cost as 0.5086763801217657 and parameters 
[-1.61121048  2.23743464 -2.12427964 -0.11653069  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552032 -1.06648325  0.60271527  1.14432445
  1.31029899 -1.87354663  0.72965047  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 565.9270906448364 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 19:05:40 2024]  Iteration number: 0 with current cost as 0.5502979012576306 and parameters 
[-1.74536127  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552044 -1.06648331  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965035  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:06:49 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743464 -2.12427964 -0.11653071  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18552014 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.87354664  0.72965049  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:09:35 2024]  Iteration number: 0 with current cost as 0.19763134090932857 and parameters 
[64.41734463  2.23744483 -2.12428983 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18554037 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87353661  0.72963042  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:11:32 2024]  Iteration number: 0 with current cost as 0.20134480339841582 and parameters 
[ 1.53284567  2.23743464 -2.12427964 -0.11652953  0.55388708 -2.77010897
  3.06858498  2.1896022   1.18552224 -1.06648383  0.60271585  1.14432445
  1.31029899 -1.87354605  0.7296493   2.88578344 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:13:30 2024]  Iteration number: 0 with current cost as 0.5086763801217657 and parameters 
[-1.61121048  2.23743464 -2.12427964 -0.11653069  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552032 -1.06648325  0.60271527  1.14432445
  1.31029899 -1.87354663  0.72965047  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 566.9111757278442 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 19:15:07 2024]  Iteration number: 0 with current cost as 0.5502979012576306 and parameters 
[-1.74536127  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552044 -1.06648331  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965035  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:16:22 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743464 -2.12427964 -0.11653071  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18552014 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.87354664  0.72965049  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:19:36 2024]  Iteration number: 0 with current cost as 0.19763134090932857 and parameters 
[64.41734463  2.23744483 -2.12428983 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18554037 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87353661  0.72963042  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:21:40 2024]  Iteration number: 0 with current cost as 0.20134480339841582 and parameters 
[ 1.53284567  2.23743464 -2.12427964 -0.11652953  0.55388708 -2.77010897
  3.06858498  2.1896022   1.18552224 -1.06648383  0.60271585  1.14432445
  1.31029899 -1.87354605  0.7296493   2.88578344 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:23:38 2024]  Iteration number: 0 with current cost as 0.5086763801217657 and parameters 
[-1.61121048  2.23743464 -2.12427964 -0.11653069  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552032 -1.06648325  0.60271527  1.14432445
  1.31029899 -1.87354663  0.72965047  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 608.3236541748047 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 19:25:16 2024]  Iteration number: 0 with current cost as 0.5502979012576306 and parameters 
[-1.74536127  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552044 -1.06648331  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965035  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:26:24 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743464 -2.12427964 -0.11653071  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18552014 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.87354664  0.72965049  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:29:18 2024]  Iteration number: 0 with current cost as 0.19763134090932857 and parameters 
[64.41734463  2.23744483 -2.12428983 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18554037 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87353661  0.72963042  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:31:15 2024]  Iteration number: 0 with current cost as 0.20134480339841582 and parameters 
[ 1.53284567  2.23743464 -2.12427964 -0.11652953  0.55388708 -2.77010897
  3.06858498  2.1896022   1.18552224 -1.06648383  0.60271585  1.14432445
  1.31029899 -1.87354605  0.7296493   2.88578344 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:33:11 2024]  Iteration number: 0 with current cost as 0.5086763801217657 and parameters 
[-1.61121048  2.23743464 -2.12427964 -0.11653069  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552032 -1.06648325  0.60271527  1.14432445
  1.31029899 -1.87354663  0.72965047  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 573.6140305995941 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 19:34:49 2024]  Iteration number: 0 with current cost as 0.5502979012576306 and parameters 
[-1.74536127  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552044 -1.06648331  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965035  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:35:57 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743464 -2.12427964 -0.11653071  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18552014 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.87354664  0.72965049  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:38:41 2024]  Iteration number: 0 with current cost as 0.19763134090932857 and parameters 
[64.41734463  2.23744483 -2.12428983 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18554037 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87353661  0.72963042  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:40:37 2024]  Iteration number: 0 with current cost as 0.20134480339841582 and parameters 
[ 1.53284567  2.23743464 -2.12427964 -0.11652953  0.55388708 -2.77010897
  3.06858498  2.1896022   1.18552224 -1.06648383  0.60271585  1.14432445
  1.31029899 -1.87354605  0.7296493   2.88578344 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:42:33 2024]  Iteration number: 0 with current cost as 0.5086763801217657 and parameters 
[-1.61121048  2.23743464 -2.12427964 -0.11653069  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552032 -1.06648325  0.60271527  1.14432445
  1.31029899 -1.87354663  0.72965047  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 561.8464553356171 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 19:44:10 2024]  Iteration number: 0 with current cost as 0.5502979012576306 and parameters 
[-1.74536127  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552044 -1.06648331  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965035  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:45:19 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743464 -2.12427964 -0.11653071  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18552014 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.87354664  0.72965049  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:48:01 2024]  Iteration number: 0 with current cost as 0.19763134090932857 and parameters 
[64.41734463  2.23744483 -2.12428983 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18554037 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87353661  0.72963042  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:49:57 2024]  Iteration number: 0 with current cost as 0.20134480339841582 and parameters 
[ 1.53284567  2.23743464 -2.12427964 -0.11652953  0.55388708 -2.77010897
  3.06858498  2.1896022   1.18552224 -1.06648383  0.60271585  1.14432445
  1.31029899 -1.87354605  0.7296493   2.88578344 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:51:54 2024]  Iteration number: 0 with current cost as 0.5086763801217657 and parameters 
[-1.61121048  2.23743464 -2.12427964 -0.11653069  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552032 -1.06648325  0.60271527  1.14432445
  1.31029899 -1.87354663  0.72965047  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 559.8177235126495 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 19:53:33 2024]  Iteration number: 0 with current cost as 0.5502979012576306 and parameters 
[-1.74536127  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552044 -1.06648331  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965035  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:54:40 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743464 -2.12427964 -0.11653071  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18552014 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.87354664  0.72965049  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:57:26 2024]  Iteration number: 0 with current cost as 0.19763134090932857 and parameters 
[64.41734463  2.23744483 -2.12428983 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18554037 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87353661  0.72963042  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:59:22 2024]  Iteration number: 0 with current cost as 0.20134480339841582 and parameters 
[ 1.53284567  2.23743464 -2.12427964 -0.11652953  0.55388708 -2.77010897
  3.06858498  2.1896022   1.18552224 -1.06648383  0.60271585  1.14432445
  1.31029899 -1.87354605  0.7296493   2.88578344 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 20:01:18 2024]  Iteration number: 0 with current cost as 0.5086763801217657 and parameters 
[-1.61121048  2.23743464 -2.12427964 -0.11653069  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552032 -1.06648325  0.60271527  1.14432445
  1.31029899 -1.87354663  0.72965047  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 563.0157911777496 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 20:02:54 2024]  Iteration number: 0 with current cost as 0.5502979012576306 and parameters 
[-1.74536127  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552044 -1.06648331  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965035  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 20:04:04 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743464 -2.12427964 -0.11653071  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18552014 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.87354664  0.72965049  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 20:06:53 2024]  Iteration number: 0 with current cost as 0.19763134090932857 and parameters 
[64.41734463  2.23744483 -2.12428983 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18554037 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87353661  0.72963042  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:08:48 2024]  Iteration number: 0 with current cost as 0.20134480339841582 and parameters 
[ 1.53284567  2.23743464 -2.12427964 -0.11652953  0.55388708 -2.77010897
  3.06858498  2.1896022   1.18552224 -1.06648383  0.60271585  1.14432445
  1.31029899 -1.87354605  0.7296493   2.88578344 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 20:10:44 2024]  Iteration number: 0 with current cost as 0.5086763801217657 and parameters 
[-1.61121048  2.23743464 -2.12427964 -0.11653069  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552032 -1.06648325  0.60271527  1.14432445
  1.31029899 -1.87354663  0.72965047  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 567.7763066291809 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 20:12:23 2024]  Iteration number: 0 with current cost as 0.5502979012576306 and parameters 
[-1.74536127  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552044 -1.06648331  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965035  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 20:13:32 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743464 -2.12427964 -0.11653071  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18552014 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.87354664  0.72965049  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 20:16:17 2024]  Iteration number: 0 with current cost as 0.19763134090932857 and parameters 
[64.41734463  2.23744483 -2.12428983 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18554037 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87353661  0.72963042  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:18:12 2024]  Iteration number: 0 with current cost as 0.20134480339841582 and parameters 
[ 1.53284567  2.23743464 -2.12427964 -0.11652953  0.55388708 -2.77010897
  3.06858498  2.1896022   1.18552224 -1.06648383  0.60271585  1.14432445
  1.31029899 -1.87354605  0.7296493   2.88578344 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 20:20:10 2024]  Iteration number: 0 with current cost as 0.5086763801217657 and parameters 
[-1.61121048  2.23743464 -2.12427964 -0.11653069  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552032 -1.06648325  0.60271527  1.14432445
  1.31029899 -1.87354663  0.72965047  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 565.9484887123108 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 20:21:47 2024]  Iteration number: 0 with current cost as 0.5502979012576306 and parameters 
[-1.74536127  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552044 -1.06648331  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965035  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 20:22:55 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743464 -2.12427964 -0.11653071  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18552014 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.87354664  0.72965049  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 20:25:41 2024]  Iteration number: 0 with current cost as 0.19763134090932857 and parameters 
[64.41734463  2.23744483 -2.12428983 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18554037 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87353661  0.72963042  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:27:37 2024]  Iteration number: 0 with current cost as 0.20134480339841582 and parameters 
[ 1.53284567  2.23743464 -2.12427964 -0.11652953  0.55388708 -2.77010897
  3.06858498  2.1896022   1.18552224 -1.06648383  0.60271585  1.14432445
  1.31029899 -1.87354605  0.7296493   2.88578344 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 20:29:42 2024]  Iteration number: 0 with current cost as 0.5086763801217657 and parameters 
[-1.61121048  2.23743464 -2.12427964 -0.11653069  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552032 -1.06648325  0.60271527  1.14432445
  1.31029899 -1.87354663  0.72965047  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 571.5717668533325 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 20:31:20 2024]  Iteration number: 0 with current cost as 0.5502979012576306 and parameters 
[-1.74536127  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552044 -1.06648331  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965035  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 20:32:28 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743464 -2.12427964 -0.11653071  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18552014 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.87354664  0.72965049  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 20:35:32 2024]  Iteration number: 0 with current cost as 0.19763134090932857 and parameters 
[64.41734463  2.23744483 -2.12428983 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18554037 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87353661  0.72963042  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:37:29 2024]  Iteration number: 0 with current cost as 0.20134480339841582 and parameters 
[ 1.53284567  2.23743464 -2.12427964 -0.11652953  0.55388708 -2.77010897
  3.06858498  2.1896022   1.18552224 -1.06648383  0.60271585  1.14432445
  1.31029899 -1.87354605  0.7296493   2.88578344 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 20:39:25 2024]  Iteration number: 0 with current cost as 0.5086763801217657 and parameters 
[-1.61121048  2.23743464 -2.12427964 -0.11653069  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552032 -1.06648325  0.60271527  1.14432445
  1.31029899 -1.87354663  0.72965047  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 583.6083180904388 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 20:41:02 2024]  Iteration number: 0 with current cost as 0.5502979012576306 and parameters 
[-1.74536127  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552044 -1.06648331  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965035  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 20:42:09 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743464 -2.12427964 -0.11653071  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18552014 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.87354664  0.72965049  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 20:44:53 2024]  Iteration number: 0 with current cost as 0.19763134090932857 and parameters 
[64.41734463  2.23744483 -2.12428983 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18554037 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87353661  0.72963042  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:46:49 2024]  Iteration number: 0 with current cost as 0.20134480339841582 and parameters 
[ 1.53284567  2.23743464 -2.12427964 -0.11652953  0.55388708 -2.77010897
  3.06858498  2.1896022   1.18552224 -1.06648383  0.60271585  1.14432445
  1.31029899 -1.87354605  0.7296493   2.88578344 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 20:48:44 2024]  Iteration number: 0 with current cost as 0.5086763801217657 and parameters 
[-1.61121048  2.23743464 -2.12427964 -0.11653069  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552032 -1.06648325  0.60271527  1.14432445
  1.31029899 -1.87354663  0.72965047  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 558.5927033424377 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 20:50:21 2024]  Iteration number: 0 with current cost as 0.5502979012576306 and parameters 
[-1.74536127  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552044 -1.06648331  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965035  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 20:51:30 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743464 -2.12427964 -0.11653071  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18552014 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.87354664  0.72965049  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 20:54:15 2024]  Iteration number: 0 with current cost as 0.19763134090932857 and parameters 
[64.41734463  2.23744483 -2.12428983 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18554037 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87353661  0.72963042  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:56:11 2024]  Iteration number: 0 with current cost as 0.20134480339841582 and parameters 
[ 1.53284567  2.23743464 -2.12427964 -0.11652953  0.55388708 -2.77010897
  3.06858498  2.1896022   1.18552224 -1.06648383  0.60271585  1.14432445
  1.31029899 -1.87354605  0.7296493   2.88578344 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 20:58:08 2024]  Iteration number: 0 with current cost as 0.5086763801217657 and parameters 
[-1.61121048  2.23743464 -2.12427964 -0.11653069  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552032 -1.06648325  0.60271527  1.14432445
  1.31029899 -1.87354663  0.72965047  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 563.8993420600891 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 20:59:44 2024]  Iteration number: 0 with current cost as 0.5502979012576306 and parameters 
[-1.74536127  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552044 -1.06648331  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965035  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 21:00:53 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743464 -2.12427964 -0.11653071  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18552014 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.87354664  0.72965049  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 21:03:38 2024]  Iteration number: 0 with current cost as 0.19763134090932857 and parameters 
[64.41734463  2.23744483 -2.12428983 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18554037 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87353661  0.72963042  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 21:05:36 2024]  Iteration number: 0 with current cost as 0.20134480339841582 and parameters 
[ 1.53284567  2.23743464 -2.12427964 -0.11652953  0.55388708 -2.77010897
  3.06858498  2.1896022   1.18552224 -1.06648383  0.60271585  1.14432445
  1.31029899 -1.87354605  0.7296493   2.88578344 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 21:07:32 2024]  Iteration number: 0 with current cost as 0.5086763801217657 and parameters 
[-1.61121048  2.23743464 -2.12427964 -0.11653069  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552032 -1.06648325  0.60271527  1.14432445
  1.31029899 -1.87354663  0.72965047  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 577.5634868144989 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 21:09:23 2024]  Iteration number: 0 with current cost as 0.5502979012576306 and parameters 
[-1.74536127  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552044 -1.06648331  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965035  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 21:10:34 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743464 -2.12427964 -0.11653071  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18552014 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.87354664  0.72965049  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 21:13:19 2024]  Iteration number: 0 with current cost as 0.19763134090932857 and parameters 
[64.41734463  2.23744483 -2.12428983 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18554037 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87353661  0.72963042  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 21:15:15 2024]  Iteration number: 0 with current cost as 0.20134480339841582 and parameters 
[ 1.53284567  2.23743464 -2.12427964 -0.11652953  0.55388708 -2.77010897
  3.06858498  2.1896022   1.18552224 -1.06648383  0.60271585  1.14432445
  1.31029899 -1.87354605  0.7296493   2.88578344 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 21:17:12 2024]  Iteration number: 0 with current cost as 0.5086763801217657 and parameters 
[-1.61121048  2.23743464 -2.12427964 -0.11653069  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552032 -1.06648325  0.60271527  1.14432445
  1.31029899 -1.87354663  0.72965047  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 566.6477990150452 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 21:18:48 2024]  Iteration number: 0 with current cost as 0.5502979012576306 and parameters 
[-1.74536127  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552044 -1.06648331  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965035  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 21:19:56 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743464 -2.12427964 -0.11653071  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18552014 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.87354664  0.72965049  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 21:22:40 2024]  Iteration number: 0 with current cost as 0.19763134090932857 and parameters 
[64.41734463  2.23744483 -2.12428983 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18554037 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87353661  0.72963042  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 21:24:37 2024]  Iteration number: 0 with current cost as 0.20134480339841582 and parameters 
[ 1.53284567  2.23743464 -2.12427964 -0.11652953  0.55388708 -2.77010897
  3.06858498  2.1896022   1.18552224 -1.06648383  0.60271585  1.14432445
  1.31029899 -1.87354605  0.7296493   2.88578344 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 21:26:33 2024]  Iteration number: 0 with current cost as 0.5086763801217657 and parameters 
[-1.61121048  2.23743464 -2.12427964 -0.11653069  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552032 -1.06648325  0.60271527  1.14432445
  1.31029899 -1.87354663  0.72965047  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 561.1511700153351 seconds. 
Discarding model... 

Training complete taking 14224.026687860489 total seconds. 
Now scoring model... 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Scoring complete taking 1.0571906566619873 seconds. 
Saved predicted values as M-M-CZ_Modified-Pauli-CRZ_predicted_values.csv
Model scores: {'MSE_train': (322.73150773416654,), 'R2_train': -0.5635258411406567, 'MAE_train': 16.55376646011674, 'MSE_test': 301.1515605306635, 'R2_test': -0.8152855655970617, 'MAE_test': 16.304477363287884}. 
Saved model results as M-M-CZ_Modified-Pauli-CRZ_results.json. 
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/circuits/Encoders.py:12: RuntimeWarning: invalid value encountered in arcsin
  if np.isnan(np.arcsin(features[feature_idx])) or np.isnan(np.arccos(features[feature_idx])):
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Thu Apr  4 11:28:42 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 11:29:51 2024]  Iteration number: 0 with current cost as 0.5502979012576306 and parameters 
[-1.74536127  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552044 -1.06648331  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965035  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 11:30:59 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743464 -2.12427964 -0.11653071  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18552014 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.87354664  0.72965049  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 11:33:53 2024]  Iteration number: 0 with current cost as 0.19763134090932857 and parameters 
[64.41734463  2.23744483 -2.12428983 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18554037 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87353661  0.72963042  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 11:35:49 2024]  Iteration number: 0 with current cost as 0.20134480339841582 and parameters 
[ 1.53284567  2.23743464 -2.12427964 -0.11652953  0.55388708 -2.77010897
  3.06858498  2.1896022   1.18552224 -1.06648383  0.60271585  1.14432445
  1.31029899 -1.87354605  0.7296493   2.88578344 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 11:37:46 2024]  Iteration number: 0 with current cost as 0.5086763801217657 and parameters 
[-1.61121048  2.23743464 -2.12427964 -0.11653069  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552032 -1.06648325  0.60271527  1.14432445
  1.31029899 -1.87354663  0.72965047  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 572.4357373714447 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 11:39:23 2024]  Iteration number: 0 with current cost as 0.5502979012576306 and parameters 
[-1.74536127  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552044 -1.06648331  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965035  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 11:40:30 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743464 -2.12427964 -0.11653071  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18552014 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.87354664  0.72965049  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 11:43:17 2024]  Iteration number: 0 with current cost as 0.19763134090932857 and parameters 
[64.41734463  2.23744483 -2.12428983 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18554037 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87353661  0.72963042  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 11:45:14 2024]  Iteration number: 0 with current cost as 0.20134480339841582 and parameters 
[ 1.53284567  2.23743464 -2.12427964 -0.11652953  0.55388708 -2.77010897
  3.06858498  2.1896022   1.18552224 -1.06648383  0.60271585  1.14432445
  1.31029899 -1.87354605  0.7296493   2.88578344 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 11:47:12 2024]  Iteration number: 0 with current cost as 0.5086763801217657 and parameters 
[-1.61121048  2.23743464 -2.12427964 -0.11653069  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552032 -1.06648325  0.60271527  1.14432445
  1.31029899 -1.87354663  0.72965047  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 565.9890727996826 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 11:48:48 2024]  Iteration number: 0 with current cost as 0.5502979012576306 and parameters 
[-1.74536127  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552044 -1.06648331  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965035  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 11:49:56 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743464 -2.12427964 -0.11653071  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18552014 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.87354664  0.72965049  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 11:52:42 2024]  Iteration number: 0 with current cost as 0.19763134090932857 and parameters 
[64.41734463  2.23744483 -2.12428983 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18554037 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87353661  0.72963042  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 11:54:40 2024]  Iteration number: 0 with current cost as 0.20134480339841582 and parameters 
[ 1.53284567  2.23743464 -2.12427964 -0.11652953  0.55388708 -2.77010897
  3.06858498  2.1896022   1.18552224 -1.06648383  0.60271585  1.14432445
  1.31029899 -1.87354605  0.7296493   2.88578344 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 11:56:53 2024]  Iteration number: 0 with current cost as 0.5086763801217657 and parameters 
[-1.61121048  2.23743464 -2.12427964 -0.11653069  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552032 -1.06648325  0.60271527  1.14432445
  1.31029899 -1.87354663  0.72965047  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 580.9975206851959 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 11:59:02 2024]  Iteration number: 0 with current cost as 0.5502979012576306 and parameters 
[-1.74536127  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552044 -1.06648331  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965035  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 12:00:11 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743464 -2.12427964 -0.11653071  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18552014 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.87354664  0.72965049  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 12:02:59 2024]  Iteration number: 0 with current cost as 0.19763134090932857 and parameters 
[64.41734463  2.23744483 -2.12428983 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18554037 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87353661  0.72963042  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:04:56 2024]  Iteration number: 0 with current cost as 0.20134480339841582 and parameters 
[ 1.53284567  2.23743464 -2.12427964 -0.11652953  0.55388708 -2.77010897
  3.06858498  2.1896022   1.18552224 -1.06648383  0.60271585  1.14432445
  1.31029899 -1.87354605  0.7296493   2.88578344 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:06:54 2024]  Iteration number: 0 with current cost as 0.5086763801217657 and parameters 
[-1.61121048  2.23743464 -2.12427964 -0.11653069  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552032 -1.06648325  0.60271527  1.14432445
  1.31029899 -1.87354663  0.72965047  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 601.1068246364594 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 12:08:31 2024]  Iteration number: 0 with current cost as 0.5502979012576306 and parameters 
[-1.74536127  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552044 -1.06648331  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965035  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 12:09:41 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743464 -2.12427964 -0.11653071  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18552014 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.87354664  0.72965049  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 12:12:26 2024]  Iteration number: 0 with current cost as 0.19763134090932857 and parameters 
[64.41734463  2.23744483 -2.12428983 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18554037 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87353661  0.72963042  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:14:23 2024]  Iteration number: 0 with current cost as 0.20134480339841582 and parameters 
[ 1.53284567  2.23743464 -2.12427964 -0.11652953  0.55388708 -2.77010897
  3.06858498  2.1896022   1.18552224 -1.06648383  0.60271585  1.14432445
  1.31029899 -1.87354605  0.7296493   2.88578344 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:16:25 2024]  Iteration number: 0 with current cost as 0.5086763801217657 and parameters 
[-1.61121048  2.23743464 -2.12427964 -0.11653069  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552032 -1.06648325  0.60271527  1.14432445
  1.31029899 -1.87354663  0.72965047  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 571.2917761802673 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 12:18:03 2024]  Iteration number: 0 with current cost as 0.5502979012576306 and parameters 
[-1.74536127  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552044 -1.06648331  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965035  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 12:19:13 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743464 -2.12427964 -0.11653071  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18552014 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.87354664  0.72965049  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 12:22:00 2024]  Iteration number: 0 with current cost as 0.19763134090932857 and parameters 
[64.41734463  2.23744483 -2.12428983 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18554037 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87353661  0.72963042  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:23:58 2024]  Iteration number: 0 with current cost as 0.20134480339841582 and parameters 
[ 1.53284567  2.23743464 -2.12427964 -0.11652953  0.55388708 -2.77010897
  3.06858498  2.1896022   1.18552224 -1.06648383  0.60271585  1.14432445
  1.31029899 -1.87354605  0.7296493   2.88578344 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:25:56 2024]  Iteration number: 0 with current cost as 0.5086763801217657 and parameters 
[-1.61121048  2.23743464 -2.12427964 -0.11653069  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552032 -1.06648325  0.60271527  1.14432445
  1.31029899 -1.87354663  0.72965047  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 571.0051381587982 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 12:27:33 2024]  Iteration number: 0 with current cost as 0.5502979012576306 and parameters 
[-1.74536127  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552044 -1.06648331  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965035  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 12:28:43 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743464 -2.12427964 -0.11653071  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18552014 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.87354664  0.72965049  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 12:31:53 2024]  Iteration number: 0 with current cost as 0.19763134090932857 and parameters 
[64.41734463  2.23744483 -2.12428983 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18554037 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87353661  0.72963042  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:33:55 2024]  Iteration number: 0 with current cost as 0.20134480339841582 and parameters 
[ 1.53284567  2.23743464 -2.12427964 -0.11652953  0.55388708 -2.77010897
  3.06858498  2.1896022   1.18552224 -1.06648383  0.60271585  1.14432445
  1.31029899 -1.87354605  0.7296493   2.88578344 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:35:53 2024]  Iteration number: 0 with current cost as 0.5086763801217657 and parameters 
[-1.61121048  2.23743464 -2.12427964 -0.11653069  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552032 -1.06648325  0.60271527  1.14432445
  1.31029899 -1.87354663  0.72965047  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 597.2310290336609 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 12:37:30 2024]  Iteration number: 0 with current cost as 0.5502979012576306 and parameters 
[-1.74536127  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552044 -1.06648331  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965035  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 12:38:39 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743464 -2.12427964 -0.11653071  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18552014 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.87354664  0.72965049  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 12:41:46 2024]  Iteration number: 0 with current cost as 0.19763134090932857 and parameters 
[64.41734463  2.23744483 -2.12428983 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18554037 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87353661  0.72963042  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:43:44 2024]  Iteration number: 0 with current cost as 0.20134480339841582 and parameters 
[ 1.53284567  2.23743464 -2.12427964 -0.11652953  0.55388708 -2.77010897
  3.06858498  2.1896022   1.18552224 -1.06648383  0.60271585  1.14432445
  1.31029899 -1.87354605  0.7296493   2.88578344 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:45:50 2024]  Iteration number: 0 with current cost as 0.5086763801217657 and parameters 
[-1.61121048  2.23743464 -2.12427964 -0.11653069  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552032 -1.06648325  0.60271527  1.14432445
  1.31029899 -1.87354663  0.72965047  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 596.7741322517395 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 12:47:27 2024]  Iteration number: 0 with current cost as 0.5502979012576306 and parameters 
[-1.74536127  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552044 -1.06648331  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965035  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 12:48:48 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743464 -2.12427964 -0.11653071  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18552014 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.87354664  0.72965049  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 12:51:33 2024]  Iteration number: 0 with current cost as 0.19763134090932857 and parameters 
[64.41734463  2.23744483 -2.12428983 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18554037 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87353661  0.72963042  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:53:32 2024]  Iteration number: 0 with current cost as 0.20134480339841582 and parameters 
[ 1.53284567  2.23743464 -2.12427964 -0.11652953  0.55388708 -2.77010897
  3.06858498  2.1896022   1.18552224 -1.06648383  0.60271585  1.14432445
  1.31029899 -1.87354605  0.7296493   2.88578344 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:55:30 2024]  Iteration number: 0 with current cost as 0.5086763801217657 and parameters 
[-1.61121048  2.23743464 -2.12427964 -0.11653069  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552032 -1.06648325  0.60271527  1.14432445
  1.31029899 -1.87354663  0.72965047  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 579.8137927055359 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 12:57:15 2024]  Iteration number: 0 with current cost as 0.5502979012576306 and parameters 
[-1.74536127  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552044 -1.06648331  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965035  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 12:58:24 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743464 -2.12427964 -0.11653071  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18552014 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.87354664  0.72965049  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 13:01:09 2024]  Iteration number: 0 with current cost as 0.19763134090932857 and parameters 
[64.41734463  2.23744483 -2.12428983 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18554037 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87353661  0.72963042  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:03:07 2024]  Iteration number: 0 with current cost as 0.20134480339841582 and parameters 
[ 1.53284567  2.23743464 -2.12427964 -0.11652953  0.55388708 -2.77010897
  3.06858498  2.1896022   1.18552224 -1.06648383  0.60271585  1.14432445
  1.31029899 -1.87354605  0.7296493   2.88578344 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:05:07 2024]  Iteration number: 0 with current cost as 0.5086763801217657 and parameters 
[-1.61121048  2.23743464 -2.12427964 -0.11653069  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552032 -1.06648325  0.60271527  1.14432445
  1.31029899 -1.87354663  0.72965047  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 581.8867285251617 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 13:07:09 2024]  Iteration number: 0 with current cost as 0.5502979012576306 and parameters 
[-1.74536127  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552044 -1.06648331  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965035  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 13:08:19 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743464 -2.12427964 -0.11653071  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18552014 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.87354664  0.72965049  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 13:11:04 2024]  Iteration number: 0 with current cost as 0.19763134090932857 and parameters 
[64.41734463  2.23744483 -2.12428983 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18554037 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87353661  0.72963042  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:13:00 2024]  Iteration number: 0 with current cost as 0.20134480339841582 and parameters 
[ 1.53284567  2.23743464 -2.12427964 -0.11652953  0.55388708 -2.77010897
  3.06858498  2.1896022   1.18552224 -1.06648383  0.60271585  1.14432445
  1.31029899 -1.87354605  0.7296493   2.88578344 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:14:57 2024]  Iteration number: 0 with current cost as 0.5086763801217657 and parameters 
[-1.61121048  2.23743464 -2.12427964 -0.11653069  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552032 -1.06648325  0.60271527  1.14432445
  1.31029899 -1.87354663  0.72965047  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 585.7455308437347 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 13:16:34 2024]  Iteration number: 0 with current cost as 0.5502979012576306 and parameters 
[-1.74536127  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552044 -1.06648331  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965035  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 13:17:44 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743464 -2.12427964 -0.11653071  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18552014 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.87354664  0.72965049  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 13:20:30 2024]  Iteration number: 0 with current cost as 0.19763134090932857 and parameters 
[64.41734463  2.23744483 -2.12428983 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18554037 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87353661  0.72963042  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:22:31 2024]  Iteration number: 0 with current cost as 0.20134480339841582 and parameters 
[ 1.53284567  2.23743464 -2.12427964 -0.11652953  0.55388708 -2.77010897
  3.06858498  2.1896022   1.18552224 -1.06648383  0.60271585  1.14432445
  1.31029899 -1.87354605  0.7296493   2.88578344 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:24:29 2024]  Iteration number: 0 with current cost as 0.5086763801217657 and parameters 
[-1.61121048  2.23743464 -2.12427964 -0.11653069  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552032 -1.06648325  0.60271527  1.14432445
  1.31029899 -1.87354663  0.72965047  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 571.54509973526 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 13:26:06 2024]  Iteration number: 0 with current cost as 0.5502979012576306 and parameters 
[-1.74536127  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552044 -1.06648331  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965035  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 13:27:14 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743464 -2.12427964 -0.11653071  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18552014 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.87354664  0.72965049  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 13:30:04 2024]  Iteration number: 0 with current cost as 0.19763134090932857 and parameters 
[64.41734463  2.23744483 -2.12428983 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18554037 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87353661  0.72963042  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:32:05 2024]  Iteration number: 0 with current cost as 0.20134480339841582 and parameters 
[ 1.53284567  2.23743464 -2.12427964 -0.11652953  0.55388708 -2.77010897
  3.06858498  2.1896022   1.18552224 -1.06648383  0.60271585  1.14432445
  1.31029899 -1.87354605  0.7296493   2.88578344 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:34:03 2024]  Iteration number: 0 with current cost as 0.5086763801217657 and parameters 
[-1.61121048  2.23743464 -2.12427964 -0.11653069  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552032 -1.06648325  0.60271527  1.14432445
  1.31029899 -1.87354663  0.72965047  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 574.2654025554657 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 13:35:40 2024]  Iteration number: 0 with current cost as 0.5502979012576306 and parameters 
[-1.74536127  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552044 -1.06648331  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965035  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 13:36:54 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743464 -2.12427964 -0.11653071  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18552014 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.87354664  0.72965049  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 13:40:00 2024]  Iteration number: 0 with current cost as 0.19763134090932857 and parameters 
[64.41734463  2.23744483 -2.12428983 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18554037 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87353661  0.72963042  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:41:57 2024]  Iteration number: 0 with current cost as 0.20134480339841582 and parameters 
[ 1.53284567  2.23743464 -2.12427964 -0.11652953  0.55388708 -2.77010897
  3.06858498  2.1896022   1.18552224 -1.06648383  0.60271585  1.14432445
  1.31029899 -1.87354605  0.7296493   2.88578344 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:43:54 2024]  Iteration number: 0 with current cost as 0.5086763801217657 and parameters 
[-1.61121048  2.23743464 -2.12427964 -0.11653069  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552032 -1.06648325  0.60271527  1.14432445
  1.31029899 -1.87354663  0.72965047  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 600.6590209007263 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 13:45:41 2024]  Iteration number: 0 with current cost as 0.5502979012576306 and parameters 
[-1.74536127  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552044 -1.06648331  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965035  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 13:46:52 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743464 -2.12427964 -0.11653071  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18552014 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.87354664  0.72965049  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 13:49:36 2024]  Iteration number: 0 with current cost as 0.19763134090932857 and parameters 
[64.41734463  2.23744483 -2.12428983 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18554037 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87353661  0.72963042  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:51:33 2024]  Iteration number: 0 with current cost as 0.20134480339841582 and parameters 
[ 1.53284567  2.23743464 -2.12427964 -0.11652953  0.55388708 -2.77010897
  3.06858498  2.1896022   1.18552224 -1.06648383  0.60271585  1.14432445
  1.31029899 -1.87354605  0.7296493   2.88578344 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:53:30 2024]  Iteration number: 0 with current cost as 0.5086763801217657 and parameters 
[-1.61121048  2.23743464 -2.12427964 -0.11653069  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552032 -1.06648325  0.60271527  1.14432445
  1.31029899 -1.87354663  0.72965047  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 566.9723763465881 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 13:55:08 2024]  Iteration number: 0 with current cost as 0.5502979012576306 and parameters 
[-1.74536127  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552044 -1.06648331  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965035  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 13:56:17 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743464 -2.12427964 -0.11653071  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18552014 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.87354664  0.72965049  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 13:59:03 2024]  Iteration number: 0 with current cost as 0.19763134090932857 and parameters 
[64.41734463  2.23744483 -2.12428983 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18554037 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87353661  0.72963042  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 14:01:00 2024]  Iteration number: 0 with current cost as 0.20134480339841582 and parameters 
[ 1.53284567  2.23743464 -2.12427964 -0.11652953  0.55388708 -2.77010897
  3.06858498  2.1896022   1.18552224 -1.06648383  0.60271585  1.14432445
  1.31029899 -1.87354605  0.7296493   2.88578344 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 14:02:56 2024]  Iteration number: 0 with current cost as 0.5086763801217657 and parameters 
[-1.61121048  2.23743464 -2.12427964 -0.11653069  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552032 -1.06648325  0.60271527  1.14432445
  1.31029899 -1.87354663  0.72965047  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 565.7049691677094 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 14:04:37 2024]  Iteration number: 0 with current cost as 0.5502979012576306 and parameters 
[-1.74536127  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552044 -1.06648331  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965035  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 14:05:57 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743464 -2.12427964 -0.11653071  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18552014 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.87354664  0.72965049  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 14:08:46 2024]  Iteration number: 0 with current cost as 0.19763134090932857 and parameters 
[64.41734463  2.23744483 -2.12428983 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18554037 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87353661  0.72963042  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 14:10:42 2024]  Iteration number: 0 with current cost as 0.20134480339841582 and parameters 
[ 1.53284567  2.23743464 -2.12427964 -0.11652953  0.55388708 -2.77010897
  3.06858498  2.1896022   1.18552224 -1.06648383  0.60271585  1.14432445
  1.31029899 -1.87354605  0.7296493   2.88578344 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 14:12:48 2024]  Iteration number: 0 with current cost as 0.5086763801217657 and parameters 
[-1.61121048  2.23743464 -2.12427964 -0.11653069  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552032 -1.06648325  0.60271527  1.14432445
  1.31029899 -1.87354663  0.72965047  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 591.5394394397736 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 14:14:25 2024]  Iteration number: 0 with current cost as 0.5502979012576306 and parameters 
[-1.74536127  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552044 -1.06648331  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965035  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 14:15:33 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743464 -2.12427964 -0.11653071  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18552014 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.87354664  0.72965049  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 14:18:19 2024]  Iteration number: 0 with current cost as 0.19763134090932857 and parameters 
[64.41734463  2.23744483 -2.12428983 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18554037 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87353661  0.72963042  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 14:20:16 2024]  Iteration number: 0 with current cost as 0.20134480339841582 and parameters 
[ 1.53284567  2.23743464 -2.12427964 -0.11652953  0.55388708 -2.77010897
  3.06858498  2.1896022   1.18552224 -1.06648383  0.60271585  1.14432445
  1.31029899 -1.87354605  0.7296493   2.88578344 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 14:22:12 2024]  Iteration number: 0 with current cost as 0.5086763801217657 and parameters 
[-1.61121048  2.23743464 -2.12427964 -0.11653069  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552032 -1.06648325  0.60271527  1.14432445
  1.31029899 -1.87354663  0.72965047  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 563.6447439193726 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 14:23:51 2024]  Iteration number: 0 with current cost as 0.5502979012576306 and parameters 
[-1.74536127  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552044 -1.06648331  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965035  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 14:24:59 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743464 -2.12427964 -0.11653071  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18552014 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.87354664  0.72965049  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 14:27:44 2024]  Iteration number: 0 with current cost as 0.19763134090932857 and parameters 
[64.41734463  2.23744483 -2.12428983 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18554037 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87353661  0.72963042  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 14:29:42 2024]  Iteration number: 0 with current cost as 0.20134480339841582 and parameters 
[ 1.53284567  2.23743464 -2.12427964 -0.11652953  0.55388708 -2.77010897
  3.06858498  2.1896022   1.18552224 -1.06648383  0.60271585  1.14432445
  1.31029899 -1.87354605  0.7296493   2.88578344 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 14:31:40 2024]  Iteration number: 0 with current cost as 0.5086763801217657 and parameters 
[-1.61121048  2.23743464 -2.12427964 -0.11653069  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552032 -1.06648325  0.60271527  1.14432445
  1.31029899 -1.87354663  0.72965047  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 568.3063037395477 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 14:33:18 2024]  Iteration number: 0 with current cost as 0.5502979012576306 and parameters 
[-1.74536127  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552044 -1.06648331  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965035  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 14:34:26 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743464 -2.12427964 -0.11653071  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18552014 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.87354664  0.72965049  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 14:37:10 2024]  Iteration number: 0 with current cost as 0.19763134090932857 and parameters 
[64.41734463  2.23744483 -2.12428983 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18554037 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87353661  0.72963042  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 14:39:09 2024]  Iteration number: 0 with current cost as 0.20134480339841582 and parameters 
[ 1.53284567  2.23743464 -2.12427964 -0.11652953  0.55388708 -2.77010897
  3.06858498  2.1896022   1.18552224 -1.06648383  0.60271585  1.14432445
  1.31029899 -1.87354605  0.7296493   2.88578344 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 14:41:07 2024]  Iteration number: 0 with current cost as 0.5086763801217657 and parameters 
[-1.61121048  2.23743464 -2.12427964 -0.11653069  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552032 -1.06648325  0.60271527  1.14432445
  1.31029899 -1.87354663  0.72965047  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 566.6427075862885 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 14:42:45 2024]  Iteration number: 0 with current cost as 0.5502979012576306 and parameters 
[-1.74536127  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552044 -1.06648331  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965035  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 14:43:55 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743464 -2.12427964 -0.11653071  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18552014 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.87354664  0.72965049  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 14:46:40 2024]  Iteration number: 0 with current cost as 0.19763134090932857 and parameters 
[64.41734463  2.23744483 -2.12428983 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18554037 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87353661  0.72963042  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 14:48:39 2024]  Iteration number: 0 with current cost as 0.20134480339841582 and parameters 
[ 1.53284567  2.23743464 -2.12427964 -0.11652953  0.55388708 -2.77010897
  3.06858498  2.1896022   1.18552224 -1.06648383  0.60271585  1.14432445
  1.31029899 -1.87354605  0.7296493   2.88578344 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 14:50:36 2024]  Iteration number: 0 with current cost as 0.5086763801217657 and parameters 
[-1.61121048  2.23743464 -2.12427964 -0.11653069  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552032 -1.06648325  0.60271527  1.14432445
  1.31029899 -1.87354663  0.72965047  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 571.5483984947205 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 14:52:15 2024]  Iteration number: 0 with current cost as 0.5502979012576306 and parameters 
[-1.74536127  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552044 -1.06648331  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965035  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 14:53:25 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743464 -2.12427964 -0.11653071  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18552014 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.87354664  0.72965049  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 14:56:09 2024]  Iteration number: 0 with current cost as 0.19763134090932857 and parameters 
[64.41734463  2.23744483 -2.12428983 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18554037 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87353661  0.72963042  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 14:58:06 2024]  Iteration number: 0 with current cost as 0.20134480339841582 and parameters 
[ 1.53284567  2.23743464 -2.12427964 -0.11652953  0.55388708 -2.77010897
  3.06858498  2.1896022   1.18552224 -1.06648383  0.60271585  1.14432445
  1.31029899 -1.87354605  0.7296493   2.88578344 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 15:00:03 2024]  Iteration number: 0 with current cost as 0.5086763801217657 and parameters 
[-1.61121048  2.23743464 -2.12427964 -0.11653069  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552032 -1.06648325  0.60271527  1.14432445
  1.31029899 -1.87354663  0.72965047  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 564.9550578594208 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 15:01:41 2024]  Iteration number: 0 with current cost as 0.5502979012576306 and parameters 
[-1.74536127  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552044 -1.06648331  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965035  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 15:02:50 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743464 -2.12427964 -0.11653071  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18552014 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.87354664  0.72965049  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 15:05:49 2024]  Iteration number: 0 with current cost as 0.19763134090932857 and parameters 
[64.41734463  2.23744483 -2.12428983 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18554037 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87353661  0.72963042  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 15:07:45 2024]  Iteration number: 0 with current cost as 0.20134480339841582 and parameters 
[ 1.53284567  2.23743464 -2.12427964 -0.11652953  0.55388708 -2.77010897
  3.06858498  2.1896022   1.18552224 -1.06648383  0.60271585  1.14432445
  1.31029899 -1.87354605  0.7296493   2.88578344 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 15:09:41 2024]  Iteration number: 0 with current cost as 0.5086763801217657 and parameters 
[-1.61121048  2.23743464 -2.12427964 -0.11653069  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552032 -1.06648325  0.60271527  1.14432445
  1.31029899 -1.87354663  0.72965047  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 577.888988494873 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 15:11:20 2024]  Iteration number: 0 with current cost as 0.5502979012576306 and parameters 
[-1.74536127  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552044 -1.06648331  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965035  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 15:12:28 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743464 -2.12427964 -0.11653071  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18552014 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.87354664  0.72965049  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 15:15:15 2024]  Iteration number: 0 with current cost as 0.19763134090932857 and parameters 
[64.41734463  2.23744483 -2.12428983 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18554037 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87353661  0.72963042  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 15:17:12 2024]  Iteration number: 0 with current cost as 0.20134480339841582 and parameters 
[ 1.53284567  2.23743464 -2.12427964 -0.11652953  0.55388708 -2.77010897
  3.06858498  2.1896022   1.18552224 -1.06648383  0.60271585  1.14432445
  1.31029899 -1.87354605  0.7296493   2.88578344 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 15:19:09 2024]  Iteration number: 0 with current cost as 0.5086763801217657 and parameters 
[-1.61121048  2.23743464 -2.12427964 -0.11653069  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552032 -1.06648325  0.60271527  1.14432445
  1.31029899 -1.87354663  0.72965047  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 567.5611326694489 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 15:20:45 2024]  Iteration number: 0 with current cost as 0.5502979012576306 and parameters 
[-1.74536127  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552044 -1.06648331  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965035  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 15:21:55 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743464 -2.12427964 -0.11653071  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18552014 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.87354664  0.72965049  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 15:24:41 2024]  Iteration number: 0 with current cost as 0.19763134090932857 and parameters 
[64.41734463  2.23744483 -2.12428983 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18554037 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87353661  0.72963042  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 15:26:39 2024]  Iteration number: 0 with current cost as 0.20134480339841582 and parameters 
[ 1.53284567  2.23743464 -2.12427964 -0.11652953  0.55388708 -2.77010897
  3.06858498  2.1896022   1.18552224 -1.06648383  0.60271585  1.14432445
  1.31029899 -1.87354605  0.7296493   2.88578344 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 15:28:36 2024]  Iteration number: 0 with current cost as 0.5086763801217657 and parameters 
[-1.61121048  2.23743464 -2.12427964 -0.11653069  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552032 -1.06648325  0.60271527  1.14432445
  1.31029899 -1.87354663  0.72965047  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 567.3457467556 seconds. 
Discarding model... 

Training complete taking 14422.858087062836 total seconds. 
Now scoring model... 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Scoring complete taking 1.125028371810913 seconds. 
Saved predicted values as M-M-CZ_Modified-Pauli-CRZ_predicted_values.csv
Model scores: {'MSE_train': (322.73150773416654,), 'R2_train': -0.5635258411406567, 'MAE_train': 16.55376646011674, 'MSE_test': 301.1515605306635, 'R2_test': -0.8152855655970617, 'MAE_test': 16.304477363287884}. 
Saved model results as M-M-CZ_Modified-Pauli-CRZ_results.json. 
