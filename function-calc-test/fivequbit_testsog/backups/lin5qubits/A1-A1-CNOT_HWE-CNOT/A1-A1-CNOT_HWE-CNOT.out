/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Sun Mar 24 17:30:14 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 17:30:25 2024]  Iteration number: 0 with current cost as 0.17395685233024477 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.03787391  0.58658104 -2.62572058
  3.03473395  2.16439753  1.25823118 -1.06953788  0.64144427  1.15272522
  1.37714082 -1.81484778  0.71559942]. 
Working on 0.4 fold... 
[Sun Mar 24 17:32:51 2024]  Iteration number: 0 with current cost as 0.1335791266521221 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.06541265  0.58115123 -2.6667207
  3.04127202  2.17025483  1.24362412 -1.06938598  0.62880195  1.1489569
  1.36295953 -1.82456036  0.72839363]. 
Working on 0.6 fold... 
[Sun Mar 24 17:35:28 2024]  Iteration number: 0 with current cost as 0.14756195771492825 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.05535157  0.58317736 -2.65167431
  3.03756892  2.16610963  1.25236878 -1.06966166  0.6367854   1.15111834
  1.36682789 -1.82286503  0.72148814]. 
Working on 0.8 fold... 
[Sun Mar 24 17:38:02 2024]  Iteration number: 0 with current cost as 0.07504599153998551 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.21867354  0.68860824 -2.16209864
  2.91077362  2.08769239  1.51561641 -1.07817476  0.77252731  1.1832251
  1.61004309 -1.60968458  0.66890713]. 
Working on 1.0 fold... 
[Sun Mar 24 17:40:22 2024]  Iteration number: 0 with current cost as 0.1492660672200189 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.05762623  0.58333495 -2.65409866
  3.03843004  2.16713351  1.25030115 -1.06943148  0.63464857  1.15072966
  1.36663952 -1.82225953  0.7242876 ]. 
Training complete taking 749.123302936554 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 17:42:54 2024]  Iteration number: 0 with current cost as 0.17395685233024477 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.03787391  0.58658104 -2.62572058
  3.03473395  2.16439753  1.25823118 -1.06953788  0.64144427  1.15272522
  1.37714082 -1.81484778  0.71559942]. 
Working on 0.4 fold... 
[Sun Mar 24 17:45:08 2024]  Iteration number: 0 with current cost as 0.1335791266521221 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.06541265  0.58115123 -2.6667207
  3.04127202  2.17025483  1.24362412 -1.06938598  0.62880195  1.1489569
  1.36295953 -1.82456036  0.72839363]. 
Working on 0.6 fold... 
[Sun Mar 24 17:47:44 2024]  Iteration number: 0 with current cost as 0.14756195771492825 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.05535157  0.58317736 -2.65167431
  3.03756892  2.16610963  1.25236878 -1.06966166  0.6367854   1.15111834
  1.36682789 -1.82286503  0.72148814]. 
Working on 0.8 fold... 
[Sun Mar 24 17:50:20 2024]  Iteration number: 0 with current cost as 0.07504599153998551 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.21867354  0.68860824 -2.16209864
  2.91077362  2.08769239  1.51561641 -1.07817476  0.77252731  1.1832251
  1.61004309 -1.60968458  0.66890713]. 
Working on 1.0 fold... 
[Sun Mar 24 17:52:51 2024]  Iteration number: 0 with current cost as 0.1492660672200189 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.05762623  0.58333495 -2.65409866
  3.03843004  2.16713351  1.25030115 -1.06943148  0.63464857  1.15072966
  1.36663952 -1.82225953  0.7242876 ]. 
Training complete taking 748.1846055984497 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 17:55:23 2024]  Iteration number: 0 with current cost as 0.17395685233024477 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.03787391  0.58658104 -2.62572058
  3.03473395  2.16439753  1.25823118 -1.06953788  0.64144427  1.15272522
  1.37714082 -1.81484778  0.71559942]. 
Working on 0.4 fold... 
[Sun Mar 24 17:57:35 2024]  Iteration number: 0 with current cost as 0.1335791266521221 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.06541265  0.58115123 -2.6667207
  3.04127202  2.17025483  1.24362412 -1.06938598  0.62880195  1.1489569
  1.36295953 -1.82456036  0.72839363]. 
Working on 0.6 fold... 
[Sun Mar 24 18:00:10 2024]  Iteration number: 0 with current cost as 0.14756195771492825 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.05535157  0.58317736 -2.65167431
  3.03756892  2.16610963  1.25236878 -1.06966166  0.6367854   1.15111834
  1.36682789 -1.82286503  0.72148814]. 
Working on 0.8 fold... 
[Sun Mar 24 18:02:44 2024]  Iteration number: 0 with current cost as 0.07504599153998551 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.21867354  0.68860824 -2.16209864
  2.91077362  2.08769239  1.51561641 -1.07817476  0.77252731  1.1832251
  1.61004309 -1.60968458  0.66890713]. 
Working on 1.0 fold... 
[Sun Mar 24 18:05:10 2024]  Iteration number: 0 with current cost as 0.1492660672200189 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.05762623  0.58333495 -2.65409866
  3.03843004  2.16713351  1.25030115 -1.06943148  0.63464857  1.15072966
  1.36663952 -1.82225953  0.7242876 ]. 
Training complete taking 741.3262059688568 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 18:07:41 2024]  Iteration number: 0 with current cost as 0.17395685233024477 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.03787391  0.58658104 -2.62572058
  3.03473395  2.16439753  1.25823118 -1.06953788  0.64144427  1.15272522
  1.37714082 -1.81484778  0.71559942]. 
Working on 0.4 fold... 
[Sun Mar 24 18:09:55 2024]  Iteration number: 0 with current cost as 0.1335791266521221 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.06541265  0.58115123 -2.6667207
  3.04127202  2.17025483  1.24362412 -1.06938598  0.62880195  1.1489569
  1.36295953 -1.82456036  0.72839363]. 
Working on 0.6 fold... 
[Sun Mar 24 18:12:30 2024]  Iteration number: 0 with current cost as 0.14756195771492825 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.05535157  0.58317736 -2.65167431
  3.03756892  2.16610963  1.25236878 -1.06966166  0.6367854   1.15111834
  1.36682789 -1.82286503  0.72148814]. 
Working on 0.8 fold... 
[Sun Mar 24 18:15:04 2024]  Iteration number: 0 with current cost as 0.07504599153998551 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.21867354  0.68860824 -2.16209864
  2.91077362  2.08769239  1.51561641 -1.07817476  0.77252731  1.1832251
  1.61004309 -1.60968458  0.66890713]. 
Working on 1.0 fold... 
[Sun Mar 24 18:17:26 2024]  Iteration number: 0 with current cost as 0.1492660672200189 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.05762623  0.58333495 -2.65409866
  3.03843004  2.16713351  1.25030115 -1.06943148  0.63464857  1.15072966
  1.36663952 -1.82225953  0.7242876 ]. 
Training complete taking 740.9649157524109 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 18:20:02 2024]  Iteration number: 0 with current cost as 0.17395685233024477 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.03787391  0.58658104 -2.62572058
  3.03473395  2.16439753  1.25823118 -1.06953788  0.64144427  1.15272522
  1.37714082 -1.81484778  0.71559942]. 
Working on 0.4 fold... 
[Sun Mar 24 18:22:14 2024]  Iteration number: 0 with current cost as 0.1335791266521221 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.06541265  0.58115123 -2.6667207
  3.04127202  2.17025483  1.24362412 -1.06938598  0.62880195  1.1489569
  1.36295953 -1.82456036  0.72839363]. 
Working on 0.6 fold... 
[Sun Mar 24 18:24:53 2024]  Iteration number: 0 with current cost as 0.14756195771492825 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.05535157  0.58317736 -2.65167431
  3.03756892  2.16610963  1.25236878 -1.06966166  0.6367854   1.15111834
  1.36682789 -1.82286503  0.72148814]. 
Working on 0.8 fold... 
[Sun Mar 24 18:27:28 2024]  Iteration number: 0 with current cost as 0.07504599153998551 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.21867354  0.68860824 -2.16209864
  2.91077362  2.08769239  1.51561641 -1.07817476  0.77252731  1.1832251
  1.61004309 -1.60968458  0.66890713]. 
Working on 1.0 fold... 
[Sun Mar 24 18:29:48 2024]  Iteration number: 0 with current cost as 0.1492660672200189 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.05762623  0.58333495 -2.65409866
  3.03843004  2.16713351  1.25030115 -1.06943148  0.63464857  1.15072966
  1.36663952 -1.82225953  0.7242876 ]. 
Training complete taking 734.0252149105072 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 18:32:16 2024]  Iteration number: 0 with current cost as 0.17395685233024477 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.03787391  0.58658104 -2.62572058
  3.03473395  2.16439753  1.25823118 -1.06953788  0.64144427  1.15272522
  1.37714082 -1.81484778  0.71559942]. 
Working on 0.4 fold... 
[Sun Mar 24 18:34:28 2024]  Iteration number: 0 with current cost as 0.1335791266521221 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.06541265  0.58115123 -2.6667207
  3.04127202  2.17025483  1.24362412 -1.06938598  0.62880195  1.1489569
  1.36295953 -1.82456036  0.72839363]. 
Working on 0.6 fold... 
[Sun Mar 24 18:37:03 2024]  Iteration number: 0 with current cost as 0.14756195771492825 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.05535157  0.58317736 -2.65167431
  3.03756892  2.16610963  1.25236878 -1.06966166  0.6367854   1.15111834
  1.36682789 -1.82286503  0.72148814]. 
Working on 0.8 fold... 
[Sun Mar 24 18:39:36 2024]  Iteration number: 0 with current cost as 0.07504599153998551 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.21867354  0.68860824 -2.16209864
  2.91077362  2.08769239  1.51561641 -1.07817476  0.77252731  1.1832251
  1.61004309 -1.60968458  0.66890713]. 
Working on 1.0 fold... 
[Sun Mar 24 18:41:56 2024]  Iteration number: 0 with current cost as 0.1492660672200189 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.05762623  0.58333495 -2.65409866
  3.03843004  2.16713351  1.25030115 -1.06943148  0.63464857  1.15072966
  1.36663952 -1.82225953  0.7242876 ]. 
Training complete taking 736.6628968715668 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 18:44:33 2024]  Iteration number: 0 with current cost as 0.17395685233024477 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.03787391  0.58658104 -2.62572058
  3.03473395  2.16439753  1.25823118 -1.06953788  0.64144427  1.15272522
  1.37714082 -1.81484778  0.71559942]. 
Working on 0.4 fold... 
[Sun Mar 24 18:46:46 2024]  Iteration number: 0 with current cost as 0.1335791266521221 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.06541265  0.58115123 -2.6667207
  3.04127202  2.17025483  1.24362412 -1.06938598  0.62880195  1.1489569
  1.36295953 -1.82456036  0.72839363]. 
Working on 0.6 fold... 
[Sun Mar 24 18:49:23 2024]  Iteration number: 0 with current cost as 0.14756195771492825 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.05535157  0.58317736 -2.65167431
  3.03756892  2.16610963  1.25236878 -1.06966166  0.6367854   1.15111834
  1.36682789 -1.82286503  0.72148814]. 
Working on 0.8 fold... 
[Sun Mar 24 18:52:07 2024]  Iteration number: 0 with current cost as 0.07504599153998551 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.21867354  0.68860824 -2.16209864
  2.91077362  2.08769239  1.51561641 -1.07817476  0.77252731  1.1832251
  1.61004309 -1.60968458  0.66890713]. 
Working on 1.0 fold... 
[Sun Mar 24 18:54:30 2024]  Iteration number: 0 with current cost as 0.1492660672200189 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.05762623  0.58333495 -2.65409866
  3.03843004  2.16713351  1.25030115 -1.06943148  0.63464857  1.15072966
  1.36663952 -1.82225953  0.7242876 ]. 
Training complete taking 743.2676813602448 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 18:57:04 2024]  Iteration number: 0 with current cost as 0.17395685233024477 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.03787391  0.58658104 -2.62572058
  3.03473395  2.16439753  1.25823118 -1.06953788  0.64144427  1.15272522
  1.37714082 -1.81484778  0.71559942]. 
Working on 0.4 fold... 
[Sun Mar 24 18:59:15 2024]  Iteration number: 0 with current cost as 0.1335791266521221 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.06541265  0.58115123 -2.6667207
  3.04127202  2.17025483  1.24362412 -1.06938598  0.62880195  1.1489569
  1.36295953 -1.82456036  0.72839363]. 
Working on 0.6 fold... 
[Sun Mar 24 19:01:47 2024]  Iteration number: 0 with current cost as 0.14756195771492825 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.05535157  0.58317736 -2.65167431
  3.03756892  2.16610963  1.25236878 -1.06966166  0.6367854   1.15111834
  1.36682789 -1.82286503  0.72148814]. 
Working on 0.8 fold... 
[Sun Mar 24 19:04:28 2024]  Iteration number: 0 with current cost as 0.07504599153998551 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.21867354  0.68860824 -2.16209864
  2.91077362  2.08769239  1.51561641 -1.07817476  0.77252731  1.1832251
  1.61004309 -1.60968458  0.66890713]. 
Working on 1.0 fold... 
[Sun Mar 24 19:06:50 2024]  Iteration number: 0 with current cost as 0.1492660672200189 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.05762623  0.58333495 -2.65409866
  3.03843004  2.16713351  1.25030115 -1.06943148  0.63464857  1.15072966
  1.36663952 -1.82225953  0.7242876 ]. 
Training complete taking 741.8426342010498 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 19:09:18 2024]  Iteration number: 0 with current cost as 0.17395685233024477 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.03787391  0.58658104 -2.62572058
  3.03473395  2.16439753  1.25823118 -1.06953788  0.64144427  1.15272522
  1.37714082 -1.81484778  0.71559942]. 
Working on 0.4 fold... 
[Sun Mar 24 19:11:30 2024]  Iteration number: 0 with current cost as 0.1335791266521221 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.06541265  0.58115123 -2.6667207
  3.04127202  2.17025483  1.24362412 -1.06938598  0.62880195  1.1489569
  1.36295953 -1.82456036  0.72839363]. 
Working on 0.6 fold... 
[Sun Mar 24 19:14:05 2024]  Iteration number: 0 with current cost as 0.14756195771492825 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.05535157  0.58317736 -2.65167431
  3.03756892  2.16610963  1.25236878 -1.06966166  0.6367854   1.15111834
  1.36682789 -1.82286503  0.72148814]. 
Working on 0.8 fold... 
[Sun Mar 24 19:16:40 2024]  Iteration number: 0 with current cost as 0.07504599153998551 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.21867354  0.68860824 -2.16209864
  2.91077362  2.08769239  1.51561641 -1.07817476  0.77252731  1.1832251
  1.61004309 -1.60968458  0.66890713]. 
Working on 1.0 fold... 
[Sun Mar 24 19:19:01 2024]  Iteration number: 0 with current cost as 0.1492660672200189 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.05762623  0.58333495 -2.65409866
  3.03843004  2.16713351  1.25030115 -1.06943148  0.63464857  1.15072966
  1.36663952 -1.82225953  0.7242876 ]. 
Training complete taking 736.6300342082977 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 19:21:35 2024]  Iteration number: 0 with current cost as 0.17395685233024477 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.03787391  0.58658104 -2.62572058
  3.03473395  2.16439753  1.25823118 -1.06953788  0.64144427  1.15272522
  1.37714082 -1.81484778  0.71559942]. 
Working on 0.4 fold... 
[Sun Mar 24 19:23:46 2024]  Iteration number: 0 with current cost as 0.1335791266521221 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.06541265  0.58115123 -2.6667207
  3.04127202  2.17025483  1.24362412 -1.06938598  0.62880195  1.1489569
  1.36295953 -1.82456036  0.72839363]. 
Working on 0.6 fold... 
[Sun Mar 24 19:26:20 2024]  Iteration number: 0 with current cost as 0.14756195771492825 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.05535157  0.58317736 -2.65167431
  3.03756892  2.16610963  1.25236878 -1.06966166  0.6367854   1.15111834
  1.36682789 -1.82286503  0.72148814]. 
Working on 0.8 fold... 
[Sun Mar 24 19:28:55 2024]  Iteration number: 0 with current cost as 0.07504599153998551 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.21867354  0.68860824 -2.16209864
  2.91077362  2.08769239  1.51561641 -1.07817476  0.77252731  1.1832251
  1.61004309 -1.60968458  0.66890713]. 
Working on 1.0 fold... 
[Sun Mar 24 19:31:15 2024]  Iteration number: 0 with current cost as 0.1492660672200189 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.05762623  0.58333495 -2.65409866
  3.03843004  2.16713351  1.25030115 -1.06943148  0.63464857  1.15072966
  1.36663952 -1.82225953  0.7242876 ]. 
Training complete taking 729.7369315624237 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 19:33:44 2024]  Iteration number: 0 with current cost as 0.17395685233024477 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.03787391  0.58658104 -2.62572058
  3.03473395  2.16439753  1.25823118 -1.06953788  0.64144427  1.15272522
  1.37714082 -1.81484778  0.71559942]. 
Working on 0.4 fold... 
[Sun Mar 24 19:35:57 2024]  Iteration number: 0 with current cost as 0.1335791266521221 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.06541265  0.58115123 -2.6667207
  3.04127202  2.17025483  1.24362412 -1.06938598  0.62880195  1.1489569
  1.36295953 -1.82456036  0.72839363]. 
Working on 0.6 fold... 
[Sun Mar 24 19:38:31 2024]  Iteration number: 0 with current cost as 0.14756195771492825 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.05535157  0.58317736 -2.65167431
  3.03756892  2.16610963  1.25236878 -1.06966166  0.6367854   1.15111834
  1.36682789 -1.82286503  0.72148814]. 
Working on 0.8 fold... 
[Sun Mar 24 19:41:08 2024]  Iteration number: 0 with current cost as 0.07504599153998551 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.21867354  0.68860824 -2.16209864
  2.91077362  2.08769239  1.51561641 -1.07817476  0.77252731  1.1832251
  1.61004309 -1.60968458  0.66890713]. 
Working on 1.0 fold... 
[Sun Mar 24 19:43:34 2024]  Iteration number: 0 with current cost as 0.1492660672200189 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.05762623  0.58333495 -2.65409866
  3.03843004  2.16713351  1.25030115 -1.06943148  0.63464857  1.15072966
  1.36663952 -1.82225953  0.7242876 ]. 
Training complete taking 740.1921317577362 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 19:46:05 2024]  Iteration number: 0 with current cost as 0.17395685233024477 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.03787391  0.58658104 -2.62572058
  3.03473395  2.16439753  1.25823118 -1.06953788  0.64144427  1.15272522
  1.37714082 -1.81484778  0.71559942]. 
Working on 0.4 fold... 
[Sun Mar 24 19:48:17 2024]  Iteration number: 0 with current cost as 0.1335791266521221 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.06541265  0.58115123 -2.6667207
  3.04127202  2.17025483  1.24362412 -1.06938598  0.62880195  1.1489569
  1.36295953 -1.82456036  0.72839363]. 
Working on 0.6 fold... 
[Sun Mar 24 19:50:51 2024]  Iteration number: 0 with current cost as 0.14756195771492825 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.05535157  0.58317736 -2.65167431
  3.03756892  2.16610963  1.25236878 -1.06966166  0.6367854   1.15111834
  1.36682789 -1.82286503  0.72148814]. 
Working on 0.8 fold... 
[Sun Mar 24 19:53:35 2024]  Iteration number: 0 with current cost as 0.07504599153998551 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.21867354  0.68860824 -2.16209864
  2.91077362  2.08769239  1.51561641 -1.07817476  0.77252731  1.1832251
  1.61004309 -1.60968458  0.66890713]. 
Working on 1.0 fold... 
[Sun Mar 24 19:56:00 2024]  Iteration number: 0 with current cost as 0.1492660672200189 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.05762623  0.58333495 -2.65409866
  3.03843004  2.16713351  1.25030115 -1.06943148  0.63464857  1.15072966
  1.36663952 -1.82225953  0.7242876 ]. 
Training complete taking 743.3097388744354 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 19:58:29 2024]  Iteration number: 0 with current cost as 0.17395685233024477 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.03787391  0.58658104 -2.62572058
  3.03473395  2.16439753  1.25823118 -1.06953788  0.64144427  1.15272522
  1.37714082 -1.81484778  0.71559942]. 
Working on 0.4 fold... 
[Sun Mar 24 20:00:41 2024]  Iteration number: 0 with current cost as 0.1335791266521221 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.06541265  0.58115123 -2.6667207
  3.04127202  2.17025483  1.24362412 -1.06938598  0.62880195  1.1489569
  1.36295953 -1.82456036  0.72839363]. 
Working on 0.6 fold... 
[Sun Mar 24 20:03:18 2024]  Iteration number: 0 with current cost as 0.14756195771492825 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.05535157  0.58317736 -2.65167431
  3.03756892  2.16610963  1.25236878 -1.06966166  0.6367854   1.15111834
  1.36682789 -1.82286503  0.72148814]. 
Working on 0.8 fold... 
[Sun Mar 24 20:05:51 2024]  Iteration number: 0 with current cost as 0.07504599153998551 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.21867354  0.68860824 -2.16209864
  2.91077362  2.08769239  1.51561641 -1.07817476  0.77252731  1.1832251
  1.61004309 -1.60968458  0.66890713]. 
Working on 1.0 fold... 
[Sun Mar 24 20:08:11 2024]  Iteration number: 0 with current cost as 0.1492660672200189 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.05762623  0.58333495 -2.65409866
  3.03843004  2.16713351  1.25030115 -1.06943148  0.63464857  1.15072966
  1.36663952 -1.82225953  0.7242876 ]. 
Training complete taking 732.4910378456116 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 20:10:40 2024]  Iteration number: 0 with current cost as 0.17395685233024477 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.03787391  0.58658104 -2.62572058
  3.03473395  2.16439753  1.25823118 -1.06953788  0.64144427  1.15272522
  1.37714082 -1.81484778  0.71559942]. 
Working on 0.4 fold... 
[Sun Mar 24 20:12:52 2024]  Iteration number: 0 with current cost as 0.1335791266521221 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.06541265  0.58115123 -2.6667207
  3.04127202  2.17025483  1.24362412 -1.06938598  0.62880195  1.1489569
  1.36295953 -1.82456036  0.72839363]. 
Working on 0.6 fold... 
[Sun Mar 24 20:15:35 2024]  Iteration number: 0 with current cost as 0.14756195771492825 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.05535157  0.58317736 -2.65167431
  3.03756892  2.16610963  1.25236878 -1.06966166  0.6367854   1.15111834
  1.36682789 -1.82286503  0.72148814]. 
Working on 0.8 fold... 
[Sun Mar 24 20:18:12 2024]  Iteration number: 0 with current cost as 0.07504599153998551 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.21867354  0.68860824 -2.16209864
  2.91077362  2.08769239  1.51561641 -1.07817476  0.77252731  1.1832251
  1.61004309 -1.60968458  0.66890713]. 
Working on 1.0 fold... 
[Sun Mar 24 20:20:32 2024]  Iteration number: 0 with current cost as 0.1492660672200189 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.05762623  0.58333495 -2.65409866
  3.03843004  2.16713351  1.25030115 -1.06943148  0.63464857  1.15072966
  1.36663952 -1.82225953  0.7242876 ]. 
Training complete taking 740.2678344249725 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 20:23:02 2024]  Iteration number: 0 with current cost as 0.17395685233024477 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.03787391  0.58658104 -2.62572058
  3.03473395  2.16439753  1.25823118 -1.06953788  0.64144427  1.15272522
  1.37714082 -1.81484778  0.71559942]. 
Working on 0.4 fold... 
[Sun Mar 24 20:25:13 2024]  Iteration number: 0 with current cost as 0.1335791266521221 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.06541265  0.58115123 -2.6667207
  3.04127202  2.17025483  1.24362412 -1.06938598  0.62880195  1.1489569
  1.36295953 -1.82456036  0.72839363]. 
Working on 0.6 fold... 
[Sun Mar 24 20:28:07 2024]  Iteration number: 0 with current cost as 0.14756195771492825 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.05535157  0.58317736 -2.65167431
  3.03756892  2.16610963  1.25236878 -1.06966166  0.6367854   1.15111834
  1.36682789 -1.82286503  0.72148814]. 
Working on 0.8 fold... 
[Sun Mar 24 20:30:42 2024]  Iteration number: 0 with current cost as 0.07504599153998551 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.21867354  0.68860824 -2.16209864
  2.91077362  2.08769239  1.51561641 -1.07817476  0.77252731  1.1832251
  1.61004309 -1.60968458  0.66890713]. 
Working on 1.0 fold... 
[Sun Mar 24 20:33:03 2024]  Iteration number: 0 with current cost as 0.1492660672200189 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.05762623  0.58333495 -2.65409866
  3.03843004  2.16713351  1.25030115 -1.06943148  0.63464857  1.15072966
  1.36663952 -1.82225953  0.7242876 ]. 
Training complete taking 749.9501864910126 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 20:35:32 2024]  Iteration number: 0 with current cost as 0.17395685233024477 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.03787391  0.58658104 -2.62572058
  3.03473395  2.16439753  1.25823118 -1.06953788  0.64144427  1.15272522
  1.37714082 -1.81484778  0.71559942]. 
Working on 0.4 fold... 
[Sun Mar 24 20:37:59 2024]  Iteration number: 0 with current cost as 0.1335791266521221 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.06541265  0.58115123 -2.6667207
  3.04127202  2.17025483  1.24362412 -1.06938598  0.62880195  1.1489569
  1.36295953 -1.82456036  0.72839363]. 
Working on 0.6 fold... 
[Sun Mar 24 20:40:33 2024]  Iteration number: 0 with current cost as 0.14756195771492825 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.05535157  0.58317736 -2.65167431
  3.03756892  2.16610963  1.25236878 -1.06966166  0.6367854   1.15111834
  1.36682789 -1.82286503  0.72148814]. 
Working on 0.8 fold... 
[Sun Mar 24 20:43:10 2024]  Iteration number: 0 with current cost as 0.07504599153998551 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.21867354  0.68860824 -2.16209864
  2.91077362  2.08769239  1.51561641 -1.07817476  0.77252731  1.1832251
  1.61004309 -1.60968458  0.66890713]. 
Working on 1.0 fold... 
[Sun Mar 24 20:45:36 2024]  Iteration number: 0 with current cost as 0.1492660672200189 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.05762623  0.58333495 -2.65409866
  3.03843004  2.16713351  1.25030115 -1.06943148  0.63464857  1.15072966
  1.36663952 -1.82225953  0.7242876 ]. 
Training complete taking 753.4868009090424 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 20:48:06 2024]  Iteration number: 0 with current cost as 0.17395685233024477 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.03787391  0.58658104 -2.62572058
  3.03473395  2.16439753  1.25823118 -1.06953788  0.64144427  1.15272522
  1.37714082 -1.81484778  0.71559942]. 
Working on 0.4 fold... 
[Sun Mar 24 20:50:21 2024]  Iteration number: 0 with current cost as 0.1335791266521221 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.06541265  0.58115123 -2.6667207
  3.04127202  2.17025483  1.24362412 -1.06938598  0.62880195  1.1489569
  1.36295953 -1.82456036  0.72839363]. 
Working on 0.6 fold... 
[Sun Mar 24 20:53:01 2024]  Iteration number: 0 with current cost as 0.14756195771492825 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.05535157  0.58317736 -2.65167431
  3.03756892  2.16610963  1.25236878 -1.06966166  0.6367854   1.15111834
  1.36682789 -1.82286503  0.72148814]. 
Working on 0.8 fold... 
[Sun Mar 24 20:55:35 2024]  Iteration number: 0 with current cost as 0.07504599153998551 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.21867354  0.68860824 -2.16209864
  2.91077362  2.08769239  1.51561641 -1.07817476  0.77252731  1.1832251
  1.61004309 -1.60968458  0.66890713]. 
Working on 1.0 fold... 
[Sun Mar 24 20:57:56 2024]  Iteration number: 0 with current cost as 0.1492660672200189 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.05762623  0.58333495 -2.65409866
  3.03843004  2.16713351  1.25030115 -1.06943148  0.63464857  1.15072966
  1.36663952 -1.82225953  0.7242876 ]. 
Training complete taking 741.777538061142 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 21:00:26 2024]  Iteration number: 0 with current cost as 0.17395685233024477 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.03787391  0.58658104 -2.62572058
  3.03473395  2.16439753  1.25823118 -1.06953788  0.64144427  1.15272522
  1.37714082 -1.81484778  0.71559942]. 
Working on 0.4 fold... 
[Sun Mar 24 21:02:57 2024]  Iteration number: 0 with current cost as 0.1335791266521221 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.06541265  0.58115123 -2.6667207
  3.04127202  2.17025483  1.24362412 -1.06938598  0.62880195  1.1489569
  1.36295953 -1.82456036  0.72839363]. 
Working on 0.6 fold... 
[Sun Mar 24 21:05:31 2024]  Iteration number: 0 with current cost as 0.14756195771492825 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.05535157  0.58317736 -2.65167431
  3.03756892  2.16610963  1.25236878 -1.06966166  0.6367854   1.15111834
  1.36682789 -1.82286503  0.72148814]. 
Working on 0.8 fold... 
[Sun Mar 24 21:08:04 2024]  Iteration number: 0 with current cost as 0.07504599153998551 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.21867354  0.68860824 -2.16209864
  2.91077362  2.08769239  1.51561641 -1.07817476  0.77252731  1.1832251
  1.61004309 -1.60968458  0.66890713]. 
Working on 1.0 fold... 
[Sun Mar 24 21:10:24 2024]  Iteration number: 0 with current cost as 0.1492660672200189 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.05762623  0.58333495 -2.65409866
  3.03843004  2.16713351  1.25030115 -1.06943148  0.63464857  1.15072966
  1.36663952 -1.82225953  0.7242876 ]. 
Training complete taking 747.7481105327606 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 21:12:54 2024]  Iteration number: 0 with current cost as 0.17395685233024477 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.03787391  0.58658104 -2.62572058
  3.03473395  2.16439753  1.25823118 -1.06953788  0.64144427  1.15272522
  1.37714082 -1.81484778  0.71559942]. 
Working on 0.4 fold... 
[Sun Mar 24 21:15:07 2024]  Iteration number: 0 with current cost as 0.1335791266521221 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.06541265  0.58115123 -2.6667207
  3.04127202  2.17025483  1.24362412 -1.06938598  0.62880195  1.1489569
  1.36295953 -1.82456036  0.72839363]. 
Working on 0.6 fold... 
[Sun Mar 24 21:17:45 2024]  Iteration number: 0 with current cost as 0.14756195771492825 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.05535157  0.58317736 -2.65167431
  3.03756892  2.16610963  1.25236878 -1.06966166  0.6367854   1.15111834
  1.36682789 -1.82286503  0.72148814]. 
Working on 0.8 fold... 
[Sun Mar 24 21:20:22 2024]  Iteration number: 0 with current cost as 0.07504599153998551 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.21867354  0.68860824 -2.16209864
  2.91077362  2.08769239  1.51561641 -1.07817476  0.77252731  1.1832251
  1.61004309 -1.60968458  0.66890713]. 
Working on 1.0 fold... 
[Sun Mar 24 21:22:43 2024]  Iteration number: 0 with current cost as 0.1492660672200189 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.05762623  0.58333495 -2.65409866
  3.03843004  2.16713351  1.25030115 -1.06943148  0.63464857  1.15072966
  1.36663952 -1.82225953  0.7242876 ]. 
Training complete taking 739.4312410354614 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 21:25:13 2024]  Iteration number: 0 with current cost as 0.17395685233024477 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.03787391  0.58658104 -2.62572058
  3.03473395  2.16439753  1.25823118 -1.06953788  0.64144427  1.15272522
  1.37714082 -1.81484778  0.71559942]. 
Working on 0.4 fold... 
[Sun Mar 24 21:27:24 2024]  Iteration number: 0 with current cost as 0.1335791266521221 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.06541265  0.58115123 -2.6667207
  3.04127202  2.17025483  1.24362412 -1.06938598  0.62880195  1.1489569
  1.36295953 -1.82456036  0.72839363]. 
Working on 0.6 fold... 
[Sun Mar 24 21:30:11 2024]  Iteration number: 0 with current cost as 0.14756195771492825 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.05535157  0.58317736 -2.65167431
  3.03756892  2.16610963  1.25236878 -1.06966166  0.6367854   1.15111834
  1.36682789 -1.82286503  0.72148814]. 
Working on 0.8 fold... 
[Sun Mar 24 21:32:45 2024]  Iteration number: 0 with current cost as 0.07504599153998551 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.21867354  0.68860824 -2.16209864
  2.91077362  2.08769239  1.51561641 -1.07817476  0.77252731  1.1832251
  1.61004309 -1.60968458  0.66890713]. 
Working on 1.0 fold... 
[Sun Mar 24 21:35:10 2024]  Iteration number: 0 with current cost as 0.1492660672200189 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.05762623  0.58333495 -2.65409866
  3.03843004  2.16713351  1.25030115 -1.06943148  0.63464857  1.15072966
  1.36663952 -1.82225953  0.7242876 ]. 
Training complete taking 747.5410449504852 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 21:37:41 2024]  Iteration number: 0 with current cost as 0.17395685233024477 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.03787391  0.58658104 -2.62572058
  3.03473395  2.16439753  1.25823118 -1.06953788  0.64144427  1.15272522
  1.37714082 -1.81484778  0.71559942]. 
Working on 0.4 fold... 
[Sun Mar 24 21:39:55 2024]  Iteration number: 0 with current cost as 0.1335791266521221 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.06541265  0.58115123 -2.6667207
  3.04127202  2.17025483  1.24362412 -1.06938598  0.62880195  1.1489569
  1.36295953 -1.82456036  0.72839363]. 
Working on 0.6 fold... 
[Sun Mar 24 21:42:31 2024]  Iteration number: 0 with current cost as 0.14756195771492825 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.05535157  0.58317736 -2.65167431
  3.03756892  2.16610963  1.25236878 -1.06966166  0.6367854   1.15111834
  1.36682789 -1.82286503  0.72148814]. 
Working on 0.8 fold... 
[Sun Mar 24 21:45:05 2024]  Iteration number: 0 with current cost as 0.07504599153998551 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.21867354  0.68860824 -2.16209864
  2.91077362  2.08769239  1.51561641 -1.07817476  0.77252731  1.1832251
  1.61004309 -1.60968458  0.66890713]. 
Working on 1.0 fold... 
[Sun Mar 24 21:47:24 2024]  Iteration number: 0 with current cost as 0.1492660672200189 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.05762623  0.58333495 -2.65409866
  3.03843004  2.16713351  1.25030115 -1.06943148  0.63464857  1.15072966
  1.36663952 -1.82225953  0.7242876 ]. 
Training complete taking 735.4478914737701 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 21:49:56 2024]  Iteration number: 0 with current cost as 0.17395685233024477 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.03787391  0.58658104 -2.62572058
  3.03473395  2.16439753  1.25823118 -1.06953788  0.64144427  1.15272522
  1.37714082 -1.81484778  0.71559942]. 
Working on 0.4 fold... 
[Sun Mar 24 21:52:09 2024]  Iteration number: 0 with current cost as 0.1335791266521221 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.06541265  0.58115123 -2.6667207
  3.04127202  2.17025483  1.24362412 -1.06938598  0.62880195  1.1489569
  1.36295953 -1.82456036  0.72839363]. 
Working on 0.6 fold... 
[Sun Mar 24 21:54:43 2024]  Iteration number: 0 with current cost as 0.14756195771492825 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.05535157  0.58317736 -2.65167431
  3.03756892  2.16610963  1.25236878 -1.06966166  0.6367854   1.15111834
  1.36682789 -1.82286503  0.72148814]. 
Working on 0.8 fold... 
[Sun Mar 24 21:57:18 2024]  Iteration number: 0 with current cost as 0.07504599153998551 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.21867354  0.68860824 -2.16209864
  2.91077362  2.08769239  1.51561641 -1.07817476  0.77252731  1.1832251
  1.61004309 -1.60968458  0.66890713]. 
Working on 1.0 fold... 
[Sun Mar 24 21:59:40 2024]  Iteration number: 0 with current cost as 0.1492660672200189 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.05762623  0.58333495 -2.65409866
  3.03843004  2.16713351  1.25030115 -1.06943148  0.63464857  1.15072966
  1.36663952 -1.82225953  0.7242876 ]. 
Training complete taking 738.5717318058014 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 22:02:15 2024]  Iteration number: 0 with current cost as 0.17395685233024477 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.03787391  0.58658104 -2.62572058
  3.03473395  2.16439753  1.25823118 -1.06953788  0.64144427  1.15272522
  1.37714082 -1.81484778  0.71559942]. 
Working on 0.4 fold... 
[Sun Mar 24 22:04:50 2024]  Iteration number: 0 with current cost as 0.1335791266521221 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.06541265  0.58115123 -2.6667207
  3.04127202  2.17025483  1.24362412 -1.06938598  0.62880195  1.1489569
  1.36295953 -1.82456036  0.72839363]. 
Working on 0.6 fold... 
[Sun Mar 24 22:07:25 2024]  Iteration number: 0 with current cost as 0.14756195771492825 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.05535157  0.58317736 -2.65167431
  3.03756892  2.16610963  1.25236878 -1.06966166  0.6367854   1.15111834
  1.36682789 -1.82286503  0.72148814]. 
Working on 0.8 fold... 
[Sun Mar 24 22:10:00 2024]  Iteration number: 0 with current cost as 0.07504599153998551 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.21867354  0.68860824 -2.16209864
  2.91077362  2.08769239  1.51561641 -1.07817476  0.77252731  1.1832251
  1.61004309 -1.60968458  0.66890713]. 
Working on 1.0 fold... 
[Sun Mar 24 22:12:21 2024]  Iteration number: 0 with current cost as 0.1492660672200189 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.05762623  0.58333495 -2.65409866
  3.03843004  2.16713351  1.25030115 -1.06943148  0.63464857  1.15072966
  1.36663952 -1.82225953  0.7242876 ]. 
Training complete taking 757.4820418357849 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 22:14:52 2024]  Iteration number: 0 with current cost as 0.17395685233024477 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.03787391  0.58658104 -2.62572058
  3.03473395  2.16439753  1.25823118 -1.06953788  0.64144427  1.15272522
  1.37714082 -1.81484778  0.71559942]. 
Working on 0.4 fold... 
[Sun Mar 24 22:17:04 2024]  Iteration number: 0 with current cost as 0.1335791266521221 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.06541265  0.58115123 -2.6667207
  3.04127202  2.17025483  1.24362412 -1.06938598  0.62880195  1.1489569
  1.36295953 -1.82456036  0.72839363]. 
Working on 0.6 fold... 
[Sun Mar 24 22:19:43 2024]  Iteration number: 0 with current cost as 0.14756195771492825 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.05535157  0.58317736 -2.65167431
  3.03756892  2.16610963  1.25236878 -1.06966166  0.6367854   1.15111834
  1.36682789 -1.82286503  0.72148814]. 
Working on 0.8 fold... 
[Sun Mar 24 22:22:17 2024]  Iteration number: 0 with current cost as 0.07504599153998551 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.21867354  0.68860824 -2.16209864
  2.91077362  2.08769239  1.51561641 -1.07817476  0.77252731  1.1832251
  1.61004309 -1.60968458  0.66890713]. 
Working on 1.0 fold... 
[Sun Mar 24 22:24:41 2024]  Iteration number: 0 with current cost as 0.1492660672200189 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.05762623  0.58333495 -2.65409866
  3.03843004  2.16713351  1.25030115 -1.06943148  0.63464857  1.15072966
  1.36663952 -1.82225953  0.7242876 ]. 
Training complete taking 736.76451420784 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 22:27:10 2024]  Iteration number: 0 with current cost as 0.17395685233024477 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.03787391  0.58658104 -2.62572058
  3.03473395  2.16439753  1.25823118 -1.06953788  0.64144427  1.15272522
  1.37714082 -1.81484778  0.71559942]. 
Working on 0.4 fold... 
[Sun Mar 24 22:29:22 2024]  Iteration number: 0 with current cost as 0.1335791266521221 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.06541265  0.58115123 -2.6667207
  3.04127202  2.17025483  1.24362412 -1.06938598  0.62880195  1.1489569
  1.36295953 -1.82456036  0.72839363]. 
Working on 0.6 fold... 
[Sun Mar 24 22:32:04 2024]  Iteration number: 0 with current cost as 0.14756195771492825 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.05535157  0.58317736 -2.65167431
  3.03756892  2.16610963  1.25236878 -1.06966166  0.6367854   1.15111834
  1.36682789 -1.82286503  0.72148814]. 
Working on 0.8 fold... 
[Sun Mar 24 22:35:08 2024]  Iteration number: 0 with current cost as 0.07504599153998551 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.21867354  0.68860824 -2.16209864
  2.91077362  2.08769239  1.51561641 -1.07817476  0.77252731  1.1832251
  1.61004309 -1.60968458  0.66890713]. 
Working on 1.0 fold... 
[Sun Mar 24 22:37:28 2024]  Iteration number: 0 with current cost as 0.1492660672200189 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.05762623  0.58333495 -2.65409866
  3.03843004  2.16713351  1.25030115 -1.06943148  0.63464857  1.15072966
  1.36663952 -1.82225953  0.7242876 ]. 
Training complete taking 768.8213634490967 seconds. 
Discarding model... 

Training complete taking 18575.050572395325 total seconds. 
Now scoring model... 
Scoring complete taking 0.8307538032531738 seconds. 
Saved predicted values as A1-A1-CNOT_HWE-CNOT_predicted_values.csv
Model scores: {'MSE_train': (27.200879039770523,), 'R2_train': 0.8682208701003133, 'MAE_train': 3.8423229693369594, 'MSE_test': 43.55784236869199, 'R2_test': 0.737441431878658, 'MAE_test': 4.960638105133766}. 
Saved model results as A1-A1-CNOT_HWE-CNOT_results.json. 
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Thu Apr  4 11:42:52 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 11:43:01 2024]  Iteration number: 0 with current cost as 0.17395685233024477 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.03787391  0.58658104 -2.62572058
  3.03473395  2.16439753  1.25823118 -1.06953788  0.64144427  1.15272522
  1.37714082 -1.81484778  0.71559942]. 
Working on 0.4 fold... 
[Thu Apr  4 11:45:13 2024]  Iteration number: 0 with current cost as 0.1335791266521221 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.06541265  0.58115123 -2.6667207
  3.04127202  2.17025483  1.24362412 -1.06938598  0.62880195  1.1489569
  1.36295953 -1.82456036  0.72839363]. 
Working on 0.6 fold... 
[Thu Apr  4 11:47:50 2024]  Iteration number: 0 with current cost as 0.14756195771492825 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.05535157  0.58317736 -2.65167431
  3.03756892  2.16610963  1.25236878 -1.06966166  0.6367854   1.15111834
  1.36682789 -1.82286503  0.72148814]. 
Working on 0.8 fold... 
[Thu Apr  4 11:50:22 2024]  Iteration number: 0 with current cost as 0.07504599153998551 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.21867354  0.68860824 -2.16209864
  2.91077362  2.08769239  1.51561641 -1.07817476  0.77252731  1.1832251
  1.61004309 -1.60968458  0.66890713]. 
Working on 1.0 fold... 
[Thu Apr  4 11:52:40 2024]  Iteration number: 0 with current cost as 0.1492660672200189 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.05762623  0.58333495 -2.65409866
  3.03843004  2.16713351  1.25030115 -1.06943148  0.63464857  1.15072966
  1.36663952 -1.82225953  0.7242876 ]. 
Training complete taking 763.9533407688141 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 11:55:45 2024]  Iteration number: 0 with current cost as 0.17395685233024477 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.03787391  0.58658104 -2.62572058
  3.03473395  2.16439753  1.25823118 -1.06953788  0.64144427  1.15272522
  1.37714082 -1.81484778  0.71559942]. 
Working on 0.4 fold... 
[Thu Apr  4 11:57:58 2024]  Iteration number: 0 with current cost as 0.1335791266521221 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.06541265  0.58115123 -2.6667207
  3.04127202  2.17025483  1.24362412 -1.06938598  0.62880195  1.1489569
  1.36295953 -1.82456036  0.72839363]. 
Working on 0.6 fold... 
[Thu Apr  4 12:00:32 2024]  Iteration number: 0 with current cost as 0.14756195771492825 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.05535157  0.58317736 -2.65167431
  3.03756892  2.16610963  1.25236878 -1.06966166  0.6367854   1.15111834
  1.36682789 -1.82286503  0.72148814]. 
Working on 0.8 fold... 
[Thu Apr  4 12:03:10 2024]  Iteration number: 0 with current cost as 0.07504599153998551 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.21867354  0.68860824 -2.16209864
  2.91077362  2.08769239  1.51561641 -1.07817476  0.77252731  1.1832251
  1.61004309 -1.60968458  0.66890713]. 
Working on 1.0 fold... 
[Thu Apr  4 12:05:30 2024]  Iteration number: 0 with current cost as 0.1492660672200189 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.05762623  0.58333495 -2.65409866
  3.03843004  2.16713351  1.25030115 -1.06943148  0.63464857  1.15072966
  1.36663952 -1.82225953  0.7242876 ]. 
Training complete taking 749.1869735717773 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 12:08:14 2024]  Iteration number: 0 with current cost as 0.17395685233024477 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.03787391  0.58658104 -2.62572058
  3.03473395  2.16439753  1.25823118 -1.06953788  0.64144427  1.15272522
  1.37714082 -1.81484778  0.71559942]. 
Working on 0.4 fold... 
[Thu Apr  4 12:10:29 2024]  Iteration number: 0 with current cost as 0.1335791266521221 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.06541265  0.58115123 -2.6667207
  3.04127202  2.17025483  1.24362412 -1.06938598  0.62880195  1.1489569
  1.36295953 -1.82456036  0.72839363]. 
Working on 0.6 fold... 
[Thu Apr  4 12:13:03 2024]  Iteration number: 0 with current cost as 0.14756195771492825 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.05535157  0.58317736 -2.65167431
  3.03756892  2.16610963  1.25236878 -1.06966166  0.6367854   1.15111834
  1.36682789 -1.82286503  0.72148814]. 
Working on 0.8 fold... 
[Thu Apr  4 12:15:52 2024]  Iteration number: 0 with current cost as 0.07504599153998551 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.21867354  0.68860824 -2.16209864
  2.91077362  2.08769239  1.51561641 -1.07817476  0.77252731  1.1832251
  1.61004309 -1.60968458  0.66890713]. 
Working on 1.0 fold... 
[Thu Apr  4 12:18:12 2024]  Iteration number: 0 with current cost as 0.1492660672200189 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.05762623  0.58333495 -2.65409866
  3.03843004  2.16713351  1.25030115 -1.06943148  0.63464857  1.15072966
  1.36663952 -1.82225953  0.7242876 ]. 
Training complete taking 746.9900097846985 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 12:20:41 2024]  Iteration number: 0 with current cost as 0.17395685233024477 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.03787391  0.58658104 -2.62572058
  3.03473395  2.16439753  1.25823118 -1.06953788  0.64144427  1.15272522
  1.37714082 -1.81484778  0.71559942]. 
Working on 0.4 fold... 
[Thu Apr  4 12:22:54 2024]  Iteration number: 0 with current cost as 0.1335791266521221 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.06541265  0.58115123 -2.6667207
  3.04127202  2.17025483  1.24362412 -1.06938598  0.62880195  1.1489569
  1.36295953 -1.82456036  0.72839363]. 
Working on 0.6 fold... 
[Thu Apr  4 12:25:46 2024]  Iteration number: 0 with current cost as 0.14756195771492825 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.05535157  0.58317736 -2.65167431
  3.03756892  2.16610963  1.25236878 -1.06966166  0.6367854   1.15111834
  1.36682789 -1.82286503  0.72148814]. 
Working on 0.8 fold... 
[Thu Apr  4 12:28:30 2024]  Iteration number: 0 with current cost as 0.07504599153998551 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.21867354  0.68860824 -2.16209864
  2.91077362  2.08769239  1.51561641 -1.07817476  0.77252731  1.1832251
  1.61004309 -1.60968458  0.66890713]. 
Working on 1.0 fold... 
[Thu Apr  4 12:30:53 2024]  Iteration number: 0 with current cost as 0.1492660672200189 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.05762623  0.58333495 -2.65409866
  3.03843004  2.16713351  1.25030115 -1.06943148  0.63464857  1.15072966
  1.36663952 -1.82225953  0.7242876 ]. 
Training complete taking 761.2704610824585 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 12:33:24 2024]  Iteration number: 0 with current cost as 0.17395685233024477 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.03787391  0.58658104 -2.62572058
  3.03473395  2.16439753  1.25823118 -1.06953788  0.64144427  1.15272522
  1.37714082 -1.81484778  0.71559942]. 
Working on 0.4 fold... 
[Thu Apr  4 12:35:39 2024]  Iteration number: 0 with current cost as 0.1335791266521221 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.06541265  0.58115123 -2.6667207
  3.04127202  2.17025483  1.24362412 -1.06938598  0.62880195  1.1489569
  1.36295953 -1.82456036  0.72839363]. 
Working on 0.6 fold... 
[Thu Apr  4 12:38:26 2024]  Iteration number: 0 with current cost as 0.14756195771492825 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.05535157  0.58317736 -2.65167431
  3.03756892  2.16610963  1.25236878 -1.06966166  0.6367854   1.15111834
  1.36682789 -1.82286503  0.72148814]. 
Working on 0.8 fold... 
[Thu Apr  4 12:41:01 2024]  Iteration number: 0 with current cost as 0.07504599153998551 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.21867354  0.68860824 -2.16209864
  2.91077362  2.08769239  1.51561641 -1.07817476  0.77252731  1.1832251
  1.61004309 -1.60968458  0.66890713]. 
Working on 1.0 fold... 
[Thu Apr  4 12:43:24 2024]  Iteration number: 0 with current cost as 0.1492660672200189 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.05762623  0.58333495 -2.65409866
  3.03843004  2.16713351  1.25030115 -1.06943148  0.63464857  1.15072966
  1.36663952 -1.82225953  0.7242876 ]. 
Training complete taking 751.3537149429321 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 12:46:06 2024]  Iteration number: 0 with current cost as 0.17395685233024477 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.03787391  0.58658104 -2.62572058
  3.03473395  2.16439753  1.25823118 -1.06953788  0.64144427  1.15272522
  1.37714082 -1.81484778  0.71559942]. 
Working on 0.4 fold... 
[Thu Apr  4 12:48:18 2024]  Iteration number: 0 with current cost as 0.1335791266521221 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.06541265  0.58115123 -2.6667207
  3.04127202  2.17025483  1.24362412 -1.06938598  0.62880195  1.1489569
  1.36295953 -1.82456036  0.72839363]. 
Working on 0.6 fold... 
[Thu Apr  4 12:50:51 2024]  Iteration number: 0 with current cost as 0.14756195771492825 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.05535157  0.58317736 -2.65167431
  3.03756892  2.16610963  1.25236878 -1.06966166  0.6367854   1.15111834
  1.36682789 -1.82286503  0.72148814]. 
Working on 0.8 fold... 
[Thu Apr  4 12:53:26 2024]  Iteration number: 0 with current cost as 0.07504599153998551 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.21867354  0.68860824 -2.16209864
  2.91077362  2.08769239  1.51561641 -1.07817476  0.77252731  1.1832251
  1.61004309 -1.60968458  0.66890713]. 
Working on 1.0 fold... 
[Thu Apr  4 12:55:55 2024]  Iteration number: 0 with current cost as 0.1492660672200189 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.05762623  0.58333495 -2.65409866
  3.03843004  2.16713351  1.25030115 -1.06943148  0.63464857  1.15072966
  1.36663952 -1.82225953  0.7242876 ]. 
Training complete taking 754.1249101161957 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 12:58:29 2024]  Iteration number: 0 with current cost as 0.17395685233024477 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.03787391  0.58658104 -2.62572058
  3.03473395  2.16439753  1.25823118 -1.06953788  0.64144427  1.15272522
  1.37714082 -1.81484778  0.71559942]. 
Working on 0.4 fold... 
[Thu Apr  4 13:00:58 2024]  Iteration number: 0 with current cost as 0.1335791266521221 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.06541265  0.58115123 -2.6667207
  3.04127202  2.17025483  1.24362412 -1.06938598  0.62880195  1.1489569
  1.36295953 -1.82456036  0.72839363]. 
Working on 0.6 fold... 
[Thu Apr  4 13:04:18 2024]  Iteration number: 0 with current cost as 0.14756195771492825 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.05535157  0.58317736 -2.65167431
  3.03756892  2.16610963  1.25236878 -1.06966166  0.6367854   1.15111834
  1.36682789 -1.82286503  0.72148814]. 
Working on 0.8 fold... 
[Thu Apr  4 13:06:52 2024]  Iteration number: 0 with current cost as 0.07504599153998551 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.21867354  0.68860824 -2.16209864
  2.91077362  2.08769239  1.51561641 -1.07817476  0.77252731  1.1832251
  1.61004309 -1.60968458  0.66890713]. 
Working on 1.0 fold... 
[Thu Apr  4 13:09:18 2024]  Iteration number: 0 with current cost as 0.1492660672200189 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.05762623  0.58333495 -2.65409866
  3.03843004  2.16713351  1.25030115 -1.06943148  0.63464857  1.15072966
  1.36663952 -1.82225953  0.7242876 ]. 
Training complete taking 800.4285802841187 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 13:11:48 2024]  Iteration number: 0 with current cost as 0.17395685233024477 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.03787391  0.58658104 -2.62572058
  3.03473395  2.16439753  1.25823118 -1.06953788  0.64144427  1.15272522
  1.37714082 -1.81484778  0.71559942]. 
Working on 0.4 fold... 
[Thu Apr  4 13:14:10 2024]  Iteration number: 0 with current cost as 0.1335791266521221 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.06541265  0.58115123 -2.6667207
  3.04127202  2.17025483  1.24362412 -1.06938598  0.62880195  1.1489569
  1.36295953 -1.82456036  0.72839363]. 
Working on 0.6 fold... 
[Thu Apr  4 13:16:59 2024]  Iteration number: 0 with current cost as 0.14756195771492825 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.05535157  0.58317736 -2.65167431
  3.03756892  2.16610963  1.25236878 -1.06966166  0.6367854   1.15111834
  1.36682789 -1.82286503  0.72148814]. 
Working on 0.8 fold... 
[Thu Apr  4 13:19:33 2024]  Iteration number: 0 with current cost as 0.07504599153998551 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.21867354  0.68860824 -2.16209864
  2.91077362  2.08769239  1.51561641 -1.07817476  0.77252731  1.1832251
  1.61004309 -1.60968458  0.66890713]. 
Working on 1.0 fold... 
[Thu Apr  4 13:21:52 2024]  Iteration number: 0 with current cost as 0.1492660672200189 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.05762623  0.58333495 -2.65409866
  3.03843004  2.16713351  1.25030115 -1.06943148  0.63464857  1.15072966
  1.36663952 -1.82225953  0.7242876 ]. 
Training complete taking 753.1394031047821 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 13:24:21 2024]  Iteration number: 0 with current cost as 0.17395685233024477 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.03787391  0.58658104 -2.62572058
  3.03473395  2.16439753  1.25823118 -1.06953788  0.64144427  1.15272522
  1.37714082 -1.81484778  0.71559942]. 
Working on 0.4 fold... 
[Thu Apr  4 13:26:34 2024]  Iteration number: 0 with current cost as 0.1335791266521221 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.06541265  0.58115123 -2.6667207
  3.04127202  2.17025483  1.24362412 -1.06938598  0.62880195  1.1489569
  1.36295953 -1.82456036  0.72839363]. 
Working on 0.6 fold... 
[Thu Apr  4 13:29:09 2024]  Iteration number: 0 with current cost as 0.14756195771492825 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.05535157  0.58317736 -2.65167431
  3.03756892  2.16610963  1.25236878 -1.06966166  0.6367854   1.15111834
  1.36682789 -1.82286503  0.72148814]. 
Working on 0.8 fold... 
[Thu Apr  4 13:31:43 2024]  Iteration number: 0 with current cost as 0.07504599153998551 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.21867354  0.68860824 -2.16209864
  2.91077362  2.08769239  1.51561641 -1.07817476  0.77252731  1.1832251
  1.61004309 -1.60968458  0.66890713]. 
Working on 1.0 fold... 
[Thu Apr  4 13:34:15 2024]  Iteration number: 0 with current cost as 0.1492660672200189 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.05762623  0.58333495 -2.65409866
  3.03843004  2.16713351  1.25030115 -1.06943148  0.63464857  1.15072966
  1.36663952 -1.82225953  0.7242876 ]. 
Training complete taking 743.6985867023468 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 13:36:45 2024]  Iteration number: 0 with current cost as 0.17395685233024477 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.03787391  0.58658104 -2.62572058
  3.03473395  2.16439753  1.25823118 -1.06953788  0.64144427  1.15272522
  1.37714082 -1.81484778  0.71559942]. 
Working on 0.4 fold... 
[Thu Apr  4 13:38:58 2024]  Iteration number: 0 with current cost as 0.1335791266521221 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.06541265  0.58115123 -2.6667207
  3.04127202  2.17025483  1.24362412 -1.06938598  0.62880195  1.1489569
  1.36295953 -1.82456036  0.72839363]. 
Working on 0.6 fold... 
[Thu Apr  4 13:41:33 2024]  Iteration number: 0 with current cost as 0.14756195771492825 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.05535157  0.58317736 -2.65167431
  3.03756892  2.16610963  1.25236878 -1.06966166  0.6367854   1.15111834
  1.36682789 -1.82286503  0.72148814]. 
Working on 0.8 fold... 
[Thu Apr  4 13:44:09 2024]  Iteration number: 0 with current cost as 0.07504599153998551 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.21867354  0.68860824 -2.16209864
  2.91077362  2.08769239  1.51561641 -1.07817476  0.77252731  1.1832251
  1.61004309 -1.60968458  0.66890713]. 
Working on 1.0 fold... 
[Thu Apr  4 13:46:30 2024]  Iteration number: 0 with current cost as 0.1492660672200189 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.05762623  0.58333495 -2.65409866
  3.03843004  2.16713351  1.25030115 -1.06943148  0.63464857  1.15072966
  1.36663952 -1.82225953  0.7242876 ]. 
Training complete taking 733.4175055027008 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 13:49:00 2024]  Iteration number: 0 with current cost as 0.17395685233024477 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.03787391  0.58658104 -2.62572058
  3.03473395  2.16439753  1.25823118 -1.06953788  0.64144427  1.15272522
  1.37714082 -1.81484778  0.71559942]. 
Working on 0.4 fold... 
[Thu Apr  4 13:51:13 2024]  Iteration number: 0 with current cost as 0.1335791266521221 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.06541265  0.58115123 -2.6667207
  3.04127202  2.17025483  1.24362412 -1.06938598  0.62880195  1.1489569
  1.36295953 -1.82456036  0.72839363]. 
Working on 0.6 fold... 
[Thu Apr  4 13:53:48 2024]  Iteration number: 0 with current cost as 0.14756195771492825 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.05535157  0.58317736 -2.65167431
  3.03756892  2.16610963  1.25236878 -1.06966166  0.6367854   1.15111834
  1.36682789 -1.82286503  0.72148814]. 
Working on 0.8 fold... 
[Thu Apr  4 13:56:21 2024]  Iteration number: 0 with current cost as 0.07504599153998551 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.21867354  0.68860824 -2.16209864
  2.91077362  2.08769239  1.51561641 -1.07817476  0.77252731  1.1832251
  1.61004309 -1.60968458  0.66890713]. 
Working on 1.0 fold... 
[Thu Apr  4 13:58:41 2024]  Iteration number: 0 with current cost as 0.1492660672200189 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.05762623  0.58333495 -2.65409866
  3.03843004  2.16713351  1.25030115 -1.06943148  0.63464857  1.15072966
  1.36663952 -1.82225953  0.7242876 ]. 
Training complete taking 731.6198918819427 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 14:01:10 2024]  Iteration number: 0 with current cost as 0.17395685233024477 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.03787391  0.58658104 -2.62572058
  3.03473395  2.16439753  1.25823118 -1.06953788  0.64144427  1.15272522
  1.37714082 -1.81484778  0.71559942]. 
Working on 0.4 fold... 
[Thu Apr  4 14:03:21 2024]  Iteration number: 0 with current cost as 0.1335791266521221 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.06541265  0.58115123 -2.6667207
  3.04127202  2.17025483  1.24362412 -1.06938598  0.62880195  1.1489569
  1.36295953 -1.82456036  0.72839363]. 
Working on 0.6 fold... 
[Thu Apr  4 14:05:54 2024]  Iteration number: 0 with current cost as 0.14756195771492825 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.05535157  0.58317736 -2.65167431
  3.03756892  2.16610963  1.25236878 -1.06966166  0.6367854   1.15111834
  1.36682789 -1.82286503  0.72148814]. 
Working on 0.8 fold... 
[Thu Apr  4 14:08:28 2024]  Iteration number: 0 with current cost as 0.07504599153998551 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.21867354  0.68860824 -2.16209864
  2.91077362  2.08769239  1.51561641 -1.07817476  0.77252731  1.1832251
  1.61004309 -1.60968458  0.66890713]. 
Working on 1.0 fold... 
[Thu Apr  4 14:10:47 2024]  Iteration number: 0 with current cost as 0.1492660672200189 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.05762623  0.58333495 -2.65409866
  3.03843004  2.16713351  1.25030115 -1.06943148  0.63464857  1.15072966
  1.36663952 -1.82225953  0.7242876 ]. 
Training complete taking 725.9345469474792 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 14:13:16 2024]  Iteration number: 0 with current cost as 0.17395685233024477 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.03787391  0.58658104 -2.62572058
  3.03473395  2.16439753  1.25823118 -1.06953788  0.64144427  1.15272522
  1.37714082 -1.81484778  0.71559942]. 
Working on 0.4 fold... 
[Thu Apr  4 14:15:25 2024]  Iteration number: 0 with current cost as 0.1335791266521221 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.06541265  0.58115123 -2.6667207
  3.04127202  2.17025483  1.24362412 -1.06938598  0.62880195  1.1489569
  1.36295953 -1.82456036  0.72839363]. 
Working on 0.6 fold... 
[Thu Apr  4 14:17:59 2024]  Iteration number: 0 with current cost as 0.14756195771492825 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.05535157  0.58317736 -2.65167431
  3.03756892  2.16610963  1.25236878 -1.06966166  0.6367854   1.15111834
  1.36682789 -1.82286503  0.72148814]. 
Working on 0.8 fold... 
[Thu Apr  4 14:20:32 2024]  Iteration number: 0 with current cost as 0.07504599153998551 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.21867354  0.68860824 -2.16209864
  2.91077362  2.08769239  1.51561641 -1.07817476  0.77252731  1.1832251
  1.61004309 -1.60968458  0.66890713]. 
Working on 1.0 fold... 
[Thu Apr  4 14:22:51 2024]  Iteration number: 0 with current cost as 0.1492660672200189 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.05762623  0.58333495 -2.65409866
  3.03843004  2.16713351  1.25030115 -1.06943148  0.63464857  1.15072966
  1.36663952 -1.82225953  0.7242876 ]. 
Training complete taking 723.1798317432404 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 14:25:19 2024]  Iteration number: 0 with current cost as 0.17395685233024477 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.03787391  0.58658104 -2.62572058
  3.03473395  2.16439753  1.25823118 -1.06953788  0.64144427  1.15272522
  1.37714082 -1.81484778  0.71559942]. 
Working on 0.4 fold... 
[Thu Apr  4 14:27:30 2024]  Iteration number: 0 with current cost as 0.1335791266521221 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.06541265  0.58115123 -2.6667207
  3.04127202  2.17025483  1.24362412 -1.06938598  0.62880195  1.1489569
  1.36295953 -1.82456036  0.72839363]. 
Working on 0.6 fold... 
[Thu Apr  4 14:30:04 2024]  Iteration number: 0 with current cost as 0.14756195771492825 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.05535157  0.58317736 -2.65167431
  3.03756892  2.16610963  1.25236878 -1.06966166  0.6367854   1.15111834
  1.36682789 -1.82286503  0.72148814]. 
Working on 0.8 fold... 
[Thu Apr  4 14:32:38 2024]  Iteration number: 0 with current cost as 0.07504599153998551 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.21867354  0.68860824 -2.16209864
  2.91077362  2.08769239  1.51561641 -1.07817476  0.77252731  1.1832251
  1.61004309 -1.60968458  0.66890713]. 
Working on 1.0 fold... 
[Thu Apr  4 14:34:57 2024]  Iteration number: 0 with current cost as 0.1492660672200189 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.05762623  0.58333495 -2.65409866
  3.03843004  2.16713351  1.25030115 -1.06943148  0.63464857  1.15072966
  1.36663952 -1.82225953  0.7242876 ]. 
Training complete taking 725.6415543556213 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 14:37:25 2024]  Iteration number: 0 with current cost as 0.17395685233024477 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.03787391  0.58658104 -2.62572058
  3.03473395  2.16439753  1.25823118 -1.06953788  0.64144427  1.15272522
  1.37714082 -1.81484778  0.71559942]. 
Working on 0.4 fold... 
[Thu Apr  4 14:39:38 2024]  Iteration number: 0 with current cost as 0.1335791266521221 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.06541265  0.58115123 -2.6667207
  3.04127202  2.17025483  1.24362412 -1.06938598  0.62880195  1.1489569
  1.36295953 -1.82456036  0.72839363]. 
Working on 0.6 fold... 
[Thu Apr  4 14:42:10 2024]  Iteration number: 0 with current cost as 0.14756195771492825 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.05535157  0.58317736 -2.65167431
  3.03756892  2.16610963  1.25236878 -1.06966166  0.6367854   1.15111834
  1.36682789 -1.82286503  0.72148814]. 
Working on 0.8 fold... 
[Thu Apr  4 14:44:46 2024]  Iteration number: 0 with current cost as 0.07504599153998551 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.21867354  0.68860824 -2.16209864
  2.91077362  2.08769239  1.51561641 -1.07817476  0.77252731  1.1832251
  1.61004309 -1.60968458  0.66890713]. 
Working on 1.0 fold... 
[Thu Apr  4 14:47:23 2024]  Iteration number: 0 with current cost as 0.1492660672200189 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.05762623  0.58333495 -2.65409866
  3.03843004  2.16713351  1.25030115 -1.06943148  0.63464857  1.15072966
  1.36663952 -1.82225953  0.7242876 ]. 
Training complete taking 745.7758085727692 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 14:49:51 2024]  Iteration number: 0 with current cost as 0.17395685233024477 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.03787391  0.58658104 -2.62572058
  3.03473395  2.16439753  1.25823118 -1.06953788  0.64144427  1.15272522
  1.37714082 -1.81484778  0.71559942]. 
Working on 0.4 fold... 
[Thu Apr  4 14:52:02 2024]  Iteration number: 0 with current cost as 0.1335791266521221 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.06541265  0.58115123 -2.6667207
  3.04127202  2.17025483  1.24362412 -1.06938598  0.62880195  1.1489569
  1.36295953 -1.82456036  0.72839363]. 
Working on 0.6 fold... 
[Thu Apr  4 14:54:35 2024]  Iteration number: 0 with current cost as 0.14756195771492825 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.05535157  0.58317736 -2.65167431
  3.03756892  2.16610963  1.25236878 -1.06966166  0.6367854   1.15111834
  1.36682789 -1.82286503  0.72148814]. 
Working on 0.8 fold... 
[Thu Apr  4 14:57:08 2024]  Iteration number: 0 with current cost as 0.07504599153998551 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.21867354  0.68860824 -2.16209864
  2.91077362  2.08769239  1.51561641 -1.07817476  0.77252731  1.1832251
  1.61004309 -1.60968458  0.66890713]. 
Working on 1.0 fold... 
[Thu Apr  4 14:59:27 2024]  Iteration number: 0 with current cost as 0.1492660672200189 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.05762623  0.58333495 -2.65409866
  3.03843004  2.16713351  1.25030115 -1.06943148  0.63464857  1.15072966
  1.36663952 -1.82225953  0.7242876 ]. 
Training complete taking 724.9021739959717 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 15:01:56 2024]  Iteration number: 0 with current cost as 0.17395685233024477 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.03787391  0.58658104 -2.62572058
  3.03473395  2.16439753  1.25823118 -1.06953788  0.64144427  1.15272522
  1.37714082 -1.81484778  0.71559942]. 
Working on 0.4 fold... 
[Thu Apr  4 15:04:07 2024]  Iteration number: 0 with current cost as 0.1335791266521221 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.06541265  0.58115123 -2.6667207
  3.04127202  2.17025483  1.24362412 -1.06938598  0.62880195  1.1489569
  1.36295953 -1.82456036  0.72839363]. 
Working on 0.6 fold... 
[Thu Apr  4 15:06:40 2024]  Iteration number: 0 with current cost as 0.14756195771492825 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.05535157  0.58317736 -2.65167431
  3.03756892  2.16610963  1.25236878 -1.06966166  0.6367854   1.15111834
  1.36682789 -1.82286503  0.72148814]. 
Working on 0.8 fold... 
[Thu Apr  4 15:09:14 2024]  Iteration number: 0 with current cost as 0.07504599153998551 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.21867354  0.68860824 -2.16209864
  2.91077362  2.08769239  1.51561641 -1.07817476  0.77252731  1.1832251
  1.61004309 -1.60968458  0.66890713]. 
Working on 1.0 fold... 
[Thu Apr  4 15:11:34 2024]  Iteration number: 0 with current cost as 0.1492660672200189 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.05762623  0.58333495 -2.65409866
  3.03843004  2.16713351  1.25030115 -1.06943148  0.63464857  1.15072966
  1.36663952 -1.82225953  0.7242876 ]. 
Training complete taking 725.8347404003143 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 15:14:03 2024]  Iteration number: 0 with current cost as 0.17395685233024477 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.03787391  0.58658104 -2.62572058
  3.03473395  2.16439753  1.25823118 -1.06953788  0.64144427  1.15272522
  1.37714082 -1.81484778  0.71559942]. 
Working on 0.4 fold... 
[Thu Apr  4 15:16:12 2024]  Iteration number: 0 with current cost as 0.1335791266521221 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.06541265  0.58115123 -2.6667207
  3.04127202  2.17025483  1.24362412 -1.06938598  0.62880195  1.1489569
  1.36295953 -1.82456036  0.72839363]. 
Working on 0.6 fold... 
[Thu Apr  4 15:19:10 2024]  Iteration number: 0 with current cost as 0.14756195771492825 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.05535157  0.58317736 -2.65167431
  3.03756892  2.16610963  1.25236878 -1.06966166  0.6367854   1.15111834
  1.36682789 -1.82286503  0.72148814]. 
Working on 0.8 fold... 
[Thu Apr  4 15:21:42 2024]  Iteration number: 0 with current cost as 0.07504599153998551 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.21867354  0.68860824 -2.16209864
  2.91077362  2.08769239  1.51561641 -1.07817476  0.77252731  1.1832251
  1.61004309 -1.60968458  0.66890713]. 
Working on 1.0 fold... 
[Thu Apr  4 15:24:03 2024]  Iteration number: 0 with current cost as 0.1492660672200189 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.05762623  0.58333495 -2.65409866
  3.03843004  2.16713351  1.25030115 -1.06943148  0.63464857  1.15072966
  1.36663952 -1.82225953  0.7242876 ]. 
Training complete taking 748.8195440769196 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 15:26:30 2024]  Iteration number: 0 with current cost as 0.17395685233024477 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.03787391  0.58658104 -2.62572058
  3.03473395  2.16439753  1.25823118 -1.06953788  0.64144427  1.15272522
  1.37714082 -1.81484778  0.71559942]. 
Working on 0.4 fold... 
[Thu Apr  4 15:28:41 2024]  Iteration number: 0 with current cost as 0.1335791266521221 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.06541265  0.58115123 -2.6667207
  3.04127202  2.17025483  1.24362412 -1.06938598  0.62880195  1.1489569
  1.36295953 -1.82456036  0.72839363]. 
Working on 0.6 fold... 
[Thu Apr  4 15:31:19 2024]  Iteration number: 0 with current cost as 0.14756195771492825 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.05535157  0.58317736 -2.65167431
  3.03756892  2.16610963  1.25236878 -1.06966166  0.6367854   1.15111834
  1.36682789 -1.82286503  0.72148814]. 
Working on 0.8 fold... 
[Thu Apr  4 15:33:57 2024]  Iteration number: 0 with current cost as 0.07504599153998551 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.21867354  0.68860824 -2.16209864
  2.91077362  2.08769239  1.51561641 -1.07817476  0.77252731  1.1832251
  1.61004309 -1.60968458  0.66890713]. 
Working on 1.0 fold... 
[Thu Apr  4 15:36:38 2024]  Iteration number: 0 with current cost as 0.1492660672200189 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.05762623  0.58333495 -2.65409866
  3.03843004  2.16713351  1.25030115 -1.06943148  0.63464857  1.15072966
  1.36663952 -1.82225953  0.7242876 ]. 
Training complete taking 754.0063140392303 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 15:39:05 2024]  Iteration number: 0 with current cost as 0.17395685233024477 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.03787391  0.58658104 -2.62572058
  3.03473395  2.16439753  1.25823118 -1.06953788  0.64144427  1.15272522
  1.37714082 -1.81484778  0.71559942]. 
Working on 0.4 fold... 
[Thu Apr  4 15:41:20 2024]  Iteration number: 0 with current cost as 0.1335791266521221 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.06541265  0.58115123 -2.6667207
  3.04127202  2.17025483  1.24362412 -1.06938598  0.62880195  1.1489569
  1.36295953 -1.82456036  0.72839363]. 
Working on 0.6 fold... 
[Thu Apr  4 15:43:53 2024]  Iteration number: 0 with current cost as 0.14756195771492825 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.05535157  0.58317736 -2.65167431
  3.03756892  2.16610963  1.25236878 -1.06966166  0.6367854   1.15111834
  1.36682789 -1.82286503  0.72148814]. 
Working on 0.8 fold... 
[Thu Apr  4 15:46:35 2024]  Iteration number: 0 with current cost as 0.07504599153998551 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.21867354  0.68860824 -2.16209864
  2.91077362  2.08769239  1.51561641 -1.07817476  0.77252731  1.1832251
  1.61004309 -1.60968458  0.66890713]. 
Working on 1.0 fold... 
[Thu Apr  4 15:48:59 2024]  Iteration number: 0 with current cost as 0.1492660672200189 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.05762623  0.58333495 -2.65409866
  3.03843004  2.16713351  1.25030115 -1.06943148  0.63464857  1.15072966
  1.36663952 -1.82225953  0.7242876 ]. 
Training complete taking 768.2857620716095 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 15:51:53 2024]  Iteration number: 0 with current cost as 0.17395685233024477 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.03787391  0.58658104 -2.62572058
  3.03473395  2.16439753  1.25823118 -1.06953788  0.64144427  1.15272522
  1.37714082 -1.81484778  0.71559942]. 
Working on 0.4 fold... 
[Thu Apr  4 15:54:16 2024]  Iteration number: 0 with current cost as 0.1335791266521221 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.06541265  0.58115123 -2.6667207
  3.04127202  2.17025483  1.24362412 -1.06938598  0.62880195  1.1489569
  1.36295953 -1.82456036  0.72839363]. 
Working on 0.6 fold... 
[Thu Apr  4 15:56:50 2024]  Iteration number: 0 with current cost as 0.14756195771492825 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.05535157  0.58317736 -2.65167431
  3.03756892  2.16610963  1.25236878 -1.06966166  0.6367854   1.15111834
  1.36682789 -1.82286503  0.72148814]. 
Working on 0.8 fold... 
[Thu Apr  4 15:59:22 2024]  Iteration number: 0 with current cost as 0.07504599153998551 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.21867354  0.68860824 -2.16209864
  2.91077362  2.08769239  1.51561641 -1.07817476  0.77252731  1.1832251
  1.61004309 -1.60968458  0.66890713]. 
Working on 1.0 fold... 
[Thu Apr  4 16:01:42 2024]  Iteration number: 0 with current cost as 0.1492660672200189 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.05762623  0.58333495 -2.65409866
  3.03843004  2.16713351  1.25030115 -1.06943148  0.63464857  1.15072966
  1.36663952 -1.82225953  0.7242876 ]. 
Training complete taking 737.2227802276611 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 16:04:12 2024]  Iteration number: 0 with current cost as 0.17395685233024477 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.03787391  0.58658104 -2.62572058
  3.03473395  2.16439753  1.25823118 -1.06953788  0.64144427  1.15272522
  1.37714082 -1.81484778  0.71559942]. 
Working on 0.4 fold... 
[Thu Apr  4 16:06:23 2024]  Iteration number: 0 with current cost as 0.1335791266521221 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.06541265  0.58115123 -2.6667207
  3.04127202  2.17025483  1.24362412 -1.06938598  0.62880195  1.1489569
  1.36295953 -1.82456036  0.72839363]. 
Working on 0.6 fold... 
[Thu Apr  4 16:08:58 2024]  Iteration number: 0 with current cost as 0.14756195771492825 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.05535157  0.58317736 -2.65167431
  3.03756892  2.16610963  1.25236878 -1.06966166  0.6367854   1.15111834
  1.36682789 -1.82286503  0.72148814]. 
Working on 0.8 fold... 
[Thu Apr  4 16:11:34 2024]  Iteration number: 0 with current cost as 0.07504599153998551 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.21867354  0.68860824 -2.16209864
  2.91077362  2.08769239  1.51561641 -1.07817476  0.77252731  1.1832251
  1.61004309 -1.60968458  0.66890713]. 
Working on 1.0 fold... 
[Thu Apr  4 16:13:54 2024]  Iteration number: 0 with current cost as 0.1492660672200189 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.05762623  0.58333495 -2.65409866
  3.03843004  2.16713351  1.25030115 -1.06943148  0.63464857  1.15072966
  1.36663952 -1.82225953  0.7242876 ]. 
Training complete taking 732.2809569835663 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 16:16:22 2024]  Iteration number: 0 with current cost as 0.17395685233024477 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.03787391  0.58658104 -2.62572058
  3.03473395  2.16439753  1.25823118 -1.06953788  0.64144427  1.15272522
  1.37714082 -1.81484778  0.71559942]. 
Working on 0.4 fold... 
[Thu Apr  4 16:18:31 2024]  Iteration number: 0 with current cost as 0.1335791266521221 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.06541265  0.58115123 -2.6667207
  3.04127202  2.17025483  1.24362412 -1.06938598  0.62880195  1.1489569
  1.36295953 -1.82456036  0.72839363]. 
Working on 0.6 fold... 
[Thu Apr  4 16:21:03 2024]  Iteration number: 0 with current cost as 0.14756195771492825 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.05535157  0.58317736 -2.65167431
  3.03756892  2.16610963  1.25236878 -1.06966166  0.6367854   1.15111834
  1.36682789 -1.82286503  0.72148814]. 
Working on 0.8 fold... 
[Thu Apr  4 16:23:39 2024]  Iteration number: 0 with current cost as 0.07504599153998551 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.21867354  0.68860824 -2.16209864
  2.91077362  2.08769239  1.51561641 -1.07817476  0.77252731  1.1832251
  1.61004309 -1.60968458  0.66890713]. 
Working on 1.0 fold... 
[Thu Apr  4 16:25:57 2024]  Iteration number: 0 with current cost as 0.1492660672200189 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.05762623  0.58333495 -2.65409866
  3.03843004  2.16713351  1.25030115 -1.06943148  0.63464857  1.15072966
  1.36663952 -1.82225953  0.7242876 ]. 
Training complete taking 725.3624668121338 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 16:28:27 2024]  Iteration number: 0 with current cost as 0.17395685233024477 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.03787391  0.58658104 -2.62572058
  3.03473395  2.16439753  1.25823118 -1.06953788  0.64144427  1.15272522
  1.37714082 -1.81484778  0.71559942]. 
Working on 0.4 fold... 
[Thu Apr  4 16:30:43 2024]  Iteration number: 0 with current cost as 0.1335791266521221 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.06541265  0.58115123 -2.6667207
  3.04127202  2.17025483  1.24362412 -1.06938598  0.62880195  1.1489569
  1.36295953 -1.82456036  0.72839363]. 
Working on 0.6 fold... 
[Thu Apr  4 16:33:16 2024]  Iteration number: 0 with current cost as 0.14756195771492825 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.05535157  0.58317736 -2.65167431
  3.03756892  2.16610963  1.25236878 -1.06966166  0.6367854   1.15111834
  1.36682789 -1.82286503  0.72148814]. 
Working on 0.8 fold... 
[Thu Apr  4 16:35:50 2024]  Iteration number: 0 with current cost as 0.07504599153998551 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.21867354  0.68860824 -2.16209864
  2.91077362  2.08769239  1.51561641 -1.07817476  0.77252731  1.1832251
  1.61004309 -1.60968458  0.66890713]. 
Working on 1.0 fold... 
[Thu Apr  4 16:38:10 2024]  Iteration number: 0 with current cost as 0.1492660672200189 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.05762623  0.58333495 -2.65409866
  3.03843004  2.16713351  1.25030115 -1.06943148  0.63464857  1.15072966
  1.36663952 -1.82225953  0.7242876 ]. 
Training complete taking 730.3706860542297 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 16:40:39 2024]  Iteration number: 0 with current cost as 0.17395685233024477 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.03787391  0.58658104 -2.62572058
  3.03473395  2.16439753  1.25823118 -1.06953788  0.64144427  1.15272522
  1.37714082 -1.81484778  0.71559942]. 
Working on 0.4 fold... 
[Thu Apr  4 16:42:51 2024]  Iteration number: 0 with current cost as 0.1335791266521221 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.06541265  0.58115123 -2.6667207
  3.04127202  2.17025483  1.24362412 -1.06938598  0.62880195  1.1489569
  1.36295953 -1.82456036  0.72839363]. 
Working on 0.6 fold... 
[Thu Apr  4 16:45:24 2024]  Iteration number: 0 with current cost as 0.14756195771492825 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.05535157  0.58317736 -2.65167431
  3.03756892  2.16610963  1.25236878 -1.06966166  0.6367854   1.15111834
  1.36682789 -1.82286503  0.72148814]. 
Working on 0.8 fold... 
[Thu Apr  4 16:47:56 2024]  Iteration number: 0 with current cost as 0.07504599153998551 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.21867354  0.68860824 -2.16209864
  2.91077362  2.08769239  1.51561641 -1.07817476  0.77252731  1.1832251
  1.61004309 -1.60968458  0.66890713]. 
Working on 1.0 fold... 
[Thu Apr  4 16:50:23 2024]  Iteration number: 0 with current cost as 0.1492660672200189 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.05762623  0.58333495 -2.65409866
  3.03843004  2.16713351  1.25030115 -1.06943148  0.63464857  1.15072966
  1.36663952 -1.82225953  0.7242876 ]. 
Training complete taking 747.5406544208527 seconds. 
Discarding model... 

Training complete taking 18604.342988967896 total seconds. 
Now scoring model... 
Scoring complete taking 0.8215005397796631 seconds. 
Saved predicted values as A1-A1-CNOT_HWE-CNOT_predicted_values.csv
Model scores: {'MSE_train': (27.200879039770523,), 'R2_train': 0.8682208701003133, 'MAE_train': 3.8423229693369594, 'MSE_test': 43.55784236869199, 'R2_test': 0.737441431878658, 'MAE_test': 4.960638105133766}. 
Saved model results as A1-A1-CNOT_HWE-CNOT_results.json. 
