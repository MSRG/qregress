/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/circuits/Encoders.py:12: RuntimeWarning: invalid value encountered in arcsin
  if np.isnan(np.arcsin(features[feature_idx])) or np.isnan(np.arccos(features[feature_idx])):
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Sun Mar 24 17:29:55 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 17:30:48 2024]  Iteration number: 0 with current cost as 0.46074512897749886 and parameters 
[-1.08295672  2.23743464 -2.12427964 -0.11653053  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 17:31:47 2024]  Iteration number: 0 with current cost as 0.37623898671970385 and parameters 
[-1.15776279  2.23743464 -2.12427951 -0.11653103  0.55388708 -2.77010897
  3.06858486  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029899 -1.87354655  0.72965055  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 17:32:53 2024]  Iteration number: 0 with current cost as 0.5001342552128882 and parameters 
[-0.07997905  2.23743521 -2.12427935 -0.11653103  0.55388708 -2.77010897
  3.06858527  2.18960174  1.18552027 -1.0664828   0.60271539  1.14432474
  1.31029927 -1.87354623  0.72965052  2.88578419 -0.54534306 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 17:34:44 2024]  Iteration number: 0 with current cost as 0.3985886622875897 and parameters 
[-1.12518698  2.23743464 -2.12427928 -0.11653067  0.55388708 -2.77010897
  3.06858516  2.18960181  1.18552016 -1.06648291  0.60271546  1.1443248
  1.31029934 -1.87354645  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 17:35:27 2024]  Iteration number: 0 with current cost as 0.40254991577958255 and parameters 
[-1.11690592  2.23743479 -2.12427948 -0.11653103  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 361.82936668395996 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 17:36:48 2024]  Iteration number: 0 with current cost as 0.46074512897749886 and parameters 
[-1.08295672  2.23743464 -2.12427964 -0.11653053  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 17:37:47 2024]  Iteration number: 0 with current cost as 0.37623898671970385 and parameters 
[-1.15776279  2.23743464 -2.12427951 -0.11653103  0.55388708 -2.77010897
  3.06858486  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029899 -1.87354655  0.72965055  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 17:38:53 2024]  Iteration number: 0 with current cost as 0.5001342552128882 and parameters 
[-0.07997905  2.23743521 -2.12427935 -0.11653103  0.55388708 -2.77010897
  3.06858527  2.18960174  1.18552027 -1.0664828   0.60271539  1.14432474
  1.31029927 -1.87354623  0.72965052  2.88578419 -0.54534306 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 17:40:44 2024]  Iteration number: 0 with current cost as 0.3985886622875897 and parameters 
[-1.12518698  2.23743464 -2.12427928 -0.11653067  0.55388708 -2.77010897
  3.06858516  2.18960181  1.18552016 -1.06648291  0.60271546  1.1443248
  1.31029934 -1.87354645  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 17:41:28 2024]  Iteration number: 0 with current cost as 0.40254991577958255 and parameters 
[-1.11690592  2.23743479 -2.12427948 -0.11653103  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 360.6556942462921 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 17:42:49 2024]  Iteration number: 0 with current cost as 0.46074512897749886 and parameters 
[-1.08295672  2.23743464 -2.12427964 -0.11653053  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 17:43:49 2024]  Iteration number: 0 with current cost as 0.37623898671970385 and parameters 
[-1.15776279  2.23743464 -2.12427951 -0.11653103  0.55388708 -2.77010897
  3.06858486  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029899 -1.87354655  0.72965055  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 17:44:55 2024]  Iteration number: 0 with current cost as 0.5001342552128882 and parameters 
[-0.07997905  2.23743521 -2.12427935 -0.11653103  0.55388708 -2.77010897
  3.06858527  2.18960174  1.18552027 -1.0664828   0.60271539  1.14432474
  1.31029927 -1.87354623  0.72965052  2.88578419 -0.54534306 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 17:46:46 2024]  Iteration number: 0 with current cost as 0.3985886622875897 and parameters 
[-1.12518698  2.23743464 -2.12427928 -0.11653067  0.55388708 -2.77010897
  3.06858516  2.18960181  1.18552016 -1.06648291  0.60271546  1.1443248
  1.31029934 -1.87354645  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 17:47:30 2024]  Iteration number: 0 with current cost as 0.40254991577958255 and parameters 
[-1.11690592  2.23743479 -2.12427948 -0.11653103  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 361.7910735607147 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 17:48:51 2024]  Iteration number: 0 with current cost as 0.46074512897749886 and parameters 
[-1.08295672  2.23743464 -2.12427964 -0.11653053  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 17:49:50 2024]  Iteration number: 0 with current cost as 0.37623898671970385 and parameters 
[-1.15776279  2.23743464 -2.12427951 -0.11653103  0.55388708 -2.77010897
  3.06858486  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029899 -1.87354655  0.72965055  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 17:50:56 2024]  Iteration number: 0 with current cost as 0.5001342552128882 and parameters 
[-0.07997905  2.23743521 -2.12427935 -0.11653103  0.55388708 -2.77010897
  3.06858527  2.18960174  1.18552027 -1.0664828   0.60271539  1.14432474
  1.31029927 -1.87354623  0.72965052  2.88578419 -0.54534306 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 17:52:48 2024]  Iteration number: 0 with current cost as 0.3985886622875897 and parameters 
[-1.12518698  2.23743464 -2.12427928 -0.11653067  0.55388708 -2.77010897
  3.06858516  2.18960181  1.18552016 -1.06648291  0.60271546  1.1443248
  1.31029934 -1.87354645  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 17:53:39 2024]  Iteration number: 0 with current cost as 0.40254991577958255 and parameters 
[-1.11690592  2.23743479 -2.12427948 -0.11653103  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 371.3097608089447 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 17:55:01 2024]  Iteration number: 0 with current cost as 0.46074512897749886 and parameters 
[-1.08295672  2.23743464 -2.12427964 -0.11653053  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 17:56:00 2024]  Iteration number: 0 with current cost as 0.37623898671970385 and parameters 
[-1.15776279  2.23743464 -2.12427951 -0.11653103  0.55388708 -2.77010897
  3.06858486  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029899 -1.87354655  0.72965055  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 17:57:08 2024]  Iteration number: 0 with current cost as 0.5001342552128882 and parameters 
[-0.07997905  2.23743521 -2.12427935 -0.11653103  0.55388708 -2.77010897
  3.06858527  2.18960174  1.18552027 -1.0664828   0.60271539  1.14432474
  1.31029927 -1.87354623  0.72965052  2.88578419 -0.54534306 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 17:58:59 2024]  Iteration number: 0 with current cost as 0.3985886622875897 and parameters 
[-1.12518698  2.23743464 -2.12427928 -0.11653067  0.55388708 -2.77010897
  3.06858516  2.18960181  1.18552016 -1.06648291  0.60271546  1.1443248
  1.31029934 -1.87354645  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 17:59:44 2024]  Iteration number: 0 with current cost as 0.40254991577958255 and parameters 
[-1.11690592  2.23743479 -2.12427948 -0.11653103  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 363.04971194267273 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 18:01:13 2024]  Iteration number: 0 with current cost as 0.46074512897749886 and parameters 
[-1.08295672  2.23743464 -2.12427964 -0.11653053  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:02:12 2024]  Iteration number: 0 with current cost as 0.37623898671970385 and parameters 
[-1.15776279  2.23743464 -2.12427951 -0.11653103  0.55388708 -2.77010897
  3.06858486  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029899 -1.87354655  0.72965055  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:03:19 2024]  Iteration number: 0 with current cost as 0.5001342552128882 and parameters 
[-0.07997905  2.23743521 -2.12427935 -0.11653103  0.55388708 -2.77010897
  3.06858527  2.18960174  1.18552027 -1.0664828   0.60271539  1.14432474
  1.31029927 -1.87354623  0.72965052  2.88578419 -0.54534306 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:05:13 2024]  Iteration number: 0 with current cost as 0.3985886622875897 and parameters 
[-1.12518698  2.23743464 -2.12427928 -0.11653067  0.55388708 -2.77010897
  3.06858516  2.18960181  1.18552016 -1.06648291  0.60271546  1.1443248
  1.31029934 -1.87354645  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:05:57 2024]  Iteration number: 0 with current cost as 0.40254991577958255 and parameters 
[-1.11690592  2.23743479 -2.12427948 -0.11653103  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 372.83530354499817 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 18:07:18 2024]  Iteration number: 0 with current cost as 0.46074512897749886 and parameters 
[-1.08295672  2.23743464 -2.12427964 -0.11653053  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:08:17 2024]  Iteration number: 0 with current cost as 0.37623898671970385 and parameters 
[-1.15776279  2.23743464 -2.12427951 -0.11653103  0.55388708 -2.77010897
  3.06858486  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029899 -1.87354655  0.72965055  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:09:23 2024]  Iteration number: 0 with current cost as 0.5001342552128882 and parameters 
[-0.07997905  2.23743521 -2.12427935 -0.11653103  0.55388708 -2.77010897
  3.06858527  2.18960174  1.18552027 -1.0664828   0.60271539  1.14432474
  1.31029927 -1.87354623  0.72965052  2.88578419 -0.54534306 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:11:15 2024]  Iteration number: 0 with current cost as 0.3985886622875897 and parameters 
[-1.12518698  2.23743464 -2.12427928 -0.11653067  0.55388708 -2.77010897
  3.06858516  2.18960181  1.18552016 -1.06648291  0.60271546  1.1443248
  1.31029934 -1.87354645  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:12:02 2024]  Iteration number: 0 with current cost as 0.40254991577958255 and parameters 
[-1.11690592  2.23743479 -2.12427948 -0.11653103  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 371.7286138534546 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 18:13:35 2024]  Iteration number: 0 with current cost as 0.46074512897749886 and parameters 
[-1.08295672  2.23743464 -2.12427964 -0.11653053  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:14:32 2024]  Iteration number: 0 with current cost as 0.37623898671970385 and parameters 
[-1.15776279  2.23743464 -2.12427951 -0.11653103  0.55388708 -2.77010897
  3.06858486  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029899 -1.87354655  0.72965055  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:15:41 2024]  Iteration number: 0 with current cost as 0.5001342552128882 and parameters 
[-0.07997905  2.23743521 -2.12427935 -0.11653103  0.55388708 -2.77010897
  3.06858527  2.18960174  1.18552027 -1.0664828   0.60271539  1.14432474
  1.31029927 -1.87354623  0.72965052  2.88578419 -0.54534306 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:17:31 2024]  Iteration number: 0 with current cost as 0.3985886622875897 and parameters 
[-1.12518698  2.23743464 -2.12427928 -0.11653067  0.55388708 -2.77010897
  3.06858516  2.18960181  1.18552016 -1.06648291  0.60271546  1.1443248
  1.31029934 -1.87354645  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:18:16 2024]  Iteration number: 0 with current cost as 0.40254991577958255 and parameters 
[-1.11690592  2.23743479 -2.12427948 -0.11653103  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 367.64192819595337 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 18:19:38 2024]  Iteration number: 0 with current cost as 0.46074512897749886 and parameters 
[-1.08295672  2.23743464 -2.12427964 -0.11653053  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:20:37 2024]  Iteration number: 0 with current cost as 0.37623898671970385 and parameters 
[-1.15776279  2.23743464 -2.12427951 -0.11653103  0.55388708 -2.77010897
  3.06858486  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029899 -1.87354655  0.72965055  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:21:45 2024]  Iteration number: 0 with current cost as 0.5001342552128882 and parameters 
[-0.07997905  2.23743521 -2.12427935 -0.11653103  0.55388708 -2.77010897
  3.06858527  2.18960174  1.18552027 -1.0664828   0.60271539  1.14432474
  1.31029927 -1.87354623  0.72965052  2.88578419 -0.54534306 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:23:39 2024]  Iteration number: 0 with current cost as 0.3985886622875897 and parameters 
[-1.12518698  2.23743464 -2.12427928 -0.11653067  0.55388708 -2.77010897
  3.06858516  2.18960181  1.18552016 -1.06648291  0.60271546  1.1443248
  1.31029934 -1.87354645  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:24:23 2024]  Iteration number: 0 with current cost as 0.40254991577958255 and parameters 
[-1.11690592  2.23743479 -2.12427948 -0.11653103  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 366.4926953315735 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 18:25:44 2024]  Iteration number: 0 with current cost as 0.46074512897749886 and parameters 
[-1.08295672  2.23743464 -2.12427964 -0.11653053  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:26:43 2024]  Iteration number: 0 with current cost as 0.37623898671970385 and parameters 
[-1.15776279  2.23743464 -2.12427951 -0.11653103  0.55388708 -2.77010897
  3.06858486  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029899 -1.87354655  0.72965055  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:27:49 2024]  Iteration number: 0 with current cost as 0.5001342552128882 and parameters 
[-0.07997905  2.23743521 -2.12427935 -0.11653103  0.55388708 -2.77010897
  3.06858527  2.18960174  1.18552027 -1.0664828   0.60271539  1.14432474
  1.31029927 -1.87354623  0.72965052  2.88578419 -0.54534306 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:29:41 2024]  Iteration number: 0 with current cost as 0.3985886622875897 and parameters 
[-1.12518698  2.23743464 -2.12427928 -0.11653067  0.55388708 -2.77010897
  3.06858516  2.18960181  1.18552016 -1.06648291  0.60271546  1.1443248
  1.31029934 -1.87354645  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:30:25 2024]  Iteration number: 0 with current cost as 0.40254991577958255 and parameters 
[-1.11690592  2.23743479 -2.12427948 -0.11653103  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 363.58758521080017 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 18:31:52 2024]  Iteration number: 0 with current cost as 0.46074512897749886 and parameters 
[-1.08295672  2.23743464 -2.12427964 -0.11653053  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:32:51 2024]  Iteration number: 0 with current cost as 0.37623898671970385 and parameters 
[-1.15776279  2.23743464 -2.12427951 -0.11653103  0.55388708 -2.77010897
  3.06858486  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029899 -1.87354655  0.72965055  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:34:01 2024]  Iteration number: 0 with current cost as 0.5001342552128882 and parameters 
[-0.07997905  2.23743521 -2.12427935 -0.11653103  0.55388708 -2.77010897
  3.06858527  2.18960174  1.18552027 -1.0664828   0.60271539  1.14432474
  1.31029927 -1.87354623  0.72965052  2.88578419 -0.54534306 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:35:52 2024]  Iteration number: 0 with current cost as 0.3985886622875897 and parameters 
[-1.12518698  2.23743464 -2.12427928 -0.11653067  0.55388708 -2.77010897
  3.06858516  2.18960181  1.18552016 -1.06648291  0.60271546  1.1443248
  1.31029934 -1.87354645  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:36:37 2024]  Iteration number: 0 with current cost as 0.40254991577958255 and parameters 
[-1.11690592  2.23743479 -2.12427948 -0.11653103  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 370.6273374557495 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 18:37:57 2024]  Iteration number: 0 with current cost as 0.46074512897749886 and parameters 
[-1.08295672  2.23743464 -2.12427964 -0.11653053  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:38:56 2024]  Iteration number: 0 with current cost as 0.37623898671970385 and parameters 
[-1.15776279  2.23743464 -2.12427951 -0.11653103  0.55388708 -2.77010897
  3.06858486  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029899 -1.87354655  0.72965055  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:40:02 2024]  Iteration number: 0 with current cost as 0.5001342552128882 and parameters 
[-0.07997905  2.23743521 -2.12427935 -0.11653103  0.55388708 -2.77010897
  3.06858527  2.18960174  1.18552027 -1.0664828   0.60271539  1.14432474
  1.31029927 -1.87354623  0.72965052  2.88578419 -0.54534306 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:41:53 2024]  Iteration number: 0 with current cost as 0.3985886622875897 and parameters 
[-1.12518698  2.23743464 -2.12427928 -0.11653067  0.55388708 -2.77010897
  3.06858516  2.18960181  1.18552016 -1.06648291  0.60271546  1.1443248
  1.31029934 -1.87354645  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:42:39 2024]  Iteration number: 0 with current cost as 0.40254991577958255 and parameters 
[-1.11690592  2.23743479 -2.12427948 -0.11653103  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 361.98055839538574 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 18:43:59 2024]  Iteration number: 0 with current cost as 0.46074512897749886 and parameters 
[-1.08295672  2.23743464 -2.12427964 -0.11653053  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:44:59 2024]  Iteration number: 0 with current cost as 0.37623898671970385 and parameters 
[-1.15776279  2.23743464 -2.12427951 -0.11653103  0.55388708 -2.77010897
  3.06858486  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029899 -1.87354655  0.72965055  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:46:06 2024]  Iteration number: 0 with current cost as 0.5001342552128882 and parameters 
[-0.07997905  2.23743521 -2.12427935 -0.11653103  0.55388708 -2.77010897
  3.06858527  2.18960174  1.18552027 -1.0664828   0.60271539  1.14432474
  1.31029927 -1.87354623  0.72965052  2.88578419 -0.54534306 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:47:57 2024]  Iteration number: 0 with current cost as 0.3985886622875897 and parameters 
[-1.12518698  2.23743464 -2.12427928 -0.11653067  0.55388708 -2.77010897
  3.06858516  2.18960181  1.18552016 -1.06648291  0.60271546  1.1443248
  1.31029934 -1.87354645  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:48:44 2024]  Iteration number: 0 with current cost as 0.40254991577958255 and parameters 
[-1.11690592  2.23743479 -2.12427948 -0.11653103  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 364.5590469837189 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 18:50:03 2024]  Iteration number: 0 with current cost as 0.46074512897749886 and parameters 
[-1.08295672  2.23743464 -2.12427964 -0.11653053  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:51:09 2024]  Iteration number: 0 with current cost as 0.37623898671970385 and parameters 
[-1.15776279  2.23743464 -2.12427951 -0.11653103  0.55388708 -2.77010897
  3.06858486  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029899 -1.87354655  0.72965055  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:52:15 2024]  Iteration number: 0 with current cost as 0.5001342552128882 and parameters 
[-0.07997905  2.23743521 -2.12427935 -0.11653103  0.55388708 -2.77010897
  3.06858527  2.18960174  1.18552027 -1.0664828   0.60271539  1.14432474
  1.31029927 -1.87354623  0.72965052  2.88578419 -0.54534306 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:54:12 2024]  Iteration number: 0 with current cost as 0.3985886622875897 and parameters 
[-1.12518698  2.23743464 -2.12427928 -0.11653067  0.55388708 -2.77010897
  3.06858516  2.18960181  1.18552016 -1.06648291  0.60271546  1.1443248
  1.31029934 -1.87354645  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:54:56 2024]  Iteration number: 0 with current cost as 0.40254991577958255 and parameters 
[-1.11690592  2.23743479 -2.12427948 -0.11653103  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 384.9750108718872 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 18:56:31 2024]  Iteration number: 0 with current cost as 0.46074512897749886 and parameters 
[-1.08295672  2.23743464 -2.12427964 -0.11653053  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:57:30 2024]  Iteration number: 0 with current cost as 0.37623898671970385 and parameters 
[-1.15776279  2.23743464 -2.12427951 -0.11653103  0.55388708 -2.77010897
  3.06858486  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029899 -1.87354655  0.72965055  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:58:38 2024]  Iteration number: 0 with current cost as 0.5001342552128882 and parameters 
[-0.07997905  2.23743521 -2.12427935 -0.11653103  0.55388708 -2.77010897
  3.06858527  2.18960174  1.18552027 -1.0664828   0.60271539  1.14432474
  1.31029927 -1.87354623  0.72965052  2.88578419 -0.54534306 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:00:27 2024]  Iteration number: 0 with current cost as 0.3985886622875897 and parameters 
[-1.12518698  2.23743464 -2.12427928 -0.11653067  0.55388708 -2.77010897
  3.06858516  2.18960181  1.18552016 -1.06648291  0.60271546  1.1443248
  1.31029934 -1.87354645  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:01:13 2024]  Iteration number: 0 with current cost as 0.40254991577958255 and parameters 
[-1.11690592  2.23743479 -2.12427948 -0.11653103  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 364.0933666229248 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 19:02:33 2024]  Iteration number: 0 with current cost as 0.46074512897749886 and parameters 
[-1.08295672  2.23743464 -2.12427964 -0.11653053  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:03:34 2024]  Iteration number: 0 with current cost as 0.37623898671970385 and parameters 
[-1.15776279  2.23743464 -2.12427951 -0.11653103  0.55388708 -2.77010897
  3.06858486  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029899 -1.87354655  0.72965055  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:04:41 2024]  Iteration number: 0 with current cost as 0.5001342552128882 and parameters 
[-0.07997905  2.23743521 -2.12427935 -0.11653103  0.55388708 -2.77010897
  3.06858527  2.18960174  1.18552027 -1.0664828   0.60271539  1.14432474
  1.31029927 -1.87354623  0.72965052  2.88578419 -0.54534306 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:06:54 2024]  Iteration number: 0 with current cost as 0.3985886622875897 and parameters 
[-1.12518698  2.23743464 -2.12427928 -0.11653067  0.55388708 -2.77010897
  3.06858516  2.18960181  1.18552016 -1.06648291  0.60271546  1.1443248
  1.31029934 -1.87354645  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:07:42 2024]  Iteration number: 0 with current cost as 0.40254991577958255 and parameters 
[-1.11690592  2.23743479 -2.12427948 -0.11653103  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 395.16211557388306 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 19:09:09 2024]  Iteration number: 0 with current cost as 0.46074512897749886 and parameters 
[-1.08295672  2.23743464 -2.12427964 -0.11653053  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:10:10 2024]  Iteration number: 0 with current cost as 0.37623898671970385 and parameters 
[-1.15776279  2.23743464 -2.12427951 -0.11653103  0.55388708 -2.77010897
  3.06858486  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029899 -1.87354655  0.72965055  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:11:16 2024]  Iteration number: 0 with current cost as 0.5001342552128882 and parameters 
[-0.07997905  2.23743521 -2.12427935 -0.11653103  0.55388708 -2.77010897
  3.06858527  2.18960174  1.18552027 -1.0664828   0.60271539  1.14432474
  1.31029927 -1.87354623  0.72965052  2.88578419 -0.54534306 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:13:12 2024]  Iteration number: 0 with current cost as 0.3985886622875897 and parameters 
[-1.12518698  2.23743464 -2.12427928 -0.11653067  0.55388708 -2.77010897
  3.06858516  2.18960181  1.18552016 -1.06648291  0.60271546  1.1443248
  1.31029934 -1.87354645  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:13:58 2024]  Iteration number: 0 with current cost as 0.40254991577958255 and parameters 
[-1.11690592  2.23743479 -2.12427948 -0.11653103  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 369.8115735054016 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 19:15:19 2024]  Iteration number: 0 with current cost as 0.46074512897749886 and parameters 
[-1.08295672  2.23743464 -2.12427964 -0.11653053  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:16:20 2024]  Iteration number: 0 with current cost as 0.37623898671970385 and parameters 
[-1.15776279  2.23743464 -2.12427951 -0.11653103  0.55388708 -2.77010897
  3.06858486  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029899 -1.87354655  0.72965055  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:17:26 2024]  Iteration number: 0 with current cost as 0.5001342552128882 and parameters 
[-0.07997905  2.23743521 -2.12427935 -0.11653103  0.55388708 -2.77010897
  3.06858527  2.18960174  1.18552027 -1.0664828   0.60271539  1.14432474
  1.31029927 -1.87354623  0.72965052  2.88578419 -0.54534306 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:19:17 2024]  Iteration number: 0 with current cost as 0.3985886622875897 and parameters 
[-1.12518698  2.23743464 -2.12427928 -0.11653067  0.55388708 -2.77010897
  3.06858516  2.18960181  1.18552016 -1.06648291  0.60271546  1.1443248
  1.31029934 -1.87354645  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:20:00 2024]  Iteration number: 0 with current cost as 0.40254991577958255 and parameters 
[-1.11690592  2.23743479 -2.12427948 -0.11653103  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 362.7535970211029 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 19:21:22 2024]  Iteration number: 0 with current cost as 0.46074512897749886 and parameters 
[-1.08295672  2.23743464 -2.12427964 -0.11653053  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:22:22 2024]  Iteration number: 0 with current cost as 0.37623898671970385 and parameters 
[-1.15776279  2.23743464 -2.12427951 -0.11653103  0.55388708 -2.77010897
  3.06858486  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029899 -1.87354655  0.72965055  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:23:29 2024]  Iteration number: 0 with current cost as 0.5001342552128882 and parameters 
[-0.07997905  2.23743521 -2.12427935 -0.11653103  0.55388708 -2.77010897
  3.06858527  2.18960174  1.18552027 -1.0664828   0.60271539  1.14432474
  1.31029927 -1.87354623  0.72965052  2.88578419 -0.54534306 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:25:20 2024]  Iteration number: 0 with current cost as 0.3985886622875897 and parameters 
[-1.12518698  2.23743464 -2.12427928 -0.11653067  0.55388708 -2.77010897
  3.06858516  2.18960181  1.18552016 -1.06648291  0.60271546  1.1443248
  1.31029934 -1.87354645  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:26:04 2024]  Iteration number: 0 with current cost as 0.40254991577958255 and parameters 
[-1.11690592  2.23743479 -2.12427948 -0.11653103  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 363.2727105617523 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 19:27:25 2024]  Iteration number: 0 with current cost as 0.46074512897749886 and parameters 
[-1.08295672  2.23743464 -2.12427964 -0.11653053  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:28:24 2024]  Iteration number: 0 with current cost as 0.37623898671970385 and parameters 
[-1.15776279  2.23743464 -2.12427951 -0.11653103  0.55388708 -2.77010897
  3.06858486  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029899 -1.87354655  0.72965055  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:29:31 2024]  Iteration number: 0 with current cost as 0.5001342552128882 and parameters 
[-0.07997905  2.23743521 -2.12427935 -0.11653103  0.55388708 -2.77010897
  3.06858527  2.18960174  1.18552027 -1.0664828   0.60271539  1.14432474
  1.31029927 -1.87354623  0.72965052  2.88578419 -0.54534306 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:31:25 2024]  Iteration number: 0 with current cost as 0.3985886622875897 and parameters 
[-1.12518698  2.23743464 -2.12427928 -0.11653067  0.55388708 -2.77010897
  3.06858516  2.18960181  1.18552016 -1.06648291  0.60271546  1.1443248
  1.31029934 -1.87354645  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:32:13 2024]  Iteration number: 0 with current cost as 0.40254991577958255 and parameters 
[-1.11690592  2.23743479 -2.12427948 -0.11653103  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 369.63901376724243 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 19:33:36 2024]  Iteration number: 0 with current cost as 0.46074512897749886 and parameters 
[-1.08295672  2.23743464 -2.12427964 -0.11653053  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:34:34 2024]  Iteration number: 0 with current cost as 0.37623898671970385 and parameters 
[-1.15776279  2.23743464 -2.12427951 -0.11653103  0.55388708 -2.77010897
  3.06858486  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029899 -1.87354655  0.72965055  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:35:41 2024]  Iteration number: 0 with current cost as 0.5001342552128882 and parameters 
[-0.07997905  2.23743521 -2.12427935 -0.11653103  0.55388708 -2.77010897
  3.06858527  2.18960174  1.18552027 -1.0664828   0.60271539  1.14432474
  1.31029927 -1.87354623  0.72965052  2.88578419 -0.54534306 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:37:29 2024]  Iteration number: 0 with current cost as 0.3985886622875897 and parameters 
[-1.12518698  2.23743464 -2.12427928 -0.11653067  0.55388708 -2.77010897
  3.06858516  2.18960181  1.18552016 -1.06648291  0.60271546  1.1443248
  1.31029934 -1.87354645  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:38:14 2024]  Iteration number: 0 with current cost as 0.40254991577958255 and parameters 
[-1.11690592  2.23743479 -2.12427948 -0.11653103  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 360.4437048435211 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 19:39:33 2024]  Iteration number: 0 with current cost as 0.46074512897749886 and parameters 
[-1.08295672  2.23743464 -2.12427964 -0.11653053  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:40:31 2024]  Iteration number: 0 with current cost as 0.37623898671970385 and parameters 
[-1.15776279  2.23743464 -2.12427951 -0.11653103  0.55388708 -2.77010897
  3.06858486  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029899 -1.87354655  0.72965055  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:41:37 2024]  Iteration number: 0 with current cost as 0.5001342552128882 and parameters 
[-0.07997905  2.23743521 -2.12427935 -0.11653103  0.55388708 -2.77010897
  3.06858527  2.18960174  1.18552027 -1.0664828   0.60271539  1.14432474
  1.31029927 -1.87354623  0.72965052  2.88578419 -0.54534306 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:43:28 2024]  Iteration number: 0 with current cost as 0.3985886622875897 and parameters 
[-1.12518698  2.23743464 -2.12427928 -0.11653067  0.55388708 -2.77010897
  3.06858516  2.18960181  1.18552016 -1.06648291  0.60271546  1.1443248
  1.31029934 -1.87354645  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:44:12 2024]  Iteration number: 0 with current cost as 0.40254991577958255 and parameters 
[-1.11690592  2.23743479 -2.12427948 -0.11653103  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 358.49071288108826 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 19:45:48 2024]  Iteration number: 0 with current cost as 0.46074512897749886 and parameters 
[-1.08295672  2.23743464 -2.12427964 -0.11653053  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:46:47 2024]  Iteration number: 0 with current cost as 0.37623898671970385 and parameters 
[-1.15776279  2.23743464 -2.12427951 -0.11653103  0.55388708 -2.77010897
  3.06858486  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029899 -1.87354655  0.72965055  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:47:52 2024]  Iteration number: 0 with current cost as 0.5001342552128882 and parameters 
[-0.07997905  2.23743521 -2.12427935 -0.11653103  0.55388708 -2.77010897
  3.06858527  2.18960174  1.18552027 -1.0664828   0.60271539  1.14432474
  1.31029927 -1.87354623  0.72965052  2.88578419 -0.54534306 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:49:43 2024]  Iteration number: 0 with current cost as 0.3985886622875897 and parameters 
[-1.12518698  2.23743464 -2.12427928 -0.11653067  0.55388708 -2.77010897
  3.06858516  2.18960181  1.18552016 -1.06648291  0.60271546  1.1443248
  1.31029934 -1.87354645  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:50:26 2024]  Iteration number: 0 with current cost as 0.40254991577958255 and parameters 
[-1.11690592  2.23743479 -2.12427948 -0.11653103  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 373.67755341529846 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 19:51:47 2024]  Iteration number: 0 with current cost as 0.46074512897749886 and parameters 
[-1.08295672  2.23743464 -2.12427964 -0.11653053  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:52:45 2024]  Iteration number: 0 with current cost as 0.37623898671970385 and parameters 
[-1.15776279  2.23743464 -2.12427951 -0.11653103  0.55388708 -2.77010897
  3.06858486  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029899 -1.87354655  0.72965055  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:53:51 2024]  Iteration number: 0 with current cost as 0.5001342552128882 and parameters 
[-0.07997905  2.23743521 -2.12427935 -0.11653103  0.55388708 -2.77010897
  3.06858527  2.18960174  1.18552027 -1.0664828   0.60271539  1.14432474
  1.31029927 -1.87354623  0.72965052  2.88578419 -0.54534306 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:55:42 2024]  Iteration number: 0 with current cost as 0.3985886622875897 and parameters 
[-1.12518698  2.23743464 -2.12427928 -0.11653067  0.55388708 -2.77010897
  3.06858516  2.18960181  1.18552016 -1.06648291  0.60271546  1.1443248
  1.31029934 -1.87354645  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:56:25 2024]  Iteration number: 0 with current cost as 0.40254991577958255 and parameters 
[-1.11690592  2.23743479 -2.12427948 -0.11653103  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 359.3538203239441 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 19:57:46 2024]  Iteration number: 0 with current cost as 0.46074512897749886 and parameters 
[-1.08295672  2.23743464 -2.12427964 -0.11653053  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:58:45 2024]  Iteration number: 0 with current cost as 0.37623898671970385 and parameters 
[-1.15776279  2.23743464 -2.12427951 -0.11653103  0.55388708 -2.77010897
  3.06858486  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029899 -1.87354655  0.72965055  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:59:51 2024]  Iteration number: 0 with current cost as 0.5001342552128882 and parameters 
[-0.07997905  2.23743521 -2.12427935 -0.11653103  0.55388708 -2.77010897
  3.06858527  2.18960174  1.18552027 -1.0664828   0.60271539  1.14432474
  1.31029927 -1.87354623  0.72965052  2.88578419 -0.54534306 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:01:51 2024]  Iteration number: 0 with current cost as 0.3985886622875897 and parameters 
[-1.12518698  2.23743464 -2.12427928 -0.11653067  0.55388708 -2.77010897
  3.06858516  2.18960181  1.18552016 -1.06648291  0.60271546  1.1443248
  1.31029934 -1.87354645  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 20:02:36 2024]  Iteration number: 0 with current cost as 0.40254991577958255 and parameters 
[-1.11690592  2.23743479 -2.12427948 -0.11653103  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 370.6228094100952 seconds. 
Discarding model... 

Training complete taking 9190.385673999786 total seconds. 
Now scoring model... 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Scoring complete taking 0.8425660133361816 seconds. 
Saved predicted values as M-A1-CNOT_Modified-Pauli-CRZ_predicted_values.csv
Model scores: {'MSE_train': (256.35371916474514,), 'R2_train': -0.24194773296428518, 'MAE_train': 14.396656389443962, 'MSE_test': 224.0725410873362, 'R2_test': -0.3506675800243102, 'MAE_test': 12.795027007942936}. 
Saved model results as M-A1-CNOT_Modified-Pauli-CRZ_results.json. 
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/circuits/Encoders.py:12: RuntimeWarning: invalid value encountered in arcsin
  if np.isnan(np.arcsin(features[feature_idx])) or np.isnan(np.arccos(features[feature_idx])):
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Thu Apr  4 11:28:45 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 11:29:39 2024]  Iteration number: 0 with current cost as 0.46074512897749886 and parameters 
[-1.08295672  2.23743464 -2.12427964 -0.11653053  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 11:30:36 2024]  Iteration number: 0 with current cost as 0.37623898671970385 and parameters 
[-1.15776279  2.23743464 -2.12427951 -0.11653103  0.55388708 -2.77010897
  3.06858486  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029899 -1.87354655  0.72965055  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 11:31:41 2024]  Iteration number: 0 with current cost as 0.5001342552128882 and parameters 
[-0.07997905  2.23743521 -2.12427935 -0.11653103  0.55388708 -2.77010897
  3.06858527  2.18960174  1.18552027 -1.0664828   0.60271539  1.14432474
  1.31029927 -1.87354623  0.72965052  2.88578419 -0.54534306 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 11:33:29 2024]  Iteration number: 0 with current cost as 0.3985886622875897 and parameters 
[-1.12518698  2.23743464 -2.12427928 -0.11653067  0.55388708 -2.77010897
  3.06858516  2.18960181  1.18552016 -1.06648291  0.60271546  1.1443248
  1.31029934 -1.87354645  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 11:34:11 2024]  Iteration number: 0 with current cost as 0.40254991577958255 and parameters 
[-1.11690592  2.23743479 -2.12427948 -0.11653103  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 354.71983575820923 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 11:35:29 2024]  Iteration number: 0 with current cost as 0.46074512897749886 and parameters 
[-1.08295672  2.23743464 -2.12427964 -0.11653053  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 11:36:27 2024]  Iteration number: 0 with current cost as 0.37623898671970385 and parameters 
[-1.15776279  2.23743464 -2.12427951 -0.11653103  0.55388708 -2.77010897
  3.06858486  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029899 -1.87354655  0.72965055  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 11:37:32 2024]  Iteration number: 0 with current cost as 0.5001342552128882 and parameters 
[-0.07997905  2.23743521 -2.12427935 -0.11653103  0.55388708 -2.77010897
  3.06858527  2.18960174  1.18552027 -1.0664828   0.60271539  1.14432474
  1.31029927 -1.87354623  0.72965052  2.88578419 -0.54534306 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 11:39:19 2024]  Iteration number: 0 with current cost as 0.3985886622875897 and parameters 
[-1.12518698  2.23743464 -2.12427928 -0.11653067  0.55388708 -2.77010897
  3.06858516  2.18960181  1.18552016 -1.06648291  0.60271546  1.1443248
  1.31029934 -1.87354645  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 11:40:01 2024]  Iteration number: 0 with current cost as 0.40254991577958255 and parameters 
[-1.11690592  2.23743479 -2.12427948 -0.11653103  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 350.3936879634857 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 11:41:20 2024]  Iteration number: 0 with current cost as 0.46074512897749886 and parameters 
[-1.08295672  2.23743464 -2.12427964 -0.11653053  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 11:42:17 2024]  Iteration number: 0 with current cost as 0.37623898671970385 and parameters 
[-1.15776279  2.23743464 -2.12427951 -0.11653103  0.55388708 -2.77010897
  3.06858486  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029899 -1.87354655  0.72965055  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 11:43:22 2024]  Iteration number: 0 with current cost as 0.5001342552128882 and parameters 
[-0.07997905  2.23743521 -2.12427935 -0.11653103  0.55388708 -2.77010897
  3.06858527  2.18960174  1.18552027 -1.0664828   0.60271539  1.14432474
  1.31029927 -1.87354623  0.72965052  2.88578419 -0.54534306 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 11:45:08 2024]  Iteration number: 0 with current cost as 0.3985886622875897 and parameters 
[-1.12518698  2.23743464 -2.12427928 -0.11653067  0.55388708 -2.77010897
  3.06858516  2.18960181  1.18552016 -1.06648291  0.60271546  1.1443248
  1.31029934 -1.87354645  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 11:45:51 2024]  Iteration number: 0 with current cost as 0.40254991577958255 and parameters 
[-1.11690592  2.23743479 -2.12427948 -0.11653103  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 349.5627884864807 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 11:47:08 2024]  Iteration number: 0 with current cost as 0.46074512897749886 and parameters 
[-1.08295672  2.23743464 -2.12427964 -0.11653053  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 11:48:05 2024]  Iteration number: 0 with current cost as 0.37623898671970385 and parameters 
[-1.15776279  2.23743464 -2.12427951 -0.11653103  0.55388708 -2.77010897
  3.06858486  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029899 -1.87354655  0.72965055  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 11:49:09 2024]  Iteration number: 0 with current cost as 0.5001342552128882 and parameters 
[-0.07997905  2.23743521 -2.12427935 -0.11653103  0.55388708 -2.77010897
  3.06858527  2.18960174  1.18552027 -1.0664828   0.60271539  1.14432474
  1.31029927 -1.87354623  0.72965052  2.88578419 -0.54534306 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 11:50:56 2024]  Iteration number: 0 with current cost as 0.3985886622875897 and parameters 
[-1.12518698  2.23743464 -2.12427928 -0.11653067  0.55388708 -2.77010897
  3.06858516  2.18960181  1.18552016 -1.06648291  0.60271546  1.1443248
  1.31029934 -1.87354645  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 11:51:39 2024]  Iteration number: 0 with current cost as 0.40254991577958255 and parameters 
[-1.11690592  2.23743479 -2.12427948 -0.11653103  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 347.7807149887085 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 11:52:59 2024]  Iteration number: 0 with current cost as 0.46074512897749886 and parameters 
[-1.08295672  2.23743464 -2.12427964 -0.11653053  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 11:53:56 2024]  Iteration number: 0 with current cost as 0.37623898671970385 and parameters 
[-1.15776279  2.23743464 -2.12427951 -0.11653103  0.55388708 -2.77010897
  3.06858486  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029899 -1.87354655  0.72965055  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 11:55:01 2024]  Iteration number: 0 with current cost as 0.5001342552128882 and parameters 
[-0.07997905  2.23743521 -2.12427935 -0.11653103  0.55388708 -2.77010897
  3.06858527  2.18960174  1.18552027 -1.0664828   0.60271539  1.14432474
  1.31029927 -1.87354623  0.72965052  2.88578419 -0.54534306 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 11:56:48 2024]  Iteration number: 0 with current cost as 0.3985886622875897 and parameters 
[-1.12518698  2.23743464 -2.12427928 -0.11653067  0.55388708 -2.77010897
  3.06858516  2.18960181  1.18552016 -1.06648291  0.60271546  1.1443248
  1.31029934 -1.87354645  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 11:57:31 2024]  Iteration number: 0 with current cost as 0.40254991577958255 and parameters 
[-1.11690592  2.23743479 -2.12427948 -0.11653103  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 351.690731048584 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 11:58:49 2024]  Iteration number: 0 with current cost as 0.46074512897749886 and parameters 
[-1.08295672  2.23743464 -2.12427964 -0.11653053  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 11:59:46 2024]  Iteration number: 0 with current cost as 0.37623898671970385 and parameters 
[-1.15776279  2.23743464 -2.12427951 -0.11653103  0.55388708 -2.77010897
  3.06858486  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029899 -1.87354655  0.72965055  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 12:00:51 2024]  Iteration number: 0 with current cost as 0.5001342552128882 and parameters 
[-0.07997905  2.23743521 -2.12427935 -0.11653103  0.55388708 -2.77010897
  3.06858527  2.18960174  1.18552027 -1.0664828   0.60271539  1.14432474
  1.31029927 -1.87354623  0.72965052  2.88578419 -0.54534306 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:02:39 2024]  Iteration number: 0 with current cost as 0.3985886622875897 and parameters 
[-1.12518698  2.23743464 -2.12427928 -0.11653067  0.55388708 -2.77010897
  3.06858516  2.18960181  1.18552016 -1.06648291  0.60271546  1.1443248
  1.31029934 -1.87354645  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:03:22 2024]  Iteration number: 0 with current cost as 0.40254991577958255 and parameters 
[-1.11690592  2.23743479 -2.12427948 -0.11653103  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 351.1855025291443 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 12:04:51 2024]  Iteration number: 0 with current cost as 0.46074512897749886 and parameters 
[-1.08295672  2.23743464 -2.12427964 -0.11653053  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 12:05:47 2024]  Iteration number: 0 with current cost as 0.37623898671970385 and parameters 
[-1.15776279  2.23743464 -2.12427951 -0.11653103  0.55388708 -2.77010897
  3.06858486  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029899 -1.87354655  0.72965055  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 12:06:51 2024]  Iteration number: 0 with current cost as 0.5001342552128882 and parameters 
[-0.07997905  2.23743521 -2.12427935 -0.11653103  0.55388708 -2.77010897
  3.06858527  2.18960174  1.18552027 -1.0664828   0.60271539  1.14432474
  1.31029927 -1.87354623  0.72965052  2.88578419 -0.54534306 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:08:38 2024]  Iteration number: 0 with current cost as 0.3985886622875897 and parameters 
[-1.12518698  2.23743464 -2.12427928 -0.11653067  0.55388708 -2.77010897
  3.06858516  2.18960181  1.18552016 -1.06648291  0.60271546  1.1443248
  1.31029934 -1.87354645  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:09:21 2024]  Iteration number: 0 with current cost as 0.40254991577958255 and parameters 
[-1.11690592  2.23743479 -2.12427948 -0.11653103  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 358.58784675598145 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 12:10:39 2024]  Iteration number: 0 with current cost as 0.46074512897749886 and parameters 
[-1.08295672  2.23743464 -2.12427964 -0.11653053  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 12:11:36 2024]  Iteration number: 0 with current cost as 0.37623898671970385 and parameters 
[-1.15776279  2.23743464 -2.12427951 -0.11653103  0.55388708 -2.77010897
  3.06858486  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029899 -1.87354655  0.72965055  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 12:12:40 2024]  Iteration number: 0 with current cost as 0.5001342552128882 and parameters 
[-0.07997905  2.23743521 -2.12427935 -0.11653103  0.55388708 -2.77010897
  3.06858527  2.18960174  1.18552027 -1.0664828   0.60271539  1.14432474
  1.31029927 -1.87354623  0.72965052  2.88578419 -0.54534306 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:14:28 2024]  Iteration number: 0 with current cost as 0.3985886622875897 and parameters 
[-1.12518698  2.23743464 -2.12427928 -0.11653067  0.55388708 -2.77010897
  3.06858516  2.18960181  1.18552016 -1.06648291  0.60271546  1.1443248
  1.31029934 -1.87354645  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:15:10 2024]  Iteration number: 0 with current cost as 0.40254991577958255 and parameters 
[-1.11690592  2.23743479 -2.12427948 -0.11653103  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 349.8965570926666 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 12:16:30 2024]  Iteration number: 0 with current cost as 0.46074512897749886 and parameters 
[-1.08295672  2.23743464 -2.12427964 -0.11653053  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 12:17:27 2024]  Iteration number: 0 with current cost as 0.37623898671970385 and parameters 
[-1.15776279  2.23743464 -2.12427951 -0.11653103  0.55388708 -2.77010897
  3.06858486  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029899 -1.87354655  0.72965055  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 12:18:30 2024]  Iteration number: 0 with current cost as 0.5001342552128882 and parameters 
[-0.07997905  2.23743521 -2.12427935 -0.11653103  0.55388708 -2.77010897
  3.06858527  2.18960174  1.18552027 -1.0664828   0.60271539  1.14432474
  1.31029927 -1.87354623  0.72965052  2.88578419 -0.54534306 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:20:15 2024]  Iteration number: 0 with current cost as 0.3985886622875897 and parameters 
[-1.12518698  2.23743464 -2.12427928 -0.11653067  0.55388708 -2.77010897
  3.06858516  2.18960181  1.18552016 -1.06648291  0.60271546  1.1443248
  1.31029934 -1.87354645  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:20:58 2024]  Iteration number: 0 with current cost as 0.40254991577958255 and parameters 
[-1.11690592  2.23743479 -2.12427948 -0.11653103  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 348.3885052204132 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 12:22:15 2024]  Iteration number: 0 with current cost as 0.46074512897749886 and parameters 
[-1.08295672  2.23743464 -2.12427964 -0.11653053  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 12:23:13 2024]  Iteration number: 0 with current cost as 0.37623898671970385 and parameters 
[-1.15776279  2.23743464 -2.12427951 -0.11653103  0.55388708 -2.77010897
  3.06858486  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029899 -1.87354655  0.72965055  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 12:24:17 2024]  Iteration number: 0 with current cost as 0.5001342552128882 and parameters 
[-0.07997905  2.23743521 -2.12427935 -0.11653103  0.55388708 -2.77010897
  3.06858527  2.18960174  1.18552027 -1.0664828   0.60271539  1.14432474
  1.31029927 -1.87354623  0.72965052  2.88578419 -0.54534306 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:26:03 2024]  Iteration number: 0 with current cost as 0.3985886622875897 and parameters 
[-1.12518698  2.23743464 -2.12427928 -0.11653067  0.55388708 -2.77010897
  3.06858516  2.18960181  1.18552016 -1.06648291  0.60271546  1.1443248
  1.31029934 -1.87354645  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:26:46 2024]  Iteration number: 0 with current cost as 0.40254991577958255 and parameters 
[-1.11690592  2.23743479 -2.12427948 -0.11653103  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 346.72208046913147 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 12:28:04 2024]  Iteration number: 0 with current cost as 0.46074512897749886 and parameters 
[-1.08295672  2.23743464 -2.12427964 -0.11653053  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 12:29:00 2024]  Iteration number: 0 with current cost as 0.37623898671970385 and parameters 
[-1.15776279  2.23743464 -2.12427951 -0.11653103  0.55388708 -2.77010897
  3.06858486  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029899 -1.87354655  0.72965055  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 12:30:04 2024]  Iteration number: 0 with current cost as 0.5001342552128882 and parameters 
[-0.07997905  2.23743521 -2.12427935 -0.11653103  0.55388708 -2.77010897
  3.06858527  2.18960174  1.18552027 -1.0664828   0.60271539  1.14432474
  1.31029927 -1.87354623  0.72965052  2.88578419 -0.54534306 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:31:50 2024]  Iteration number: 0 with current cost as 0.3985886622875897 and parameters 
[-1.12518698  2.23743464 -2.12427928 -0.11653067  0.55388708 -2.77010897
  3.06858516  2.18960181  1.18552016 -1.06648291  0.60271546  1.1443248
  1.31029934 -1.87354645  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:32:33 2024]  Iteration number: 0 with current cost as 0.40254991577958255 and parameters 
[-1.11690592  2.23743479 -2.12427948 -0.11653103  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 347.1681730747223 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 12:33:51 2024]  Iteration number: 0 with current cost as 0.46074512897749886 and parameters 
[-1.08295672  2.23743464 -2.12427964 -0.11653053  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 12:34:47 2024]  Iteration number: 0 with current cost as 0.37623898671970385 and parameters 
[-1.15776279  2.23743464 -2.12427951 -0.11653103  0.55388708 -2.77010897
  3.06858486  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029899 -1.87354655  0.72965055  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 12:35:51 2024]  Iteration number: 0 with current cost as 0.5001342552128882 and parameters 
[-0.07997905  2.23743521 -2.12427935 -0.11653103  0.55388708 -2.77010897
  3.06858527  2.18960174  1.18552027 -1.0664828   0.60271539  1.14432474
  1.31029927 -1.87354623  0.72965052  2.88578419 -0.54534306 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:37:37 2024]  Iteration number: 0 with current cost as 0.3985886622875897 and parameters 
[-1.12518698  2.23743464 -2.12427928 -0.11653067  0.55388708 -2.77010897
  3.06858516  2.18960181  1.18552016 -1.06648291  0.60271546  1.1443248
  1.31029934 -1.87354645  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:38:22 2024]  Iteration number: 0 with current cost as 0.40254991577958255 and parameters 
[-1.11690592  2.23743479 -2.12427948 -0.11653103  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 348.7460994720459 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 12:39:39 2024]  Iteration number: 0 with current cost as 0.46074512897749886 and parameters 
[-1.08295672  2.23743464 -2.12427964 -0.11653053  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 12:40:36 2024]  Iteration number: 0 with current cost as 0.37623898671970385 and parameters 
[-1.15776279  2.23743464 -2.12427951 -0.11653103  0.55388708 -2.77010897
  3.06858486  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029899 -1.87354655  0.72965055  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 12:41:40 2024]  Iteration number: 0 with current cost as 0.5001342552128882 and parameters 
[-0.07997905  2.23743521 -2.12427935 -0.11653103  0.55388708 -2.77010897
  3.06858527  2.18960174  1.18552027 -1.0664828   0.60271539  1.14432474
  1.31029927 -1.87354623  0.72965052  2.88578419 -0.54534306 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:43:27 2024]  Iteration number: 0 with current cost as 0.3985886622875897 and parameters 
[-1.12518698  2.23743464 -2.12427928 -0.11653067  0.55388708 -2.77010897
  3.06858516  2.18960181  1.18552016 -1.06648291  0.60271546  1.1443248
  1.31029934 -1.87354645  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:44:08 2024]  Iteration number: 0 with current cost as 0.40254991577958255 and parameters 
[-1.11690592  2.23743479 -2.12427948 -0.11653103  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 347.29806566238403 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 12:45:27 2024]  Iteration number: 0 with current cost as 0.46074512897749886 and parameters 
[-1.08295672  2.23743464 -2.12427964 -0.11653053  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 12:46:23 2024]  Iteration number: 0 with current cost as 0.37623898671970385 and parameters 
[-1.15776279  2.23743464 -2.12427951 -0.11653103  0.55388708 -2.77010897
  3.06858486  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029899 -1.87354655  0.72965055  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 12:47:27 2024]  Iteration number: 0 with current cost as 0.5001342552128882 and parameters 
[-0.07997905  2.23743521 -2.12427935 -0.11653103  0.55388708 -2.77010897
  3.06858527  2.18960174  1.18552027 -1.0664828   0.60271539  1.14432474
  1.31029927 -1.87354623  0.72965052  2.88578419 -0.54534306 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:49:12 2024]  Iteration number: 0 with current cost as 0.3985886622875897 and parameters 
[-1.12518698  2.23743464 -2.12427928 -0.11653067  0.55388708 -2.77010897
  3.06858516  2.18960181  1.18552016 -1.06648291  0.60271546  1.1443248
  1.31029934 -1.87354645  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:49:55 2024]  Iteration number: 0 with current cost as 0.40254991577958255 and parameters 
[-1.11690592  2.23743479 -2.12427948 -0.11653103  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 347.75419211387634 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 12:51:13 2024]  Iteration number: 0 with current cost as 0.46074512897749886 and parameters 
[-1.08295672  2.23743464 -2.12427964 -0.11653053  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 12:52:10 2024]  Iteration number: 0 with current cost as 0.37623898671970385 and parameters 
[-1.15776279  2.23743464 -2.12427951 -0.11653103  0.55388708 -2.77010897
  3.06858486  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029899 -1.87354655  0.72965055  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 12:53:15 2024]  Iteration number: 0 with current cost as 0.5001342552128882 and parameters 
[-0.07997905  2.23743521 -2.12427935 -0.11653103  0.55388708 -2.77010897
  3.06858527  2.18960174  1.18552027 -1.0664828   0.60271539  1.14432474
  1.31029927 -1.87354623  0.72965052  2.88578419 -0.54534306 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:55:01 2024]  Iteration number: 0 with current cost as 0.3985886622875897 and parameters 
[-1.12518698  2.23743464 -2.12427928 -0.11653067  0.55388708 -2.77010897
  3.06858516  2.18960181  1.18552016 -1.06648291  0.60271546  1.1443248
  1.31029934 -1.87354645  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:55:43 2024]  Iteration number: 0 with current cost as 0.40254991577958255 and parameters 
[-1.11690592  2.23743479 -2.12427948 -0.11653103  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 346.0314350128174 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 12:57:00 2024]  Iteration number: 0 with current cost as 0.46074512897749886 and parameters 
[-1.08295672  2.23743464 -2.12427964 -0.11653053  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 12:57:57 2024]  Iteration number: 0 with current cost as 0.37623898671970385 and parameters 
[-1.15776279  2.23743464 -2.12427951 -0.11653103  0.55388708 -2.77010897
  3.06858486  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029899 -1.87354655  0.72965055  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 12:59:01 2024]  Iteration number: 0 with current cost as 0.5001342552128882 and parameters 
[-0.07997905  2.23743521 -2.12427935 -0.11653103  0.55388708 -2.77010897
  3.06858527  2.18960174  1.18552027 -1.0664828   0.60271539  1.14432474
  1.31029927 -1.87354623  0.72965052  2.88578419 -0.54534306 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:00:48 2024]  Iteration number: 0 with current cost as 0.3985886622875897 and parameters 
[-1.12518698  2.23743464 -2.12427928 -0.11653067  0.55388708 -2.77010897
  3.06858516  2.18960181  1.18552016 -1.06648291  0.60271546  1.1443248
  1.31029934 -1.87354645  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:01:31 2024]  Iteration number: 0 with current cost as 0.40254991577958255 and parameters 
[-1.11690592  2.23743479 -2.12427948 -0.11653103  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 348.4492721557617 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 13:02:49 2024]  Iteration number: 0 with current cost as 0.46074512897749886 and parameters 
[-1.08295672  2.23743464 -2.12427964 -0.11653053  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 13:03:47 2024]  Iteration number: 0 with current cost as 0.37623898671970385 and parameters 
[-1.15776279  2.23743464 -2.12427951 -0.11653103  0.55388708 -2.77010897
  3.06858486  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029899 -1.87354655  0.72965055  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 13:04:51 2024]  Iteration number: 0 with current cost as 0.5001342552128882 and parameters 
[-0.07997905  2.23743521 -2.12427935 -0.11653103  0.55388708 -2.77010897
  3.06858527  2.18960174  1.18552027 -1.0664828   0.60271539  1.14432474
  1.31029927 -1.87354623  0.72965052  2.88578419 -0.54534306 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:06:37 2024]  Iteration number: 0 with current cost as 0.3985886622875897 and parameters 
[-1.12518698  2.23743464 -2.12427928 -0.11653067  0.55388708 -2.77010897
  3.06858516  2.18960181  1.18552016 -1.06648291  0.60271546  1.1443248
  1.31029934 -1.87354645  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:07:18 2024]  Iteration number: 0 with current cost as 0.40254991577958255 and parameters 
[-1.11690592  2.23743479 -2.12427948 -0.11653103  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 347.8291244506836 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 13:08:37 2024]  Iteration number: 0 with current cost as 0.46074512897749886 and parameters 
[-1.08295672  2.23743464 -2.12427964 -0.11653053  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 13:09:34 2024]  Iteration number: 0 with current cost as 0.37623898671970385 and parameters 
[-1.15776279  2.23743464 -2.12427951 -0.11653103  0.55388708 -2.77010897
  3.06858486  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029899 -1.87354655  0.72965055  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 13:10:41 2024]  Iteration number: 0 with current cost as 0.5001342552128882 and parameters 
[-0.07997905  2.23743521 -2.12427935 -0.11653103  0.55388708 -2.77010897
  3.06858527  2.18960174  1.18552027 -1.0664828   0.60271539  1.14432474
  1.31029927 -1.87354623  0.72965052  2.88578419 -0.54534306 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:12:28 2024]  Iteration number: 0 with current cost as 0.3985886622875897 and parameters 
[-1.12518698  2.23743464 -2.12427928 -0.11653067  0.55388708 -2.77010897
  3.06858516  2.18960181  1.18552016 -1.06648291  0.60271546  1.1443248
  1.31029934 -1.87354645  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:13:10 2024]  Iteration number: 0 with current cost as 0.40254991577958255 and parameters 
[-1.11690592  2.23743479 -2.12427948 -0.11653103  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 351.59180331230164 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 13:14:28 2024]  Iteration number: 0 with current cost as 0.46074512897749886 and parameters 
[-1.08295672  2.23743464 -2.12427964 -0.11653053  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 13:15:25 2024]  Iteration number: 0 with current cost as 0.37623898671970385 and parameters 
[-1.15776279  2.23743464 -2.12427951 -0.11653103  0.55388708 -2.77010897
  3.06858486  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029899 -1.87354655  0.72965055  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 13:16:30 2024]  Iteration number: 0 with current cost as 0.5001342552128882 and parameters 
[-0.07997905  2.23743521 -2.12427935 -0.11653103  0.55388708 -2.77010897
  3.06858527  2.18960174  1.18552027 -1.0664828   0.60271539  1.14432474
  1.31029927 -1.87354623  0.72965052  2.88578419 -0.54534306 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:18:15 2024]  Iteration number: 0 with current cost as 0.3985886622875897 and parameters 
[-1.12518698  2.23743464 -2.12427928 -0.11653067  0.55388708 -2.77010897
  3.06858516  2.18960181  1.18552016 -1.06648291  0.60271546  1.1443248
  1.31029934 -1.87354645  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:18:57 2024]  Iteration number: 0 with current cost as 0.40254991577958255 and parameters 
[-1.11690592  2.23743479 -2.12427948 -0.11653103  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 347.80900144577026 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 13:20:15 2024]  Iteration number: 0 with current cost as 0.46074512897749886 and parameters 
[-1.08295672  2.23743464 -2.12427964 -0.11653053  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 13:21:12 2024]  Iteration number: 0 with current cost as 0.37623898671970385 and parameters 
[-1.15776279  2.23743464 -2.12427951 -0.11653103  0.55388708 -2.77010897
  3.06858486  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029899 -1.87354655  0.72965055  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 13:22:15 2024]  Iteration number: 0 with current cost as 0.5001342552128882 and parameters 
[-0.07997905  2.23743521 -2.12427935 -0.11653103  0.55388708 -2.77010897
  3.06858527  2.18960174  1.18552027 -1.0664828   0.60271539  1.14432474
  1.31029927 -1.87354623  0.72965052  2.88578419 -0.54534306 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:24:02 2024]  Iteration number: 0 with current cost as 0.3985886622875897 and parameters 
[-1.12518698  2.23743464 -2.12427928 -0.11653067  0.55388708 -2.77010897
  3.06858516  2.18960181  1.18552016 -1.06648291  0.60271546  1.1443248
  1.31029934 -1.87354645  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:24:45 2024]  Iteration number: 0 with current cost as 0.40254991577958255 and parameters 
[-1.11690592  2.23743479 -2.12427948 -0.11653103  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 346.0936574935913 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 13:26:11 2024]  Iteration number: 0 with current cost as 0.46074512897749886 and parameters 
[-1.08295672  2.23743464 -2.12427964 -0.11653053  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 13:27:07 2024]  Iteration number: 0 with current cost as 0.37623898671970385 and parameters 
[-1.15776279  2.23743464 -2.12427951 -0.11653103  0.55388708 -2.77010897
  3.06858486  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029899 -1.87354655  0.72965055  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 13:28:11 2024]  Iteration number: 0 with current cost as 0.5001342552128882 and parameters 
[-0.07997905  2.23743521 -2.12427935 -0.11653103  0.55388708 -2.77010897
  3.06858527  2.18960174  1.18552027 -1.0664828   0.60271539  1.14432474
  1.31029927 -1.87354623  0.72965052  2.88578419 -0.54534306 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:29:58 2024]  Iteration number: 0 with current cost as 0.3985886622875897 and parameters 
[-1.12518698  2.23743464 -2.12427928 -0.11653067  0.55388708 -2.77010897
  3.06858516  2.18960181  1.18552016 -1.06648291  0.60271546  1.1443248
  1.31029934 -1.87354645  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:30:41 2024]  Iteration number: 0 with current cost as 0.40254991577958255 and parameters 
[-1.11690592  2.23743479 -2.12427948 -0.11653103  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 363.47507882118225 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 13:32:06 2024]  Iteration number: 0 with current cost as 0.46074512897749886 and parameters 
[-1.08295672  2.23743464 -2.12427964 -0.11653053  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 13:33:02 2024]  Iteration number: 0 with current cost as 0.37623898671970385 and parameters 
[-1.15776279  2.23743464 -2.12427951 -0.11653103  0.55388708 -2.77010897
  3.06858486  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029899 -1.87354655  0.72965055  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 13:34:07 2024]  Iteration number: 0 with current cost as 0.5001342552128882 and parameters 
[-0.07997905  2.23743521 -2.12427935 -0.11653103  0.55388708 -2.77010897
  3.06858527  2.18960174  1.18552027 -1.0664828   0.60271539  1.14432474
  1.31029927 -1.87354623  0.72965052  2.88578419 -0.54534306 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:35:53 2024]  Iteration number: 0 with current cost as 0.3985886622875897 and parameters 
[-1.12518698  2.23743464 -2.12427928 -0.11653067  0.55388708 -2.77010897
  3.06858516  2.18960181  1.18552016 -1.06648291  0.60271546  1.1443248
  1.31029934 -1.87354645  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:36:36 2024]  Iteration number: 0 with current cost as 0.40254991577958255 and parameters 
[-1.11690592  2.23743479 -2.12427948 -0.11653103  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 348.36864018440247 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 13:37:54 2024]  Iteration number: 0 with current cost as 0.46074512897749886 and parameters 
[-1.08295672  2.23743464 -2.12427964 -0.11653053  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 13:38:51 2024]  Iteration number: 0 with current cost as 0.37623898671970385 and parameters 
[-1.15776279  2.23743464 -2.12427951 -0.11653103  0.55388708 -2.77010897
  3.06858486  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029899 -1.87354655  0.72965055  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 13:39:55 2024]  Iteration number: 0 with current cost as 0.5001342552128882 and parameters 
[-0.07997905  2.23743521 -2.12427935 -0.11653103  0.55388708 -2.77010897
  3.06858527  2.18960174  1.18552027 -1.0664828   0.60271539  1.14432474
  1.31029927 -1.87354623  0.72965052  2.88578419 -0.54534306 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:41:42 2024]  Iteration number: 0 with current cost as 0.3985886622875897 and parameters 
[-1.12518698  2.23743464 -2.12427928 -0.11653067  0.55388708 -2.77010897
  3.06858516  2.18960181  1.18552016 -1.06648291  0.60271546  1.1443248
  1.31029934 -1.87354645  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:42:25 2024]  Iteration number: 0 with current cost as 0.40254991577958255 and parameters 
[-1.11690592  2.23743479 -2.12427948 -0.11653103  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 348.35920429229736 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 13:43:42 2024]  Iteration number: 0 with current cost as 0.46074512897749886 and parameters 
[-1.08295672  2.23743464 -2.12427964 -0.11653053  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 13:44:39 2024]  Iteration number: 0 with current cost as 0.37623898671970385 and parameters 
[-1.15776279  2.23743464 -2.12427951 -0.11653103  0.55388708 -2.77010897
  3.06858486  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029899 -1.87354655  0.72965055  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 13:45:42 2024]  Iteration number: 0 with current cost as 0.5001342552128882 and parameters 
[-0.07997905  2.23743521 -2.12427935 -0.11653103  0.55388708 -2.77010897
  3.06858527  2.18960174  1.18552027 -1.0664828   0.60271539  1.14432474
  1.31029927 -1.87354623  0.72965052  2.88578419 -0.54534306 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:47:29 2024]  Iteration number: 0 with current cost as 0.3985886622875897 and parameters 
[-1.12518698  2.23743464 -2.12427928 -0.11653067  0.55388708 -2.77010897
  3.06858516  2.18960181  1.18552016 -1.06648291  0.60271546  1.1443248
  1.31029934 -1.87354645  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:48:11 2024]  Iteration number: 0 with current cost as 0.40254991577958255 and parameters 
[-1.11690592  2.23743479 -2.12427948 -0.11653103  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 348.1947193145752 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 13:49:31 2024]  Iteration number: 0 with current cost as 0.46074512897749886 and parameters 
[-1.08295672  2.23743464 -2.12427964 -0.11653053  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 13:50:28 2024]  Iteration number: 0 with current cost as 0.37623898671970385 and parameters 
[-1.15776279  2.23743464 -2.12427951 -0.11653103  0.55388708 -2.77010897
  3.06858486  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029899 -1.87354655  0.72965055  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 13:51:33 2024]  Iteration number: 0 with current cost as 0.5001342552128882 and parameters 
[-0.07997905  2.23743521 -2.12427935 -0.11653103  0.55388708 -2.77010897
  3.06858527  2.18960174  1.18552027 -1.0664828   0.60271539  1.14432474
  1.31029927 -1.87354623  0.72965052  2.88578419 -0.54534306 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:53:18 2024]  Iteration number: 0 with current cost as 0.3985886622875897 and parameters 
[-1.12518698  2.23743464 -2.12427928 -0.11653067  0.55388708 -2.77010897
  3.06858516  2.18960181  1.18552016 -1.06648291  0.60271546  1.1443248
  1.31029934 -1.87354645  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:54:03 2024]  Iteration number: 0 with current cost as 0.40254991577958255 and parameters 
[-1.11690592  2.23743479 -2.12427948 -0.11653103  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 350.7615296840668 seconds. 
Discarding model... 

Training complete taking 8746.859285116196 total seconds. 
Now scoring model... 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Scoring complete taking 0.9537086486816406 seconds. 
Saved predicted values as M-A1-CNOT_Modified-Pauli-CRZ_predicted_values.csv
Model scores: {'MSE_train': (256.35371916474514,), 'R2_train': -0.24194773296428518, 'MAE_train': 14.396656389443962, 'MSE_test': 224.0725410873362, 'R2_test': -0.3506675800243102, 'MAE_test': 12.795027007942936}. 
Saved model results as M-A1-CNOT_Modified-Pauli-CRZ_results.json. 
