/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Sun Mar 24 17:36:44 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 17:37:51 2024]  Iteration number: 0 with current cost as 0.4020692184897885 and parameters 
[-4.2542592   2.23743464 -2.12427948 -0.11653118  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.06648324  0.60271526  1.14432429
  1.31029883 -1.87354696]. 
Working on 0.4 fold... 
[Sun Mar 24 17:40:01 2024]  Iteration number: 0 with current cost as 0.3243252347146518 and parameters 
[-4.44706128  2.23743464 -2.12427949 -0.11653117  0.55388708 -2.77010912
  3.06858484  2.18960145  1.18552013 -1.06648323  0.6027151   1.14432431
  1.31029884 -1.87354694]. 
Working on 0.6 fold... 
[Sun Mar 24 17:42:12 2024]  Iteration number: 0 with current cost as 0.36777230979622877 and parameters 
[-4.3440556   2.23743487 -2.12427916 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552022 -1.06648308  0.60271534  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 17:44:24 2024]  Iteration number: 0 with current cost as 0.363780831535238 and parameters 
[-4.35460091  2.23743448 -2.12427964 -0.11653118  0.55388692 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.6027151   1.14432429
  1.31029867 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 17:46:35 2024]  Iteration number: 0 with current cost as 0.3597146207892242 and parameters 
[-7.59805192  2.23743527 -2.12427837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 640.5127563476562 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 17:48:28 2024]  Iteration number: 0 with current cost as 0.4020692184897885 and parameters 
[-4.2542592   2.23743464 -2.12427948 -0.11653118  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.06648324  0.60271526  1.14432429
  1.31029883 -1.87354696]. 
Working on 0.4 fold... 
[Sun Mar 24 17:50:36 2024]  Iteration number: 0 with current cost as 0.3243252347146518 and parameters 
[-4.44706128  2.23743464 -2.12427949 -0.11653117  0.55388708 -2.77010912
  3.06858484  2.18960145  1.18552013 -1.06648323  0.6027151   1.14432431
  1.31029884 -1.87354694]. 
Working on 0.6 fold... 
[Sun Mar 24 17:52:46 2024]  Iteration number: 0 with current cost as 0.36777230979622877 and parameters 
[-4.3440556   2.23743487 -2.12427916 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552022 -1.06648308  0.60271534  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 17:54:55 2024]  Iteration number: 0 with current cost as 0.363780831535238 and parameters 
[-4.35460091  2.23743448 -2.12427964 -0.11653118  0.55388692 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.6027151   1.14432429
  1.31029867 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 17:57:01 2024]  Iteration number: 0 with current cost as 0.3597146207892242 and parameters 
[-7.59805192  2.23743527 -2.12427837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 624.1151120662689 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 17:58:54 2024]  Iteration number: 0 with current cost as 0.4020692184897885 and parameters 
[-4.2542592   2.23743464 -2.12427948 -0.11653118  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.06648324  0.60271526  1.14432429
  1.31029883 -1.87354696]. 
Working on 0.4 fold... 
[Sun Mar 24 18:01:04 2024]  Iteration number: 0 with current cost as 0.3243252347146518 and parameters 
[-4.44706128  2.23743464 -2.12427949 -0.11653117  0.55388708 -2.77010912
  3.06858484  2.18960145  1.18552013 -1.06648323  0.6027151   1.14432431
  1.31029884 -1.87354694]. 
Working on 0.6 fold... 
[Sun Mar 24 18:03:10 2024]  Iteration number: 0 with current cost as 0.36777230979622877 and parameters 
[-4.3440556   2.23743487 -2.12427916 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552022 -1.06648308  0.60271534  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:05:17 2024]  Iteration number: 0 with current cost as 0.363780831535238 and parameters 
[-4.35460091  2.23743448 -2.12427964 -0.11653118  0.55388692 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.6027151   1.14432429
  1.31029867 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:07:30 2024]  Iteration number: 0 with current cost as 0.3597146207892242 and parameters 
[-7.59805192  2.23743527 -2.12427837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 630.6253051757812 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 18:09:24 2024]  Iteration number: 0 with current cost as 0.4020692184897885 and parameters 
[-4.2542592   2.23743464 -2.12427948 -0.11653118  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.06648324  0.60271526  1.14432429
  1.31029883 -1.87354696]. 
Working on 0.4 fold... 
[Sun Mar 24 18:11:36 2024]  Iteration number: 0 with current cost as 0.3243252347146518 and parameters 
[-4.44706128  2.23743464 -2.12427949 -0.11653117  0.55388708 -2.77010912
  3.06858484  2.18960145  1.18552013 -1.06648323  0.6027151   1.14432431
  1.31029884 -1.87354694]. 
Working on 0.6 fold... 
[Sun Mar 24 18:13:48 2024]  Iteration number: 0 with current cost as 0.36777230979622877 and parameters 
[-4.3440556   2.23743487 -2.12427916 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552022 -1.06648308  0.60271534  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:15:59 2024]  Iteration number: 0 with current cost as 0.363780831535238 and parameters 
[-4.35460091  2.23743448 -2.12427964 -0.11653118  0.55388692 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.6027151   1.14432429
  1.31029867 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:18:11 2024]  Iteration number: 0 with current cost as 0.3597146207892242 and parameters 
[-7.59805192  2.23743527 -2.12427837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 637.9935648441315 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 18:20:02 2024]  Iteration number: 0 with current cost as 0.4020692184897885 and parameters 
[-4.2542592   2.23743464 -2.12427948 -0.11653118  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.06648324  0.60271526  1.14432429
  1.31029883 -1.87354696]. 
Working on 0.4 fold... 
[Sun Mar 24 18:22:12 2024]  Iteration number: 0 with current cost as 0.3243252347146518 and parameters 
[-4.44706128  2.23743464 -2.12427949 -0.11653117  0.55388708 -2.77010912
  3.06858484  2.18960145  1.18552013 -1.06648323  0.6027151   1.14432431
  1.31029884 -1.87354694]. 
Working on 0.6 fold... 
[Sun Mar 24 18:24:20 2024]  Iteration number: 0 with current cost as 0.36777230979622877 and parameters 
[-4.3440556   2.23743487 -2.12427916 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552022 -1.06648308  0.60271534  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:26:28 2024]  Iteration number: 0 with current cost as 0.363780831535238 and parameters 
[-4.35460091  2.23743448 -2.12427964 -0.11653118  0.55388692 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.6027151   1.14432429
  1.31029867 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:28:41 2024]  Iteration number: 0 with current cost as 0.3597146207892242 and parameters 
[-7.59805192  2.23743527 -2.12427837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 633.6009624004364 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 18:30:38 2024]  Iteration number: 0 with current cost as 0.4020692184897885 and parameters 
[-4.2542592   2.23743464 -2.12427948 -0.11653118  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.06648324  0.60271526  1.14432429
  1.31029883 -1.87354696]. 
Working on 0.4 fold... 
[Sun Mar 24 18:32:48 2024]  Iteration number: 0 with current cost as 0.3243252347146518 and parameters 
[-4.44706128  2.23743464 -2.12427949 -0.11653117  0.55388708 -2.77010912
  3.06858484  2.18960145  1.18552013 -1.06648323  0.6027151   1.14432431
  1.31029884 -1.87354694]. 
Working on 0.6 fold... 
[Sun Mar 24 18:34:59 2024]  Iteration number: 0 with current cost as 0.36777230979622877 and parameters 
[-4.3440556   2.23743487 -2.12427916 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552022 -1.06648308  0.60271534  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:37:08 2024]  Iteration number: 0 with current cost as 0.363780831535238 and parameters 
[-4.35460091  2.23743448 -2.12427964 -0.11653118  0.55388692 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.6027151   1.14432429
  1.31029867 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:39:15 2024]  Iteration number: 0 with current cost as 0.3597146207892242 and parameters 
[-7.59805192  2.23743527 -2.12427837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 631.1669149398804 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 18:41:07 2024]  Iteration number: 0 with current cost as 0.4020692184897885 and parameters 
[-4.2542592   2.23743464 -2.12427948 -0.11653118  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.06648324  0.60271526  1.14432429
  1.31029883 -1.87354696]. 
Working on 0.4 fold... 
[Sun Mar 24 18:43:18 2024]  Iteration number: 0 with current cost as 0.3243252347146518 and parameters 
[-4.44706128  2.23743464 -2.12427949 -0.11653117  0.55388708 -2.77010912
  3.06858484  2.18960145  1.18552013 -1.06648323  0.6027151   1.14432431
  1.31029884 -1.87354694]. 
Working on 0.6 fold... 
[Sun Mar 24 18:45:27 2024]  Iteration number: 0 with current cost as 0.36777230979622877 and parameters 
[-4.3440556   2.23743487 -2.12427916 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552022 -1.06648308  0.60271534  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:47:36 2024]  Iteration number: 0 with current cost as 0.363780831535238 and parameters 
[-4.35460091  2.23743448 -2.12427964 -0.11653118  0.55388692 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.6027151   1.14432429
  1.31029867 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:49:45 2024]  Iteration number: 0 with current cost as 0.3597146207892242 and parameters 
[-7.59805192  2.23743527 -2.12427837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 629.2418103218079 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 18:51:40 2024]  Iteration number: 0 with current cost as 0.4020692184897885 and parameters 
[-4.2542592   2.23743464 -2.12427948 -0.11653118  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.06648324  0.60271526  1.14432429
  1.31029883 -1.87354696]. 
Working on 0.4 fold... 
[Sun Mar 24 18:53:50 2024]  Iteration number: 0 with current cost as 0.3243252347146518 and parameters 
[-4.44706128  2.23743464 -2.12427949 -0.11653117  0.55388708 -2.77010912
  3.06858484  2.18960145  1.18552013 -1.06648323  0.6027151   1.14432431
  1.31029884 -1.87354694]. 
Working on 0.6 fold... 
[Sun Mar 24 18:56:00 2024]  Iteration number: 0 with current cost as 0.36777230979622877 and parameters 
[-4.3440556   2.23743487 -2.12427916 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552022 -1.06648308  0.60271534  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:58:09 2024]  Iteration number: 0 with current cost as 0.363780831535238 and parameters 
[-4.35460091  2.23743448 -2.12427964 -0.11653118  0.55388692 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.6027151   1.14432429
  1.31029867 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:00:20 2024]  Iteration number: 0 with current cost as 0.3597146207892242 and parameters 
[-7.59805192  2.23743527 -2.12427837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 637.0800359249115 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 19:02:15 2024]  Iteration number: 0 with current cost as 0.4020692184897885 and parameters 
[-4.2542592   2.23743464 -2.12427948 -0.11653118  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.06648324  0.60271526  1.14432429
  1.31029883 -1.87354696]. 
Working on 0.4 fold... 
[Sun Mar 24 19:04:22 2024]  Iteration number: 0 with current cost as 0.3243252347146518 and parameters 
[-4.44706128  2.23743464 -2.12427949 -0.11653117  0.55388708 -2.77010912
  3.06858484  2.18960145  1.18552013 -1.06648323  0.6027151   1.14432431
  1.31029884 -1.87354694]. 
Working on 0.6 fold... 
[Sun Mar 24 19:06:32 2024]  Iteration number: 0 with current cost as 0.36777230979622877 and parameters 
[-4.3440556   2.23743487 -2.12427916 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552022 -1.06648308  0.60271534  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:08:41 2024]  Iteration number: 0 with current cost as 0.363780831535238 and parameters 
[-4.35460091  2.23743448 -2.12427964 -0.11653118  0.55388692 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.6027151   1.14432429
  1.31029867 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:10:55 2024]  Iteration number: 0 with current cost as 0.3597146207892242 and parameters 
[-7.59805192  2.23743527 -2.12427837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 633.8008329868317 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 19:12:50 2024]  Iteration number: 0 with current cost as 0.4020692184897885 and parameters 
[-4.2542592   2.23743464 -2.12427948 -0.11653118  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.06648324  0.60271526  1.14432429
  1.31029883 -1.87354696]. 
Working on 0.4 fold... 
[Sun Mar 24 19:15:04 2024]  Iteration number: 0 with current cost as 0.3243252347146518 and parameters 
[-4.44706128  2.23743464 -2.12427949 -0.11653117  0.55388708 -2.77010912
  3.06858484  2.18960145  1.18552013 -1.06648323  0.6027151   1.14432431
  1.31029884 -1.87354694]. 
Working on 0.6 fold... 
[Sun Mar 24 19:17:20 2024]  Iteration number: 0 with current cost as 0.36777230979622877 and parameters 
[-4.3440556   2.23743487 -2.12427916 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552022 -1.06648308  0.60271534  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:19:32 2024]  Iteration number: 0 with current cost as 0.363780831535238 and parameters 
[-4.35460091  2.23743448 -2.12427964 -0.11653118  0.55388692 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.6027151   1.14432429
  1.31029867 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:21:44 2024]  Iteration number: 0 with current cost as 0.3597146207892242 and parameters 
[-7.59805192  2.23743527 -2.12427837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 650.7525835037231 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 19:23:39 2024]  Iteration number: 0 with current cost as 0.4020692184897885 and parameters 
[-4.2542592   2.23743464 -2.12427948 -0.11653118  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.06648324  0.60271526  1.14432429
  1.31029883 -1.87354696]. 
Working on 0.4 fold... 
[Sun Mar 24 19:25:49 2024]  Iteration number: 0 with current cost as 0.3243252347146518 and parameters 
[-4.44706128  2.23743464 -2.12427949 -0.11653117  0.55388708 -2.77010912
  3.06858484  2.18960145  1.18552013 -1.06648323  0.6027151   1.14432431
  1.31029884 -1.87354694]. 
Working on 0.6 fold... 
[Sun Mar 24 19:27:58 2024]  Iteration number: 0 with current cost as 0.36777230979622877 and parameters 
[-4.3440556   2.23743487 -2.12427916 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552022 -1.06648308  0.60271534  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:30:08 2024]  Iteration number: 0 with current cost as 0.363780831535238 and parameters 
[-4.35460091  2.23743448 -2.12427964 -0.11653118  0.55388692 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.6027151   1.14432429
  1.31029867 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:32:18 2024]  Iteration number: 0 with current cost as 0.3597146207892242 and parameters 
[-7.59805192  2.23743527 -2.12427837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 633.9411008358002 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 19:34:11 2024]  Iteration number: 0 with current cost as 0.4020692184897885 and parameters 
[-4.2542592   2.23743464 -2.12427948 -0.11653118  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.06648324  0.60271526  1.14432429
  1.31029883 -1.87354696]. 
Working on 0.4 fold... 
[Sun Mar 24 19:36:28 2024]  Iteration number: 0 with current cost as 0.3243252347146518 and parameters 
[-4.44706128  2.23743464 -2.12427949 -0.11653117  0.55388708 -2.77010912
  3.06858484  2.18960145  1.18552013 -1.06648323  0.6027151   1.14432431
  1.31029884 -1.87354694]. 
Working on 0.6 fold... 
[Sun Mar 24 19:38:44 2024]  Iteration number: 0 with current cost as 0.36777230979622877 and parameters 
[-4.3440556   2.23743487 -2.12427916 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552022 -1.06648308  0.60271534  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:40:54 2024]  Iteration number: 0 with current cost as 0.363780831535238 and parameters 
[-4.35460091  2.23743448 -2.12427964 -0.11653118  0.55388692 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.6027151   1.14432429
  1.31029867 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:43:04 2024]  Iteration number: 0 with current cost as 0.3597146207892242 and parameters 
[-7.59805192  2.23743527 -2.12427837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 648.7272839546204 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 19:45:02 2024]  Iteration number: 0 with current cost as 0.4020692184897885 and parameters 
[-4.2542592   2.23743464 -2.12427948 -0.11653118  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.06648324  0.60271526  1.14432429
  1.31029883 -1.87354696]. 
Working on 0.4 fold... 
[Sun Mar 24 19:47:16 2024]  Iteration number: 0 with current cost as 0.3243252347146518 and parameters 
[-4.44706128  2.23743464 -2.12427949 -0.11653117  0.55388708 -2.77010912
  3.06858484  2.18960145  1.18552013 -1.06648323  0.6027151   1.14432431
  1.31029884 -1.87354694]. 
Working on 0.6 fold... 
[Sun Mar 24 19:49:23 2024]  Iteration number: 0 with current cost as 0.36777230979622877 and parameters 
[-4.3440556   2.23743487 -2.12427916 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552022 -1.06648308  0.60271534  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:51:34 2024]  Iteration number: 0 with current cost as 0.363780831535238 and parameters 
[-4.35460091  2.23743448 -2.12427964 -0.11653118  0.55388692 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.6027151   1.14432429
  1.31029867 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:53:46 2024]  Iteration number: 0 with current cost as 0.3597146207892242 and parameters 
[-7.59805192  2.23743527 -2.12427837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 639.7811000347137 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 19:55:42 2024]  Iteration number: 0 with current cost as 0.4020692184897885 and parameters 
[-4.2542592   2.23743464 -2.12427948 -0.11653118  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.06648324  0.60271526  1.14432429
  1.31029883 -1.87354696]. 
Working on 0.4 fold... 
[Sun Mar 24 19:57:53 2024]  Iteration number: 0 with current cost as 0.3243252347146518 and parameters 
[-4.44706128  2.23743464 -2.12427949 -0.11653117  0.55388708 -2.77010912
  3.06858484  2.18960145  1.18552013 -1.06648323  0.6027151   1.14432431
  1.31029884 -1.87354694]. 
Working on 0.6 fold... 
[Sun Mar 24 20:00:06 2024]  Iteration number: 0 with current cost as 0.36777230979622877 and parameters 
[-4.3440556   2.23743487 -2.12427916 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552022 -1.06648308  0.60271534  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:02:20 2024]  Iteration number: 0 with current cost as 0.363780831535238 and parameters 
[-4.35460091  2.23743448 -2.12427964 -0.11653118  0.55388692 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.6027151   1.14432429
  1.31029867 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 20:04:37 2024]  Iteration number: 0 with current cost as 0.3597146207892242 and parameters 
[-7.59805192  2.23743527 -2.12427837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 652.6335859298706 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 20:06:33 2024]  Iteration number: 0 with current cost as 0.4020692184897885 and parameters 
[-4.2542592   2.23743464 -2.12427948 -0.11653118  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.06648324  0.60271526  1.14432429
  1.31029883 -1.87354696]. 
Working on 0.4 fold... 
[Sun Mar 24 20:08:42 2024]  Iteration number: 0 with current cost as 0.3243252347146518 and parameters 
[-4.44706128  2.23743464 -2.12427949 -0.11653117  0.55388708 -2.77010912
  3.06858484  2.18960145  1.18552013 -1.06648323  0.6027151   1.14432431
  1.31029884 -1.87354694]. 
Working on 0.6 fold... 
[Sun Mar 24 20:10:52 2024]  Iteration number: 0 with current cost as 0.36777230979622877 and parameters 
[-4.3440556   2.23743487 -2.12427916 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552022 -1.06648308  0.60271534  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:13:03 2024]  Iteration number: 0 with current cost as 0.363780831535238 and parameters 
[-4.35460091  2.23743448 -2.12427964 -0.11653118  0.55388692 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.6027151   1.14432429
  1.31029867 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 20:15:11 2024]  Iteration number: 0 with current cost as 0.3597146207892242 and parameters 
[-7.59805192  2.23743527 -2.12427837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 630.2014527320862 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 20:17:03 2024]  Iteration number: 0 with current cost as 0.4020692184897885 and parameters 
[-4.2542592   2.23743464 -2.12427948 -0.11653118  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.06648324  0.60271526  1.14432429
  1.31029883 -1.87354696]. 
Working on 0.4 fold... 
[Sun Mar 24 20:19:16 2024]  Iteration number: 0 with current cost as 0.3243252347146518 and parameters 
[-4.44706128  2.23743464 -2.12427949 -0.11653117  0.55388708 -2.77010912
  3.06858484  2.18960145  1.18552013 -1.06648323  0.6027151   1.14432431
  1.31029884 -1.87354694]. 
Working on 0.6 fold... 
[Sun Mar 24 20:21:25 2024]  Iteration number: 0 with current cost as 0.36777230979622877 and parameters 
[-4.3440556   2.23743487 -2.12427916 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552022 -1.06648308  0.60271534  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:23:37 2024]  Iteration number: 0 with current cost as 0.363780831535238 and parameters 
[-4.35460091  2.23743448 -2.12427964 -0.11653118  0.55388692 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.6027151   1.14432429
  1.31029867 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 20:25:45 2024]  Iteration number: 0 with current cost as 0.3597146207892242 and parameters 
[-7.59805192  2.23743527 -2.12427837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 634.1050884723663 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 20:27:38 2024]  Iteration number: 0 with current cost as 0.4020692184897885 and parameters 
[-4.2542592   2.23743464 -2.12427948 -0.11653118  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.06648324  0.60271526  1.14432429
  1.31029883 -1.87354696]. 
Working on 0.4 fold... 
[Sun Mar 24 20:29:53 2024]  Iteration number: 0 with current cost as 0.3243252347146518 and parameters 
[-4.44706128  2.23743464 -2.12427949 -0.11653117  0.55388708 -2.77010912
  3.06858484  2.18960145  1.18552013 -1.06648323  0.6027151   1.14432431
  1.31029884 -1.87354694]. 
Working on 0.6 fold... 
[Sun Mar 24 20:32:04 2024]  Iteration number: 0 with current cost as 0.36777230979622877 and parameters 
[-4.3440556   2.23743487 -2.12427916 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552022 -1.06648308  0.60271534  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:34:16 2024]  Iteration number: 0 with current cost as 0.363780831535238 and parameters 
[-4.35460091  2.23743448 -2.12427964 -0.11653118  0.55388692 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.6027151   1.14432429
  1.31029867 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 20:36:26 2024]  Iteration number: 0 with current cost as 0.3597146207892242 and parameters 
[-7.59805192  2.23743527 -2.12427837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 643.0113098621368 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 20:38:24 2024]  Iteration number: 0 with current cost as 0.4020692184897885 and parameters 
[-4.2542592   2.23743464 -2.12427948 -0.11653118  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.06648324  0.60271526  1.14432429
  1.31029883 -1.87354696]. 
Working on 0.4 fold... 
[Sun Mar 24 20:40:31 2024]  Iteration number: 0 with current cost as 0.3243252347146518 and parameters 
[-4.44706128  2.23743464 -2.12427949 -0.11653117  0.55388708 -2.77010912
  3.06858484  2.18960145  1.18552013 -1.06648323  0.6027151   1.14432431
  1.31029884 -1.87354694]. 
Working on 0.6 fold... 
[Sun Mar 24 20:42:41 2024]  Iteration number: 0 with current cost as 0.36777230979622877 and parameters 
[-4.3440556   2.23743487 -2.12427916 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552022 -1.06648308  0.60271534  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:44:56 2024]  Iteration number: 0 with current cost as 0.363780831535238 and parameters 
[-4.35460091  2.23743448 -2.12427964 -0.11653118  0.55388692 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.6027151   1.14432429
  1.31029867 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 20:47:06 2024]  Iteration number: 0 with current cost as 0.3597146207892242 and parameters 
[-7.59805192  2.23743527 -2.12427837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 640.170667886734 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 20:49:00 2024]  Iteration number: 0 with current cost as 0.4020692184897885 and parameters 
[-4.2542592   2.23743464 -2.12427948 -0.11653118  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.06648324  0.60271526  1.14432429
  1.31029883 -1.87354696]. 
Working on 0.4 fold... 
[Sun Mar 24 20:51:06 2024]  Iteration number: 0 with current cost as 0.3243252347146518 and parameters 
[-4.44706128  2.23743464 -2.12427949 -0.11653117  0.55388708 -2.77010912
  3.06858484  2.18960145  1.18552013 -1.06648323  0.6027151   1.14432431
  1.31029884 -1.87354694]. 
Working on 0.6 fold... 
[Sun Mar 24 20:53:15 2024]  Iteration number: 0 with current cost as 0.36777230979622877 and parameters 
[-4.3440556   2.23743487 -2.12427916 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552022 -1.06648308  0.60271534  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:55:26 2024]  Iteration number: 0 with current cost as 0.363780831535238 and parameters 
[-4.35460091  2.23743448 -2.12427964 -0.11653118  0.55388692 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.6027151   1.14432429
  1.31029867 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 20:57:45 2024]  Iteration number: 0 with current cost as 0.3597146207892242 and parameters 
[-7.59805192  2.23743527 -2.12427837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 640.0490095615387 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 20:59:43 2024]  Iteration number: 0 with current cost as 0.4020692184897885 and parameters 
[-4.2542592   2.23743464 -2.12427948 -0.11653118  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.06648324  0.60271526  1.14432429
  1.31029883 -1.87354696]. 
Working on 0.4 fold... 
[Sun Mar 24 21:01:52 2024]  Iteration number: 0 with current cost as 0.3243252347146518 and parameters 
[-4.44706128  2.23743464 -2.12427949 -0.11653117  0.55388708 -2.77010912
  3.06858484  2.18960145  1.18552013 -1.06648323  0.6027151   1.14432431
  1.31029884 -1.87354694]. 
Working on 0.6 fold... 
[Sun Mar 24 21:04:08 2024]  Iteration number: 0 with current cost as 0.36777230979622877 and parameters 
[-4.3440556   2.23743487 -2.12427916 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552022 -1.06648308  0.60271534  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 21:06:17 2024]  Iteration number: 0 with current cost as 0.363780831535238 and parameters 
[-4.35460091  2.23743448 -2.12427964 -0.11653118  0.55388692 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.6027151   1.14432429
  1.31029867 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 21:08:30 2024]  Iteration number: 0 with current cost as 0.3597146207892242 and parameters 
[-7.59805192  2.23743527 -2.12427837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 642.2176547050476 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 21:10:25 2024]  Iteration number: 0 with current cost as 0.4020692184897885 and parameters 
[-4.2542592   2.23743464 -2.12427948 -0.11653118  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.06648324  0.60271526  1.14432429
  1.31029883 -1.87354696]. 
Working on 0.4 fold... 
[Sun Mar 24 21:12:36 2024]  Iteration number: 0 with current cost as 0.3243252347146518 and parameters 
[-4.44706128  2.23743464 -2.12427949 -0.11653117  0.55388708 -2.77010912
  3.06858484  2.18960145  1.18552013 -1.06648323  0.6027151   1.14432431
  1.31029884 -1.87354694]. 
Working on 0.6 fold... 
[Sun Mar 24 21:14:45 2024]  Iteration number: 0 with current cost as 0.36777230979622877 and parameters 
[-4.3440556   2.23743487 -2.12427916 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552022 -1.06648308  0.60271534  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 21:17:00 2024]  Iteration number: 0 with current cost as 0.363780831535238 and parameters 
[-4.35460091  2.23743448 -2.12427964 -0.11653118  0.55388692 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.6027151   1.14432429
  1.31029867 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 21:19:12 2024]  Iteration number: 0 with current cost as 0.3597146207892242 and parameters 
[-7.59805192  2.23743527 -2.12427837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 645.2181301116943 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 21:21:06 2024]  Iteration number: 0 with current cost as 0.4020692184897885 and parameters 
[-4.2542592   2.23743464 -2.12427948 -0.11653118  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.06648324  0.60271526  1.14432429
  1.31029883 -1.87354696]. 
Working on 0.4 fold... 
[Sun Mar 24 21:23:14 2024]  Iteration number: 0 with current cost as 0.3243252347146518 and parameters 
[-4.44706128  2.23743464 -2.12427949 -0.11653117  0.55388708 -2.77010912
  3.06858484  2.18960145  1.18552013 -1.06648323  0.6027151   1.14432431
  1.31029884 -1.87354694]. 
Working on 0.6 fold... 
[Sun Mar 24 21:25:22 2024]  Iteration number: 0 with current cost as 0.36777230979622877 and parameters 
[-4.3440556   2.23743487 -2.12427916 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552022 -1.06648308  0.60271534  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 21:27:31 2024]  Iteration number: 0 with current cost as 0.363780831535238 and parameters 
[-4.35460091  2.23743448 -2.12427964 -0.11653118  0.55388692 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.6027151   1.14432429
  1.31029867 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 21:29:44 2024]  Iteration number: 0 with current cost as 0.3597146207892242 and parameters 
[-7.59805192  2.23743527 -2.12427837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 630.9152045249939 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 21:31:38 2024]  Iteration number: 0 with current cost as 0.4020692184897885 and parameters 
[-4.2542592   2.23743464 -2.12427948 -0.11653118  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.06648324  0.60271526  1.14432429
  1.31029883 -1.87354696]. 
Working on 0.4 fold... 
[Sun Mar 24 21:33:46 2024]  Iteration number: 0 with current cost as 0.3243252347146518 and parameters 
[-4.44706128  2.23743464 -2.12427949 -0.11653117  0.55388708 -2.77010912
  3.06858484  2.18960145  1.18552013 -1.06648323  0.6027151   1.14432431
  1.31029884 -1.87354694]. 
Working on 0.6 fold... 
[Sun Mar 24 21:35:55 2024]  Iteration number: 0 with current cost as 0.36777230979622877 and parameters 
[-4.3440556   2.23743487 -2.12427916 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552022 -1.06648308  0.60271534  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 21:38:08 2024]  Iteration number: 0 with current cost as 0.363780831535238 and parameters 
[-4.35460091  2.23743448 -2.12427964 -0.11653118  0.55388692 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.6027151   1.14432429
  1.31029867 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 21:40:20 2024]  Iteration number: 0 with current cost as 0.3597146207892242 and parameters 
[-7.59805192  2.23743527 -2.12427837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 635.6539356708527 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 21:42:11 2024]  Iteration number: 0 with current cost as 0.4020692184897885 and parameters 
[-4.2542592   2.23743464 -2.12427948 -0.11653118  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.06648324  0.60271526  1.14432429
  1.31029883 -1.87354696]. 
Working on 0.4 fold... 
[Sun Mar 24 21:44:22 2024]  Iteration number: 0 with current cost as 0.3243252347146518 and parameters 
[-4.44706128  2.23743464 -2.12427949 -0.11653117  0.55388708 -2.77010912
  3.06858484  2.18960145  1.18552013 -1.06648323  0.6027151   1.14432431
  1.31029884 -1.87354694]. 
Working on 0.6 fold... 
[Sun Mar 24 21:46:27 2024]  Iteration number: 0 with current cost as 0.36777230979622877 and parameters 
[-4.3440556   2.23743487 -2.12427916 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552022 -1.06648308  0.60271534  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 21:48:37 2024]  Iteration number: 0 with current cost as 0.363780831535238 and parameters 
[-4.35460091  2.23743448 -2.12427964 -0.11653118  0.55388692 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.6027151   1.14432429
  1.31029867 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 21:50:46 2024]  Iteration number: 0 with current cost as 0.3597146207892242 and parameters 
[-7.59805192  2.23743527 -2.12427837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 622.6958150863647 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 21:52:35 2024]  Iteration number: 0 with current cost as 0.4020692184897885 and parameters 
[-4.2542592   2.23743464 -2.12427948 -0.11653118  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.06648324  0.60271526  1.14432429
  1.31029883 -1.87354696]. 
Working on 0.4 fold... 
[Sun Mar 24 21:54:39 2024]  Iteration number: 0 with current cost as 0.3243252347146518 and parameters 
[-4.44706128  2.23743464 -2.12427949 -0.11653117  0.55388708 -2.77010912
  3.06858484  2.18960145  1.18552013 -1.06648323  0.6027151   1.14432431
  1.31029884 -1.87354694]. 
Working on 0.6 fold... 
[Sun Mar 24 21:56:44 2024]  Iteration number: 0 with current cost as 0.36777230979622877 and parameters 
[-4.3440556   2.23743487 -2.12427916 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552022 -1.06648308  0.60271534  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 21:58:49 2024]  Iteration number: 0 with current cost as 0.363780831535238 and parameters 
[-4.35460091  2.23743448 -2.12427964 -0.11653118  0.55388692 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.6027151   1.14432429
  1.31029867 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 22:00:52 2024]  Iteration number: 0 with current cost as 0.3597146207892242 and parameters 
[-7.59805192  2.23743527 -2.12427837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 606.3818590641022 seconds. 
Discarding model... 

Training complete taking 15894.593666315079 total seconds. 
Now scoring model... 
Scoring complete taking 1.7977135181427002 seconds. 
Saved predicted values as A2-A2-CZ_Efficient-CRZ_predicted_values.csv
Model scores: {'MSE_train': (227.6305360764044,), 'R2_train': -0.1027935508587472, 'MAE_train': 13.231385708407208, 'MSE_test': 214.8609325783672, 'R2_test': -0.2951417181201017, 'MAE_test': 12.615998634448655}. 
Saved model results as A2-A2-CZ_Efficient-CRZ_results.json. 
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Thu Apr  4 12:21:49 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 12:23:09 2024]  Iteration number: 0 with current cost as 0.4020692184897885 and parameters 
[-4.2542592   2.23743464 -2.12427948 -0.11653118  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.06648324  0.60271526  1.14432429
  1.31029883 -1.87354696]. 
Working on 0.4 fold... 
[Thu Apr  4 12:25:38 2024]  Iteration number: 0 with current cost as 0.3243252347146518 and parameters 
[-4.44706128  2.23743464 -2.12427949 -0.11653117  0.55388708 -2.77010912
  3.06858484  2.18960145  1.18552013 -1.06648323  0.6027151   1.14432431
  1.31029884 -1.87354694]. 
Working on 0.6 fold... 
[Thu Apr  4 12:28:08 2024]  Iteration number: 0 with current cost as 0.36777230979622877 and parameters 
[-4.3440556   2.23743487 -2.12427916 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552022 -1.06648308  0.60271534  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:30:40 2024]  Iteration number: 0 with current cost as 0.363780831535238 and parameters 
[-4.35460091  2.23743448 -2.12427964 -0.11653118  0.55388692 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.6027151   1.14432429
  1.31029867 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:33:11 2024]  Iteration number: 0 with current cost as 0.3597146207892242 and parameters 
[-7.59805192  2.23743527 -2.12427837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 735.5617532730103 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 12:35:20 2024]  Iteration number: 0 with current cost as 0.4020692184897885 and parameters 
[-4.2542592   2.23743464 -2.12427948 -0.11653118  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.06648324  0.60271526  1.14432429
  1.31029883 -1.87354696]. 
Working on 0.4 fold... 
[Thu Apr  4 12:37:46 2024]  Iteration number: 0 with current cost as 0.3243252347146518 and parameters 
[-4.44706128  2.23743464 -2.12427949 -0.11653117  0.55388708 -2.77010912
  3.06858484  2.18960145  1.18552013 -1.06648323  0.6027151   1.14432431
  1.31029884 -1.87354694]. 
Working on 0.6 fold... 
[Thu Apr  4 12:40:14 2024]  Iteration number: 0 with current cost as 0.36777230979622877 and parameters 
[-4.3440556   2.23743487 -2.12427916 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552022 -1.06648308  0.60271534  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:42:46 2024]  Iteration number: 0 with current cost as 0.363780831535238 and parameters 
[-4.35460091  2.23743448 -2.12427964 -0.11653118  0.55388692 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.6027151   1.14432429
  1.31029867 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:45:14 2024]  Iteration number: 0 with current cost as 0.3597146207892242 and parameters 
[-7.59805192  2.23743527 -2.12427837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 722.5007612705231 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 12:47:18 2024]  Iteration number: 0 with current cost as 0.4020692184897885 and parameters 
[-4.2542592   2.23743464 -2.12427948 -0.11653118  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.06648324  0.60271526  1.14432429
  1.31029883 -1.87354696]. 
Working on 0.4 fold... 
[Thu Apr  4 12:49:50 2024]  Iteration number: 0 with current cost as 0.3243252347146518 and parameters 
[-4.44706128  2.23743464 -2.12427949 -0.11653117  0.55388708 -2.77010912
  3.06858484  2.18960145  1.18552013 -1.06648323  0.6027151   1.14432431
  1.31029884 -1.87354694]. 
Working on 0.6 fold... 
[Thu Apr  4 12:52:27 2024]  Iteration number: 0 with current cost as 0.36777230979622877 and parameters 
[-4.3440556   2.23743487 -2.12427916 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552022 -1.06648308  0.60271534  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:54:56 2024]  Iteration number: 0 with current cost as 0.363780831535238 and parameters 
[-4.35460091  2.23743448 -2.12427964 -0.11653118  0.55388692 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.6027151   1.14432429
  1.31029867 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:57:21 2024]  Iteration number: 0 with current cost as 0.3597146207892242 and parameters 
[-7.59805192  2.23743527 -2.12427837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 727.5538151264191 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 12:59:29 2024]  Iteration number: 0 with current cost as 0.4020692184897885 and parameters 
[-4.2542592   2.23743464 -2.12427948 -0.11653118  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.06648324  0.60271526  1.14432429
  1.31029883 -1.87354696]. 
Working on 0.4 fold... 
[Thu Apr  4 13:02:01 2024]  Iteration number: 0 with current cost as 0.3243252347146518 and parameters 
[-4.44706128  2.23743464 -2.12427949 -0.11653117  0.55388708 -2.77010912
  3.06858484  2.18960145  1.18552013 -1.06648323  0.6027151   1.14432431
  1.31029884 -1.87354694]. 
Working on 0.6 fold... 
[Thu Apr  4 13:04:41 2024]  Iteration number: 0 with current cost as 0.36777230979622877 and parameters 
[-4.3440556   2.23743487 -2.12427916 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552022 -1.06648308  0.60271534  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:07:10 2024]  Iteration number: 0 with current cost as 0.363780831535238 and parameters 
[-4.35460091  2.23743448 -2.12427964 -0.11653118  0.55388692 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.6027151   1.14432429
  1.31029867 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:09:45 2024]  Iteration number: 0 with current cost as 0.3597146207892242 and parameters 
[-7.59805192  2.23743527 -2.12427837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 746.0125412940979 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 13:11:57 2024]  Iteration number: 0 with current cost as 0.4020692184897885 and parameters 
[-4.2542592   2.23743464 -2.12427948 -0.11653118  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.06648324  0.60271526  1.14432429
  1.31029883 -1.87354696]. 
Working on 0.4 fold... 
[Thu Apr  4 13:14:36 2024]  Iteration number: 0 with current cost as 0.3243252347146518 and parameters 
[-4.44706128  2.23743464 -2.12427949 -0.11653117  0.55388708 -2.77010912
  3.06858484  2.18960145  1.18552013 -1.06648323  0.6027151   1.14432431
  1.31029884 -1.87354694]. 
Working on 0.6 fold... 
[Thu Apr  4 13:17:02 2024]  Iteration number: 0 with current cost as 0.36777230979622877 and parameters 
[-4.3440556   2.23743487 -2.12427916 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552022 -1.06648308  0.60271534  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:19:31 2024]  Iteration number: 0 with current cost as 0.363780831535238 and parameters 
[-4.35460091  2.23743448 -2.12427964 -0.11653118  0.55388692 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.6027151   1.14432429
  1.31029867 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:22:03 2024]  Iteration number: 0 with current cost as 0.3597146207892242 and parameters 
[-7.59805192  2.23743527 -2.12427837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 738.8055734634399 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 13:24:22 2024]  Iteration number: 0 with current cost as 0.4020692184897885 and parameters 
[-4.2542592   2.23743464 -2.12427948 -0.11653118  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.06648324  0.60271526  1.14432429
  1.31029883 -1.87354696]. 
Working on 0.4 fold... 
[Thu Apr  4 13:26:54 2024]  Iteration number: 0 with current cost as 0.3243252347146518 and parameters 
[-4.44706128  2.23743464 -2.12427949 -0.11653117  0.55388708 -2.77010912
  3.06858484  2.18960145  1.18552013 -1.06648323  0.6027151   1.14432431
  1.31029884 -1.87354694]. 
Working on 0.6 fold... 
[Thu Apr  4 13:29:15 2024]  Iteration number: 0 with current cost as 0.36777230979622877 and parameters 
[-4.3440556   2.23743487 -2.12427916 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552022 -1.06648308  0.60271534  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:31:53 2024]  Iteration number: 0 with current cost as 0.363780831535238 and parameters 
[-4.35460091  2.23743448 -2.12427964 -0.11653118  0.55388692 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.6027151   1.14432429
  1.31029867 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:34:25 2024]  Iteration number: 0 with current cost as 0.3597146207892242 and parameters 
[-7.59805192  2.23743527 -2.12427837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 743.760614156723 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 13:36:40 2024]  Iteration number: 0 with current cost as 0.4020692184897885 and parameters 
[-4.2542592   2.23743464 -2.12427948 -0.11653118  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.06648324  0.60271526  1.14432429
  1.31029883 -1.87354696]. 
Working on 0.4 fold... 
[Thu Apr  4 13:39:13 2024]  Iteration number: 0 with current cost as 0.3243252347146518 and parameters 
[-4.44706128  2.23743464 -2.12427949 -0.11653117  0.55388708 -2.77010912
  3.06858484  2.18960145  1.18552013 -1.06648323  0.6027151   1.14432431
  1.31029884 -1.87354694]. 
Working on 0.6 fold... 
[Thu Apr  4 13:41:52 2024]  Iteration number: 0 with current cost as 0.36777230979622877 and parameters 
[-4.3440556   2.23743487 -2.12427916 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552022 -1.06648308  0.60271534  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:44:27 2024]  Iteration number: 0 with current cost as 0.363780831535238 and parameters 
[-4.35460091  2.23743448 -2.12427964 -0.11653118  0.55388692 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.6027151   1.14432429
  1.31029867 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:46:58 2024]  Iteration number: 0 with current cost as 0.3597146207892242 and parameters 
[-7.59805192  2.23743527 -2.12427837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 749.3394899368286 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 13:49:16 2024]  Iteration number: 0 with current cost as 0.4020692184897885 and parameters 
[-4.2542592   2.23743464 -2.12427948 -0.11653118  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.06648324  0.60271526  1.14432429
  1.31029883 -1.87354696]. 
Working on 0.4 fold... 
[Thu Apr  4 13:52:16 2024]  Iteration number: 0 with current cost as 0.3243252347146518 and parameters 
[-4.44706128  2.23743464 -2.12427949 -0.11653117  0.55388708 -2.77010912
  3.06858484  2.18960145  1.18552013 -1.06648323  0.6027151   1.14432431
  1.31029884 -1.87354694]. 
Working on 0.6 fold... 
[Thu Apr  4 13:54:46 2024]  Iteration number: 0 with current cost as 0.36777230979622877 and parameters 
[-4.3440556   2.23743487 -2.12427916 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552022 -1.06648308  0.60271534  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:57:14 2024]  Iteration number: 0 with current cost as 0.363780831535238 and parameters 
[-4.35460091  2.23743448 -2.12427964 -0.11653118  0.55388692 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.6027151   1.14432429
  1.31029867 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:59:44 2024]  Iteration number: 0 with current cost as 0.3597146207892242 and parameters 
[-7.59805192  2.23743527 -2.12427837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 765.9139211177826 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 14:01:56 2024]  Iteration number: 0 with current cost as 0.4020692184897885 and parameters 
[-4.2542592   2.23743464 -2.12427948 -0.11653118  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.06648324  0.60271526  1.14432429
  1.31029883 -1.87354696]. 
Working on 0.4 fold... 
[Thu Apr  4 14:04:30 2024]  Iteration number: 0 with current cost as 0.3243252347146518 and parameters 
[-4.44706128  2.23743464 -2.12427949 -0.11653117  0.55388708 -2.77010912
  3.06858484  2.18960145  1.18552013 -1.06648323  0.6027151   1.14432431
  1.31029884 -1.87354694]. 
Working on 0.6 fold... 
[Thu Apr  4 14:06:57 2024]  Iteration number: 0 with current cost as 0.36777230979622877 and parameters 
[-4.3440556   2.23743487 -2.12427916 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552022 -1.06648308  0.60271534  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 14:09:34 2024]  Iteration number: 0 with current cost as 0.363780831535238 and parameters 
[-4.35460091  2.23743448 -2.12427964 -0.11653118  0.55388692 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.6027151   1.14432429
  1.31029867 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 14:12:04 2024]  Iteration number: 0 with current cost as 0.3597146207892242 and parameters 
[-7.59805192  2.23743527 -2.12427837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 748.4670140743256 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 14:14:33 2024]  Iteration number: 0 with current cost as 0.4020692184897885 and parameters 
[-4.2542592   2.23743464 -2.12427948 -0.11653118  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.06648324  0.60271526  1.14432429
  1.31029883 -1.87354696]. 
Working on 0.4 fold... 
[Thu Apr  4 14:17:07 2024]  Iteration number: 0 with current cost as 0.3243252347146518 and parameters 
[-4.44706128  2.23743464 -2.12427949 -0.11653117  0.55388708 -2.77010912
  3.06858484  2.18960145  1.18552013 -1.06648323  0.6027151   1.14432431
  1.31029884 -1.87354694]. 
Working on 0.6 fold... 
[Thu Apr  4 14:19:40 2024]  Iteration number: 0 with current cost as 0.36777230979622877 and parameters 
[-4.3440556   2.23743487 -2.12427916 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552022 -1.06648308  0.60271534  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 14:22:08 2024]  Iteration number: 0 with current cost as 0.363780831535238 and parameters 
[-4.35460091  2.23743448 -2.12427964 -0.11653118  0.55388692 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.6027151   1.14432429
  1.31029867 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 14:24:45 2024]  Iteration number: 0 with current cost as 0.3597146207892242 and parameters 
[-7.59805192  2.23743527 -2.12427837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 757.1268119812012 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 14:27:01 2024]  Iteration number: 0 with current cost as 0.4020692184897885 and parameters 
[-4.2542592   2.23743464 -2.12427948 -0.11653118  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.06648324  0.60271526  1.14432429
  1.31029883 -1.87354696]. 
Working on 0.4 fold... 
[Thu Apr  4 14:29:38 2024]  Iteration number: 0 with current cost as 0.3243252347146518 and parameters 
[-4.44706128  2.23743464 -2.12427949 -0.11653117  0.55388708 -2.77010912
  3.06858484  2.18960145  1.18552013 -1.06648323  0.6027151   1.14432431
  1.31029884 -1.87354694]. 
Working on 0.6 fold... 
[Thu Apr  4 14:32:13 2024]  Iteration number: 0 with current cost as 0.36777230979622877 and parameters 
[-4.3440556   2.23743487 -2.12427916 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552022 -1.06648308  0.60271534  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 14:34:51 2024]  Iteration number: 0 with current cost as 0.363780831535238 and parameters 
[-4.35460091  2.23743448 -2.12427964 -0.11653118  0.55388692 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.6027151   1.14432429
  1.31029867 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 14:37:20 2024]  Iteration number: 0 with current cost as 0.3597146207892242 and parameters 
[-7.59805192  2.23743527 -2.12427837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 761.7649960517883 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 14:39:44 2024]  Iteration number: 0 with current cost as 0.4020692184897885 and parameters 
[-4.2542592   2.23743464 -2.12427948 -0.11653118  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.06648324  0.60271526  1.14432429
  1.31029883 -1.87354696]. 
Working on 0.4 fold... 
[Thu Apr  4 14:42:19 2024]  Iteration number: 0 with current cost as 0.3243252347146518 and parameters 
[-4.44706128  2.23743464 -2.12427949 -0.11653117  0.55388708 -2.77010912
  3.06858484  2.18960145  1.18552013 -1.06648323  0.6027151   1.14432431
  1.31029884 -1.87354694]. 
Working on 0.6 fold... 
[Thu Apr  4 14:44:52 2024]  Iteration number: 0 with current cost as 0.36777230979622877 and parameters 
[-4.3440556   2.23743487 -2.12427916 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552022 -1.06648308  0.60271534  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 14:47:20 2024]  Iteration number: 0 with current cost as 0.363780831535238 and parameters 
[-4.35460091  2.23743448 -2.12427964 -0.11653118  0.55388692 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.6027151   1.14432429
  1.31029867 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 14:49:57 2024]  Iteration number: 0 with current cost as 0.3597146207892242 and parameters 
[-7.59805192  2.23743527 -2.12427837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 748.0887558460236 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 14:52:15 2024]  Iteration number: 0 with current cost as 0.4020692184897885 and parameters 
[-4.2542592   2.23743464 -2.12427948 -0.11653118  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.06648324  0.60271526  1.14432429
  1.31029883 -1.87354696]. 
Working on 0.4 fold... 
[Thu Apr  4 14:55:08 2024]  Iteration number: 0 with current cost as 0.3243252347146518 and parameters 
[-4.44706128  2.23743464 -2.12427949 -0.11653117  0.55388708 -2.77010912
  3.06858484  2.18960145  1.18552013 -1.06648323  0.6027151   1.14432431
  1.31029884 -1.87354694]. 
Working on 0.6 fold... 
[Thu Apr  4 14:57:48 2024]  Iteration number: 0 with current cost as 0.36777230979622877 and parameters 
[-4.3440556   2.23743487 -2.12427916 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552022 -1.06648308  0.60271534  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 15:00:33 2024]  Iteration number: 0 with current cost as 0.363780831535238 and parameters 
[-4.35460091  2.23743448 -2.12427964 -0.11653118  0.55388692 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.6027151   1.14432429
  1.31029867 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 15:03:15 2024]  Iteration number: 0 with current cost as 0.3597146207892242 and parameters 
[-7.59805192  2.23743527 -2.12427837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 798.1223711967468 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 15:05:36 2024]  Iteration number: 0 with current cost as 0.4020692184897885 and parameters 
[-4.2542592   2.23743464 -2.12427948 -0.11653118  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.06648324  0.60271526  1.14432429
  1.31029883 -1.87354696]. 
Working on 0.4 fold... 
[Thu Apr  4 15:08:16 2024]  Iteration number: 0 with current cost as 0.3243252347146518 and parameters 
[-4.44706128  2.23743464 -2.12427949 -0.11653117  0.55388708 -2.77010912
  3.06858484  2.18960145  1.18552013 -1.06648323  0.6027151   1.14432431
  1.31029884 -1.87354694]. 
Working on 0.6 fold... 
[Thu Apr  4 15:11:20 2024]  Iteration number: 0 with current cost as 0.36777230979622877 and parameters 
[-4.3440556   2.23743487 -2.12427916 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552022 -1.06648308  0.60271534  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 15:14:04 2024]  Iteration number: 0 with current cost as 0.363780831535238 and parameters 
[-4.35460091  2.23743448 -2.12427964 -0.11653118  0.55388692 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.6027151   1.14432429
  1.31029867 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 15:16:45 2024]  Iteration number: 0 with current cost as 0.3597146207892242 and parameters 
[-7.59805192  2.23743527 -2.12427837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 816.217841386795 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 15:19:11 2024]  Iteration number: 0 with current cost as 0.4020692184897885 and parameters 
[-4.2542592   2.23743464 -2.12427948 -0.11653118  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.06648324  0.60271526  1.14432429
  1.31029883 -1.87354696]. 
Working on 0.4 fold... 
[Thu Apr  4 15:22:04 2024]  Iteration number: 0 with current cost as 0.3243252347146518 and parameters 
[-4.44706128  2.23743464 -2.12427949 -0.11653117  0.55388708 -2.77010912
  3.06858484  2.18960145  1.18552013 -1.06648323  0.6027151   1.14432431
  1.31029884 -1.87354694]. 
Working on 0.6 fold... 
[Thu Apr  4 15:25:02 2024]  Iteration number: 0 with current cost as 0.36777230979622877 and parameters 
[-4.3440556   2.23743487 -2.12427916 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552022 -1.06648308  0.60271534  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 15:27:51 2024]  Iteration number: 0 with current cost as 0.363780831535238 and parameters 
[-4.35460091  2.23743448 -2.12427964 -0.11653118  0.55388692 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.6027151   1.14432429
  1.31029867 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 15:30:29 2024]  Iteration number: 0 with current cost as 0.3597146207892242 and parameters 
[-7.59805192  2.23743527 -2.12427837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 818.2279562950134 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 15:32:41 2024]  Iteration number: 0 with current cost as 0.4020692184897885 and parameters 
[-4.2542592   2.23743464 -2.12427948 -0.11653118  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.06648324  0.60271526  1.14432429
  1.31029883 -1.87354696]. 
Working on 0.4 fold... 
[Thu Apr  4 15:35:16 2024]  Iteration number: 0 with current cost as 0.3243252347146518 and parameters 
[-4.44706128  2.23743464 -2.12427949 -0.11653117  0.55388708 -2.77010912
  3.06858484  2.18960145  1.18552013 -1.06648323  0.6027151   1.14432431
  1.31029884 -1.87354694]. 
Working on 0.6 fold... 
[Thu Apr  4 15:38:12 2024]  Iteration number: 0 with current cost as 0.36777230979622877 and parameters 
[-4.3440556   2.23743487 -2.12427916 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552022 -1.06648308  0.60271534  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 15:40:58 2024]  Iteration number: 0 with current cost as 0.363780831535238 and parameters 
[-4.35460091  2.23743448 -2.12427964 -0.11653118  0.55388692 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.6027151   1.14432429
  1.31029867 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 15:43:47 2024]  Iteration number: 0 with current cost as 0.3597146207892242 and parameters 
[-7.59805192  2.23743527 -2.12427837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 807.2590417861938 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 15:46:17 2024]  Iteration number: 0 with current cost as 0.4020692184897885 and parameters 
[-4.2542592   2.23743464 -2.12427948 -0.11653118  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.06648324  0.60271526  1.14432429
  1.31029883 -1.87354696]. 
Working on 0.4 fold... 
[Thu Apr  4 15:49:17 2024]  Iteration number: 0 with current cost as 0.3243252347146518 and parameters 
[-4.44706128  2.23743464 -2.12427949 -0.11653117  0.55388708 -2.77010912
  3.06858484  2.18960145  1.18552013 -1.06648323  0.6027151   1.14432431
  1.31029884 -1.87354694]. 
Working on 0.6 fold... 
[Thu Apr  4 15:52:05 2024]  Iteration number: 0 with current cost as 0.36777230979622877 and parameters 
[-4.3440556   2.23743487 -2.12427916 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552022 -1.06648308  0.60271534  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 15:54:49 2024]  Iteration number: 0 with current cost as 0.363780831535238 and parameters 
[-4.35460091  2.23743448 -2.12427964 -0.11653118  0.55388692 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.6027151   1.14432429
  1.31029867 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 15:57:31 2024]  Iteration number: 0 with current cost as 0.3597146207892242 and parameters 
[-7.59805192  2.23743527 -2.12427837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 822.2351129055023 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 16:00:02 2024]  Iteration number: 0 with current cost as 0.4020692184897885 and parameters 
[-4.2542592   2.23743464 -2.12427948 -0.11653118  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.06648324  0.60271526  1.14432429
  1.31029883 -1.87354696]. 
Working on 0.4 fold... 
[Thu Apr  4 16:02:57 2024]  Iteration number: 0 with current cost as 0.3243252347146518 and parameters 
[-4.44706128  2.23743464 -2.12427949 -0.11653117  0.55388708 -2.77010912
  3.06858484  2.18960145  1.18552013 -1.06648323  0.6027151   1.14432431
  1.31029884 -1.87354694]. 
Working on 0.6 fold... 
[Thu Apr  4 16:05:35 2024]  Iteration number: 0 with current cost as 0.36777230979622877 and parameters 
[-4.3440556   2.23743487 -2.12427916 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552022 -1.06648308  0.60271534  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 16:08:31 2024]  Iteration number: 0 with current cost as 0.363780831535238 and parameters 
[-4.35460091  2.23743448 -2.12427964 -0.11653118  0.55388692 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.6027151   1.14432429
  1.31029867 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 16:11:25 2024]  Iteration number: 0 with current cost as 0.3597146207892242 and parameters 
[-7.59805192  2.23743527 -2.12427837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 839.5778365135193 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 16:13:55 2024]  Iteration number: 0 with current cost as 0.4020692184897885 and parameters 
[-4.2542592   2.23743464 -2.12427948 -0.11653118  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.06648324  0.60271526  1.14432429
  1.31029883 -1.87354696]. 
Working on 0.4 fold... 
[Thu Apr  4 16:16:32 2024]  Iteration number: 0 with current cost as 0.3243252347146518 and parameters 
[-4.44706128  2.23743464 -2.12427949 -0.11653117  0.55388708 -2.77010912
  3.06858484  2.18960145  1.18552013 -1.06648323  0.6027151   1.14432431
  1.31029884 -1.87354694]. 
Working on 0.6 fold... 
[Thu Apr  4 16:19:34 2024]  Iteration number: 0 with current cost as 0.36777230979622877 and parameters 
[-4.3440556   2.23743487 -2.12427916 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552022 -1.06648308  0.60271534  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 16:22:17 2024]  Iteration number: 0 with current cost as 0.363780831535238 and parameters 
[-4.35460091  2.23743448 -2.12427964 -0.11653118  0.55388692 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.6027151   1.14432429
  1.31029867 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 16:24:53 2024]  Iteration number: 0 with current cost as 0.3597146207892242 and parameters 
[-7.59805192  2.23743527 -2.12427837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 794.9219446182251 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 16:27:05 2024]  Iteration number: 0 with current cost as 0.4020692184897885 and parameters 
[-4.2542592   2.23743464 -2.12427948 -0.11653118  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.06648324  0.60271526  1.14432429
  1.31029883 -1.87354696]. 
Working on 0.4 fold... 
[Thu Apr  4 16:29:41 2024]  Iteration number: 0 with current cost as 0.3243252347146518 and parameters 
[-4.44706128  2.23743464 -2.12427949 -0.11653117  0.55388708 -2.77010912
  3.06858484  2.18960145  1.18552013 -1.06648323  0.6027151   1.14432431
  1.31029884 -1.87354694]. 
Working on 0.6 fold... 
[Thu Apr  4 16:32:13 2024]  Iteration number: 0 with current cost as 0.36777230979622877 and parameters 
[-4.3440556   2.23743487 -2.12427916 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552022 -1.06648308  0.60271534  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 16:34:57 2024]  Iteration number: 0 with current cost as 0.363780831535238 and parameters 
[-4.35460091  2.23743448 -2.12427964 -0.11653118  0.55388692 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.6027151   1.14432429
  1.31029867 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 16:37:53 2024]  Iteration number: 0 with current cost as 0.3597146207892242 and parameters 
[-7.59805192  2.23743527 -2.12427837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 790.5629684925079 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 16:40:22 2024]  Iteration number: 0 with current cost as 0.4020692184897885 and parameters 
[-4.2542592   2.23743464 -2.12427948 -0.11653118  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.06648324  0.60271526  1.14432429
  1.31029883 -1.87354696]. 
Working on 0.4 fold... 
[Thu Apr  4 16:43:01 2024]  Iteration number: 0 with current cost as 0.3243252347146518 and parameters 
[-4.44706128  2.23743464 -2.12427949 -0.11653117  0.55388708 -2.77010912
  3.06858484  2.18960145  1.18552013 -1.06648323  0.6027151   1.14432431
  1.31029884 -1.87354694]. 
Working on 0.6 fold... 
[Thu Apr  4 16:45:50 2024]  Iteration number: 0 with current cost as 0.36777230979622877 and parameters 
[-4.3440556   2.23743487 -2.12427916 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552022 -1.06648308  0.60271534  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 16:48:43 2024]  Iteration number: 0 with current cost as 0.363780831535238 and parameters 
[-4.35460091  2.23743448 -2.12427964 -0.11653118  0.55388692 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.6027151   1.14432429
  1.31029867 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 16:51:29 2024]  Iteration number: 0 with current cost as 0.3597146207892242 and parameters 
[-7.59805192  2.23743527 -2.12427837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 807.2182102203369 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 16:53:45 2024]  Iteration number: 0 with current cost as 0.4020692184897885 and parameters 
[-4.2542592   2.23743464 -2.12427948 -0.11653118  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.06648324  0.60271526  1.14432429
  1.31029883 -1.87354696]. 
Working on 0.4 fold... 
[Thu Apr  4 16:56:36 2024]  Iteration number: 0 with current cost as 0.3243252347146518 and parameters 
[-4.44706128  2.23743464 -2.12427949 -0.11653117  0.55388708 -2.77010912
  3.06858484  2.18960145  1.18552013 -1.06648323  0.6027151   1.14432431
  1.31029884 -1.87354694]. 
Working on 0.6 fold... 
[Thu Apr  4 16:59:26 2024]  Iteration number: 0 with current cost as 0.36777230979622877 and parameters 
[-4.3440556   2.23743487 -2.12427916 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552022 -1.06648308  0.60271534  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 17:02:11 2024]  Iteration number: 0 with current cost as 0.363780831535238 and parameters 
[-4.35460091  2.23743448 -2.12427964 -0.11653118  0.55388692 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.6027151   1.14432429
  1.31029867 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 17:05:12 2024]  Iteration number: 0 with current cost as 0.3597146207892242 and parameters 
[-7.59805192  2.23743527 -2.12427837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 823.0181245803833 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 17:07:32 2024]  Iteration number: 0 with current cost as 0.4020692184897885 and parameters 
[-4.2542592   2.23743464 -2.12427948 -0.11653118  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.06648324  0.60271526  1.14432429
  1.31029883 -1.87354696]. 
Working on 0.4 fold... 
[Thu Apr  4 17:10:20 2024]  Iteration number: 0 with current cost as 0.3243252347146518 and parameters 
[-4.44706128  2.23743464 -2.12427949 -0.11653117  0.55388708 -2.77010912
  3.06858484  2.18960145  1.18552013 -1.06648323  0.6027151   1.14432431
  1.31029884 -1.87354694]. 
Working on 0.6 fold... 
[Thu Apr  4 17:13:11 2024]  Iteration number: 0 with current cost as 0.36777230979622877 and parameters 
[-4.3440556   2.23743487 -2.12427916 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552022 -1.06648308  0.60271534  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 17:15:44 2024]  Iteration number: 0 with current cost as 0.363780831535238 and parameters 
[-4.35460091  2.23743448 -2.12427964 -0.11653118  0.55388692 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.6027151   1.14432429
  1.31029867 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 17:18:20 2024]  Iteration number: 0 with current cost as 0.3597146207892242 and parameters 
[-7.59805192  2.23743527 -2.12427837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 794.5456273555756 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 17:20:43 2024]  Iteration number: 0 with current cost as 0.4020692184897885 and parameters 
[-4.2542592   2.23743464 -2.12427948 -0.11653118  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.06648324  0.60271526  1.14432429
  1.31029883 -1.87354696]. 
Working on 0.4 fold... 
[Thu Apr  4 17:23:28 2024]  Iteration number: 0 with current cost as 0.3243252347146518 and parameters 
[-4.44706128  2.23743464 -2.12427949 -0.11653117  0.55388708 -2.77010912
  3.06858484  2.18960145  1.18552013 -1.06648323  0.6027151   1.14432431
  1.31029884 -1.87354694]. 
Working on 0.6 fold... 
[Thu Apr  4 17:26:08 2024]  Iteration number: 0 with current cost as 0.36777230979622877 and parameters 
[-4.3440556   2.23743487 -2.12427916 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552022 -1.06648308  0.60271534  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 17:28:50 2024]  Iteration number: 0 with current cost as 0.363780831535238 and parameters 
[-4.35460091  2.23743448 -2.12427964 -0.11653118  0.55388692 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.6027151   1.14432429
  1.31029867 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 17:31:26 2024]  Iteration number: 0 with current cost as 0.3597146207892242 and parameters 
[-7.59805192  2.23743527 -2.12427837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 779.429034948349 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 17:33:40 2024]  Iteration number: 0 with current cost as 0.4020692184897885 and parameters 
[-4.2542592   2.23743464 -2.12427948 -0.11653118  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.06648324  0.60271526  1.14432429
  1.31029883 -1.87354696]. 
Working on 0.4 fold... 
[Thu Apr  4 17:36:31 2024]  Iteration number: 0 with current cost as 0.3243252347146518 and parameters 
[-4.44706128  2.23743464 -2.12427949 -0.11653117  0.55388708 -2.77010912
  3.06858484  2.18960145  1.18552013 -1.06648323  0.6027151   1.14432431
  1.31029884 -1.87354694]. 
Working on 0.6 fold... 
[Thu Apr  4 17:39:24 2024]  Iteration number: 0 with current cost as 0.36777230979622877 and parameters 
[-4.3440556   2.23743487 -2.12427916 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552022 -1.06648308  0.60271534  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 17:42:14 2024]  Iteration number: 0 with current cost as 0.363780831535238 and parameters 
[-4.35460091  2.23743448 -2.12427964 -0.11653118  0.55388692 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.6027151   1.14432429
  1.31029867 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 17:45:11 2024]  Iteration number: 0 with current cost as 0.3597146207892242 and parameters 
[-7.59805192  2.23743527 -2.12427837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 826.2707366943359 seconds. 
Discarding model... 

Training complete taking 19462.503534317017 total seconds. 
Now scoring model... 
Scoring complete taking 1.9816029071807861 seconds. 
Saved predicted values as A2-A2-CZ_Efficient-CRZ_predicted_values.csv
Model scores: {'MSE_train': (227.6305360764044,), 'R2_train': -0.1027935508587472, 'MAE_train': 13.231385708407208, 'MSE_test': 214.8609325783672, 'R2_test': -0.2951417181201017, 'MAE_test': 12.615998634448655}. 
Saved model results as A2-A2-CZ_Efficient-CRZ_results.json. 
