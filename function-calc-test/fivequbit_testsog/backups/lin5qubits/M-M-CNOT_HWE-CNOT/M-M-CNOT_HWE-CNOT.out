/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/circuits/Encoders.py:12: RuntimeWarning: invalid value encountered in arcsin
  if np.isnan(np.arcsin(features[feature_idx])) or np.isnan(np.arccos(features[feature_idx])):
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Sun Mar 24 17:30:26 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 17:30:39 2024]  Iteration number: 0 with current cost as 0.3158571118914961 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10012876  0.52964358 -2.78931118
  3.09312933  2.13569493  1.17390637 -1.01420303  0.61632942  1.21204318
  1.26093092 -1.92610255  0.70708687]. 
[Sun Mar 24 17:35:47 2024]  Iteration number: 50 with current cost as 0.15802374603925967 and parameters 
[-2.90318331  2.23743454 -2.12427992  0.57600278 -1.09959662 -2.77856063
  3.00188253  2.19177881  2.78862963 -0.79742068  0.47991528  2.5058187
  0.21984448 -2.19917186 -0.37957797]. 
Working on 0.4 fold... 
[Sun Mar 24 17:36:00 2024]  Iteration number: 0 with current cost as 0.2673213341857351 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10627884  0.54312293 -2.77514319
  3.08820188  2.17010728  1.16280328 -1.03368807  0.62922536  1.19242255
  1.24759391 -1.92957904  0.73937425]. 
Working on 0.6 fold... 
[Sun Mar 24 17:42:12 2024]  Iteration number: 0 with current cost as 0.2731554112126978 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.11062596  0.5351918  -2.79284563
  3.0831292   2.15169526  1.18203395 -1.02262672  0.62505989  1.20454741
  1.25119578 -1.93043863  0.7242161 ]. 
Working on 0.8 fold... 
[Sun Mar 24 17:47:07 2024]  Iteration number: 0 with current cost as 0.28042696181453697 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.11373358  0.53829865 -2.79156727
  3.08791444  2.15531654  1.17172212 -1.02289931  0.6260587   1.2045288
  1.24455613 -1.93628971  0.72553925]. 
[Sun Mar 24 17:52:12 2024]  Iteration number: 50 with current cost as 0.09146224204641965 and parameters 
[-2.90318329  2.2374346  -2.12427967  0.61924569  0.34600076 -1.66836898
  3.35170411  3.61778649  1.51476541  1.04008414  1.9611062   2.42925986
  1.60510093 -1.50222167  0.39166901]. 
Working on 1.0 fold... 
[Sun Mar 24 17:52:42 2024]  Iteration number: 0 with current cost as 0.2802935606361306 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.11012259  0.53584413 -2.79121815
  3.09782146  2.14161863  1.1624438  -1.02215111  0.61379969  1.20160416
  1.25287728 -1.93215614  0.71242765]. 
[Sun Mar 24 17:59:08 2024]  Iteration number: 50 with current cost as 0.09883688874401034 and parameters 
[-2.90318355  2.2374346  -2.12427967  0.95537948  0.25355227 -1.9243262
  3.92730251  0.49465136  1.19597701  1.12624557  2.07980672  2.4578889
 -0.34773904 -3.6943621   1.17075566]. 
Training complete taking 1727.621199131012 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 17:59:24 2024]  Iteration number: 0 with current cost as 0.3158571118914961 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10012876  0.52964358 -2.78931118
  3.09312933  2.13569493  1.17390637 -1.01420303  0.61632942  1.21204318
  1.26093092 -1.92610255  0.70708687]. 
[Sun Mar 24 18:04:51 2024]  Iteration number: 50 with current cost as 0.15802374603925967 and parameters 
[-2.90318331  2.23743454 -2.12427992  0.57600278 -1.09959662 -2.77856063
  3.00188253  2.19177881  2.78862963 -0.79742068  0.47991528  2.5058187
  0.21984448 -2.19917186 -0.37957797]. 
Working on 0.4 fold... 
[Sun Mar 24 18:05:04 2024]  Iteration number: 0 with current cost as 0.2673213341857351 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10627884  0.54312293 -2.77514319
  3.08820188  2.17010728  1.16280328 -1.03368807  0.62922536  1.19242255
  1.24759391 -1.92957904  0.73937425]. 
Working on 0.6 fold... 
[Sun Mar 24 18:10:39 2024]  Iteration number: 0 with current cost as 0.2731554112126978 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.11062596  0.5351918  -2.79284563
  3.0831292   2.15169526  1.18203395 -1.02262672  0.62505989  1.20454741
  1.25119578 -1.93043863  0.7242161 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:15:17 2024]  Iteration number: 0 with current cost as 0.28042696181453697 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.11373358  0.53829865 -2.79156727
  3.08791444  2.15531654  1.17172212 -1.02289931  0.6260587   1.2045288
  1.24455613 -1.93628971  0.72553925]. 
[Sun Mar 24 18:20:30 2024]  Iteration number: 50 with current cost as 0.09146224204641965 and parameters 
[-2.90318329  2.2374346  -2.12427967  0.61924569  0.34600076 -1.66836898
  3.35170411  3.61778649  1.51476541  1.04008414  1.9611062   2.42925986
  1.60510093 -1.50222167  0.39166901]. 
Working on 1.0 fold... 
[Sun Mar 24 18:21:03 2024]  Iteration number: 0 with current cost as 0.2802935606361306 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.11012259  0.53584413 -2.79121815
  3.09782146  2.14161863  1.1624438  -1.02215111  0.61379969  1.20160416
  1.25287728 -1.93215614  0.71242765]. 
[Sun Mar 24 18:27:10 2024]  Iteration number: 50 with current cost as 0.09883688874401034 and parameters 
[-2.90318355  2.2374346  -2.12427967  0.95537948  0.25355227 -1.9243262
  3.92730251  0.49465136  1.19597701  1.12624557  2.07980672  2.4578889
 -0.34773904 -3.6943621   1.17075566]. 
Training complete taking 1682.3084406852722 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 18:27:27 2024]  Iteration number: 0 with current cost as 0.3158571118914961 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10012876  0.52964358 -2.78931118
  3.09312933  2.13569493  1.17390637 -1.01420303  0.61632942  1.21204318
  1.26093092 -1.92610255  0.70708687]. 
[Sun Mar 24 18:32:38 2024]  Iteration number: 50 with current cost as 0.15802374603925967 and parameters 
[-2.90318331  2.23743454 -2.12427992  0.57600278 -1.09959662 -2.77856063
  3.00188253  2.19177881  2.78862963 -0.79742068  0.47991528  2.5058187
  0.21984448 -2.19917186 -0.37957797]. 
Working on 0.4 fold... 
[Sun Mar 24 18:32:49 2024]  Iteration number: 0 with current cost as 0.2673213341857351 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10627884  0.54312293 -2.77514319
  3.08820188  2.17010728  1.16280328 -1.03368807  0.62922536  1.19242255
  1.24759391 -1.92957904  0.73937425]. 
Working on 0.6 fold... 
[Sun Mar 24 18:38:38 2024]  Iteration number: 0 with current cost as 0.2731554112126978 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.11062596  0.5351918  -2.79284563
  3.0831292   2.15169526  1.18203395 -1.02262672  0.62505989  1.20454741
  1.25119578 -1.93043863  0.7242161 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:43:17 2024]  Iteration number: 0 with current cost as 0.28042696181453697 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.11373358  0.53829865 -2.79156727
  3.08791444  2.15531654  1.17172212 -1.02289931  0.6260587   1.2045288
  1.24455613 -1.93628971  0.72553925]. 
[Sun Mar 24 18:48:25 2024]  Iteration number: 50 with current cost as 0.09146224204641965 and parameters 
[-2.90318329  2.2374346  -2.12427967  0.61924569  0.34600076 -1.66836898
  3.35170411  3.61778649  1.51476541  1.04008414  1.9611062   2.42925986
  1.60510093 -1.50222167  0.39166901]. 
Working on 1.0 fold... 
[Sun Mar 24 18:48:56 2024]  Iteration number: 0 with current cost as 0.2802935606361306 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.11012259  0.53584413 -2.79121815
  3.09782146  2.14161863  1.1624438  -1.02215111  0.61379969  1.20160416
  1.25287728 -1.93215614  0.71242765]. 
[Sun Mar 24 18:55:06 2024]  Iteration number: 50 with current cost as 0.09883688874401034 and parameters 
[-2.90318355  2.2374346  -2.12427967  0.95537948  0.25355227 -1.9243262
  3.92730251  0.49465136  1.19597701  1.12624557  2.07980672  2.4578889
 -0.34773904 -3.6943621   1.17075566]. 
Training complete taking 1675.967299938202 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 18:55:23 2024]  Iteration number: 0 with current cost as 0.3158571118914961 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10012876  0.52964358 -2.78931118
  3.09312933  2.13569493  1.17390637 -1.01420303  0.61632942  1.21204318
  1.26093092 -1.92610255  0.70708687]. 
[Sun Mar 24 19:00:33 2024]  Iteration number: 50 with current cost as 0.15802374603925967 and parameters 
[-2.90318331  2.23743454 -2.12427992  0.57600278 -1.09959662 -2.77856063
  3.00188253  2.19177881  2.78862963 -0.79742068  0.47991528  2.5058187
  0.21984448 -2.19917186 -0.37957797]. 
Working on 0.4 fold... 
[Sun Mar 24 19:00:45 2024]  Iteration number: 0 with current cost as 0.2673213341857351 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10627884  0.54312293 -2.77514319
  3.08820188  2.17010728  1.16280328 -1.03368807  0.62922536  1.19242255
  1.24759391 -1.92957904  0.73937425]. 
Working on 0.6 fold... 
[Sun Mar 24 19:06:43 2024]  Iteration number: 0 with current cost as 0.2731554112126978 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.11062596  0.5351918  -2.79284563
  3.0831292   2.15169526  1.18203395 -1.02262672  0.62505989  1.20454741
  1.25119578 -1.93043863  0.7242161 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:11:28 2024]  Iteration number: 0 with current cost as 0.28042696181453697 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.11373358  0.53829865 -2.79156727
  3.08791444  2.15531654  1.17172212 -1.02289931  0.6260587   1.2045288
  1.24455613 -1.93628971  0.72553925]. 
[Sun Mar 24 19:16:40 2024]  Iteration number: 50 with current cost as 0.09146224204641965 and parameters 
[-2.90318329  2.2374346  -2.12427967  0.61924569  0.34600076 -1.66836898
  3.35170411  3.61778649  1.51476541  1.04008414  1.9611062   2.42925986
  1.60510093 -1.50222167  0.39166901]. 
Working on 1.0 fold... 
[Sun Mar 24 19:17:10 2024]  Iteration number: 0 with current cost as 0.2802935606361306 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.11012259  0.53584413 -2.79121815
  3.09782146  2.14161863  1.1624438  -1.02215111  0.61379969  1.20160416
  1.25287728 -1.93215614  0.71242765]. 
[Sun Mar 24 19:23:11 2024]  Iteration number: 50 with current cost as 0.09883688874401034 and parameters 
[-2.90318355  2.2374346  -2.12427967  0.95537948  0.25355227 -1.9243262
  3.92730251  0.49465136  1.19597701  1.12624557  2.07980672  2.4578889
 -0.34773904 -3.6943621   1.17075566]. 
Training complete taking 1685.1850514411926 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 19:23:28 2024]  Iteration number: 0 with current cost as 0.3158571118914961 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10012876  0.52964358 -2.78931118
  3.09312933  2.13569493  1.17390637 -1.01420303  0.61632942  1.21204318
  1.26093092 -1.92610255  0.70708687]. 
[Sun Mar 24 19:28:38 2024]  Iteration number: 50 with current cost as 0.15802374603925967 and parameters 
[-2.90318331  2.23743454 -2.12427992  0.57600278 -1.09959662 -2.77856063
  3.00188253  2.19177881  2.78862963 -0.79742068  0.47991528  2.5058187
  0.21984448 -2.19917186 -0.37957797]. 
Working on 0.4 fold... 
[Sun Mar 24 19:28:50 2024]  Iteration number: 0 with current cost as 0.2673213341857351 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10627884  0.54312293 -2.77514319
  3.08820188  2.17010728  1.16280328 -1.03368807  0.62922536  1.19242255
  1.24759391 -1.92957904  0.73937425]. 
Working on 0.6 fold... 
[Sun Mar 24 19:34:43 2024]  Iteration number: 0 with current cost as 0.2731554112126978 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.11062596  0.5351918  -2.79284563
  3.0831292   2.15169526  1.18203395 -1.02262672  0.62505989  1.20454741
  1.25119578 -1.93043863  0.7242161 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:39:22 2024]  Iteration number: 0 with current cost as 0.28042696181453697 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.11373358  0.53829865 -2.79156727
  3.08791444  2.15531654  1.17172212 -1.02289931  0.6260587   1.2045288
  1.24455613 -1.93628971  0.72553925]. 
[Sun Mar 24 19:44:28 2024]  Iteration number: 50 with current cost as 0.09146224204641965 and parameters 
[-2.90318329  2.2374346  -2.12427967  0.61924569  0.34600076 -1.66836898
  3.35170411  3.61778649  1.51476541  1.04008414  1.9611062   2.42925986
  1.60510093 -1.50222167  0.39166901]. 
Working on 1.0 fold... 
[Sun Mar 24 19:44:58 2024]  Iteration number: 0 with current cost as 0.2802935606361306 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.11012259  0.53584413 -2.79121815
  3.09782146  2.14161863  1.1624438  -1.02215111  0.61379969  1.20160416
  1.25287728 -1.93215614  0.71242765]. 
[Sun Mar 24 19:50:59 2024]  Iteration number: 50 with current cost as 0.09883688874401034 and parameters 
[-2.90318355  2.2374346  -2.12427967  0.95537948  0.25355227 -1.9243262
  3.92730251  0.49465136  1.19597701  1.12624557  2.07980672  2.4578889
 -0.34773904 -3.6943621   1.17075566]. 
Training complete taking 1670.429637670517 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 19:51:18 2024]  Iteration number: 0 with current cost as 0.3158571118914961 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10012876  0.52964358 -2.78931118
  3.09312933  2.13569493  1.17390637 -1.01420303  0.61632942  1.21204318
  1.26093092 -1.92610255  0.70708687]. 
[Sun Mar 24 19:56:25 2024]  Iteration number: 50 with current cost as 0.15802374603925967 and parameters 
[-2.90318331  2.23743454 -2.12427992  0.57600278 -1.09959662 -2.77856063
  3.00188253  2.19177881  2.78862963 -0.79742068  0.47991528  2.5058187
  0.21984448 -2.19917186 -0.37957797]. 
Working on 0.4 fold... 
[Sun Mar 24 19:56:38 2024]  Iteration number: 0 with current cost as 0.2673213341857351 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10627884  0.54312293 -2.77514319
  3.08820188  2.17010728  1.16280328 -1.03368807  0.62922536  1.19242255
  1.24759391 -1.92957904  0.73937425]. 
Working on 0.6 fold... 
[Sun Mar 24 20:02:15 2024]  Iteration number: 0 with current cost as 0.2731554112126978 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.11062596  0.5351918  -2.79284563
  3.0831292   2.15169526  1.18203395 -1.02262672  0.62505989  1.20454741
  1.25119578 -1.93043863  0.7242161 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:06:52 2024]  Iteration number: 0 with current cost as 0.28042696181453697 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.11373358  0.53829865 -2.79156727
  3.08791444  2.15531654  1.17172212 -1.02289931  0.6260587   1.2045288
  1.24455613 -1.93628971  0.72553925]. 
[Sun Mar 24 20:11:59 2024]  Iteration number: 50 with current cost as 0.09146224204641965 and parameters 
[-2.90318329  2.2374346  -2.12427967  0.61924569  0.34600076 -1.66836898
  3.35170411  3.61778649  1.51476541  1.04008414  1.9611062   2.42925986
  1.60510093 -1.50222167  0.39166901]. 
Working on 1.0 fold... 
[Sun Mar 24 20:12:29 2024]  Iteration number: 0 with current cost as 0.2802935606361306 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.11012259  0.53584413 -2.79121815
  3.09782146  2.14161863  1.1624438  -1.02215111  0.61379969  1.20160416
  1.25287728 -1.93215614  0.71242765]. 
[Sun Mar 24 20:18:26 2024]  Iteration number: 50 with current cost as 0.09883688874401034 and parameters 
[-2.90318355  2.2374346  -2.12427967  0.95537948  0.25355227 -1.9243262
  3.92730251  0.49465136  1.19597701  1.12624557  2.07980672  2.4578889
 -0.34773904 -3.6943621   1.17075566]. 
Training complete taking 1644.2831473350525 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 20:18:44 2024]  Iteration number: 0 with current cost as 0.3158571118914961 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10012876  0.52964358 -2.78931118
  3.09312933  2.13569493  1.17390637 -1.01420303  0.61632942  1.21204318
  1.26093092 -1.92610255  0.70708687]. 
[Sun Mar 24 20:23:48 2024]  Iteration number: 50 with current cost as 0.15802374603925967 and parameters 
[-2.90318331  2.23743454 -2.12427992  0.57600278 -1.09959662 -2.77856063
  3.00188253  2.19177881  2.78862963 -0.79742068  0.47991528  2.5058187
  0.21984448 -2.19917186 -0.37957797]. 
Working on 0.4 fold... 
[Sun Mar 24 20:23:59 2024]  Iteration number: 0 with current cost as 0.2673213341857351 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10627884  0.54312293 -2.77514319
  3.08820188  2.17010728  1.16280328 -1.03368807  0.62922536  1.19242255
  1.24759391 -1.92957904  0.73937425]. 
Working on 0.6 fold... 
[Sun Mar 24 20:29:30 2024]  Iteration number: 0 with current cost as 0.2731554112126978 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.11062596  0.5351918  -2.79284563
  3.0831292   2.15169526  1.18203395 -1.02262672  0.62505989  1.20454741
  1.25119578 -1.93043863  0.7242161 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:34:04 2024]  Iteration number: 0 with current cost as 0.28042696181453697 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.11373358  0.53829865 -2.79156727
  3.08791444  2.15531654  1.17172212 -1.02289931  0.6260587   1.2045288
  1.24455613 -1.93628971  0.72553925]. 
[Sun Mar 24 20:39:11 2024]  Iteration number: 50 with current cost as 0.09146224204641965 and parameters 
[-2.90318329  2.2374346  -2.12427967  0.61924569  0.34600076 -1.66836898
  3.35170411  3.61778649  1.51476541  1.04008414  1.9611062   2.42925986
  1.60510093 -1.50222167  0.39166901]. 
Working on 1.0 fold... 
[Sun Mar 24 20:39:42 2024]  Iteration number: 0 with current cost as 0.2802935606361306 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.11012259  0.53584413 -2.79121815
  3.09782146  2.14161863  1.1624438  -1.02215111  0.61379969  1.20160416
  1.25287728 -1.93215614  0.71242765]. 
[Sun Mar 24 20:45:41 2024]  Iteration number: 50 with current cost as 0.09883688874401034 and parameters 
[-2.90318355  2.2374346  -2.12427967  0.95537948  0.25355227 -1.9243262
  3.92730251  0.49465136  1.19597701  1.12624557  2.07980672  2.4578889
 -0.34773904 -3.6943621   1.17075566]. 
Training complete taking 1634.96741604805 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 20:45:57 2024]  Iteration number: 0 with current cost as 0.3158571118914961 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10012876  0.52964358 -2.78931118
  3.09312933  2.13569493  1.17390637 -1.01420303  0.61632942  1.21204318
  1.26093092 -1.92610255  0.70708687]. 
[Sun Mar 24 20:51:07 2024]  Iteration number: 50 with current cost as 0.15802374603925967 and parameters 
[-2.90318331  2.23743454 -2.12427992  0.57600278 -1.09959662 -2.77856063
  3.00188253  2.19177881  2.78862963 -0.79742068  0.47991528  2.5058187
  0.21984448 -2.19917186 -0.37957797]. 
Working on 0.4 fold... 
[Sun Mar 24 20:51:21 2024]  Iteration number: 0 with current cost as 0.2673213341857351 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10627884  0.54312293 -2.77514319
  3.08820188  2.17010728  1.16280328 -1.03368807  0.62922536  1.19242255
  1.24759391 -1.92957904  0.73937425]. 
Working on 0.6 fold... 
[Sun Mar 24 20:57:04 2024]  Iteration number: 0 with current cost as 0.2731554112126978 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.11062596  0.5351918  -2.79284563
  3.0831292   2.15169526  1.18203395 -1.02262672  0.62505989  1.20454741
  1.25119578 -1.93043863  0.7242161 ]. 
Working on 0.8 fold... 
[Sun Mar 24 21:01:44 2024]  Iteration number: 0 with current cost as 0.28042696181453697 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.11373358  0.53829865 -2.79156727
  3.08791444  2.15531654  1.17172212 -1.02289931  0.6260587   1.2045288
  1.24455613 -1.93628971  0.72553925]. 
[Sun Mar 24 21:06:48 2024]  Iteration number: 50 with current cost as 0.09146224204641965 and parameters 
[-2.90318329  2.2374346  -2.12427967  0.61924569  0.34600076 -1.66836898
  3.35170411  3.61778649  1.51476541  1.04008414  1.9611062   2.42925986
  1.60510093 -1.50222167  0.39166901]. 
Working on 1.0 fold... 
[Sun Mar 24 21:07:18 2024]  Iteration number: 0 with current cost as 0.2802935606361306 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.11012259  0.53584413 -2.79121815
  3.09782146  2.14161863  1.1624438  -1.02215111  0.61379969  1.20160416
  1.25287728 -1.93215614  0.71242765]. 
[Sun Mar 24 21:13:15 2024]  Iteration number: 50 with current cost as 0.09883688874401034 and parameters 
[-2.90318355  2.2374346  -2.12427967  0.95537948  0.25355227 -1.9243262
  3.92730251  0.49465136  1.19597701  1.12624557  2.07980672  2.4578889
 -0.34773904 -3.6943621   1.17075566]. 
Training complete taking 1654.4175448417664 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 21:13:32 2024]  Iteration number: 0 with current cost as 0.3158571118914961 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10012876  0.52964358 -2.78931118
  3.09312933  2.13569493  1.17390637 -1.01420303  0.61632942  1.21204318
  1.26093092 -1.92610255  0.70708687]. 
[Sun Mar 24 21:18:37 2024]  Iteration number: 50 with current cost as 0.15802374603925967 and parameters 
[-2.90318331  2.23743454 -2.12427992  0.57600278 -1.09959662 -2.77856063
  3.00188253  2.19177881  2.78862963 -0.79742068  0.47991528  2.5058187
  0.21984448 -2.19917186 -0.37957797]. 
Working on 0.4 fold... 
[Sun Mar 24 21:18:50 2024]  Iteration number: 0 with current cost as 0.2673213341857351 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10627884  0.54312293 -2.77514319
  3.08820188  2.17010728  1.16280328 -1.03368807  0.62922536  1.19242255
  1.24759391 -1.92957904  0.73937425]. 
Working on 0.6 fold... 
[Sun Mar 24 21:24:27 2024]  Iteration number: 0 with current cost as 0.2731554112126978 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.11062596  0.5351918  -2.79284563
  3.0831292   2.15169526  1.18203395 -1.02262672  0.62505989  1.20454741
  1.25119578 -1.93043863  0.7242161 ]. 
Working on 0.8 fold... 
[Sun Mar 24 21:29:01 2024]  Iteration number: 0 with current cost as 0.28042696181453697 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.11373358  0.53829865 -2.79156727
  3.08791444  2.15531654  1.17172212 -1.02289931  0.6260587   1.2045288
  1.24455613 -1.93628971  0.72553925]. 
[Sun Mar 24 21:34:06 2024]  Iteration number: 50 with current cost as 0.09146224204641965 and parameters 
[-2.90318329  2.2374346  -2.12427967  0.61924569  0.34600076 -1.66836898
  3.35170411  3.61778649  1.51476541  1.04008414  1.9611062   2.42925986
  1.60510093 -1.50222167  0.39166901]. 
Working on 1.0 fold... 
[Sun Mar 24 21:34:38 2024]  Iteration number: 0 with current cost as 0.2802935606361306 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.11012259  0.53584413 -2.79121815
  3.09782146  2.14161863  1.1624438  -1.02215111  0.61379969  1.20160416
  1.25287728 -1.93215614  0.71242765]. 
[Sun Mar 24 21:40:40 2024]  Iteration number: 50 with current cost as 0.09883688874401034 and parameters 
[-2.90318355  2.2374346  -2.12427967  0.95537948  0.25355227 -1.9243262
  3.92730251  0.49465136  1.19597701  1.12624557  2.07980672  2.4578889
 -0.34773904 -3.6943621   1.17075566]. 
Training complete taking 1647.2059786319733 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 21:40:59 2024]  Iteration number: 0 with current cost as 0.3158571118914961 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10012876  0.52964358 -2.78931118
  3.09312933  2.13569493  1.17390637 -1.01420303  0.61632942  1.21204318
  1.26093092 -1.92610255  0.70708687]. 
[Sun Mar 24 21:46:04 2024]  Iteration number: 50 with current cost as 0.15802374603925967 and parameters 
[-2.90318331  2.23743454 -2.12427992  0.57600278 -1.09959662 -2.77856063
  3.00188253  2.19177881  2.78862963 -0.79742068  0.47991528  2.5058187
  0.21984448 -2.19917186 -0.37957797]. 
Working on 0.4 fold... 
[Sun Mar 24 21:46:17 2024]  Iteration number: 0 with current cost as 0.2673213341857351 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10627884  0.54312293 -2.77514319
  3.08820188  2.17010728  1.16280328 -1.03368807  0.62922536  1.19242255
  1.24759391 -1.92957904  0.73937425]. 
Working on 0.6 fold... 
[Sun Mar 24 21:51:50 2024]  Iteration number: 0 with current cost as 0.2731554112126978 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.11062596  0.5351918  -2.79284563
  3.0831292   2.15169526  1.18203395 -1.02262672  0.62505989  1.20454741
  1.25119578 -1.93043863  0.7242161 ]. 
Working on 0.8 fold... 
[Sun Mar 24 21:56:22 2024]  Iteration number: 0 with current cost as 0.28042696181453697 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.11373358  0.53829865 -2.79156727
  3.08791444  2.15531654  1.17172212 -1.02289931  0.6260587   1.2045288
  1.24455613 -1.93628971  0.72553925]. 
[Sun Mar 24 22:01:36 2024]  Iteration number: 50 with current cost as 0.09146224204641965 and parameters 
[-2.90318329  2.2374346  -2.12427967  0.61924569  0.34600076 -1.66836898
  3.35170411  3.61778649  1.51476541  1.04008414  1.9611062   2.42925986
  1.60510093 -1.50222167  0.39166901]. 
Working on 1.0 fold... 
[Sun Mar 24 22:02:10 2024]  Iteration number: 0 with current cost as 0.2802935606361306 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.11012259  0.53584413 -2.79121815
  3.09782146  2.14161863  1.1624438  -1.02215111  0.61379969  1.20160416
  1.25287728 -1.93215614  0.71242765]. 
[Sun Mar 24 22:08:21 2024]  Iteration number: 50 with current cost as 0.09883688874401034 and parameters 
[-2.90318355  2.2374346  -2.12427967  0.95537948  0.25355227 -1.9243262
  3.92730251  0.49465136  1.19597701  1.12624557  2.07980672  2.4578889
 -0.34773904 -3.6943621   1.17075566]. 
Training complete taking 1658.3552012443542 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 22:08:37 2024]  Iteration number: 0 with current cost as 0.3158571118914961 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10012876  0.52964358 -2.78931118
  3.09312933  2.13569493  1.17390637 -1.01420303  0.61632942  1.21204318
  1.26093092 -1.92610255  0.70708687]. 
[Sun Mar 24 22:13:43 2024]  Iteration number: 50 with current cost as 0.15802374603925967 and parameters 
[-2.90318331  2.23743454 -2.12427992  0.57600278 -1.09959662 -2.77856063
  3.00188253  2.19177881  2.78862963 -0.79742068  0.47991528  2.5058187
  0.21984448 -2.19917186 -0.37957797]. 
Working on 0.4 fold... 
[Sun Mar 24 22:13:56 2024]  Iteration number: 0 with current cost as 0.2673213341857351 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10627884  0.54312293 -2.77514319
  3.08820188  2.17010728  1.16280328 -1.03368807  0.62922536  1.19242255
  1.24759391 -1.92957904  0.73937425]. 
Working on 0.6 fold... 
[Sun Mar 24 22:19:32 2024]  Iteration number: 0 with current cost as 0.2731554112126978 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.11062596  0.5351918  -2.79284563
  3.0831292   2.15169526  1.18203395 -1.02262672  0.62505989  1.20454741
  1.25119578 -1.93043863  0.7242161 ]. 
Working on 0.8 fold... 
[Sun Mar 24 22:24:07 2024]  Iteration number: 0 with current cost as 0.28042696181453697 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.11373358  0.53829865 -2.79156727
  3.08791444  2.15531654  1.17172212 -1.02289931  0.6260587   1.2045288
  1.24455613 -1.93628971  0.72553925]. 
[Sun Mar 24 22:29:10 2024]  Iteration number: 50 with current cost as 0.09146224204641965 and parameters 
[-2.90318329  2.2374346  -2.12427967  0.61924569  0.34600076 -1.66836898
  3.35170411  3.61778649  1.51476541  1.04008414  1.9611062   2.42925986
  1.60510093 -1.50222167  0.39166901]. 
Working on 1.0 fold... 
[Sun Mar 24 22:29:40 2024]  Iteration number: 0 with current cost as 0.2802935606361306 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.11012259  0.53584413 -2.79121815
  3.09782146  2.14161863  1.1624438  -1.02215111  0.61379969  1.20160416
  1.25287728 -1.93215614  0.71242765]. 
[Sun Mar 24 22:35:41 2024]  Iteration number: 50 with current cost as 0.09883688874401034 and parameters 
[-2.90318355  2.2374346  -2.12427967  0.95537948  0.25355227 -1.9243262
  3.92730251  0.49465136  1.19597701  1.12624557  2.07980672  2.4578889
 -0.34773904 -3.6943621   1.17075566]. 
Training complete taking 1642.5004017353058 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 22:36:00 2024]  Iteration number: 0 with current cost as 0.3158571118914961 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10012876  0.52964358 -2.78931118
  3.09312933  2.13569493  1.17390637 -1.01420303  0.61632942  1.21204318
  1.26093092 -1.92610255  0.70708687]. 
[Sun Mar 24 22:41:12 2024]  Iteration number: 50 with current cost as 0.15802374603925967 and parameters 
[-2.90318331  2.23743454 -2.12427992  0.57600278 -1.09959662 -2.77856063
  3.00188253  2.19177881  2.78862963 -0.79742068  0.47991528  2.5058187
  0.21984448 -2.19917186 -0.37957797]. 
Working on 0.4 fold... 
[Sun Mar 24 22:41:25 2024]  Iteration number: 0 with current cost as 0.2673213341857351 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10627884  0.54312293 -2.77514319
  3.08820188  2.17010728  1.16280328 -1.03368807  0.62922536  1.19242255
  1.24759391 -1.92957904  0.73937425]. 
Working on 0.6 fold... 
[Sun Mar 24 22:47:07 2024]  Iteration number: 0 with current cost as 0.2731554112126978 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.11062596  0.5351918  -2.79284563
  3.0831292   2.15169526  1.18203395 -1.02262672  0.62505989  1.20454741
  1.25119578 -1.93043863  0.7242161 ]. 
Working on 0.8 fold... 
[Sun Mar 24 22:52:05 2024]  Iteration number: 0 with current cost as 0.28042696181453697 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.11373358  0.53829865 -2.79156727
  3.08791444  2.15531654  1.17172212 -1.02289931  0.6260587   1.2045288
  1.24455613 -1.93628971  0.72553925]. 
[Sun Mar 24 22:57:19 2024]  Iteration number: 50 with current cost as 0.09146224204641965 and parameters 
[-2.90318329  2.2374346  -2.12427967  0.61924569  0.34600076 -1.66836898
  3.35170411  3.61778649  1.51476541  1.04008414  1.9611062   2.42925986
  1.60510093 -1.50222167  0.39166901]. 
Working on 1.0 fold... 
[Sun Mar 24 22:57:49 2024]  Iteration number: 0 with current cost as 0.2802935606361306 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.11012259  0.53584413 -2.79121815
  3.09782146  2.14161863  1.1624438  -1.02215111  0.61379969  1.20160416
  1.25287728 -1.93215614  0.71242765]. 
[Sun Mar 24 23:03:46 2024]  Iteration number: 50 with current cost as 0.09883688874401034 and parameters 
[-2.90318355  2.2374346  -2.12427967  0.95537948  0.25355227 -1.9243262
  3.92730251  0.49465136  1.19597701  1.12624557  2.07980672  2.4578889
 -0.34773904 -3.6943621   1.17075566]. 
Training complete taking 1684.7663629055023 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 23:04:05 2024]  Iteration number: 0 with current cost as 0.3158571118914961 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10012876  0.52964358 -2.78931118
  3.09312933  2.13569493  1.17390637 -1.01420303  0.61632942  1.21204318
  1.26093092 -1.92610255  0.70708687]. 
[Sun Mar 24 23:09:09 2024]  Iteration number: 50 with current cost as 0.15802374603925967 and parameters 
[-2.90318331  2.23743454 -2.12427992  0.57600278 -1.09959662 -2.77856063
  3.00188253  2.19177881  2.78862963 -0.79742068  0.47991528  2.5058187
  0.21984448 -2.19917186 -0.37957797]. 
Working on 0.4 fold... 
[Sun Mar 24 23:09:23 2024]  Iteration number: 0 with current cost as 0.2673213341857351 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10627884  0.54312293 -2.77514319
  3.08820188  2.17010728  1.16280328 -1.03368807  0.62922536  1.19242255
  1.24759391 -1.92957904  0.73937425]. 
Working on 0.6 fold... 
[Sun Mar 24 23:14:58 2024]  Iteration number: 0 with current cost as 0.2731554112126978 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.11062596  0.5351918  -2.79284563
  3.0831292   2.15169526  1.18203395 -1.02262672  0.62505989  1.20454741
  1.25119578 -1.93043863  0.7242161 ]. 
Working on 0.8 fold... 
[Sun Mar 24 23:19:37 2024]  Iteration number: 0 with current cost as 0.28042696181453697 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.11373358  0.53829865 -2.79156727
  3.08791444  2.15531654  1.17172212 -1.02289931  0.6260587   1.2045288
  1.24455613 -1.93628971  0.72553925]. 
[Sun Mar 24 23:25:13 2024]  Iteration number: 50 with current cost as 0.09146224204641965 and parameters 
[-2.90318329  2.2374346  -2.12427967  0.61924569  0.34600076 -1.66836898
  3.35170411  3.61778649  1.51476541  1.04008414  1.9611062   2.42925986
  1.60510093 -1.50222167  0.39166901]. 
Working on 1.0 fold... 
[Sun Mar 24 23:25:43 2024]  Iteration number: 0 with current cost as 0.2802935606361306 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.11012259  0.53584413 -2.79121815
  3.09782146  2.14161863  1.1624438  -1.02215111  0.61379969  1.20160416
  1.25287728 -1.93215614  0.71242765]. 
[Sun Mar 24 23:31:40 2024]  Iteration number: 50 with current cost as 0.09883688874401034 and parameters 
[-2.90318355  2.2374346  -2.12427967  0.95537948  0.25355227 -1.9243262
  3.92730251  0.49465136  1.19597701  1.12624557  2.07980672  2.4578889
 -0.34773904 -3.6943621   1.17075566]. 
Training complete taking 1672.432103395462 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 23:31:59 2024]  Iteration number: 0 with current cost as 0.3158571118914961 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10012876  0.52964358 -2.78931118
  3.09312933  2.13569493  1.17390637 -1.01420303  0.61632942  1.21204318
  1.26093092 -1.92610255  0.70708687]. 
[Sun Mar 24 23:37:03 2024]  Iteration number: 50 with current cost as 0.15802374603925967 and parameters 
[-2.90318331  2.23743454 -2.12427992  0.57600278 -1.09959662 -2.77856063
  3.00188253  2.19177881  2.78862963 -0.79742068  0.47991528  2.5058187
  0.21984448 -2.19917186 -0.37957797]. 
Working on 0.4 fold... 
[Sun Mar 24 23:37:14 2024]  Iteration number: 0 with current cost as 0.2673213341857351 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10627884  0.54312293 -2.77514319
  3.08820188  2.17010728  1.16280328 -1.03368807  0.62922536  1.19242255
  1.24759391 -1.92957904  0.73937425]. 
Working on 0.6 fold... 
[Sun Mar 24 23:42:50 2024]  Iteration number: 0 with current cost as 0.2731554112126978 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.11062596  0.5351918  -2.79284563
  3.0831292   2.15169526  1.18203395 -1.02262672  0.62505989  1.20454741
  1.25119578 -1.93043863  0.7242161 ]. 
Working on 0.8 fold... 
[Sun Mar 24 23:47:25 2024]  Iteration number: 0 with current cost as 0.28042696181453697 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.11373358  0.53829865 -2.79156727
  3.08791444  2.15531654  1.17172212 -1.02289931  0.6260587   1.2045288
  1.24455613 -1.93628971  0.72553925]. 
[Sun Mar 24 23:52:30 2024]  Iteration number: 50 with current cost as 0.09146224204641965 and parameters 
[-2.90318329  2.2374346  -2.12427967  0.61924569  0.34600076 -1.66836898
  3.35170411  3.61778649  1.51476541  1.04008414  1.9611062   2.42925986
  1.60510093 -1.50222167  0.39166901]. 
Working on 1.0 fold... 
[Sun Mar 24 23:53:00 2024]  Iteration number: 0 with current cost as 0.2802935606361306 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.11012259  0.53584413 -2.79121815
  3.09782146  2.14161863  1.1624438  -1.02215111  0.61379969  1.20160416
  1.25287728 -1.93215614  0.71242765]. 
[Sun Mar 24 23:59:09 2024]  Iteration number: 50 with current cost as 0.09883688874401034 and parameters 
[-2.90318355  2.2374346  -2.12427967  0.95537948  0.25355227 -1.9243262
  3.92730251  0.49465136  1.19597701  1.12624557  2.07980672  2.4578889
 -0.34773904 -3.6943621   1.17075566]. 
Training complete taking 1648.1622014045715 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 23:59:27 2024]  Iteration number: 0 with current cost as 0.3158571118914961 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10012876  0.52964358 -2.78931118
  3.09312933  2.13569493  1.17390637 -1.01420303  0.61632942  1.21204318
  1.26093092 -1.92610255  0.70708687]. 
[Mon Mar 25 00:04:35 2024]  Iteration number: 50 with current cost as 0.15802374603925967 and parameters 
[-2.90318331  2.23743454 -2.12427992  0.57600278 -1.09959662 -2.77856063
  3.00188253  2.19177881  2.78862963 -0.79742068  0.47991528  2.5058187
  0.21984448 -2.19917186 -0.37957797]. 
Working on 0.4 fold... 
[Mon Mar 25 00:04:46 2024]  Iteration number: 0 with current cost as 0.2673213341857351 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10627884  0.54312293 -2.77514319
  3.08820188  2.17010728  1.16280328 -1.03368807  0.62922536  1.19242255
  1.24759391 -1.92957904  0.73937425]. 
Working on 0.6 fold... 
[Mon Mar 25 00:10:19 2024]  Iteration number: 0 with current cost as 0.2731554112126978 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.11062596  0.5351918  -2.79284563
  3.0831292   2.15169526  1.18203395 -1.02262672  0.62505989  1.20454741
  1.25119578 -1.93043863  0.7242161 ]. 
Working on 0.8 fold... 
[Mon Mar 25 00:14:56 2024]  Iteration number: 0 with current cost as 0.28042696181453697 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.11373358  0.53829865 -2.79156727
  3.08791444  2.15531654  1.17172212 -1.02289931  0.6260587   1.2045288
  1.24455613 -1.93628971  0.72553925]. 
[Mon Mar 25 00:20:01 2024]  Iteration number: 50 with current cost as 0.09146224204641965 and parameters 
[-2.90318329  2.2374346  -2.12427967  0.61924569  0.34600076 -1.66836898
  3.35170411  3.61778649  1.51476541  1.04008414  1.9611062   2.42925986
  1.60510093 -1.50222167  0.39166901]. 
Working on 1.0 fold... 
[Mon Mar 25 00:20:31 2024]  Iteration number: 0 with current cost as 0.2802935606361306 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.11012259  0.53584413 -2.79121815
  3.09782146  2.14161863  1.1624438  -1.02215111  0.61379969  1.20160416
  1.25287728 -1.93215614  0.71242765]. 
[Mon Mar 25 00:26:29 2024]  Iteration number: 50 with current cost as 0.09883688874401034 and parameters 
[-2.90318355  2.2374346  -2.12427967  0.95537948  0.25355227 -1.9243262
  3.92730251  0.49465136  1.19597701  1.12624557  2.07980672  2.4578889
 -0.34773904 -3.6943621   1.17075566]. 
Training complete taking 1640.5747020244598 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 00:26:46 2024]  Iteration number: 0 with current cost as 0.3158571118914961 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10012876  0.52964358 -2.78931118
  3.09312933  2.13569493  1.17390637 -1.01420303  0.61632942  1.21204318
  1.26093092 -1.92610255  0.70708687]. 
[Mon Mar 25 00:31:51 2024]  Iteration number: 50 with current cost as 0.15802374603925967 and parameters 
[-2.90318331  2.23743454 -2.12427992  0.57600278 -1.09959662 -2.77856063
  3.00188253  2.19177881  2.78862963 -0.79742068  0.47991528  2.5058187
  0.21984448 -2.19917186 -0.37957797]. 
Working on 0.4 fold... 
[Mon Mar 25 00:32:04 2024]  Iteration number: 0 with current cost as 0.2673213341857351 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10627884  0.54312293 -2.77514319
  3.08820188  2.17010728  1.16280328 -1.03368807  0.62922536  1.19242255
  1.24759391 -1.92957904  0.73937425]. 
Working on 0.6 fold... 
[Mon Mar 25 00:37:48 2024]  Iteration number: 0 with current cost as 0.2731554112126978 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.11062596  0.5351918  -2.79284563
  3.0831292   2.15169526  1.18203395 -1.02262672  0.62505989  1.20454741
  1.25119578 -1.93043863  0.7242161 ]. 
Working on 0.8 fold... 
[Mon Mar 25 00:42:32 2024]  Iteration number: 0 with current cost as 0.28042696181453697 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.11373358  0.53829865 -2.79156727
  3.08791444  2.15531654  1.17172212 -1.02289931  0.6260587   1.2045288
  1.24455613 -1.93628971  0.72553925]. 
[Mon Mar 25 00:47:35 2024]  Iteration number: 50 with current cost as 0.09146224204641965 and parameters 
[-2.90318329  2.2374346  -2.12427967  0.61924569  0.34600076 -1.66836898
  3.35170411  3.61778649  1.51476541  1.04008414  1.9611062   2.42925986
  1.60510093 -1.50222167  0.39166901]. 
Working on 1.0 fold... 
[Mon Mar 25 00:48:05 2024]  Iteration number: 0 with current cost as 0.2802935606361306 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.11012259  0.53584413 -2.79121815
  3.09782146  2.14161863  1.1624438  -1.02215111  0.61379969  1.20160416
  1.25287728 -1.93215614  0.71242765]. 
[Mon Mar 25 00:54:04 2024]  Iteration number: 50 with current cost as 0.09883688874401034 and parameters 
[-2.90318355  2.2374346  -2.12427967  0.95537948  0.25355227 -1.9243262
  3.92730251  0.49465136  1.19597701  1.12624557  2.07980672  2.4578889
 -0.34773904 -3.6943621   1.17075566]. 
Training complete taking 1654.548422574997 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 00:54:20 2024]  Iteration number: 0 with current cost as 0.3158571118914961 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10012876  0.52964358 -2.78931118
  3.09312933  2.13569493  1.17390637 -1.01420303  0.61632942  1.21204318
  1.26093092 -1.92610255  0.70708687]. 
[Mon Mar 25 00:59:58 2024]  Iteration number: 50 with current cost as 0.15802374603925967 and parameters 
[-2.90318331  2.23743454 -2.12427992  0.57600278 -1.09959662 -2.77856063
  3.00188253  2.19177881  2.78862963 -0.79742068  0.47991528  2.5058187
  0.21984448 -2.19917186 -0.37957797]. 
Working on 0.4 fold... 
[Mon Mar 25 01:00:09 2024]  Iteration number: 0 with current cost as 0.2673213341857351 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10627884  0.54312293 -2.77514319
  3.08820188  2.17010728  1.16280328 -1.03368807  0.62922536  1.19242255
  1.24759391 -1.92957904  0.73937425]. 
Working on 0.6 fold... 
[Mon Mar 25 01:05:46 2024]  Iteration number: 0 with current cost as 0.2731554112126978 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.11062596  0.5351918  -2.79284563
  3.0831292   2.15169526  1.18203395 -1.02262672  0.62505989  1.20454741
  1.25119578 -1.93043863  0.7242161 ]. 
Working on 0.8 fold... 
[Mon Mar 25 01:10:33 2024]  Iteration number: 0 with current cost as 0.28042696181453697 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.11373358  0.53829865 -2.79156727
  3.08791444  2.15531654  1.17172212 -1.02289931  0.6260587   1.2045288
  1.24455613 -1.93628971  0.72553925]. 
[Mon Mar 25 01:15:38 2024]  Iteration number: 50 with current cost as 0.09146224204641965 and parameters 
[-2.90318329  2.2374346  -2.12427967  0.61924569  0.34600076 -1.66836898
  3.35170411  3.61778649  1.51476541  1.04008414  1.9611062   2.42925986
  1.60510093 -1.50222167  0.39166901]. 
Working on 1.0 fold... 
[Mon Mar 25 01:16:08 2024]  Iteration number: 0 with current cost as 0.2802935606361306 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.11012259  0.53584413 -2.79121815
  3.09782146  2.14161863  1.1624438  -1.02215111  0.61379969  1.20160416
  1.25287728 -1.93215614  0.71242765]. 
[Mon Mar 25 01:22:16 2024]  Iteration number: 50 with current cost as 0.09883688874401034 and parameters 
[-2.90318355  2.2374346  -2.12427967  0.95537948  0.25355227 -1.9243262
  3.92730251  0.49465136  1.19597701  1.12624557  2.07980672  2.4578889
 -0.34773904 -3.6943621   1.17075566]. 
Training complete taking 1692.2192471027374 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 01:22:35 2024]  Iteration number: 0 with current cost as 0.3158571118914961 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10012876  0.52964358 -2.78931118
  3.09312933  2.13569493  1.17390637 -1.01420303  0.61632942  1.21204318
  1.26093092 -1.92610255  0.70708687]. 
[Mon Mar 25 01:27:40 2024]  Iteration number: 50 with current cost as 0.15802374603925967 and parameters 
[-2.90318331  2.23743454 -2.12427992  0.57600278 -1.09959662 -2.77856063
  3.00188253  2.19177881  2.78862963 -0.79742068  0.47991528  2.5058187
  0.21984448 -2.19917186 -0.37957797]. 
Working on 0.4 fold... 
[Mon Mar 25 01:27:51 2024]  Iteration number: 0 with current cost as 0.2673213341857351 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10627884  0.54312293 -2.77514319
  3.08820188  2.17010728  1.16280328 -1.03368807  0.62922536  1.19242255
  1.24759391 -1.92957904  0.73937425]. 
Working on 0.6 fold... 
[Mon Mar 25 01:33:28 2024]  Iteration number: 0 with current cost as 0.2731554112126978 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.11062596  0.5351918  -2.79284563
  3.0831292   2.15169526  1.18203395 -1.02262672  0.62505989  1.20454741
  1.25119578 -1.93043863  0.7242161 ]. 
Working on 0.8 fold... 
[Mon Mar 25 01:38:03 2024]  Iteration number: 0 with current cost as 0.28042696181453697 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.11373358  0.53829865 -2.79156727
  3.08791444  2.15531654  1.17172212 -1.02289931  0.6260587   1.2045288
  1.24455613 -1.93628971  0.72553925]. 
[Mon Mar 25 01:43:12 2024]  Iteration number: 50 with current cost as 0.09146224204641965 and parameters 
[-2.90318329  2.2374346  -2.12427967  0.61924569  0.34600076 -1.66836898
  3.35170411  3.61778649  1.51476541  1.04008414  1.9611062   2.42925986
  1.60510093 -1.50222167  0.39166901]. 
Working on 1.0 fold... 
[Mon Mar 25 01:43:42 2024]  Iteration number: 0 with current cost as 0.2802935606361306 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.11012259  0.53584413 -2.79121815
  3.09782146  2.14161863  1.1624438  -1.02215111  0.61379969  1.20160416
  1.25287728 -1.93215614  0.71242765]. 
[Mon Mar 25 01:49:41 2024]  Iteration number: 50 with current cost as 0.09883688874401034 and parameters 
[-2.90318355  2.2374346  -2.12427967  0.95537948  0.25355227 -1.9243262
  3.92730251  0.49465136  1.19597701  1.12624557  2.07980672  2.4578889
 -0.34773904 -3.6943621   1.17075566]. 
Training complete taking 1645.5400862693787 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 01:50:00 2024]  Iteration number: 0 with current cost as 0.3158571118914961 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10012876  0.52964358 -2.78931118
  3.09312933  2.13569493  1.17390637 -1.01420303  0.61632942  1.21204318
  1.26093092 -1.92610255  0.70708687]. 
[Mon Mar 25 01:55:14 2024]  Iteration number: 50 with current cost as 0.15802374603925967 and parameters 
[-2.90318331  2.23743454 -2.12427992  0.57600278 -1.09959662 -2.77856063
  3.00188253  2.19177881  2.78862963 -0.79742068  0.47991528  2.5058187
  0.21984448 -2.19917186 -0.37957797]. 
Working on 0.4 fold... 
[Mon Mar 25 01:55:25 2024]  Iteration number: 0 with current cost as 0.2673213341857351 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10627884  0.54312293 -2.77514319
  3.08820188  2.17010728  1.16280328 -1.03368807  0.62922536  1.19242255
  1.24759391 -1.92957904  0.73937425]. 
Working on 0.6 fold... 
[Mon Mar 25 02:01:06 2024]  Iteration number: 0 with current cost as 0.2731554112126978 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.11062596  0.5351918  -2.79284563
  3.0831292   2.15169526  1.18203395 -1.02262672  0.62505989  1.20454741
  1.25119578 -1.93043863  0.7242161 ]. 
Working on 0.8 fold... 
[Mon Mar 25 02:06:04 2024]  Iteration number: 0 with current cost as 0.28042696181453697 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.11373358  0.53829865 -2.79156727
  3.08791444  2.15531654  1.17172212 -1.02289931  0.6260587   1.2045288
  1.24455613 -1.93628971  0.72553925]. 
[Mon Mar 25 02:11:17 2024]  Iteration number: 50 with current cost as 0.09146224204641965 and parameters 
[-2.90318329  2.2374346  -2.12427967  0.61924569  0.34600076 -1.66836898
  3.35170411  3.61778649  1.51476541  1.04008414  1.9611062   2.42925986
  1.60510093 -1.50222167  0.39166901]. 
Working on 1.0 fold... 
[Mon Mar 25 02:11:47 2024]  Iteration number: 0 with current cost as 0.2802935606361306 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.11012259  0.53584413 -2.79121815
  3.09782146  2.14161863  1.1624438  -1.02215111  0.61379969  1.20160416
  1.25287728 -1.93215614  0.71242765]. 
[Mon Mar 25 02:18:05 2024]  Iteration number: 50 with current cost as 0.09883688874401034 and parameters 
[-2.90318355  2.2374346  -2.12427967  0.95537948  0.25355227 -1.9243262
  3.92730251  0.49465136  1.19597701  1.12624557  2.07980672  2.4578889
 -0.34773904 -3.6943621   1.17075566]. 
Training complete taking 1703.3571717739105 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 02:18:22 2024]  Iteration number: 0 with current cost as 0.3158571118914961 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10012876  0.52964358 -2.78931118
  3.09312933  2.13569493  1.17390637 -1.01420303  0.61632942  1.21204318
  1.26093092 -1.92610255  0.70708687]. 
[Mon Mar 25 02:23:35 2024]  Iteration number: 50 with current cost as 0.15802374603925967 and parameters 
[-2.90318331  2.23743454 -2.12427992  0.57600278 -1.09959662 -2.77856063
  3.00188253  2.19177881  2.78862963 -0.79742068  0.47991528  2.5058187
  0.21984448 -2.19917186 -0.37957797]. 
Working on 0.4 fold... 
[Mon Mar 25 02:23:46 2024]  Iteration number: 0 with current cost as 0.2673213341857351 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10627884  0.54312293 -2.77514319
  3.08820188  2.17010728  1.16280328 -1.03368807  0.62922536  1.19242255
  1.24759391 -1.92957904  0.73937425]. 
Working on 0.6 fold... 
[Mon Mar 25 02:29:21 2024]  Iteration number: 0 with current cost as 0.2731554112126978 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.11062596  0.5351918  -2.79284563
  3.0831292   2.15169526  1.18203395 -1.02262672  0.62505989  1.20454741
  1.25119578 -1.93043863  0.7242161 ]. 
Working on 0.8 fold... 
[Mon Mar 25 02:33:56 2024]  Iteration number: 0 with current cost as 0.28042696181453697 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.11373358  0.53829865 -2.79156727
  3.08791444  2.15531654  1.17172212 -1.02289931  0.6260587   1.2045288
  1.24455613 -1.93628971  0.72553925]. 
[Mon Mar 25 02:39:03 2024]  Iteration number: 50 with current cost as 0.09146224204641965 and parameters 
[-2.90318329  2.2374346  -2.12427967  0.61924569  0.34600076 -1.66836898
  3.35170411  3.61778649  1.51476541  1.04008414  1.9611062   2.42925986
  1.60510093 -1.50222167  0.39166901]. 
Working on 1.0 fold... 
[Mon Mar 25 02:39:33 2024]  Iteration number: 0 with current cost as 0.2802935606361306 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.11012259  0.53584413 -2.79121815
  3.09782146  2.14161863  1.1624438  -1.02215111  0.61379969  1.20160416
  1.25287728 -1.93215614  0.71242765]. 
[Mon Mar 25 02:45:47 2024]  Iteration number: 50 with current cost as 0.09883688874401034 and parameters 
[-2.90318355  2.2374346  -2.12427967  0.95537948  0.25355227 -1.9243262
  3.92730251  0.49465136  1.19597701  1.12624557  2.07980672  2.4578889
 -0.34773904 -3.6943621   1.17075566]. 
Training complete taking 1661.7026283740997 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 02:46:05 2024]  Iteration number: 0 with current cost as 0.3158571118914961 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10012876  0.52964358 -2.78931118
  3.09312933  2.13569493  1.17390637 -1.01420303  0.61632942  1.21204318
  1.26093092 -1.92610255  0.70708687]. 
[Mon Mar 25 02:51:26 2024]  Iteration number: 50 with current cost as 0.15802374603925967 and parameters 
[-2.90318331  2.23743454 -2.12427992  0.57600278 -1.09959662 -2.77856063
  3.00188253  2.19177881  2.78862963 -0.79742068  0.47991528  2.5058187
  0.21984448 -2.19917186 -0.37957797]. 
Working on 0.4 fold... 
[Mon Mar 25 02:51:39 2024]  Iteration number: 0 with current cost as 0.2673213341857351 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10627884  0.54312293 -2.77514319
  3.08820188  2.17010728  1.16280328 -1.03368807  0.62922536  1.19242255
  1.24759391 -1.92957904  0.73937425]. 
Working on 0.6 fold... 
[Mon Mar 25 02:57:27 2024]  Iteration number: 0 with current cost as 0.2731554112126978 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.11062596  0.5351918  -2.79284563
  3.0831292   2.15169526  1.18203395 -1.02262672  0.62505989  1.20454741
  1.25119578 -1.93043863  0.7242161 ]. 
Working on 0.8 fold... 
[Mon Mar 25 03:02:06 2024]  Iteration number: 0 with current cost as 0.28042696181453697 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.11373358  0.53829865 -2.79156727
  3.08791444  2.15531654  1.17172212 -1.02289931  0.6260587   1.2045288
  1.24455613 -1.93628971  0.72553925]. 
[Mon Mar 25 03:07:21 2024]  Iteration number: 50 with current cost as 0.09146224204641965 and parameters 
[-2.90318329  2.2374346  -2.12427967  0.61924569  0.34600076 -1.66836898
  3.35170411  3.61778649  1.51476541  1.04008414  1.9611062   2.42925986
  1.60510093 -1.50222167  0.39166901]. 
Working on 1.0 fold... 
[Mon Mar 25 03:07:51 2024]  Iteration number: 0 with current cost as 0.2802935606361306 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.11012259  0.53584413 -2.79121815
  3.09782146  2.14161863  1.1624438  -1.02215111  0.61379969  1.20160416
  1.25287728 -1.93215614  0.71242765]. 
[Mon Mar 25 03:13:49 2024]  Iteration number: 50 with current cost as 0.09883688874401034 and parameters 
[-2.90318355  2.2374346  -2.12427967  0.95537948  0.25355227 -1.9243262
  3.92730251  0.49465136  1.19597701  1.12624557  2.07980672  2.4578889
 -0.34773904 -3.6943621   1.17075566]. 
Training complete taking 1682.4063618183136 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 03:14:08 2024]  Iteration number: 0 with current cost as 0.3158571118914961 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10012876  0.52964358 -2.78931118
  3.09312933  2.13569493  1.17390637 -1.01420303  0.61632942  1.21204318
  1.26093092 -1.92610255  0.70708687]. 
[Mon Mar 25 03:19:29 2024]  Iteration number: 50 with current cost as 0.15802374603925967 and parameters 
[-2.90318331  2.23743454 -2.12427992  0.57600278 -1.09959662 -2.77856063
  3.00188253  2.19177881  2.78862963 -0.79742068  0.47991528  2.5058187
  0.21984448 -2.19917186 -0.37957797]. 
Working on 0.4 fold... 
[Mon Mar 25 03:19:42 2024]  Iteration number: 0 with current cost as 0.2673213341857351 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10627884  0.54312293 -2.77514319
  3.08820188  2.17010728  1.16280328 -1.03368807  0.62922536  1.19242255
  1.24759391 -1.92957904  0.73937425]. 
Working on 0.6 fold... 
[Mon Mar 25 03:25:22 2024]  Iteration number: 0 with current cost as 0.2731554112126978 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.11062596  0.5351918  -2.79284563
  3.0831292   2.15169526  1.18203395 -1.02262672  0.62505989  1.20454741
  1.25119578 -1.93043863  0.7242161 ]. 
Working on 0.8 fold... 
[Mon Mar 25 03:29:57 2024]  Iteration number: 0 with current cost as 0.28042696181453697 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.11373358  0.53829865 -2.79156727
  3.08791444  2.15531654  1.17172212 -1.02289931  0.6260587   1.2045288
  1.24455613 -1.93628971  0.72553925]. 
[Mon Mar 25 03:35:22 2024]  Iteration number: 50 with current cost as 0.09146224204641965 and parameters 
[-2.90318329  2.2374346  -2.12427967  0.61924569  0.34600076 -1.66836898
  3.35170411  3.61778649  1.51476541  1.04008414  1.9611062   2.42925986
  1.60510093 -1.50222167  0.39166901]. 
Working on 1.0 fold... 
[Mon Mar 25 03:35:52 2024]  Iteration number: 0 with current cost as 0.2802935606361306 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.11012259  0.53584413 -2.79121815
  3.09782146  2.14161863  1.1624438  -1.02215111  0.61379969  1.20160416
  1.25287728 -1.93215614  0.71242765]. 
[Mon Mar 25 03:41:55 2024]  Iteration number: 50 with current cost as 0.09883688874401034 and parameters 
[-2.90318355  2.2374346  -2.12427967  0.95537948  0.25355227 -1.9243262
  3.92730251  0.49465136  1.19597701  1.12624557  2.07980672  2.4578889
 -0.34773904 -3.6943621   1.17075566]. 
Training complete taking 1686.5570352077484 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 03:42:15 2024]  Iteration number: 0 with current cost as 0.3158571118914961 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10012876  0.52964358 -2.78931118
  3.09312933  2.13569493  1.17390637 -1.01420303  0.61632942  1.21204318
  1.26093092 -1.92610255  0.70708687]. 
[Mon Mar 25 03:47:22 2024]  Iteration number: 50 with current cost as 0.15802374603925967 and parameters 
[-2.90318331  2.23743454 -2.12427992  0.57600278 -1.09959662 -2.77856063
  3.00188253  2.19177881  2.78862963 -0.79742068  0.47991528  2.5058187
  0.21984448 -2.19917186 -0.37957797]. 
Working on 0.4 fold... 
[Mon Mar 25 03:47:33 2024]  Iteration number: 0 with current cost as 0.2673213341857351 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10627884  0.54312293 -2.77514319
  3.08820188  2.17010728  1.16280328 -1.03368807  0.62922536  1.19242255
  1.24759391 -1.92957904  0.73937425]. 
Working on 0.6 fold... 
[Mon Mar 25 03:53:09 2024]  Iteration number: 0 with current cost as 0.2731554112126978 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.11062596  0.5351918  -2.79284563
  3.0831292   2.15169526  1.18203395 -1.02262672  0.62505989  1.20454741
  1.25119578 -1.93043863  0.7242161 ]. 
Working on 0.8 fold... 
[Mon Mar 25 03:57:43 2024]  Iteration number: 0 with current cost as 0.28042696181453697 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.11373358  0.53829865 -2.79156727
  3.08791444  2.15531654  1.17172212 -1.02289931  0.6260587   1.2045288
  1.24455613 -1.93628971  0.72553925]. 
[Mon Mar 25 04:03:12 2024]  Iteration number: 50 with current cost as 0.09146224204641965 and parameters 
[-2.90318329  2.2374346  -2.12427967  0.61924569  0.34600076 -1.66836898
  3.35170411  3.61778649  1.51476541  1.04008414  1.9611062   2.42925986
  1.60510093 -1.50222167  0.39166901]. 
Working on 1.0 fold... 
[Mon Mar 25 04:03:42 2024]  Iteration number: 0 with current cost as 0.2802935606361306 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.11012259  0.53584413 -2.79121815
  3.09782146  2.14161863  1.1624438  -1.02215111  0.61379969  1.20160416
  1.25287728 -1.93215614  0.71242765]. 
[Mon Mar 25 04:09:55 2024]  Iteration number: 50 with current cost as 0.09883688874401034 and parameters 
[-2.90318355  2.2374346  -2.12427967  0.95537948  0.25355227 -1.9243262
  3.92730251  0.49465136  1.19597701  1.12624557  2.07980672  2.4578889
 -0.34773904 -3.6943621   1.17075566]. 
Training complete taking 1680.9680576324463 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 04:10:16 2024]  Iteration number: 0 with current cost as 0.3158571118914961 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10012876  0.52964358 -2.78931118
  3.09312933  2.13569493  1.17390637 -1.01420303  0.61632942  1.21204318
  1.26093092 -1.92610255  0.70708687]. 
[Mon Mar 25 04:15:21 2024]  Iteration number: 50 with current cost as 0.15802374603925967 and parameters 
[-2.90318331  2.23743454 -2.12427992  0.57600278 -1.09959662 -2.77856063
  3.00188253  2.19177881  2.78862963 -0.79742068  0.47991528  2.5058187
  0.21984448 -2.19917186 -0.37957797]. 
Working on 0.4 fold... 
[Mon Mar 25 04:15:32 2024]  Iteration number: 0 with current cost as 0.2673213341857351 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10627884  0.54312293 -2.77514319
  3.08820188  2.17010728  1.16280328 -1.03368807  0.62922536  1.19242255
  1.24759391 -1.92957904  0.73937425]. 
Working on 0.6 fold... 
[Mon Mar 25 04:21:09 2024]  Iteration number: 0 with current cost as 0.2731554112126978 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.11062596  0.5351918  -2.79284563
  3.0831292   2.15169526  1.18203395 -1.02262672  0.62505989  1.20454741
  1.25119578 -1.93043863  0.7242161 ]. 
Working on 0.8 fold... 
[Mon Mar 25 04:25:45 2024]  Iteration number: 0 with current cost as 0.28042696181453697 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.11373358  0.53829865 -2.79156727
  3.08791444  2.15531654  1.17172212 -1.02289931  0.6260587   1.2045288
  1.24455613 -1.93628971  0.72553925]. 
[Mon Mar 25 04:30:51 2024]  Iteration number: 50 with current cost as 0.09146224204641965 and parameters 
[-2.90318329  2.2374346  -2.12427967  0.61924569  0.34600076 -1.66836898
  3.35170411  3.61778649  1.51476541  1.04008414  1.9611062   2.42925986
  1.60510093 -1.50222167  0.39166901]. 
Working on 1.0 fold... 
[Mon Mar 25 04:31:20 2024]  Iteration number: 0 with current cost as 0.2802935606361306 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.11012259  0.53584413 -2.79121815
  3.09782146  2.14161863  1.1624438  -1.02215111  0.61379969  1.20160416
  1.25287728 -1.93215614  0.71242765]. 
[Mon Mar 25 04:37:17 2024]  Iteration number: 50 with current cost as 0.09883688874401034 and parameters 
[-2.90318355  2.2374346  -2.12427967  0.95537948  0.25355227 -1.9243262
  3.92730251  0.49465136  1.19597701  1.12624557  2.07980672  2.4578889
 -0.34773904 -3.6943621   1.17075566]. 
Training complete taking 1640.466403722763 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 04:37:34 2024]  Iteration number: 0 with current cost as 0.3158571118914961 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10012876  0.52964358 -2.78931118
  3.09312933  2.13569493  1.17390637 -1.01420303  0.61632942  1.21204318
  1.26093092 -1.92610255  0.70708687]. 
[Mon Mar 25 04:43:07 2024]  Iteration number: 50 with current cost as 0.15802374603925967 and parameters 
[-2.90318331  2.23743454 -2.12427992  0.57600278 -1.09959662 -2.77856063
  3.00188253  2.19177881  2.78862963 -0.79742068  0.47991528  2.5058187
  0.21984448 -2.19917186 -0.37957797]. 
Working on 0.4 fold... 
[Mon Mar 25 04:43:20 2024]  Iteration number: 0 with current cost as 0.2673213341857351 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10627884  0.54312293 -2.77514319
  3.08820188  2.17010728  1.16280328 -1.03368807  0.62922536  1.19242255
  1.24759391 -1.92957904  0.73937425]. 
Working on 0.6 fold... 
[Mon Mar 25 04:49:24 2024]  Iteration number: 0 with current cost as 0.2731554112126978 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.11062596  0.5351918  -2.79284563
  3.0831292   2.15169526  1.18203395 -1.02262672  0.62505989  1.20454741
  1.25119578 -1.93043863  0.7242161 ]. 
Working on 0.8 fold... 
[Mon Mar 25 04:54:28 2024]  Iteration number: 0 with current cost as 0.28042696181453697 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.11373358  0.53829865 -2.79156727
  3.08791444  2.15531654  1.17172212 -1.02289931  0.6260587   1.2045288
  1.24455613 -1.93628971  0.72553925]. 
[Mon Mar 25 04:59:51 2024]  Iteration number: 50 with current cost as 0.09146224204641965 and parameters 
[-2.90318329  2.2374346  -2.12427967  0.61924569  0.34600076 -1.66836898
  3.35170411  3.61778649  1.51476541  1.04008414  1.9611062   2.42925986
  1.60510093 -1.50222167  0.39166901]. 
Working on 1.0 fold... 
[Mon Mar 25 05:00:21 2024]  Iteration number: 0 with current cost as 0.2802935606361306 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.11012259  0.53584413 -2.79121815
  3.09782146  2.14161863  1.1624438  -1.02215111  0.61379969  1.20160416
  1.25287728 -1.93215614  0.71242765]. 
[Mon Mar 25 05:06:28 2024]  Iteration number: 50 with current cost as 0.09883688874401034 and parameters 
[-2.90318355  2.2374346  -2.12427967  0.95537948  0.25355227 -1.9243262
  3.92730251  0.49465136  1.19597701  1.12624557  2.07980672  2.4578889
 -0.34773904 -3.6943621   1.17075566]. 
Training complete taking 1751.5056965351105 seconds. 
Discarding model... 

Training complete taking 41768.449214458466 total seconds. 
Now scoring model... 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Scoring complete taking 0.9245359897613525 seconds. 
Saved predicted values as M-M-CNOT_HWE-CNOT_predicted_values.csv
Model scores: {'MSE_train': (60.082760029532594,), 'R2_train': 0.7089191923876048, 'MAE_train': 5.410453326163731, 'MSE_test': 84.90095578694752, 'R2_test': 0.48823283773172554, 'MAE_test': 5.963098382934954}. 
Saved model results as M-M-CNOT_HWE-CNOT_results.json. 
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/circuits/Encoders.py:12: RuntimeWarning: invalid value encountered in arcsin
  if np.isnan(np.arcsin(features[feature_idx])) or np.isnan(np.arccos(features[feature_idx])):
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Thu Apr  4 11:42:54 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 11:43:05 2024]  Iteration number: 0 with current cost as 0.3158571118914961 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10012876  0.52964358 -2.78931118
  3.09312933  2.13569493  1.17390637 -1.01420303  0.61632942  1.21204318
  1.26093092 -1.92610255  0.70708687]. 
[Thu Apr  4 11:48:03 2024]  Iteration number: 50 with current cost as 0.15802374603925967 and parameters 
[-2.90318331  2.23743454 -2.12427992  0.57600278 -1.09959662 -2.77856063
  3.00188253  2.19177881  2.78862963 -0.79742068  0.47991528  2.5058187
  0.21984448 -2.19917186 -0.37957797]. 
Working on 0.4 fold... 
[Thu Apr  4 11:48:14 2024]  Iteration number: 0 with current cost as 0.2673213341857351 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10627884  0.54312293 -2.77514319
  3.08820188  2.17010728  1.16280328 -1.03368807  0.62922536  1.19242255
  1.24759391 -1.92957904  0.73937425]. 
Working on 0.6 fold... 
[Thu Apr  4 11:53:45 2024]  Iteration number: 0 with current cost as 0.2731554112126978 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.11062596  0.5351918  -2.79284563
  3.0831292   2.15169526  1.18203395 -1.02262672  0.62505989  1.20454741
  1.25119578 -1.93043863  0.7242161 ]. 
Working on 0.8 fold... 
[Thu Apr  4 11:58:28 2024]  Iteration number: 0 with current cost as 0.28042696181453697 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.11373358  0.53829865 -2.79156727
  3.08791444  2.15531654  1.17172212 -1.02289931  0.6260587   1.2045288
  1.24455613 -1.93628971  0.72553925]. 
[Thu Apr  4 12:03:36 2024]  Iteration number: 50 with current cost as 0.09146224204641965 and parameters 
[-2.90318329  2.2374346  -2.12427967  0.61924569  0.34600076 -1.66836898
  3.35170411  3.61778649  1.51476541  1.04008414  1.9611062   2.42925986
  1.60510093 -1.50222167  0.39166901]. 
Working on 1.0 fold... 
[Thu Apr  4 12:04:06 2024]  Iteration number: 0 with current cost as 0.2802935606361306 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.11012259  0.53584413 -2.79121815
  3.09782146  2.14161863  1.1624438  -1.02215111  0.61379969  1.20160416
  1.25287728 -1.93215614  0.71242765]. 
[Thu Apr  4 12:10:01 2024]  Iteration number: 50 with current cost as 0.09883688874401034 and parameters 
[-2.90318355  2.2374346  -2.12427967  0.95537948  0.25355227 -1.9243262
  3.92730251  0.49465136  1.19597701  1.12624557  2.07980672  2.4578889
 -0.34773904 -3.6943621   1.17075566]. 
Training complete taking 1633.118770122528 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 12:10:21 2024]  Iteration number: 0 with current cost as 0.3158571118914961 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10012876  0.52964358 -2.78931118
  3.09312933  2.13569493  1.17390637 -1.01420303  0.61632942  1.21204318
  1.26093092 -1.92610255  0.70708687]. 
[Thu Apr  4 12:15:23 2024]  Iteration number: 50 with current cost as 0.15802374603925967 and parameters 
[-2.90318331  2.23743454 -2.12427992  0.57600278 -1.09959662 -2.77856063
  3.00188253  2.19177881  2.78862963 -0.79742068  0.47991528  2.5058187
  0.21984448 -2.19917186 -0.37957797]. 
Working on 0.4 fold... 
[Thu Apr  4 12:15:35 2024]  Iteration number: 0 with current cost as 0.2673213341857351 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10627884  0.54312293 -2.77514319
  3.08820188  2.17010728  1.16280328 -1.03368807  0.62922536  1.19242255
  1.24759391 -1.92957904  0.73937425]. 
Working on 0.6 fold... 
[Thu Apr  4 12:21:07 2024]  Iteration number: 0 with current cost as 0.2731554112126978 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.11062596  0.5351918  -2.79284563
  3.0831292   2.15169526  1.18203395 -1.02262672  0.62505989  1.20454741
  1.25119578 -1.93043863  0.7242161 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:25:38 2024]  Iteration number: 0 with current cost as 0.28042696181453697 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.11373358  0.53829865 -2.79156727
  3.08791444  2.15531654  1.17172212 -1.02289931  0.6260587   1.2045288
  1.24455613 -1.93628971  0.72553925]. 
[Thu Apr  4 12:30:37 2024]  Iteration number: 50 with current cost as 0.09146224204641965 and parameters 
[-2.90318329  2.2374346  -2.12427967  0.61924569  0.34600076 -1.66836898
  3.35170411  3.61778649  1.51476541  1.04008414  1.9611062   2.42925986
  1.60510093 -1.50222167  0.39166901]. 
Working on 1.0 fold... 
[Thu Apr  4 12:31:08 2024]  Iteration number: 0 with current cost as 0.2802935606361306 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.11012259  0.53584413 -2.79121815
  3.09782146  2.14161863  1.1624438  -1.02215111  0.61379969  1.20160416
  1.25287728 -1.93215614  0.71242765]. 
[Thu Apr  4 12:37:00 2024]  Iteration number: 50 with current cost as 0.09883688874401034 and parameters 
[-2.90318355  2.2374346  -2.12427967  0.95537948  0.25355227 -1.9243262
  3.92730251  0.49465136  1.19597701  1.12624557  2.07980672  2.4578889
 -0.34773904 -3.6943621   1.17075566]. 
Training complete taking 1618.8900227546692 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 12:37:17 2024]  Iteration number: 0 with current cost as 0.3158571118914961 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10012876  0.52964358 -2.78931118
  3.09312933  2.13569493  1.17390637 -1.01420303  0.61632942  1.21204318
  1.26093092 -1.92610255  0.70708687]. 
[Thu Apr  4 12:42:20 2024]  Iteration number: 50 with current cost as 0.15802374603925967 and parameters 
[-2.90318331  2.23743454 -2.12427992  0.57600278 -1.09959662 -2.77856063
  3.00188253  2.19177881  2.78862963 -0.79742068  0.47991528  2.5058187
  0.21984448 -2.19917186 -0.37957797]. 
Working on 0.4 fold... 
[Thu Apr  4 12:42:32 2024]  Iteration number: 0 with current cost as 0.2673213341857351 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10627884  0.54312293 -2.77514319
  3.08820188  2.17010728  1.16280328 -1.03368807  0.62922536  1.19242255
  1.24759391 -1.92957904  0.73937425]. 
Working on 0.6 fold... 
[Thu Apr  4 12:48:04 2024]  Iteration number: 0 with current cost as 0.2731554112126978 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.11062596  0.5351918  -2.79284563
  3.0831292   2.15169526  1.18203395 -1.02262672  0.62505989  1.20454741
  1.25119578 -1.93043863  0.7242161 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:52:33 2024]  Iteration number: 0 with current cost as 0.28042696181453697 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.11373358  0.53829865 -2.79156727
  3.08791444  2.15531654  1.17172212 -1.02289931  0.6260587   1.2045288
  1.24455613 -1.93628971  0.72553925]. 
[Thu Apr  4 12:57:46 2024]  Iteration number: 50 with current cost as 0.09146224204641965 and parameters 
[-2.90318329  2.2374346  -2.12427967  0.61924569  0.34600076 -1.66836898
  3.35170411  3.61778649  1.51476541  1.04008414  1.9611062   2.42925986
  1.60510093 -1.50222167  0.39166901]. 
Working on 1.0 fold... 
[Thu Apr  4 12:58:15 2024]  Iteration number: 0 with current cost as 0.2802935606361306 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.11012259  0.53584413 -2.79121815
  3.09782146  2.14161863  1.1624438  -1.02215111  0.61379969  1.20160416
  1.25287728 -1.93215614  0.71242765]. 
[Thu Apr  4 13:04:08 2024]  Iteration number: 50 with current cost as 0.09883688874401034 and parameters 
[-2.90318355  2.2374346  -2.12427967  0.95537948  0.25355227 -1.9243262
  3.92730251  0.49465136  1.19597701  1.12624557  2.07980672  2.4578889
 -0.34773904 -3.6943621   1.17075566]. 
Training complete taking 1627.5776977539062 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 13:04:26 2024]  Iteration number: 0 with current cost as 0.3158571118914961 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10012876  0.52964358 -2.78931118
  3.09312933  2.13569493  1.17390637 -1.01420303  0.61632942  1.21204318
  1.26093092 -1.92610255  0.70708687]. 
[Thu Apr  4 13:09:26 2024]  Iteration number: 50 with current cost as 0.15802374603925967 and parameters 
[-2.90318331  2.23743454 -2.12427992  0.57600278 -1.09959662 -2.77856063
  3.00188253  2.19177881  2.78862963 -0.79742068  0.47991528  2.5058187
  0.21984448 -2.19917186 -0.37957797]. 
Working on 0.4 fold... 
[Thu Apr  4 13:09:38 2024]  Iteration number: 0 with current cost as 0.2673213341857351 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10627884  0.54312293 -2.77514319
  3.08820188  2.17010728  1.16280328 -1.03368807  0.62922536  1.19242255
  1.24759391 -1.92957904  0.73937425]. 
Working on 0.6 fold... 
[Thu Apr  4 13:15:05 2024]  Iteration number: 0 with current cost as 0.2731554112126978 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.11062596  0.5351918  -2.79284563
  3.0831292   2.15169526  1.18203395 -1.02262672  0.62505989  1.20454741
  1.25119578 -1.93043863  0.7242161 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:19:37 2024]  Iteration number: 0 with current cost as 0.28042696181453697 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.11373358  0.53829865 -2.79156727
  3.08791444  2.15531654  1.17172212 -1.02289931  0.6260587   1.2045288
  1.24455613 -1.93628971  0.72553925]. 
[Thu Apr  4 13:24:35 2024]  Iteration number: 50 with current cost as 0.09146224204641965 and parameters 
[-2.90318329  2.2374346  -2.12427967  0.61924569  0.34600076 -1.66836898
  3.35170411  3.61778649  1.51476541  1.04008414  1.9611062   2.42925986
  1.60510093 -1.50222167  0.39166901]. 
Working on 1.0 fold... 
[Thu Apr  4 13:25:04 2024]  Iteration number: 0 with current cost as 0.2802935606361306 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.11012259  0.53584413 -2.79121815
  3.09782146  2.14161863  1.1624438  -1.02215111  0.61379969  1.20160416
  1.25287728 -1.93215614  0.71242765]. 
[Thu Apr  4 13:31:10 2024]  Iteration number: 50 with current cost as 0.09883688874401034 and parameters 
[-2.90318355  2.2374346  -2.12427967  0.95537948  0.25355227 -1.9243262
  3.92730251  0.49465136  1.19597701  1.12624557  2.07980672  2.4578889
 -0.34773904 -3.6943621   1.17075566]. 
Training complete taking 1622.0443959236145 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 13:31:28 2024]  Iteration number: 0 with current cost as 0.3158571118914961 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10012876  0.52964358 -2.78931118
  3.09312933  2.13569493  1.17390637 -1.01420303  0.61632942  1.21204318
  1.26093092 -1.92610255  0.70708687]. 
[Thu Apr  4 13:36:28 2024]  Iteration number: 50 with current cost as 0.15802374603925967 and parameters 
[-2.90318331  2.23743454 -2.12427992  0.57600278 -1.09959662 -2.77856063
  3.00188253  2.19177881  2.78862963 -0.79742068  0.47991528  2.5058187
  0.21984448 -2.19917186 -0.37957797]. 
Working on 0.4 fold... 
[Thu Apr  4 13:36:40 2024]  Iteration number: 0 with current cost as 0.2673213341857351 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10627884  0.54312293 -2.77514319
  3.08820188  2.17010728  1.16280328 -1.03368807  0.62922536  1.19242255
  1.24759391 -1.92957904  0.73937425]. 
Working on 0.6 fold... 
[Thu Apr  4 13:42:27 2024]  Iteration number: 0 with current cost as 0.2731554112126978 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.11062596  0.5351918  -2.79284563
  3.0831292   2.15169526  1.18203395 -1.02262672  0.62505989  1.20454741
  1.25119578 -1.93043863  0.7242161 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:47:06 2024]  Iteration number: 0 with current cost as 0.28042696181453697 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.11373358  0.53829865 -2.79156727
  3.08791444  2.15531654  1.17172212 -1.02289931  0.6260587   1.2045288
  1.24455613 -1.93628971  0.72553925]. 
[Thu Apr  4 13:52:06 2024]  Iteration number: 50 with current cost as 0.09146224204641965 and parameters 
[-2.90318329  2.2374346  -2.12427967  0.61924569  0.34600076 -1.66836898
  3.35170411  3.61778649  1.51476541  1.04008414  1.9611062   2.42925986
  1.60510093 -1.50222167  0.39166901]. 
Working on 1.0 fold... 
[Thu Apr  4 13:52:36 2024]  Iteration number: 0 with current cost as 0.2802935606361306 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.11012259  0.53584413 -2.79121815
  3.09782146  2.14161863  1.1624438  -1.02215111  0.61379969  1.20160416
  1.25287728 -1.93215614  0.71242765]. 
[Thu Apr  4 13:58:28 2024]  Iteration number: 50 with current cost as 0.09883688874401034 and parameters 
[-2.90318355  2.2374346  -2.12427967  0.95537948  0.25355227 -1.9243262
  3.92730251  0.49465136  1.19597701  1.12624557  2.07980672  2.4578889
 -0.34773904 -3.6943621   1.17075566]. 
Training complete taking 1641.3870649337769 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 13:58:48 2024]  Iteration number: 0 with current cost as 0.3158571118914961 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10012876  0.52964358 -2.78931118
  3.09312933  2.13569493  1.17390637 -1.01420303  0.61632942  1.21204318
  1.26093092 -1.92610255  0.70708687]. 
[Thu Apr  4 14:03:46 2024]  Iteration number: 50 with current cost as 0.15802374603925967 and parameters 
[-2.90318331  2.23743454 -2.12427992  0.57600278 -1.09959662 -2.77856063
  3.00188253  2.19177881  2.78862963 -0.79742068  0.47991528  2.5058187
  0.21984448 -2.19917186 -0.37957797]. 
Working on 0.4 fold... 
[Thu Apr  4 14:03:59 2024]  Iteration number: 0 with current cost as 0.2673213341857351 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10627884  0.54312293 -2.77514319
  3.08820188  2.17010728  1.16280328 -1.03368807  0.62922536  1.19242255
  1.24759391 -1.92957904  0.73937425]. 
Working on 0.6 fold... 
[Thu Apr  4 14:09:27 2024]  Iteration number: 0 with current cost as 0.2731554112126978 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.11062596  0.5351918  -2.79284563
  3.0831292   2.15169526  1.18203395 -1.02262672  0.62505989  1.20454741
  1.25119578 -1.93043863  0.7242161 ]. 
Working on 0.8 fold... 
[Thu Apr  4 14:13:56 2024]  Iteration number: 0 with current cost as 0.28042696181453697 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.11373358  0.53829865 -2.79156727
  3.08791444  2.15531654  1.17172212 -1.02289931  0.6260587   1.2045288
  1.24455613 -1.93628971  0.72553925]. 
[Thu Apr  4 14:19:02 2024]  Iteration number: 50 with current cost as 0.09146224204641965 and parameters 
[-2.90318329  2.2374346  -2.12427967  0.61924569  0.34600076 -1.66836898
  3.35170411  3.61778649  1.51476541  1.04008414  1.9611062   2.42925986
  1.60510093 -1.50222167  0.39166901]. 
Working on 1.0 fold... 
[Thu Apr  4 14:19:31 2024]  Iteration number: 0 with current cost as 0.2802935606361306 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.11012259  0.53584413 -2.79121815
  3.09782146  2.14161863  1.1624438  -1.02215111  0.61379969  1.20160416
  1.25287728 -1.93215614  0.71242765]. 
[Thu Apr  4 14:25:34 2024]  Iteration number: 50 with current cost as 0.09883688874401034 and parameters 
[-2.90318355  2.2374346  -2.12427967  0.95537948  0.25355227 -1.9243262
  3.92730251  0.49465136  1.19597701  1.12624557  2.07980672  2.4578889
 -0.34773904 -3.6943621   1.17075566]. 
Training complete taking 1622.7350108623505 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 14:25:51 2024]  Iteration number: 0 with current cost as 0.3158571118914961 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10012876  0.52964358 -2.78931118
  3.09312933  2.13569493  1.17390637 -1.01420303  0.61632942  1.21204318
  1.26093092 -1.92610255  0.70708687]. 
[Thu Apr  4 14:30:51 2024]  Iteration number: 50 with current cost as 0.15802374603925967 and parameters 
[-2.90318331  2.23743454 -2.12427992  0.57600278 -1.09959662 -2.77856063
  3.00188253  2.19177881  2.78862963 -0.79742068  0.47991528  2.5058187
  0.21984448 -2.19917186 -0.37957797]. 
Working on 0.4 fold... 
[Thu Apr  4 14:31:04 2024]  Iteration number: 0 with current cost as 0.2673213341857351 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10627884  0.54312293 -2.77514319
  3.08820188  2.17010728  1.16280328 -1.03368807  0.62922536  1.19242255
  1.24759391 -1.92957904  0.73937425]. 
Working on 0.6 fold... 
[Thu Apr  4 14:36:32 2024]  Iteration number: 0 with current cost as 0.2731554112126978 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.11062596  0.5351918  -2.79284563
  3.0831292   2.15169526  1.18203395 -1.02262672  0.62505989  1.20454741
  1.25119578 -1.93043863  0.7242161 ]. 
Working on 0.8 fold... 
[Thu Apr  4 14:41:20 2024]  Iteration number: 0 with current cost as 0.28042696181453697 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.11373358  0.53829865 -2.79156727
  3.08791444  2.15531654  1.17172212 -1.02289931  0.6260587   1.2045288
  1.24455613 -1.93628971  0.72553925]. 
[Thu Apr  4 14:46:21 2024]  Iteration number: 50 with current cost as 0.09146224204641965 and parameters 
[-2.90318329  2.2374346  -2.12427967  0.61924569  0.34600076 -1.66836898
  3.35170411  3.61778649  1.51476541  1.04008414  1.9611062   2.42925986
  1.60510093 -1.50222167  0.39166901]. 
Working on 1.0 fold... 
[Thu Apr  4 14:46:51 2024]  Iteration number: 0 with current cost as 0.2802935606361306 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.11012259  0.53584413 -2.79121815
  3.09782146  2.14161863  1.1624438  -1.02215111  0.61379969  1.20160416
  1.25287728 -1.93215614  0.71242765]. 
[Thu Apr  4 14:52:41 2024]  Iteration number: 50 with current cost as 0.09883688874401034 and parameters 
[-2.90318355  2.2374346  -2.12427967  0.95537948  0.25355227 -1.9243262
  3.92730251  0.49465136  1.19597701  1.12624557  2.07980672  2.4578889
 -0.34773904 -3.6943621   1.17075566]. 
Training complete taking 1626.9689764976501 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 14:52:59 2024]  Iteration number: 0 with current cost as 0.3158571118914961 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10012876  0.52964358 -2.78931118
  3.09312933  2.13569493  1.17390637 -1.01420303  0.61632942  1.21204318
  1.26093092 -1.92610255  0.70708687]. 
[Thu Apr  4 14:57:56 2024]  Iteration number: 50 with current cost as 0.15802374603925967 and parameters 
[-2.90318331  2.23743454 -2.12427992  0.57600278 -1.09959662 -2.77856063
  3.00188253  2.19177881  2.78862963 -0.79742068  0.47991528  2.5058187
  0.21984448 -2.19917186 -0.37957797]. 
Working on 0.4 fold... 
[Thu Apr  4 14:58:08 2024]  Iteration number: 0 with current cost as 0.2673213341857351 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10627884  0.54312293 -2.77514319
  3.08820188  2.17010728  1.16280328 -1.03368807  0.62922536  1.19242255
  1.24759391 -1.92957904  0.73937425]. 
Working on 0.6 fold... 
[Thu Apr  4 15:03:35 2024]  Iteration number: 0 with current cost as 0.2731554112126978 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.11062596  0.5351918  -2.79284563
  3.0831292   2.15169526  1.18203395 -1.02262672  0.62505989  1.20454741
  1.25119578 -1.93043863  0.7242161 ]. 
Working on 0.8 fold... 
[Thu Apr  4 15:08:08 2024]  Iteration number: 0 with current cost as 0.28042696181453697 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.11373358  0.53829865 -2.79156727
  3.08791444  2.15531654  1.17172212 -1.02289931  0.6260587   1.2045288
  1.24455613 -1.93628971  0.72553925]. 
[Thu Apr  4 15:13:05 2024]  Iteration number: 50 with current cost as 0.09146224204641965 and parameters 
[-2.90318329  2.2374346  -2.12427967  0.61924569  0.34600076 -1.66836898
  3.35170411  3.61778649  1.51476541  1.04008414  1.9611062   2.42925986
  1.60510093 -1.50222167  0.39166901]. 
Working on 1.0 fold... 
[Thu Apr  4 15:13:37 2024]  Iteration number: 0 with current cost as 0.2802935606361306 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.11012259  0.53584413 -2.79121815
  3.09782146  2.14161863  1.1624438  -1.02215111  0.61379969  1.20160416
  1.25287728 -1.93215614  0.71242765]. 
[Thu Apr  4 15:19:37 2024]  Iteration number: 50 with current cost as 0.09883688874401034 and parameters 
[-2.90318355  2.2374346  -2.12427967  0.95537948  0.25355227 -1.9243262
  3.92730251  0.49465136  1.19597701  1.12624557  2.07980672  2.4578889
 -0.34773904 -3.6943621   1.17075566]. 
Training complete taking 1615.6401197910309 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 15:19:53 2024]  Iteration number: 0 with current cost as 0.3158571118914961 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10012876  0.52964358 -2.78931118
  3.09312933  2.13569493  1.17390637 -1.01420303  0.61632942  1.21204318
  1.26093092 -1.92610255  0.70708687]. 
[Thu Apr  4 15:24:52 2024]  Iteration number: 50 with current cost as 0.15802374603925967 and parameters 
[-2.90318331  2.23743454 -2.12427992  0.57600278 -1.09959662 -2.77856063
  3.00188253  2.19177881  2.78862963 -0.79742068  0.47991528  2.5058187
  0.21984448 -2.19917186 -0.37957797]. 
Working on 0.4 fold... 
[Thu Apr  4 15:25:04 2024]  Iteration number: 0 with current cost as 0.2673213341857351 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10627884  0.54312293 -2.77514319
  3.08820188  2.17010728  1.16280328 -1.03368807  0.62922536  1.19242255
  1.24759391 -1.92957904  0.73937425]. 
Working on 0.6 fold... 
[Thu Apr  4 15:30:33 2024]  Iteration number: 0 with current cost as 0.2731554112126978 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.11062596  0.5351918  -2.79284563
  3.0831292   2.15169526  1.18203395 -1.02262672  0.62505989  1.20454741
  1.25119578 -1.93043863  0.7242161 ]. 
Working on 0.8 fold... 
[Thu Apr  4 15:35:02 2024]  Iteration number: 0 with current cost as 0.28042696181453697 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.11373358  0.53829865 -2.79156727
  3.08791444  2.15531654  1.17172212 -1.02289931  0.6260587   1.2045288
  1.24455613 -1.93628971  0.72553925]. 
[Thu Apr  4 15:39:58 2024]  Iteration number: 50 with current cost as 0.09146224204641965 and parameters 
[-2.90318329  2.2374346  -2.12427967  0.61924569  0.34600076 -1.66836898
  3.35170411  3.61778649  1.51476541  1.04008414  1.9611062   2.42925986
  1.60510093 -1.50222167  0.39166901]. 
Working on 1.0 fold... 
[Thu Apr  4 15:40:28 2024]  Iteration number: 0 with current cost as 0.2802935606361306 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.11012259  0.53584413 -2.79121815
  3.09782146  2.14161863  1.1624438  -1.02215111  0.61379969  1.20160416
  1.25287728 -1.93215614  0.71242765]. 
[Thu Apr  4 15:46:17 2024]  Iteration number: 50 with current cost as 0.09883688874401034 and parameters 
[-2.90318355  2.2374346  -2.12427967  0.95537948  0.25355227 -1.9243262
  3.92730251  0.49465136  1.19597701  1.12624557  2.07980672  2.4578889
 -0.34773904 -3.6943621   1.17075566]. 
Training complete taking 1599.9169735908508 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 15:46:35 2024]  Iteration number: 0 with current cost as 0.3158571118914961 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10012876  0.52964358 -2.78931118
  3.09312933  2.13569493  1.17390637 -1.01420303  0.61632942  1.21204318
  1.26093092 -1.92610255  0.70708687]. 
[Thu Apr  4 15:51:28 2024]  Iteration number: 50 with current cost as 0.15802374603925967 and parameters 
[-2.90318331  2.23743454 -2.12427992  0.57600278 -1.09959662 -2.77856063
  3.00188253  2.19177881  2.78862963 -0.79742068  0.47991528  2.5058187
  0.21984448 -2.19917186 -0.37957797]. 
Working on 0.4 fold... 
[Thu Apr  4 15:51:40 2024]  Iteration number: 0 with current cost as 0.2673213341857351 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10627884  0.54312293 -2.77514319
  3.08820188  2.17010728  1.16280328 -1.03368807  0.62922536  1.19242255
  1.24759391 -1.92957904  0.73937425]. 
Working on 0.6 fold... 
[Thu Apr  4 15:57:11 2024]  Iteration number: 0 with current cost as 0.2731554112126978 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.11062596  0.5351918  -2.79284563
  3.0831292   2.15169526  1.18203395 -1.02262672  0.62505989  1.20454741
  1.25119578 -1.93043863  0.7242161 ]. 
Working on 0.8 fold... 
[Thu Apr  4 16:01:40 2024]  Iteration number: 0 with current cost as 0.28042696181453697 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.11373358  0.53829865 -2.79156727
  3.08791444  2.15531654  1.17172212 -1.02289931  0.6260587   1.2045288
  1.24455613 -1.93628971  0.72553925]. 
[Thu Apr  4 16:06:54 2024]  Iteration number: 50 with current cost as 0.09146224204641965 and parameters 
[-2.90318329  2.2374346  -2.12427967  0.61924569  0.34600076 -1.66836898
  3.35170411  3.61778649  1.51476541  1.04008414  1.9611062   2.42925986
  1.60510093 -1.50222167  0.39166901]. 
Working on 1.0 fold... 
[Thu Apr  4 16:07:23 2024]  Iteration number: 0 with current cost as 0.2802935606361306 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.11012259  0.53584413 -2.79121815
  3.09782146  2.14161863  1.1624438  -1.02215111  0.61379969  1.20160416
  1.25287728 -1.93215614  0.71242765]. 
[Thu Apr  4 16:13:13 2024]  Iteration number: 50 with current cost as 0.09883688874401034 and parameters 
[-2.90318355  2.2374346  -2.12427967  0.95537948  0.25355227 -1.9243262
  3.92730251  0.49465136  1.19597701  1.12624557  2.07980672  2.4578889
 -0.34773904 -3.6943621   1.17075566]. 
Training complete taking 1616.6078748703003 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 16:13:30 2024]  Iteration number: 0 with current cost as 0.3158571118914961 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10012876  0.52964358 -2.78931118
  3.09312933  2.13569493  1.17390637 -1.01420303  0.61632942  1.21204318
  1.26093092 -1.92610255  0.70708687]. 
[Thu Apr  4 16:18:33 2024]  Iteration number: 50 with current cost as 0.15802374603925967 and parameters 
[-2.90318331  2.23743454 -2.12427992  0.57600278 -1.09959662 -2.77856063
  3.00188253  2.19177881  2.78862963 -0.79742068  0.47991528  2.5058187
  0.21984448 -2.19917186 -0.37957797]. 
Working on 0.4 fold... 
[Thu Apr  4 16:18:44 2024]  Iteration number: 0 with current cost as 0.2673213341857351 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10627884  0.54312293 -2.77514319
  3.08820188  2.17010728  1.16280328 -1.03368807  0.62922536  1.19242255
  1.24759391 -1.92957904  0.73937425]. 
Working on 0.6 fold... 
[Thu Apr  4 16:24:12 2024]  Iteration number: 0 with current cost as 0.2731554112126978 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.11062596  0.5351918  -2.79284563
  3.0831292   2.15169526  1.18203395 -1.02262672  0.62505989  1.20454741
  1.25119578 -1.93043863  0.7242161 ]. 
Working on 0.8 fold... 
[Thu Apr  4 16:28:39 2024]  Iteration number: 0 with current cost as 0.28042696181453697 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.11373358  0.53829865 -2.79156727
  3.08791444  2.15531654  1.17172212 -1.02289931  0.6260587   1.2045288
  1.24455613 -1.93628971  0.72553925]. 
[Thu Apr  4 16:33:34 2024]  Iteration number: 50 with current cost as 0.09146224204641965 and parameters 
[-2.90318329  2.2374346  -2.12427967  0.61924569  0.34600076 -1.66836898
  3.35170411  3.61778649  1.51476541  1.04008414  1.9611062   2.42925986
  1.60510093 -1.50222167  0.39166901]. 
Working on 1.0 fold... 
[Thu Apr  4 16:34:02 2024]  Iteration number: 0 with current cost as 0.2802935606361306 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.11012259  0.53584413 -2.79121815
  3.09782146  2.14161863  1.1624438  -1.02215111  0.61379969  1.20160416
  1.25287728 -1.93215614  0.71242765]. 
[Thu Apr  4 16:39:56 2024]  Iteration number: 50 with current cost as 0.09883688874401034 and parameters 
[-2.90318355  2.2374346  -2.12427967  0.95537948  0.25355227 -1.9243262
  3.92730251  0.49465136  1.19597701  1.12624557  2.07980672  2.4578889
 -0.34773904 -3.6943621   1.17075566]. 
Training complete taking 1602.723076581955 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 16:40:14 2024]  Iteration number: 0 with current cost as 0.3158571118914961 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10012876  0.52964358 -2.78931118
  3.09312933  2.13569493  1.17390637 -1.01420303  0.61632942  1.21204318
  1.26093092 -1.92610255  0.70708687]. 
[Thu Apr  4 16:45:18 2024]  Iteration number: 50 with current cost as 0.15802374603925967 and parameters 
[-2.90318331  2.23743454 -2.12427992  0.57600278 -1.09959662 -2.77856063
  3.00188253  2.19177881  2.78862963 -0.79742068  0.47991528  2.5058187
  0.21984448 -2.19917186 -0.37957797]. 
Working on 0.4 fold... 
[Thu Apr  4 16:45:29 2024]  Iteration number: 0 with current cost as 0.2673213341857351 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10627884  0.54312293 -2.77514319
  3.08820188  2.17010728  1.16280328 -1.03368807  0.62922536  1.19242255
  1.24759391 -1.92957904  0.73937425]. 
Working on 0.6 fold... 
[Thu Apr  4 16:50:58 2024]  Iteration number: 0 with current cost as 0.2731554112126978 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.11062596  0.5351918  -2.79284563
  3.0831292   2.15169526  1.18203395 -1.02262672  0.62505989  1.20454741
  1.25119578 -1.93043863  0.7242161 ]. 
Working on 0.8 fold... 
[Thu Apr  4 16:55:27 2024]  Iteration number: 0 with current cost as 0.28042696181453697 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.11373358  0.53829865 -2.79156727
  3.08791444  2.15531654  1.17172212 -1.02289931  0.6260587   1.2045288
  1.24455613 -1.93628971  0.72553925]. 
[Thu Apr  4 17:00:39 2024]  Iteration number: 50 with current cost as 0.09146224204641965 and parameters 
[-2.90318329  2.2374346  -2.12427967  0.61924569  0.34600076 -1.66836898
  3.35170411  3.61778649  1.51476541  1.04008414  1.9611062   2.42925986
  1.60510093 -1.50222167  0.39166901]. 
Working on 1.0 fold... 
[Thu Apr  4 17:01:08 2024]  Iteration number: 0 with current cost as 0.2802935606361306 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.11012259  0.53584413 -2.79121815
  3.09782146  2.14161863  1.1624438  -1.02215111  0.61379969  1.20160416
  1.25287728 -1.93215614  0.71242765]. 
[Thu Apr  4 17:07:21 2024]  Iteration number: 50 with current cost as 0.09883688874401034 and parameters 
[-2.90318355  2.2374346  -2.12427967  0.95537948  0.25355227 -1.9243262
  3.92730251  0.49465136  1.19597701  1.12624557  2.07980672  2.4578889
 -0.34773904 -3.6943621   1.17075566]. 
Training complete taking 1645.4638276100159 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 17:07:41 2024]  Iteration number: 0 with current cost as 0.3158571118914961 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10012876  0.52964358 -2.78931118
  3.09312933  2.13569493  1.17390637 -1.01420303  0.61632942  1.21204318
  1.26093092 -1.92610255  0.70708687]. 
[Thu Apr  4 17:12:38 2024]  Iteration number: 50 with current cost as 0.15802374603925967 and parameters 
[-2.90318331  2.23743454 -2.12427992  0.57600278 -1.09959662 -2.77856063
  3.00188253  2.19177881  2.78862963 -0.79742068  0.47991528  2.5058187
  0.21984448 -2.19917186 -0.37957797]. 
Working on 0.4 fold... 
[Thu Apr  4 17:12:49 2024]  Iteration number: 0 with current cost as 0.2673213341857351 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10627884  0.54312293 -2.77514319
  3.08820188  2.17010728  1.16280328 -1.03368807  0.62922536  1.19242255
  1.24759391 -1.92957904  0.73937425]. 
Working on 0.6 fold... 
[Thu Apr  4 17:18:40 2024]  Iteration number: 0 with current cost as 0.2731554112126978 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.11062596  0.5351918  -2.79284563
  3.0831292   2.15169526  1.18203395 -1.02262672  0.62505989  1.20454741
  1.25119578 -1.93043863  0.7242161 ]. 
Working on 0.8 fold... 
[Thu Apr  4 17:23:24 2024]  Iteration number: 0 with current cost as 0.28042696181453697 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.11373358  0.53829865 -2.79156727
  3.08791444  2.15531654  1.17172212 -1.02289931  0.6260587   1.2045288
  1.24455613 -1.93628971  0.72553925]. 
[Thu Apr  4 17:28:25 2024]  Iteration number: 50 with current cost as 0.09146224204641965 and parameters 
[-2.90318329  2.2374346  -2.12427967  0.61924569  0.34600076 -1.66836898
  3.35170411  3.61778649  1.51476541  1.04008414  1.9611062   2.42925986
  1.60510093 -1.50222167  0.39166901]. 
Working on 1.0 fold... 
[Thu Apr  4 17:28:55 2024]  Iteration number: 0 with current cost as 0.2802935606361306 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.11012259  0.53584413 -2.79121815
  3.09782146  2.14161863  1.1624438  -1.02215111  0.61379969  1.20160416
  1.25287728 -1.93215614  0.71242765]. 
[Thu Apr  4 17:34:47 2024]  Iteration number: 50 with current cost as 0.09883688874401034 and parameters 
[-2.90318355  2.2374346  -2.12427967  0.95537948  0.25355227 -1.9243262
  3.92730251  0.49465136  1.19597701  1.12624557  2.07980672  2.4578889
 -0.34773904 -3.6943621   1.17075566]. 
Training complete taking 1645.0679392814636 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 17:35:03 2024]  Iteration number: 0 with current cost as 0.3158571118914961 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10012876  0.52964358 -2.78931118
  3.09312933  2.13569493  1.17390637 -1.01420303  0.61632942  1.21204318
  1.26093092 -1.92610255  0.70708687]. 
[Thu Apr  4 17:40:00 2024]  Iteration number: 50 with current cost as 0.15802374603925967 and parameters 
[-2.90318331  2.23743454 -2.12427992  0.57600278 -1.09959662 -2.77856063
  3.00188253  2.19177881  2.78862963 -0.79742068  0.47991528  2.5058187
  0.21984448 -2.19917186 -0.37957797]. 
Working on 0.4 fold... 
[Thu Apr  4 17:40:13 2024]  Iteration number: 0 with current cost as 0.2673213341857351 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10627884  0.54312293 -2.77514319
  3.08820188  2.17010728  1.16280328 -1.03368807  0.62922536  1.19242255
  1.24759391 -1.92957904  0.73937425]. 
Working on 0.6 fold... 
[Thu Apr  4 17:45:41 2024]  Iteration number: 0 with current cost as 0.2731554112126978 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.11062596  0.5351918  -2.79284563
  3.0831292   2.15169526  1.18203395 -1.02262672  0.62505989  1.20454741
  1.25119578 -1.93043863  0.7242161 ]. 
Working on 0.8 fold... 
[Thu Apr  4 17:50:07 2024]  Iteration number: 0 with current cost as 0.28042696181453697 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.11373358  0.53829865 -2.79156727
  3.08791444  2.15531654  1.17172212 -1.02289931  0.6260587   1.2045288
  1.24455613 -1.93628971  0.72553925]. 
[Thu Apr  4 17:55:05 2024]  Iteration number: 50 with current cost as 0.09146224204641965 and parameters 
[-2.90318329  2.2374346  -2.12427967  0.61924569  0.34600076 -1.66836898
  3.35170411  3.61778649  1.51476541  1.04008414  1.9611062   2.42925986
  1.60510093 -1.50222167  0.39166901]. 
Working on 1.0 fold... 
[Thu Apr  4 17:55:34 2024]  Iteration number: 0 with current cost as 0.2802935606361306 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.11012259  0.53584413 -2.79121815
  3.09782146  2.14161863  1.1624438  -1.02215111  0.61379969  1.20160416
  1.25287728 -1.93215614  0.71242765]. 
[Thu Apr  4 18:01:25 2024]  Iteration number: 50 with current cost as 0.09883688874401034 and parameters 
[-2.90318355  2.2374346  -2.12427967  0.95537948  0.25355227 -1.9243262
  3.92730251  0.49465136  1.19597701  1.12624557  2.07980672  2.4578889
 -0.34773904 -3.6943621   1.17075566]. 
Training complete taking 1598.4880175590515 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 18:01:44 2024]  Iteration number: 0 with current cost as 0.3158571118914961 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10012876  0.52964358 -2.78931118
  3.09312933  2.13569493  1.17390637 -1.01420303  0.61632942  1.21204318
  1.26093092 -1.92610255  0.70708687]. 
[Thu Apr  4 18:06:54 2024]  Iteration number: 50 with current cost as 0.15802374603925967 and parameters 
[-2.90318331  2.23743454 -2.12427992  0.57600278 -1.09959662 -2.77856063
  3.00188253  2.19177881  2.78862963 -0.79742068  0.47991528  2.5058187
  0.21984448 -2.19917186 -0.37957797]. 
Working on 0.4 fold... 
[Thu Apr  4 18:07:05 2024]  Iteration number: 0 with current cost as 0.2673213341857351 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10627884  0.54312293 -2.77514319
  3.08820188  2.17010728  1.16280328 -1.03368807  0.62922536  1.19242255
  1.24759391 -1.92957904  0.73937425]. 
Working on 0.6 fold... 
[Thu Apr  4 18:12:35 2024]  Iteration number: 0 with current cost as 0.2731554112126978 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.11062596  0.5351918  -2.79284563
  3.0831292   2.15169526  1.18203395 -1.02262672  0.62505989  1.20454741
  1.25119578 -1.93043863  0.7242161 ]. 
Working on 0.8 fold... 
[Thu Apr  4 18:17:06 2024]  Iteration number: 0 with current cost as 0.28042696181453697 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.11373358  0.53829865 -2.79156727
  3.08791444  2.15531654  1.17172212 -1.02289931  0.6260587   1.2045288
  1.24455613 -1.93628971  0.72553925]. 
[Thu Apr  4 18:22:08 2024]  Iteration number: 50 with current cost as 0.09146224204641965 and parameters 
[-2.90318329  2.2374346  -2.12427967  0.61924569  0.34600076 -1.66836898
  3.35170411  3.61778649  1.51476541  1.04008414  1.9611062   2.42925986
  1.60510093 -1.50222167  0.39166901]. 
Working on 1.0 fold... 
[Thu Apr  4 18:22:37 2024]  Iteration number: 0 with current cost as 0.2802935606361306 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.11012259  0.53584413 -2.79121815
  3.09782146  2.14161863  1.1624438  -1.02215111  0.61379969  1.20160416
  1.25287728 -1.93215614  0.71242765]. 
[Thu Apr  4 18:28:38 2024]  Iteration number: 50 with current cost as 0.09883688874401034 and parameters 
[-2.90318355  2.2374346  -2.12427967  0.95537948  0.25355227 -1.9243262
  3.92730251  0.49465136  1.19597701  1.12624557  2.07980672  2.4578889
 -0.34773904 -3.6943621   1.17075566]. 
Training complete taking 1634.61807513237 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 18:28:56 2024]  Iteration number: 0 with current cost as 0.3158571118914961 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10012876  0.52964358 -2.78931118
  3.09312933  2.13569493  1.17390637 -1.01420303  0.61632942  1.21204318
  1.26093092 -1.92610255  0.70708687]. 
[Thu Apr  4 18:34:18 2024]  Iteration number: 50 with current cost as 0.15802374603925967 and parameters 
[-2.90318331  2.23743454 -2.12427992  0.57600278 -1.09959662 -2.77856063
  3.00188253  2.19177881  2.78862963 -0.79742068  0.47991528  2.5058187
  0.21984448 -2.19917186 -0.37957797]. 
Working on 0.4 fold... 
[Thu Apr  4 18:34:29 2024]  Iteration number: 0 with current cost as 0.2673213341857351 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10627884  0.54312293 -2.77514319
  3.08820188  2.17010728  1.16280328 -1.03368807  0.62922536  1.19242255
  1.24759391 -1.92957904  0.73937425]. 
Working on 0.6 fold... 
[Thu Apr  4 18:40:01 2024]  Iteration number: 0 with current cost as 0.2731554112126978 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.11062596  0.5351918  -2.79284563
  3.0831292   2.15169526  1.18203395 -1.02262672  0.62505989  1.20454741
  1.25119578 -1.93043863  0.7242161 ]. 
Working on 0.8 fold... 
[Thu Apr  4 18:44:28 2024]  Iteration number: 0 with current cost as 0.28042696181453697 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.11373358  0.53829865 -2.79156727
  3.08791444  2.15531654  1.17172212 -1.02289931  0.6260587   1.2045288
  1.24455613 -1.93628971  0.72553925]. 
[Thu Apr  4 18:49:56 2024]  Iteration number: 50 with current cost as 0.09146224204641965 and parameters 
[-2.90318329  2.2374346  -2.12427967  0.61924569  0.34600076 -1.66836898
  3.35170411  3.61778649  1.51476541  1.04008414  1.9611062   2.42925986
  1.60510093 -1.50222167  0.39166901]. 
Working on 1.0 fold... 
[Thu Apr  4 18:50:25 2024]  Iteration number: 0 with current cost as 0.2802935606361306 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.11012259  0.53584413 -2.79121815
  3.09782146  2.14161863  1.1624438  -1.02215111  0.61379969  1.20160416
  1.25287728 -1.93215614  0.71242765]. 
[Thu Apr  4 18:56:22 2024]  Iteration number: 50 with current cost as 0.09883688874401034 and parameters 
[-2.90318355  2.2374346  -2.12427967  0.95537948  0.25355227 -1.9243262
  3.92730251  0.49465136  1.19597701  1.12624557  2.07980672  2.4578889
 -0.34773904 -3.6943621   1.17075566]. 
Training complete taking 1667.4254076480865 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 18:56:45 2024]  Iteration number: 0 with current cost as 0.3158571118914961 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10012876  0.52964358 -2.78931118
  3.09312933  2.13569493  1.17390637 -1.01420303  0.61632942  1.21204318
  1.26093092 -1.92610255  0.70708687]. 
[Thu Apr  4 19:01:46 2024]  Iteration number: 50 with current cost as 0.15802374603925967 and parameters 
[-2.90318331  2.23743454 -2.12427992  0.57600278 -1.09959662 -2.77856063
  3.00188253  2.19177881  2.78862963 -0.79742068  0.47991528  2.5058187
  0.21984448 -2.19917186 -0.37957797]. 
Working on 0.4 fold... 
[Thu Apr  4 19:01:58 2024]  Iteration number: 0 with current cost as 0.2673213341857351 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10627884  0.54312293 -2.77514319
  3.08820188  2.17010728  1.16280328 -1.03368807  0.62922536  1.19242255
  1.24759391 -1.92957904  0.73937425]. 
Working on 0.6 fold... 
[Thu Apr  4 19:07:39 2024]  Iteration number: 0 with current cost as 0.2731554112126978 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.11062596  0.5351918  -2.79284563
  3.0831292   2.15169526  1.18203395 -1.02262672  0.62505989  1.20454741
  1.25119578 -1.93043863  0.7242161 ]. 
Working on 0.8 fold... 
[Thu Apr  4 19:12:08 2024]  Iteration number: 0 with current cost as 0.28042696181453697 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.11373358  0.53829865 -2.79156727
  3.08791444  2.15531654  1.17172212 -1.02289931  0.6260587   1.2045288
  1.24455613 -1.93628971  0.72553925]. 
[Thu Apr  4 19:17:06 2024]  Iteration number: 50 with current cost as 0.09146224204641965 and parameters 
[-2.90318329  2.2374346  -2.12427967  0.61924569  0.34600076 -1.66836898
  3.35170411  3.61778649  1.51476541  1.04008414  1.9611062   2.42925986
  1.60510093 -1.50222167  0.39166901]. 
Working on 1.0 fold... 
[Thu Apr  4 19:17:36 2024]  Iteration number: 0 with current cost as 0.2802935606361306 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.11012259  0.53584413 -2.79121815
  3.09782146  2.14161863  1.1624438  -1.02215111  0.61379969  1.20160416
  1.25287728 -1.93215614  0.71242765]. 
[Thu Apr  4 19:23:26 2024]  Iteration number: 50 with current cost as 0.09883688874401034 and parameters 
[-2.90318355  2.2374346  -2.12427967  0.95537948  0.25355227 -1.9243262
  3.92730251  0.49465136  1.19597701  1.12624557  2.07980672  2.4578889
 -0.34773904 -3.6943621   1.17075566]. 
Training complete taking 1619.4021887779236 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 19:23:43 2024]  Iteration number: 0 with current cost as 0.3158571118914961 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10012876  0.52964358 -2.78931118
  3.09312933  2.13569493  1.17390637 -1.01420303  0.61632942  1.21204318
  1.26093092 -1.92610255  0.70708687]. 
[Thu Apr  4 19:28:38 2024]  Iteration number: 50 with current cost as 0.15802374603925967 and parameters 
[-2.90318331  2.23743454 -2.12427992  0.57600278 -1.09959662 -2.77856063
  3.00188253  2.19177881  2.78862963 -0.79742068  0.47991528  2.5058187
  0.21984448 -2.19917186 -0.37957797]. 
Working on 0.4 fold... 
[Thu Apr  4 19:28:51 2024]  Iteration number: 0 with current cost as 0.2673213341857351 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10627884  0.54312293 -2.77514319
  3.08820188  2.17010728  1.16280328 -1.03368807  0.62922536  1.19242255
  1.24759391 -1.92957904  0.73937425]. 
Working on 0.6 fold... 
[Thu Apr  4 19:34:19 2024]  Iteration number: 0 with current cost as 0.2731554112126978 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.11062596  0.5351918  -2.79284563
  3.0831292   2.15169526  1.18203395 -1.02262672  0.62505989  1.20454741
  1.25119578 -1.93043863  0.7242161 ]. 
Working on 0.8 fold... 
[Thu Apr  4 19:38:48 2024]  Iteration number: 0 with current cost as 0.28042696181453697 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.11373358  0.53829865 -2.79156727
  3.08791444  2.15531654  1.17172212 -1.02289931  0.6260587   1.2045288
  1.24455613 -1.93628971  0.72553925]. 
[Thu Apr  4 19:43:54 2024]  Iteration number: 50 with current cost as 0.09146224204641965 and parameters 
[-2.90318329  2.2374346  -2.12427967  0.61924569  0.34600076 -1.66836898
  3.35170411  3.61778649  1.51476541  1.04008414  1.9611062   2.42925986
  1.60510093 -1.50222167  0.39166901]. 
Working on 1.0 fold... 
[Thu Apr  4 19:44:23 2024]  Iteration number: 0 with current cost as 0.2802935606361306 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.11012259  0.53584413 -2.79121815
  3.09782146  2.14161863  1.1624438  -1.02215111  0.61379969  1.20160416
  1.25287728 -1.93215614  0.71242765]. 
[Thu Apr  4 19:50:13 2024]  Iteration number: 50 with current cost as 0.09883688874401034 and parameters 
[-2.90318355  2.2374346  -2.12427967  0.95537948  0.25355227 -1.9243262
  3.92730251  0.49465136  1.19597701  1.12624557  2.07980672  2.4578889
 -0.34773904 -3.6943621   1.17075566]. 
Training complete taking 1608.4725091457367 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 19:50:32 2024]  Iteration number: 0 with current cost as 0.3158571118914961 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10012876  0.52964358 -2.78931118
  3.09312933  2.13569493  1.17390637 -1.01420303  0.61632942  1.21204318
  1.26093092 -1.92610255  0.70708687]. 
[Thu Apr  4 19:55:30 2024]  Iteration number: 50 with current cost as 0.15802374603925967 and parameters 
[-2.90318331  2.23743454 -2.12427992  0.57600278 -1.09959662 -2.77856063
  3.00188253  2.19177881  2.78862963 -0.79742068  0.47991528  2.5058187
  0.21984448 -2.19917186 -0.37957797]. 
Working on 0.4 fold... 
[Thu Apr  4 19:55:41 2024]  Iteration number: 0 with current cost as 0.2673213341857351 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10627884  0.54312293 -2.77514319
  3.08820188  2.17010728  1.16280328 -1.03368807  0.62922536  1.19242255
  1.24759391 -1.92957904  0.73937425]. 
Working on 0.6 fold... 
[Thu Apr  4 20:01:07 2024]  Iteration number: 0 with current cost as 0.2731554112126978 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.11062596  0.5351918  -2.79284563
  3.0831292   2.15169526  1.18203395 -1.02262672  0.62505989  1.20454741
  1.25119578 -1.93043863  0.7242161 ]. 
Working on 0.8 fold... 
[Thu Apr  4 20:05:36 2024]  Iteration number: 0 with current cost as 0.28042696181453697 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.11373358  0.53829865 -2.79156727
  3.08791444  2.15531654  1.17172212 -1.02289931  0.6260587   1.2045288
  1.24455613 -1.93628971  0.72553925]. 
[Thu Apr  4 20:10:33 2024]  Iteration number: 50 with current cost as 0.09146224204641965 and parameters 
[-2.90318329  2.2374346  -2.12427967  0.61924569  0.34600076 -1.66836898
  3.35170411  3.61778649  1.51476541  1.04008414  1.9611062   2.42925986
  1.60510093 -1.50222167  0.39166901]. 
Working on 1.0 fold... 
[Thu Apr  4 20:11:02 2024]  Iteration number: 0 with current cost as 0.2802935606361306 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.11012259  0.53584413 -2.79121815
  3.09782146  2.14161863  1.1624438  -1.02215111  0.61379969  1.20160416
  1.25287728 -1.93215614  0.71242765]. 
[Thu Apr  4 20:17:15 2024]  Iteration number: 50 with current cost as 0.09883688874401034 and parameters 
[-2.90318355  2.2374346  -2.12427967  0.95537948  0.25355227 -1.9243262
  3.92730251  0.49465136  1.19597701  1.12624557  2.07980672  2.4578889
 -0.34773904 -3.6943621   1.17075566]. 
Training complete taking 1621.1139175891876 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 20:17:33 2024]  Iteration number: 0 with current cost as 0.3158571118914961 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10012876  0.52964358 -2.78931118
  3.09312933  2.13569493  1.17390637 -1.01420303  0.61632942  1.21204318
  1.26093092 -1.92610255  0.70708687]. 
[Thu Apr  4 20:22:42 2024]  Iteration number: 50 with current cost as 0.15802374603925967 and parameters 
[-2.90318331  2.23743454 -2.12427992  0.57600278 -1.09959662 -2.77856063
  3.00188253  2.19177881  2.78862963 -0.79742068  0.47991528  2.5058187
  0.21984448 -2.19917186 -0.37957797]. 
Working on 0.4 fold... 
[Thu Apr  4 20:22:55 2024]  Iteration number: 0 with current cost as 0.2673213341857351 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10627884  0.54312293 -2.77514319
  3.08820188  2.17010728  1.16280328 -1.03368807  0.62922536  1.19242255
  1.24759391 -1.92957904  0.73937425]. 
Working on 0.6 fold... 
[Thu Apr  4 20:28:31 2024]  Iteration number: 0 with current cost as 0.2731554112126978 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.11062596  0.5351918  -2.79284563
  3.0831292   2.15169526  1.18203395 -1.02262672  0.62505989  1.20454741
  1.25119578 -1.93043863  0.7242161 ]. 
Working on 0.8 fold... 
[Thu Apr  4 20:33:15 2024]  Iteration number: 0 with current cost as 0.28042696181453697 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.11373358  0.53829865 -2.79156727
  3.08791444  2.15531654  1.17172212 -1.02289931  0.6260587   1.2045288
  1.24455613 -1.93628971  0.72553925]. 
[Thu Apr  4 20:38:23 2024]  Iteration number: 50 with current cost as 0.09146224204641965 and parameters 
[-2.90318329  2.2374346  -2.12427967  0.61924569  0.34600076 -1.66836898
  3.35170411  3.61778649  1.51476541  1.04008414  1.9611062   2.42925986
  1.60510093 -1.50222167  0.39166901]. 
Working on 1.0 fold... 
[Thu Apr  4 20:38:52 2024]  Iteration number: 0 with current cost as 0.2802935606361306 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.11012259  0.53584413 -2.79121815
  3.09782146  2.14161863  1.1624438  -1.02215111  0.61379969  1.20160416
  1.25287728 -1.93215614  0.71242765]. 
[Thu Apr  4 20:44:53 2024]  Iteration number: 50 with current cost as 0.09883688874401034 and parameters 
[-2.90318355  2.2374346  -2.12427967  0.95537948  0.25355227 -1.9243262
  3.92730251  0.49465136  1.19597701  1.12624557  2.07980672  2.4578889
 -0.34773904 -3.6943621   1.17075566]. 
Training complete taking 1657.315131187439 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 20:45:10 2024]  Iteration number: 0 with current cost as 0.3158571118914961 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10012876  0.52964358 -2.78931118
  3.09312933  2.13569493  1.17390637 -1.01420303  0.61632942  1.21204318
  1.26093092 -1.92610255  0.70708687]. 
[Thu Apr  4 20:50:07 2024]  Iteration number: 50 with current cost as 0.15802374603925967 and parameters 
[-2.90318331  2.23743454 -2.12427992  0.57600278 -1.09959662 -2.77856063
  3.00188253  2.19177881  2.78862963 -0.79742068  0.47991528  2.5058187
  0.21984448 -2.19917186 -0.37957797]. 
Working on 0.4 fold... 
[Thu Apr  4 20:50:18 2024]  Iteration number: 0 with current cost as 0.2673213341857351 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10627884  0.54312293 -2.77514319
  3.08820188  2.17010728  1.16280328 -1.03368807  0.62922536  1.19242255
  1.24759391 -1.92957904  0.73937425]. 
Working on 0.6 fold... 
[Thu Apr  4 20:55:43 2024]  Iteration number: 0 with current cost as 0.2731554112126978 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.11062596  0.5351918  -2.79284563
  3.0831292   2.15169526  1.18203395 -1.02262672  0.62505989  1.20454741
  1.25119578 -1.93043863  0.7242161 ]. 
Working on 0.8 fold... 
[Thu Apr  4 21:00:11 2024]  Iteration number: 0 with current cost as 0.28042696181453697 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.11373358  0.53829865 -2.79156727
  3.08791444  2.15531654  1.17172212 -1.02289931  0.6260587   1.2045288
  1.24455613 -1.93628971  0.72553925]. 
[Thu Apr  4 21:05:20 2024]  Iteration number: 50 with current cost as 0.09146224204641965 and parameters 
[-2.90318329  2.2374346  -2.12427967  0.61924569  0.34600076 -1.66836898
  3.35170411  3.61778649  1.51476541  1.04008414  1.9611062   2.42925986
  1.60510093 -1.50222167  0.39166901]. 
Working on 1.0 fold... 
[Thu Apr  4 21:05:49 2024]  Iteration number: 0 with current cost as 0.2802935606361306 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.11012259  0.53584413 -2.79121815
  3.09782146  2.14161863  1.1624438  -1.02215111  0.61379969  1.20160416
  1.25287728 -1.93215614  0.71242765]. 
[Thu Apr  4 21:11:42 2024]  Iteration number: 50 with current cost as 0.09883688874401034 and parameters 
[-2.90318355  2.2374346  -2.12427967  0.95537948  0.25355227 -1.9243262
  3.92730251  0.49465136  1.19597701  1.12624557  2.07980672  2.4578889
 -0.34773904 -3.6943621   1.17075566]. 
Training complete taking 1608.674008846283 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 21:12:00 2024]  Iteration number: 0 with current cost as 0.3158571118914961 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10012876  0.52964358 -2.78931118
  3.09312933  2.13569493  1.17390637 -1.01420303  0.61632942  1.21204318
  1.26093092 -1.92610255  0.70708687]. 
[Thu Apr  4 21:17:05 2024]  Iteration number: 50 with current cost as 0.15802374603925967 and parameters 
[-2.90318331  2.23743454 -2.12427992  0.57600278 -1.09959662 -2.77856063
  3.00188253  2.19177881  2.78862963 -0.79742068  0.47991528  2.5058187
  0.21984448 -2.19917186 -0.37957797]. 
Working on 0.4 fold... 
[Thu Apr  4 21:17:16 2024]  Iteration number: 0 with current cost as 0.2673213341857351 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10627884  0.54312293 -2.77514319
  3.08820188  2.17010728  1.16280328 -1.03368807  0.62922536  1.19242255
  1.24759391 -1.92957904  0.73937425]. 
Working on 0.6 fold... 
[Thu Apr  4 21:23:01 2024]  Iteration number: 0 with current cost as 0.2731554112126978 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.11062596  0.5351918  -2.79284563
  3.0831292   2.15169526  1.18203395 -1.02262672  0.62505989  1.20454741
  1.25119578 -1.93043863  0.7242161 ]. 
Working on 0.8 fold... 
[Thu Apr  4 21:27:30 2024]  Iteration number: 0 with current cost as 0.28042696181453697 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.11373358  0.53829865 -2.79156727
  3.08791444  2.15531654  1.17172212 -1.02289931  0.6260587   1.2045288
  1.24455613 -1.93628971  0.72553925]. 
[Thu Apr  4 21:32:28 2024]  Iteration number: 50 with current cost as 0.09146224204641965 and parameters 
[-2.90318329  2.2374346  -2.12427967  0.61924569  0.34600076 -1.66836898
  3.35170411  3.61778649  1.51476541  1.04008414  1.9611062   2.42925986
  1.60510093 -1.50222167  0.39166901]. 
Working on 1.0 fold... 
[Thu Apr  4 21:32:57 2024]  Iteration number: 0 with current cost as 0.2802935606361306 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.11012259  0.53584413 -2.79121815
  3.09782146  2.14161863  1.1624438  -1.02215111  0.61379969  1.20160416
  1.25287728 -1.93215614  0.71242765]. 
[Thu Apr  4 21:38:51 2024]  Iteration number: 50 with current cost as 0.09883688874401034 and parameters 
[-2.90318355  2.2374346  -2.12427967  0.95537948  0.25355227 -1.9243262
  3.92730251  0.49465136  1.19597701  1.12624557  2.07980672  2.4578889
 -0.34773904 -3.6943621   1.17075566]. 
Training complete taking 1630.471109867096 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 21:39:09 2024]  Iteration number: 0 with current cost as 0.3158571118914961 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10012876  0.52964358 -2.78931118
  3.09312933  2.13569493  1.17390637 -1.01420303  0.61632942  1.21204318
  1.26093092 -1.92610255  0.70708687]. 
[Thu Apr  4 21:44:20 2024]  Iteration number: 50 with current cost as 0.15802374603925967 and parameters 
[-2.90318331  2.23743454 -2.12427992  0.57600278 -1.09959662 -2.77856063
  3.00188253  2.19177881  2.78862963 -0.79742068  0.47991528  2.5058187
  0.21984448 -2.19917186 -0.37957797]. 
Working on 0.4 fold... 
[Thu Apr  4 21:44:33 2024]  Iteration number: 0 with current cost as 0.2673213341857351 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10627884  0.54312293 -2.77514319
  3.08820188  2.17010728  1.16280328 -1.03368807  0.62922536  1.19242255
  1.24759391 -1.92957904  0.73937425]. 
Working on 0.6 fold... 
[Thu Apr  4 21:50:02 2024]  Iteration number: 0 with current cost as 0.2731554112126978 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.11062596  0.5351918  -2.79284563
  3.0831292   2.15169526  1.18203395 -1.02262672  0.62505989  1.20454741
  1.25119578 -1.93043863  0.7242161 ]. 
Working on 0.8 fold... 
[Thu Apr  4 21:54:31 2024]  Iteration number: 0 with current cost as 0.28042696181453697 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.11373358  0.53829865 -2.79156727
  3.08791444  2.15531654  1.17172212 -1.02289931  0.6260587   1.2045288
  1.24455613 -1.93628971  0.72553925]. 
[Thu Apr  4 21:59:28 2024]  Iteration number: 50 with current cost as 0.09146224204641965 and parameters 
[-2.90318329  2.2374346  -2.12427967  0.61924569  0.34600076 -1.66836898
  3.35170411  3.61778649  1.51476541  1.04008414  1.9611062   2.42925986
  1.60510093 -1.50222167  0.39166901]. 
Working on 1.0 fold... 
[Thu Apr  4 21:59:58 2024]  Iteration number: 0 with current cost as 0.2802935606361306 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.11012259  0.53584413 -2.79121815
  3.09782146  2.14161863  1.1624438  -1.02215111  0.61379969  1.20160416
  1.25287728 -1.93215614  0.71242765]. 
[Thu Apr  4 22:05:48 2024]  Iteration number: 50 with current cost as 0.09883688874401034 and parameters 
[-2.90318355  2.2374346  -2.12427967  0.95537948  0.25355227 -1.9243262
  3.92730251  0.49465136  1.19597701  1.12624557  2.07980672  2.4578889
 -0.34773904 -3.6943621   1.17075566]. 
Training complete taking 1616.939858675003 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 22:06:06 2024]  Iteration number: 0 with current cost as 0.3158571118914961 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10012876  0.52964358 -2.78931118
  3.09312933  2.13569493  1.17390637 -1.01420303  0.61632942  1.21204318
  1.26093092 -1.92610255  0.70708687]. 
[Thu Apr  4 22:11:22 2024]  Iteration number: 50 with current cost as 0.15802374603925967 and parameters 
[-2.90318331  2.23743454 -2.12427992  0.57600278 -1.09959662 -2.77856063
  3.00188253  2.19177881  2.78862963 -0.79742068  0.47991528  2.5058187
  0.21984448 -2.19917186 -0.37957797]. 
Working on 0.4 fold... 
[Thu Apr  4 22:11:36 2024]  Iteration number: 0 with current cost as 0.2673213341857351 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10627884  0.54312293 -2.77514319
  3.08820188  2.17010728  1.16280328 -1.03368807  0.62922536  1.19242255
  1.24759391 -1.92957904  0.73937425]. 
Working on 0.6 fold... 
[Thu Apr  4 22:17:03 2024]  Iteration number: 0 with current cost as 0.2731554112126978 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.11062596  0.5351918  -2.79284563
  3.0831292   2.15169526  1.18203395 -1.02262672  0.62505989  1.20454741
  1.25119578 -1.93043863  0.7242161 ]. 
Working on 0.8 fold... 
[Thu Apr  4 22:21:30 2024]  Iteration number: 0 with current cost as 0.28042696181453697 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.11373358  0.53829865 -2.79156727
  3.08791444  2.15531654  1.17172212 -1.02289931  0.6260587   1.2045288
  1.24455613 -1.93628971  0.72553925]. 
[Thu Apr  4 22:26:33 2024]  Iteration number: 50 with current cost as 0.09146224204641965 and parameters 
[-2.90318329  2.2374346  -2.12427967  0.61924569  0.34600076 -1.66836898
  3.35170411  3.61778649  1.51476541  1.04008414  1.9611062   2.42925986
  1.60510093 -1.50222167  0.39166901]. 
Working on 1.0 fold... 
[Thu Apr  4 22:27:03 2024]  Iteration number: 0 with current cost as 0.2802935606361306 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.11012259  0.53584413 -2.79121815
  3.09782146  2.14161863  1.1624438  -1.02215111  0.61379969  1.20160416
  1.25287728 -1.93215614  0.71242765]. 
[Thu Apr  4 22:33:17 2024]  Iteration number: 50 with current cost as 0.09883688874401034 and parameters 
[-2.90318355  2.2374346  -2.12427967  0.95537948  0.25355227 -1.9243262
  3.92730251  0.49465136  1.19597701  1.12624557  2.07980672  2.4578889
 -0.34773904 -3.6943621   1.17075566]. 
Training complete taking 1647.9176433086395 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 22:33:36 2024]  Iteration number: 0 with current cost as 0.3158571118914961 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10012876  0.52964358 -2.78931118
  3.09312933  2.13569493  1.17390637 -1.01420303  0.61632942  1.21204318
  1.26093092 -1.92610255  0.70708687]. 
[Thu Apr  4 22:38:33 2024]  Iteration number: 50 with current cost as 0.15802374603925967 and parameters 
[-2.90318331  2.23743454 -2.12427992  0.57600278 -1.09959662 -2.77856063
  3.00188253  2.19177881  2.78862963 -0.79742068  0.47991528  2.5058187
  0.21984448 -2.19917186 -0.37957797]. 
Working on 0.4 fold... 
[Thu Apr  4 22:38:44 2024]  Iteration number: 0 with current cost as 0.2673213341857351 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10627884  0.54312293 -2.77514319
  3.08820188  2.17010728  1.16280328 -1.03368807  0.62922536  1.19242255
  1.24759391 -1.92957904  0.73937425]. 
Working on 0.6 fold... 
[Thu Apr  4 22:44:15 2024]  Iteration number: 0 with current cost as 0.2731554112126978 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.11062596  0.5351918  -2.79284563
  3.0831292   2.15169526  1.18203395 -1.02262672  0.62505989  1.20454741
  1.25119578 -1.93043863  0.7242161 ]. 
Working on 0.8 fold... 
[Thu Apr  4 22:48:44 2024]  Iteration number: 0 with current cost as 0.28042696181453697 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.11373358  0.53829865 -2.79156727
  3.08791444  2.15531654  1.17172212 -1.02289931  0.6260587   1.2045288
  1.24455613 -1.93628971  0.72553925]. 
[Thu Apr  4 22:53:48 2024]  Iteration number: 50 with current cost as 0.09146224204641965 and parameters 
[-2.90318329  2.2374346  -2.12427967  0.61924569  0.34600076 -1.66836898
  3.35170411  3.61778649  1.51476541  1.04008414  1.9611062   2.42925986
  1.60510093 -1.50222167  0.39166901]. 
Working on 1.0 fold... 
[Thu Apr  4 22:54:17 2024]  Iteration number: 0 with current cost as 0.2802935606361306 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.11012259  0.53584413 -2.79121815
  3.09782146  2.14161863  1.1624438  -1.02215111  0.61379969  1.20160416
  1.25287728 -1.93215614  0.71242765]. 
[Thu Apr  4 23:00:09 2024]  Iteration number: 50 with current cost as 0.09883688874401034 and parameters 
[-2.90318355  2.2374346  -2.12427967  0.95537948  0.25355227 -1.9243262
  3.92730251  0.49465136  1.19597701  1.12624557  2.07980672  2.4578889
 -0.34773904 -3.6943621   1.17075566]. 
Training complete taking 1611.3852715492249 seconds. 
Discarding model... 

Training complete taking 40640.366255283356 total seconds. 
Now scoring model... 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Scoring complete taking 0.9777786731719971 seconds. 
Saved predicted values as M-M-CNOT_HWE-CNOT_predicted_values.csv
Model scores: {'MSE_train': (60.082760029532594,), 'R2_train': 0.7089191923876048, 'MAE_train': 5.410453326163731, 'MSE_test': 84.90095578694752, 'R2_test': 0.48823283773172554, 'MAE_test': 5.963098382934954}. 
Saved model results as M-M-CNOT_HWE-CNOT_results.json. 
