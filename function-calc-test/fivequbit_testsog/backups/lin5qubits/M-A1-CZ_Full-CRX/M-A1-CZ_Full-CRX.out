/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/circuits/Encoders.py:12: RuntimeWarning: invalid value encountered in arcsin
  if np.isnan(np.arcsin(features[feature_idx])) or np.isnan(np.arccos(features[feature_idx])):
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Sun Mar 24 17:35:00 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 17:38:51 2024]  Iteration number: 0 with current cost as 0.10822095887456029 and parameters 
[-4.66319339  2.23743464 -2.12427957 -0.11653096  0.55388701 -2.77010904
  3.06858491  2.18960145  1.18552006 -1.06648316  0.60271517  1.14432452
  1.31029899 -1.87354673  0.72965073  2.88578412 -0.54534342 -0.47522478
 -2.02654248  0.72897362  1.60512671  2.830771   -1.26456703 -0.25136098
 -2.39279211 -2.27309767  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 17:44:33 2024]  Iteration number: 0 with current cost as 0.09061534838173133 and parameters 
[-4.6900951   2.23743481 -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432454
  1.31029907 -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522468
 -2.0265424   0.7289737   1.60512664  2.83077099 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 17:50:15 2024]  Iteration number: 0 with current cost as 0.08455690777284282 and parameters 
[-4.66452267  2.23743464 -2.12427949 -0.11653103  0.553887   -2.77010912
  3.06858498  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578412 -0.54534343 -0.47522485
 -2.02654248  0.72897362  1.60512656  2.83077092 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 17:55:57 2024]  Iteration number: 0 with current cost as 0.08874339146370301 and parameters 
[-4.68039068  2.23743464 -2.12427935 -0.11653088  0.55388715 -2.7701089
  3.06858505  2.18960152  1.18552006 -1.06648308  0.60271532  1.1443246
  1.31029913 -1.87354666  0.72965095  2.88578419 -0.54534335 -0.47522471
 -2.0265424   0.7289737   1.60512671  2.83077107 -1.2645671  -0.25136097
 -2.39279211 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 18:01:39 2024]  Iteration number: 0 with current cost as 0.09587137153921405 and parameters 
[-4.6746318   2.2374348  -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432453
  1.31029907 -1.87354672  0.72965089  2.88578419 -0.54534335 -0.47522477
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136096
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550779 -2.69002202]. 
Training complete taking 1713.8114485740662 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 18:07:23 2024]  Iteration number: 0 with current cost as 0.10822095887456029 and parameters 
[-4.66319339  2.23743464 -2.12427957 -0.11653096  0.55388701 -2.77010904
  3.06858491  2.18960145  1.18552006 -1.06648316  0.60271517  1.14432452
  1.31029899 -1.87354673  0.72965073  2.88578412 -0.54534342 -0.47522478
 -2.02654248  0.72897362  1.60512671  2.830771   -1.26456703 -0.25136098
 -2.39279211 -2.27309767  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 18:13:04 2024]  Iteration number: 0 with current cost as 0.09061534838173133 and parameters 
[-4.6900951   2.23743481 -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432454
  1.31029907 -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522468
 -2.0265424   0.7289737   1.60512664  2.83077099 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 18:18:49 2024]  Iteration number: 0 with current cost as 0.08455690777284282 and parameters 
[-4.66452267  2.23743464 -2.12427949 -0.11653103  0.553887   -2.77010912
  3.06858498  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578412 -0.54534343 -0.47522485
 -2.02654248  0.72897362  1.60512656  2.83077092 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 18:24:35 2024]  Iteration number: 0 with current cost as 0.08874339146370301 and parameters 
[-4.68039068  2.23743464 -2.12427935 -0.11653088  0.55388715 -2.7701089
  3.06858505  2.18960152  1.18552006 -1.06648308  0.60271532  1.1443246
  1.31029913 -1.87354666  0.72965095  2.88578419 -0.54534335 -0.47522471
 -2.0265424   0.7289737   1.60512671  2.83077107 -1.2645671  -0.25136097
 -2.39279211 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 18:30:18 2024]  Iteration number: 0 with current cost as 0.09587137153921405 and parameters 
[-4.6746318   2.2374348  -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432453
  1.31029907 -1.87354672  0.72965089  2.88578419 -0.54534335 -0.47522477
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136096
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550779 -2.69002202]. 
Training complete taking 1717.1739716529846 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 18:36:03 2024]  Iteration number: 0 with current cost as 0.10822095887456029 and parameters 
[-4.66319339  2.23743464 -2.12427957 -0.11653096  0.55388701 -2.77010904
  3.06858491  2.18960145  1.18552006 -1.06648316  0.60271517  1.14432452
  1.31029899 -1.87354673  0.72965073  2.88578412 -0.54534342 -0.47522478
 -2.02654248  0.72897362  1.60512671  2.830771   -1.26456703 -0.25136098
 -2.39279211 -2.27309767  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 18:41:49 2024]  Iteration number: 0 with current cost as 0.09061534838173133 and parameters 
[-4.6900951   2.23743481 -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432454
  1.31029907 -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522468
 -2.0265424   0.7289737   1.60512664  2.83077099 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 18:47:30 2024]  Iteration number: 0 with current cost as 0.08455690777284282 and parameters 
[-4.66452267  2.23743464 -2.12427949 -0.11653103  0.553887   -2.77010912
  3.06858498  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578412 -0.54534343 -0.47522485
 -2.02654248  0.72897362  1.60512656  2.83077092 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 18:53:12 2024]  Iteration number: 0 with current cost as 0.08874339146370301 and parameters 
[-4.68039068  2.23743464 -2.12427935 -0.11653088  0.55388715 -2.7701089
  3.06858505  2.18960152  1.18552006 -1.06648308  0.60271532  1.1443246
  1.31029913 -1.87354666  0.72965095  2.88578419 -0.54534335 -0.47522471
 -2.0265424   0.7289737   1.60512671  2.83077107 -1.2645671  -0.25136097
 -2.39279211 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 18:58:48 2024]  Iteration number: 0 with current cost as 0.09587137153921405 and parameters 
[-4.6746318   2.2374348  -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432453
  1.31029907 -1.87354672  0.72965089  2.88578419 -0.54534335 -0.47522477
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136096
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550779 -2.69002202]. 
Training complete taking 1710.8120477199554 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 19:04:29 2024]  Iteration number: 0 with current cost as 0.10822095887456029 and parameters 
[-4.66319339  2.23743464 -2.12427957 -0.11653096  0.55388701 -2.77010904
  3.06858491  2.18960145  1.18552006 -1.06648316  0.60271517  1.14432452
  1.31029899 -1.87354673  0.72965073  2.88578412 -0.54534342 -0.47522478
 -2.02654248  0.72897362  1.60512671  2.830771   -1.26456703 -0.25136098
 -2.39279211 -2.27309767  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 19:10:10 2024]  Iteration number: 0 with current cost as 0.09061534838173133 and parameters 
[-4.6900951   2.23743481 -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432454
  1.31029907 -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522468
 -2.0265424   0.7289737   1.60512664  2.83077099 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 19:15:52 2024]  Iteration number: 0 with current cost as 0.08455690777284282 and parameters 
[-4.66452267  2.23743464 -2.12427949 -0.11653103  0.553887   -2.77010912
  3.06858498  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578412 -0.54534343 -0.47522485
 -2.02654248  0.72897362  1.60512656  2.83077092 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 19:21:30 2024]  Iteration number: 0 with current cost as 0.08874339146370301 and parameters 
[-4.68039068  2.23743464 -2.12427935 -0.11653088  0.55388715 -2.7701089
  3.06858505  2.18960152  1.18552006 -1.06648308  0.60271532  1.1443246
  1.31029913 -1.87354666  0.72965095  2.88578419 -0.54534335 -0.47522471
 -2.0265424   0.7289737   1.60512671  2.83077107 -1.2645671  -0.25136097
 -2.39279211 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 19:27:12 2024]  Iteration number: 0 with current cost as 0.09587137153921405 and parameters 
[-4.6746318   2.2374348  -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432453
  1.31029907 -1.87354672  0.72965089  2.88578419 -0.54534335 -0.47522477
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136096
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550779 -2.69002202]. 
Training complete taking 1703.2376594543457 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 19:32:54 2024]  Iteration number: 0 with current cost as 0.10822095887456029 and parameters 
[-4.66319339  2.23743464 -2.12427957 -0.11653096  0.55388701 -2.77010904
  3.06858491  2.18960145  1.18552006 -1.06648316  0.60271517  1.14432452
  1.31029899 -1.87354673  0.72965073  2.88578412 -0.54534342 -0.47522478
 -2.02654248  0.72897362  1.60512671  2.830771   -1.26456703 -0.25136098
 -2.39279211 -2.27309767  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 19:38:36 2024]  Iteration number: 0 with current cost as 0.09061534838173133 and parameters 
[-4.6900951   2.23743481 -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432454
  1.31029907 -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522468
 -2.0265424   0.7289737   1.60512664  2.83077099 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 19:44:17 2024]  Iteration number: 0 with current cost as 0.08455690777284282 and parameters 
[-4.66452267  2.23743464 -2.12427949 -0.11653103  0.553887   -2.77010912
  3.06858498  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578412 -0.54534343 -0.47522485
 -2.02654248  0.72897362  1.60512656  2.83077092 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 19:50:01 2024]  Iteration number: 0 with current cost as 0.08874339146370301 and parameters 
[-4.68039068  2.23743464 -2.12427935 -0.11653088  0.55388715 -2.7701089
  3.06858505  2.18960152  1.18552006 -1.06648308  0.60271532  1.1443246
  1.31029913 -1.87354666  0.72965095  2.88578419 -0.54534335 -0.47522471
 -2.0265424   0.7289737   1.60512671  2.83077107 -1.2645671  -0.25136097
 -2.39279211 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 19:55:43 2024]  Iteration number: 0 with current cost as 0.09587137153921405 and parameters 
[-4.6746318   2.2374348  -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432453
  1.31029907 -1.87354672  0.72965089  2.88578419 -0.54534335 -0.47522477
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136096
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550779 -2.69002202]. 
Training complete taking 1710.529131412506 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 20:01:24 2024]  Iteration number: 0 with current cost as 0.10822095887456029 and parameters 
[-4.66319339  2.23743464 -2.12427957 -0.11653096  0.55388701 -2.77010904
  3.06858491  2.18960145  1.18552006 -1.06648316  0.60271517  1.14432452
  1.31029899 -1.87354673  0.72965073  2.88578412 -0.54534342 -0.47522478
 -2.02654248  0.72897362  1.60512671  2.830771   -1.26456703 -0.25136098
 -2.39279211 -2.27309767  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 20:07:05 2024]  Iteration number: 0 with current cost as 0.09061534838173133 and parameters 
[-4.6900951   2.23743481 -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432454
  1.31029907 -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522468
 -2.0265424   0.7289737   1.60512664  2.83077099 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 20:12:46 2024]  Iteration number: 0 with current cost as 0.08455690777284282 and parameters 
[-4.66452267  2.23743464 -2.12427949 -0.11653103  0.553887   -2.77010912
  3.06858498  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578412 -0.54534343 -0.47522485
 -2.02654248  0.72897362  1.60512656  2.83077092 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 20:18:36 2024]  Iteration number: 0 with current cost as 0.08874339146370301 and parameters 
[-4.68039068  2.23743464 -2.12427935 -0.11653088  0.55388715 -2.7701089
  3.06858505  2.18960152  1.18552006 -1.06648308  0.60271532  1.1443246
  1.31029913 -1.87354666  0.72965095  2.88578419 -0.54534335 -0.47522471
 -2.0265424   0.7289737   1.60512671  2.83077107 -1.2645671  -0.25136097
 -2.39279211 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 20:24:18 2024]  Iteration number: 0 with current cost as 0.09587137153921405 and parameters 
[-4.6746318   2.2374348  -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432453
  1.31029907 -1.87354672  0.72965089  2.88578419 -0.54534335 -0.47522477
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136096
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550779 -2.69002202]. 
Training complete taking 1717.9258215427399 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 20:30:05 2024]  Iteration number: 0 with current cost as 0.10822095887456029 and parameters 
[-4.66319339  2.23743464 -2.12427957 -0.11653096  0.55388701 -2.77010904
  3.06858491  2.18960145  1.18552006 -1.06648316  0.60271517  1.14432452
  1.31029899 -1.87354673  0.72965073  2.88578412 -0.54534342 -0.47522478
 -2.02654248  0.72897362  1.60512671  2.830771   -1.26456703 -0.25136098
 -2.39279211 -2.27309767  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 20:35:48 2024]  Iteration number: 0 with current cost as 0.09061534838173133 and parameters 
[-4.6900951   2.23743481 -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432454
  1.31029907 -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522468
 -2.0265424   0.7289737   1.60512664  2.83077099 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 20:41:30 2024]  Iteration number: 0 with current cost as 0.08455690777284282 and parameters 
[-4.66452267  2.23743464 -2.12427949 -0.11653103  0.553887   -2.77010912
  3.06858498  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578412 -0.54534343 -0.47522485
 -2.02654248  0.72897362  1.60512656  2.83077092 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 20:47:15 2024]  Iteration number: 0 with current cost as 0.08874339146370301 and parameters 
[-4.68039068  2.23743464 -2.12427935 -0.11653088  0.55388715 -2.7701089
  3.06858505  2.18960152  1.18552006 -1.06648308  0.60271532  1.1443246
  1.31029913 -1.87354666  0.72965095  2.88578419 -0.54534335 -0.47522471
 -2.0265424   0.7289737   1.60512671  2.83077107 -1.2645671  -0.25136097
 -2.39279211 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 20:52:59 2024]  Iteration number: 0 with current cost as 0.09587137153921405 and parameters 
[-4.6746318   2.2374348  -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432453
  1.31029907 -1.87354672  0.72965089  2.88578419 -0.54534335 -0.47522477
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136096
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550779 -2.69002202]. 
Training complete taking 1718.2109291553497 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 20:58:43 2024]  Iteration number: 0 with current cost as 0.10822095887456029 and parameters 
[-4.66319339  2.23743464 -2.12427957 -0.11653096  0.55388701 -2.77010904
  3.06858491  2.18960145  1.18552006 -1.06648316  0.60271517  1.14432452
  1.31029899 -1.87354673  0.72965073  2.88578412 -0.54534342 -0.47522478
 -2.02654248  0.72897362  1.60512671  2.830771   -1.26456703 -0.25136098
 -2.39279211 -2.27309767  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 21:04:30 2024]  Iteration number: 0 with current cost as 0.09061534838173133 and parameters 
[-4.6900951   2.23743481 -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432454
  1.31029907 -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522468
 -2.0265424   0.7289737   1.60512664  2.83077099 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 21:10:14 2024]  Iteration number: 0 with current cost as 0.08455690777284282 and parameters 
[-4.66452267  2.23743464 -2.12427949 -0.11653103  0.553887   -2.77010912
  3.06858498  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578412 -0.54534343 -0.47522485
 -2.02654248  0.72897362  1.60512656  2.83077092 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 21:16:01 2024]  Iteration number: 0 with current cost as 0.08874339146370301 and parameters 
[-4.68039068  2.23743464 -2.12427935 -0.11653088  0.55388715 -2.7701089
  3.06858505  2.18960152  1.18552006 -1.06648308  0.60271532  1.1443246
  1.31029913 -1.87354666  0.72965095  2.88578419 -0.54534335 -0.47522471
 -2.0265424   0.7289737   1.60512671  2.83077107 -1.2645671  -0.25136097
 -2.39279211 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 21:21:43 2024]  Iteration number: 0 with current cost as 0.09587137153921405 and parameters 
[-4.6746318   2.2374348  -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432453
  1.31029907 -1.87354672  0.72965089  2.88578419 -0.54534335 -0.47522477
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136096
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550779 -2.69002202]. 
Training complete taking 1726.1392328739166 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 21:27:30 2024]  Iteration number: 0 with current cost as 0.10822095887456029 and parameters 
[-4.66319339  2.23743464 -2.12427957 -0.11653096  0.55388701 -2.77010904
  3.06858491  2.18960145  1.18552006 -1.06648316  0.60271517  1.14432452
  1.31029899 -1.87354673  0.72965073  2.88578412 -0.54534342 -0.47522478
 -2.02654248  0.72897362  1.60512671  2.830771   -1.26456703 -0.25136098
 -2.39279211 -2.27309767  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 21:33:15 2024]  Iteration number: 0 with current cost as 0.09061534838173133 and parameters 
[-4.6900951   2.23743481 -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432454
  1.31029907 -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522468
 -2.0265424   0.7289737   1.60512664  2.83077099 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 21:39:03 2024]  Iteration number: 0 with current cost as 0.08455690777284282 and parameters 
[-4.66452267  2.23743464 -2.12427949 -0.11653103  0.553887   -2.77010912
  3.06858498  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578412 -0.54534343 -0.47522485
 -2.02654248  0.72897362  1.60512656  2.83077092 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 21:44:47 2024]  Iteration number: 0 with current cost as 0.08874339146370301 and parameters 
[-4.68039068  2.23743464 -2.12427935 -0.11653088  0.55388715 -2.7701089
  3.06858505  2.18960152  1.18552006 -1.06648308  0.60271532  1.1443246
  1.31029913 -1.87354666  0.72965095  2.88578419 -0.54534335 -0.47522471
 -2.0265424   0.7289737   1.60512671  2.83077107 -1.2645671  -0.25136097
 -2.39279211 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 21:50:43 2024]  Iteration number: 0 with current cost as 0.09587137153921405 and parameters 
[-4.6746318   2.2374348  -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432453
  1.31029907 -1.87354672  0.72965089  2.88578419 -0.54534335 -0.47522477
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136096
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550779 -2.69002202]. 
Training complete taking 1740.8994324207306 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 21:56:30 2024]  Iteration number: 0 with current cost as 0.10822095887456029 and parameters 
[-4.66319339  2.23743464 -2.12427957 -0.11653096  0.55388701 -2.77010904
  3.06858491  2.18960145  1.18552006 -1.06648316  0.60271517  1.14432452
  1.31029899 -1.87354673  0.72965073  2.88578412 -0.54534342 -0.47522478
 -2.02654248  0.72897362  1.60512671  2.830771   -1.26456703 -0.25136098
 -2.39279211 -2.27309767  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 22:02:16 2024]  Iteration number: 0 with current cost as 0.09061534838173133 and parameters 
[-4.6900951   2.23743481 -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432454
  1.31029907 -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522468
 -2.0265424   0.7289737   1.60512664  2.83077099 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 22:07:58 2024]  Iteration number: 0 with current cost as 0.08455690777284282 and parameters 
[-4.66452267  2.23743464 -2.12427949 -0.11653103  0.553887   -2.77010912
  3.06858498  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578412 -0.54534343 -0.47522485
 -2.02654248  0.72897362  1.60512656  2.83077092 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 22:13:43 2024]  Iteration number: 0 with current cost as 0.08874339146370301 and parameters 
[-4.68039068  2.23743464 -2.12427935 -0.11653088  0.55388715 -2.7701089
  3.06858505  2.18960152  1.18552006 -1.06648308  0.60271532  1.1443246
  1.31029913 -1.87354666  0.72965095  2.88578419 -0.54534335 -0.47522471
 -2.0265424   0.7289737   1.60512671  2.83077107 -1.2645671  -0.25136097
 -2.39279211 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 22:19:23 2024]  Iteration number: 0 with current cost as 0.09587137153921405 and parameters 
[-4.6746318   2.2374348  -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432453
  1.31029907 -1.87354672  0.72965089  2.88578419 -0.54534335 -0.47522477
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136096
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550779 -2.69002202]. 
Training complete taking 1719.8662219047546 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 22:25:13 2024]  Iteration number: 0 with current cost as 0.10822095887456029 and parameters 
[-4.66319339  2.23743464 -2.12427957 -0.11653096  0.55388701 -2.77010904
  3.06858491  2.18960145  1.18552006 -1.06648316  0.60271517  1.14432452
  1.31029899 -1.87354673  0.72965073  2.88578412 -0.54534342 -0.47522478
 -2.02654248  0.72897362  1.60512671  2.830771   -1.26456703 -0.25136098
 -2.39279211 -2.27309767  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 22:31:01 2024]  Iteration number: 0 with current cost as 0.09061534838173133 and parameters 
[-4.6900951   2.23743481 -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432454
  1.31029907 -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522468
 -2.0265424   0.7289737   1.60512664  2.83077099 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 22:36:47 2024]  Iteration number: 0 with current cost as 0.08455690777284282 and parameters 
[-4.66452267  2.23743464 -2.12427949 -0.11653103  0.553887   -2.77010912
  3.06858498  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578412 -0.54534343 -0.47522485
 -2.02654248  0.72897362  1.60512656  2.83077092 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 22:42:35 2024]  Iteration number: 0 with current cost as 0.08874339146370301 and parameters 
[-4.68039068  2.23743464 -2.12427935 -0.11653088  0.55388715 -2.7701089
  3.06858505  2.18960152  1.18552006 -1.06648308  0.60271532  1.1443246
  1.31029913 -1.87354666  0.72965095  2.88578419 -0.54534335 -0.47522471
 -2.0265424   0.7289737   1.60512671  2.83077107 -1.2645671  -0.25136097
 -2.39279211 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 22:48:19 2024]  Iteration number: 0 with current cost as 0.09587137153921405 and parameters 
[-4.6746318   2.2374348  -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432453
  1.31029907 -1.87354672  0.72965089  2.88578419 -0.54534335 -0.47522477
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136096
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550779 -2.69002202]. 
Training complete taking 1735.1731595993042 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 22:54:04 2024]  Iteration number: 0 with current cost as 0.10822095887456029 and parameters 
[-4.66319339  2.23743464 -2.12427957 -0.11653096  0.55388701 -2.77010904
  3.06858491  2.18960145  1.18552006 -1.06648316  0.60271517  1.14432452
  1.31029899 -1.87354673  0.72965073  2.88578412 -0.54534342 -0.47522478
 -2.02654248  0.72897362  1.60512671  2.830771   -1.26456703 -0.25136098
 -2.39279211 -2.27309767  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 22:59:51 2024]  Iteration number: 0 with current cost as 0.09061534838173133 and parameters 
[-4.6900951   2.23743481 -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432454
  1.31029907 -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522468
 -2.0265424   0.7289737   1.60512664  2.83077099 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 23:05:35 2024]  Iteration number: 0 with current cost as 0.08455690777284282 and parameters 
[-4.66452267  2.23743464 -2.12427949 -0.11653103  0.553887   -2.77010912
  3.06858498  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578412 -0.54534343 -0.47522485
 -2.02654248  0.72897362  1.60512656  2.83077092 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 23:11:18 2024]  Iteration number: 0 with current cost as 0.08874339146370301 and parameters 
[-4.68039068  2.23743464 -2.12427935 -0.11653088  0.55388715 -2.7701089
  3.06858505  2.18960152  1.18552006 -1.06648308  0.60271532  1.1443246
  1.31029913 -1.87354666  0.72965095  2.88578419 -0.54534335 -0.47522471
 -2.0265424   0.7289737   1.60512671  2.83077107 -1.2645671  -0.25136097
 -2.39279211 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 23:17:02 2024]  Iteration number: 0 with current cost as 0.09587137153921405 and parameters 
[-4.6746318   2.2374348  -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432453
  1.31029907 -1.87354672  0.72965089  2.88578419 -0.54534335 -0.47522477
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136096
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550779 -2.69002202]. 
Training complete taking 1723.8583071231842 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 23:22:49 2024]  Iteration number: 0 with current cost as 0.10822095887456029 and parameters 
[-4.66319339  2.23743464 -2.12427957 -0.11653096  0.55388701 -2.77010904
  3.06858491  2.18960145  1.18552006 -1.06648316  0.60271517  1.14432452
  1.31029899 -1.87354673  0.72965073  2.88578412 -0.54534342 -0.47522478
 -2.02654248  0.72897362  1.60512671  2.830771   -1.26456703 -0.25136098
 -2.39279211 -2.27309767  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 23:28:36 2024]  Iteration number: 0 with current cost as 0.09061534838173133 and parameters 
[-4.6900951   2.23743481 -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432454
  1.31029907 -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522468
 -2.0265424   0.7289737   1.60512664  2.83077099 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 23:34:24 2024]  Iteration number: 0 with current cost as 0.08455690777284282 and parameters 
[-4.66452267  2.23743464 -2.12427949 -0.11653103  0.553887   -2.77010912
  3.06858498  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578412 -0.54534343 -0.47522485
 -2.02654248  0.72897362  1.60512656  2.83077092 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 23:40:14 2024]  Iteration number: 0 with current cost as 0.08874339146370301 and parameters 
[-4.68039068  2.23743464 -2.12427935 -0.11653088  0.55388715 -2.7701089
  3.06858505  2.18960152  1.18552006 -1.06648308  0.60271532  1.1443246
  1.31029913 -1.87354666  0.72965095  2.88578419 -0.54534335 -0.47522471
 -2.0265424   0.7289737   1.60512671  2.83077107 -1.2645671  -0.25136097
 -2.39279211 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 23:46:08 2024]  Iteration number: 0 with current cost as 0.09587137153921405 and parameters 
[-4.6746318   2.2374348  -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432453
  1.31029907 -1.87354672  0.72965089  2.88578419 -0.54534335 -0.47522477
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136096
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550779 -2.69002202]. 
Training complete taking 1745.680858373642 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 23:51:59 2024]  Iteration number: 0 with current cost as 0.10822095887456029 and parameters 
[-4.66319339  2.23743464 -2.12427957 -0.11653096  0.55388701 -2.77010904
  3.06858491  2.18960145  1.18552006 -1.06648316  0.60271517  1.14432452
  1.31029899 -1.87354673  0.72965073  2.88578412 -0.54534342 -0.47522478
 -2.02654248  0.72897362  1.60512671  2.830771   -1.26456703 -0.25136098
 -2.39279211 -2.27309767  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 23:57:46 2024]  Iteration number: 0 with current cost as 0.09061534838173133 and parameters 
[-4.6900951   2.23743481 -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432454
  1.31029907 -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522468
 -2.0265424   0.7289737   1.60512664  2.83077099 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 00:03:33 2024]  Iteration number: 0 with current cost as 0.08455690777284282 and parameters 
[-4.66452267  2.23743464 -2.12427949 -0.11653103  0.553887   -2.77010912
  3.06858498  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578412 -0.54534343 -0.47522485
 -2.02654248  0.72897362  1.60512656  2.83077092 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 00:09:24 2024]  Iteration number: 0 with current cost as 0.08874339146370301 and parameters 
[-4.68039068  2.23743464 -2.12427935 -0.11653088  0.55388715 -2.7701089
  3.06858505  2.18960152  1.18552006 -1.06648308  0.60271532  1.1443246
  1.31029913 -1.87354666  0.72965095  2.88578419 -0.54534335 -0.47522471
 -2.0265424   0.7289737   1.60512671  2.83077107 -1.2645671  -0.25136097
 -2.39279211 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 00:15:08 2024]  Iteration number: 0 with current cost as 0.09587137153921405 and parameters 
[-4.6746318   2.2374348  -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432453
  1.31029907 -1.87354672  0.72965089  2.88578419 -0.54534335 -0.47522477
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136096
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550779 -2.69002202]. 
Training complete taking 1739.3424365520477 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 00:20:54 2024]  Iteration number: 0 with current cost as 0.10822095887456029 and parameters 
[-4.66319339  2.23743464 -2.12427957 -0.11653096  0.55388701 -2.77010904
  3.06858491  2.18960145  1.18552006 -1.06648316  0.60271517  1.14432452
  1.31029899 -1.87354673  0.72965073  2.88578412 -0.54534342 -0.47522478
 -2.02654248  0.72897362  1.60512671  2.830771   -1.26456703 -0.25136098
 -2.39279211 -2.27309767  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 00:26:45 2024]  Iteration number: 0 with current cost as 0.09061534838173133 and parameters 
[-4.6900951   2.23743481 -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432454
  1.31029907 -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522468
 -2.0265424   0.7289737   1.60512664  2.83077099 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 00:32:28 2024]  Iteration number: 0 with current cost as 0.08455690777284282 and parameters 
[-4.66452267  2.23743464 -2.12427949 -0.11653103  0.553887   -2.77010912
  3.06858498  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578412 -0.54534343 -0.47522485
 -2.02654248  0.72897362  1.60512656  2.83077092 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 00:38:13 2024]  Iteration number: 0 with current cost as 0.08874339146370301 and parameters 
[-4.68039068  2.23743464 -2.12427935 -0.11653088  0.55388715 -2.7701089
  3.06858505  2.18960152  1.18552006 -1.06648308  0.60271532  1.1443246
  1.31029913 -1.87354666  0.72965095  2.88578419 -0.54534335 -0.47522471
 -2.0265424   0.7289737   1.60512671  2.83077107 -1.2645671  -0.25136097
 -2.39279211 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 00:43:59 2024]  Iteration number: 0 with current cost as 0.09587137153921405 and parameters 
[-4.6746318   2.2374348  -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432453
  1.31029907 -1.87354672  0.72965089  2.88578419 -0.54534335 -0.47522477
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136096
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550779 -2.69002202]. 
Training complete taking 1733.4163126945496 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 00:49:46 2024]  Iteration number: 0 with current cost as 0.10822095887456029 and parameters 
[-4.66319339  2.23743464 -2.12427957 -0.11653096  0.55388701 -2.77010904
  3.06858491  2.18960145  1.18552006 -1.06648316  0.60271517  1.14432452
  1.31029899 -1.87354673  0.72965073  2.88578412 -0.54534342 -0.47522478
 -2.02654248  0.72897362  1.60512671  2.830771   -1.26456703 -0.25136098
 -2.39279211 -2.27309767  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 00:55:36 2024]  Iteration number: 0 with current cost as 0.09061534838173133 and parameters 
[-4.6900951   2.23743481 -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432454
  1.31029907 -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522468
 -2.0265424   0.7289737   1.60512664  2.83077099 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 01:01:26 2024]  Iteration number: 0 with current cost as 0.08455690777284282 and parameters 
[-4.66452267  2.23743464 -2.12427949 -0.11653103  0.553887   -2.77010912
  3.06858498  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578412 -0.54534343 -0.47522485
 -2.02654248  0.72897362  1.60512656  2.83077092 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 01:07:20 2024]  Iteration number: 0 with current cost as 0.08874339146370301 and parameters 
[-4.68039068  2.23743464 -2.12427935 -0.11653088  0.55388715 -2.7701089
  3.06858505  2.18960152  1.18552006 -1.06648308  0.60271532  1.1443246
  1.31029913 -1.87354666  0.72965095  2.88578419 -0.54534335 -0.47522471
 -2.0265424   0.7289737   1.60512671  2.83077107 -1.2645671  -0.25136097
 -2.39279211 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 01:13:04 2024]  Iteration number: 0 with current cost as 0.09587137153921405 and parameters 
[-4.6746318   2.2374348  -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432453
  1.31029907 -1.87354672  0.72965089  2.88578419 -0.54534335 -0.47522477
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136096
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550779 -2.69002202]. 
Training complete taking 1742.0111002922058 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 01:18:50 2024]  Iteration number: 0 with current cost as 0.10822095887456029 and parameters 
[-4.66319339  2.23743464 -2.12427957 -0.11653096  0.55388701 -2.77010904
  3.06858491  2.18960145  1.18552006 -1.06648316  0.60271517  1.14432452
  1.31029899 -1.87354673  0.72965073  2.88578412 -0.54534342 -0.47522478
 -2.02654248  0.72897362  1.60512671  2.830771   -1.26456703 -0.25136098
 -2.39279211 -2.27309767  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 01:24:31 2024]  Iteration number: 0 with current cost as 0.09061534838173133 and parameters 
[-4.6900951   2.23743481 -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432454
  1.31029907 -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522468
 -2.0265424   0.7289737   1.60512664  2.83077099 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 01:30:15 2024]  Iteration number: 0 with current cost as 0.08455690777284282 and parameters 
[-4.66452267  2.23743464 -2.12427949 -0.11653103  0.553887   -2.77010912
  3.06858498  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578412 -0.54534343 -0.47522485
 -2.02654248  0.72897362  1.60512656  2.83077092 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 01:36:05 2024]  Iteration number: 0 with current cost as 0.08874339146370301 and parameters 
[-4.68039068  2.23743464 -2.12427935 -0.11653088  0.55388715 -2.7701089
  3.06858505  2.18960152  1.18552006 -1.06648308  0.60271532  1.1443246
  1.31029913 -1.87354666  0.72965095  2.88578419 -0.54534335 -0.47522471
 -2.0265424   0.7289737   1.60512671  2.83077107 -1.2645671  -0.25136097
 -2.39279211 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 01:41:53 2024]  Iteration number: 0 with current cost as 0.09587137153921405 and parameters 
[-4.6746318   2.2374348  -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432453
  1.31029907 -1.87354672  0.72965089  2.88578419 -0.54534335 -0.47522477
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136096
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550779 -2.69002202]. 
Training complete taking 1730.2858335971832 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 01:47:42 2024]  Iteration number: 0 with current cost as 0.10822095887456029 and parameters 
[-4.66319339  2.23743464 -2.12427957 -0.11653096  0.55388701 -2.77010904
  3.06858491  2.18960145  1.18552006 -1.06648316  0.60271517  1.14432452
  1.31029899 -1.87354673  0.72965073  2.88578412 -0.54534342 -0.47522478
 -2.02654248  0.72897362  1.60512671  2.830771   -1.26456703 -0.25136098
 -2.39279211 -2.27309767  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 01:53:32 2024]  Iteration number: 0 with current cost as 0.09061534838173133 and parameters 
[-4.6900951   2.23743481 -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432454
  1.31029907 -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522468
 -2.0265424   0.7289737   1.60512664  2.83077099 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 01:59:15 2024]  Iteration number: 0 with current cost as 0.08455690777284282 and parameters 
[-4.66452267  2.23743464 -2.12427949 -0.11653103  0.553887   -2.77010912
  3.06858498  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578412 -0.54534343 -0.47522485
 -2.02654248  0.72897362  1.60512656  2.83077092 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 02:05:10 2024]  Iteration number: 0 with current cost as 0.08874339146370301 and parameters 
[-4.68039068  2.23743464 -2.12427935 -0.11653088  0.55388715 -2.7701089
  3.06858505  2.18960152  1.18552006 -1.06648308  0.60271532  1.1443246
  1.31029913 -1.87354666  0.72965095  2.88578419 -0.54534335 -0.47522471
 -2.0265424   0.7289737   1.60512671  2.83077107 -1.2645671  -0.25136097
 -2.39279211 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 02:10:57 2024]  Iteration number: 0 with current cost as 0.09587137153921405 and parameters 
[-4.6746318   2.2374348  -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432453
  1.31029907 -1.87354672  0.72965089  2.88578419 -0.54534335 -0.47522477
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136096
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550779 -2.69002202]. 
Training complete taking 1742.3031151294708 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 02:16:41 2024]  Iteration number: 0 with current cost as 0.10822095887456029 and parameters 
[-4.66319339  2.23743464 -2.12427957 -0.11653096  0.55388701 -2.77010904
  3.06858491  2.18960145  1.18552006 -1.06648316  0.60271517  1.14432452
  1.31029899 -1.87354673  0.72965073  2.88578412 -0.54534342 -0.47522478
 -2.02654248  0.72897362  1.60512671  2.830771   -1.26456703 -0.25136098
 -2.39279211 -2.27309767  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 02:22:27 2024]  Iteration number: 0 with current cost as 0.09061534838173133 and parameters 
[-4.6900951   2.23743481 -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432454
  1.31029907 -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522468
 -2.0265424   0.7289737   1.60512664  2.83077099 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 02:28:24 2024]  Iteration number: 0 with current cost as 0.08455690777284282 and parameters 
[-4.66452267  2.23743464 -2.12427949 -0.11653103  0.553887   -2.77010912
  3.06858498  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578412 -0.54534343 -0.47522485
 -2.02654248  0.72897362  1.60512656  2.83077092 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 02:34:17 2024]  Iteration number: 0 with current cost as 0.08874339146370301 and parameters 
[-4.68039068  2.23743464 -2.12427935 -0.11653088  0.55388715 -2.7701089
  3.06858505  2.18960152  1.18552006 -1.06648308  0.60271532  1.1443246
  1.31029913 -1.87354666  0.72965095  2.88578419 -0.54534335 -0.47522471
 -2.0265424   0.7289737   1.60512671  2.83077107 -1.2645671  -0.25136097
 -2.39279211 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 02:40:14 2024]  Iteration number: 0 with current cost as 0.09587137153921405 and parameters 
[-4.6746318   2.2374348  -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432453
  1.31029907 -1.87354672  0.72965089  2.88578419 -0.54534335 -0.47522477
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136096
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550779 -2.69002202]. 
Training complete taking 1760.147672176361 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 02:46:08 2024]  Iteration number: 0 with current cost as 0.10822095887456029 and parameters 
[-4.66319339  2.23743464 -2.12427957 -0.11653096  0.55388701 -2.77010904
  3.06858491  2.18960145  1.18552006 -1.06648316  0.60271517  1.14432452
  1.31029899 -1.87354673  0.72965073  2.88578412 -0.54534342 -0.47522478
 -2.02654248  0.72897362  1.60512671  2.830771   -1.26456703 -0.25136098
 -2.39279211 -2.27309767  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 02:52:06 2024]  Iteration number: 0 with current cost as 0.09061534838173133 and parameters 
[-4.6900951   2.23743481 -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432454
  1.31029907 -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522468
 -2.0265424   0.7289737   1.60512664  2.83077099 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 02:58:00 2024]  Iteration number: 0 with current cost as 0.08455690777284282 and parameters 
[-4.66452267  2.23743464 -2.12427949 -0.11653103  0.553887   -2.77010912
  3.06858498  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578412 -0.54534343 -0.47522485
 -2.02654248  0.72897362  1.60512656  2.83077092 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 03:03:52 2024]  Iteration number: 0 with current cost as 0.08874339146370301 and parameters 
[-4.68039068  2.23743464 -2.12427935 -0.11653088  0.55388715 -2.7701089
  3.06858505  2.18960152  1.18552006 -1.06648308  0.60271532  1.1443246
  1.31029913 -1.87354666  0.72965095  2.88578419 -0.54534335 -0.47522471
 -2.0265424   0.7289737   1.60512671  2.83077107 -1.2645671  -0.25136097
 -2.39279211 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 03:09:53 2024]  Iteration number: 0 with current cost as 0.09587137153921405 and parameters 
[-4.6746318   2.2374348  -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432453
  1.31029907 -1.87354672  0.72965089  2.88578419 -0.54534335 -0.47522477
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136096
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550779 -2.69002202]. 
Training complete taking 1786.9935791492462 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 03:15:55 2024]  Iteration number: 0 with current cost as 0.10822095887456029 and parameters 
[-4.66319339  2.23743464 -2.12427957 -0.11653096  0.55388701 -2.77010904
  3.06858491  2.18960145  1.18552006 -1.06648316  0.60271517  1.14432452
  1.31029899 -1.87354673  0.72965073  2.88578412 -0.54534342 -0.47522478
 -2.02654248  0.72897362  1.60512671  2.830771   -1.26456703 -0.25136098
 -2.39279211 -2.27309767  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 03:21:54 2024]  Iteration number: 0 with current cost as 0.09061534838173133 and parameters 
[-4.6900951   2.23743481 -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432454
  1.31029907 -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522468
 -2.0265424   0.7289737   1.60512664  2.83077099 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 03:27:51 2024]  Iteration number: 0 with current cost as 0.08455690777284282 and parameters 
[-4.66452267  2.23743464 -2.12427949 -0.11653103  0.553887   -2.77010912
  3.06858498  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578412 -0.54534343 -0.47522485
 -2.02654248  0.72897362  1.60512656  2.83077092 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 03:33:37 2024]  Iteration number: 0 with current cost as 0.08874339146370301 and parameters 
[-4.68039068  2.23743464 -2.12427935 -0.11653088  0.55388715 -2.7701089
  3.06858505  2.18960152  1.18552006 -1.06648308  0.60271532  1.1443246
  1.31029913 -1.87354666  0.72965095  2.88578419 -0.54534335 -0.47522471
 -2.0265424   0.7289737   1.60512671  2.83077107 -1.2645671  -0.25136097
 -2.39279211 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 03:39:25 2024]  Iteration number: 0 with current cost as 0.09587137153921405 and parameters 
[-4.6746318   2.2374348  -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432453
  1.31029907 -1.87354672  0.72965089  2.88578419 -0.54534335 -0.47522477
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136096
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550779 -2.69002202]. 
Training complete taking 1765.2162370681763 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 03:45:12 2024]  Iteration number: 0 with current cost as 0.10822095887456029 and parameters 
[-4.66319339  2.23743464 -2.12427957 -0.11653096  0.55388701 -2.77010904
  3.06858491  2.18960145  1.18552006 -1.06648316  0.60271517  1.14432452
  1.31029899 -1.87354673  0.72965073  2.88578412 -0.54534342 -0.47522478
 -2.02654248  0.72897362  1.60512671  2.830771   -1.26456703 -0.25136098
 -2.39279211 -2.27309767  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 03:51:03 2024]  Iteration number: 0 with current cost as 0.09061534838173133 and parameters 
[-4.6900951   2.23743481 -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432454
  1.31029907 -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522468
 -2.0265424   0.7289737   1.60512664  2.83077099 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 03:56:50 2024]  Iteration number: 0 with current cost as 0.08455690777284282 and parameters 
[-4.66452267  2.23743464 -2.12427949 -0.11653103  0.553887   -2.77010912
  3.06858498  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578412 -0.54534343 -0.47522485
 -2.02654248  0.72897362  1.60512656  2.83077092 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 04:02:33 2024]  Iteration number: 0 with current cost as 0.08874339146370301 and parameters 
[-4.68039068  2.23743464 -2.12427935 -0.11653088  0.55388715 -2.7701089
  3.06858505  2.18960152  1.18552006 -1.06648308  0.60271532  1.1443246
  1.31029913 -1.87354666  0.72965095  2.88578419 -0.54534335 -0.47522471
 -2.0265424   0.7289737   1.60512671  2.83077107 -1.2645671  -0.25136097
 -2.39279211 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 04:08:17 2024]  Iteration number: 0 with current cost as 0.09587137153921405 and parameters 
[-4.6746318   2.2374348  -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432453
  1.31029907 -1.87354672  0.72965089  2.88578419 -0.54534335 -0.47522477
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136096
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550779 -2.69002202]. 
Training complete taking 1728.7434570789337 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 04:14:07 2024]  Iteration number: 0 with current cost as 0.10822095887456029 and parameters 
[-4.66319339  2.23743464 -2.12427957 -0.11653096  0.55388701 -2.77010904
  3.06858491  2.18960145  1.18552006 -1.06648316  0.60271517  1.14432452
  1.31029899 -1.87354673  0.72965073  2.88578412 -0.54534342 -0.47522478
 -2.02654248  0.72897362  1.60512671  2.830771   -1.26456703 -0.25136098
 -2.39279211 -2.27309767  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 04:19:52 2024]  Iteration number: 0 with current cost as 0.09061534838173133 and parameters 
[-4.6900951   2.23743481 -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432454
  1.31029907 -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522468
 -2.0265424   0.7289737   1.60512664  2.83077099 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 04:25:33 2024]  Iteration number: 0 with current cost as 0.08455690777284282 and parameters 
[-4.66452267  2.23743464 -2.12427949 -0.11653103  0.553887   -2.77010912
  3.06858498  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578412 -0.54534343 -0.47522485
 -2.02654248  0.72897362  1.60512656  2.83077092 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 04:31:16 2024]  Iteration number: 0 with current cost as 0.08874339146370301 and parameters 
[-4.68039068  2.23743464 -2.12427935 -0.11653088  0.55388715 -2.7701089
  3.06858505  2.18960152  1.18552006 -1.06648308  0.60271532  1.1443246
  1.31029913 -1.87354666  0.72965095  2.88578419 -0.54534335 -0.47522471
 -2.0265424   0.7289737   1.60512671  2.83077107 -1.2645671  -0.25136097
 -2.39279211 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 04:36:57 2024]  Iteration number: 0 with current cost as 0.09587137153921405 and parameters 
[-4.6746318   2.2374348  -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432453
  1.31029907 -1.87354672  0.72965089  2.88578419 -0.54534335 -0.47522477
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136096
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550779 -2.69002202]. 
Training complete taking 1721.5632882118225 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 04:42:44 2024]  Iteration number: 0 with current cost as 0.10822095887456029 and parameters 
[-4.66319339  2.23743464 -2.12427957 -0.11653096  0.55388701 -2.77010904
  3.06858491  2.18960145  1.18552006 -1.06648316  0.60271517  1.14432452
  1.31029899 -1.87354673  0.72965073  2.88578412 -0.54534342 -0.47522478
 -2.02654248  0.72897362  1.60512671  2.830771   -1.26456703 -0.25136098
 -2.39279211 -2.27309767  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 04:48:32 2024]  Iteration number: 0 with current cost as 0.09061534838173133 and parameters 
[-4.6900951   2.23743481 -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432454
  1.31029907 -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522468
 -2.0265424   0.7289737   1.60512664  2.83077099 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 04:54:26 2024]  Iteration number: 0 with current cost as 0.08455690777284282 and parameters 
[-4.66452267  2.23743464 -2.12427949 -0.11653103  0.553887   -2.77010912
  3.06858498  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578412 -0.54534343 -0.47522485
 -2.02654248  0.72897362  1.60512656  2.83077092 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 05:00:10 2024]  Iteration number: 0 with current cost as 0.08874339146370301 and parameters 
[-4.68039068  2.23743464 -2.12427935 -0.11653088  0.55388715 -2.7701089
  3.06858505  2.18960152  1.18552006 -1.06648308  0.60271532  1.1443246
  1.31029913 -1.87354666  0.72965095  2.88578419 -0.54534335 -0.47522471
 -2.0265424   0.7289737   1.60512671  2.83077107 -1.2645671  -0.25136097
 -2.39279211 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 05:05:55 2024]  Iteration number: 0 with current cost as 0.09587137153921405 and parameters 
[-4.6746318   2.2374348  -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432453
  1.31029907 -1.87354672  0.72965089  2.88578419 -0.54534335 -0.47522477
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136096
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550779 -2.69002202]. 
Training complete taking 1737.5965354442596 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 05:11:40 2024]  Iteration number: 0 with current cost as 0.10822095887456029 and parameters 
[-4.66319339  2.23743464 -2.12427957 -0.11653096  0.55388701 -2.77010904
  3.06858491  2.18960145  1.18552006 -1.06648316  0.60271517  1.14432452
  1.31029899 -1.87354673  0.72965073  2.88578412 -0.54534342 -0.47522478
 -2.02654248  0.72897362  1.60512671  2.830771   -1.26456703 -0.25136098
 -2.39279211 -2.27309767  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 05:17:22 2024]  Iteration number: 0 with current cost as 0.09061534838173133 and parameters 
[-4.6900951   2.23743481 -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432454
  1.31029907 -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522468
 -2.0265424   0.7289737   1.60512664  2.83077099 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 05:23:07 2024]  Iteration number: 0 with current cost as 0.08455690777284282 and parameters 
[-4.66452267  2.23743464 -2.12427949 -0.11653103  0.553887   -2.77010912
  3.06858498  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578412 -0.54534343 -0.47522485
 -2.02654248  0.72897362  1.60512656  2.83077092 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 05:28:49 2024]  Iteration number: 0 with current cost as 0.08874339146370301 and parameters 
[-4.68039068  2.23743464 -2.12427935 -0.11653088  0.55388715 -2.7701089
  3.06858505  2.18960152  1.18552006 -1.06648308  0.60271532  1.1443246
  1.31029913 -1.87354666  0.72965095  2.88578419 -0.54534335 -0.47522471
 -2.0265424   0.7289737   1.60512671  2.83077107 -1.2645671  -0.25136097
 -2.39279211 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 05:34:33 2024]  Iteration number: 0 with current cost as 0.09587137153921405 and parameters 
[-4.6746318   2.2374348  -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432453
  1.31029907 -1.87354672  0.72965089  2.88578419 -0.54534335 -0.47522477
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136096
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550779 -2.69002202]. 
Training complete taking 1715.629340171814 seconds. 
Discarding model... 

Training complete taking 43286.567755937576 total seconds. 
Now scoring model... 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Scoring complete taking 2.1562013626098633 seconds. 
Saved predicted values as M-A1-CZ_Full-CRX_predicted_values.csv
Model scores: {'MSE_train': (58.35083788403513,), 'R2_train': 0.7173097739218985, 'MAE_train': 5.013475892245843, 'MSE_test': 77.63284334844319, 'R2_test': 0.5320436669882785, 'MAE_test': 5.13789221227575}. 
Saved model results as M-A1-CZ_Full-CRX_results.json. 
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/circuits/Encoders.py:12: RuntimeWarning: invalid value encountered in arcsin
  if np.isnan(np.arcsin(features[feature_idx])) or np.isnan(np.arccos(features[feature_idx])):
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Thu Apr  4 12:07:36 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 12:11:32 2024]  Iteration number: 0 with current cost as 0.10822095887456029 and parameters 
[-4.66319339  2.23743464 -2.12427957 -0.11653096  0.55388701 -2.77010904
  3.06858491  2.18960145  1.18552006 -1.06648316  0.60271517  1.14432452
  1.31029899 -1.87354673  0.72965073  2.88578412 -0.54534342 -0.47522478
 -2.02654248  0.72897362  1.60512671  2.830771   -1.26456703 -0.25136098
 -2.39279211 -2.27309767  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 12:17:19 2024]  Iteration number: 0 with current cost as 0.09061534838173133 and parameters 
[-4.6900951   2.23743481 -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432454
  1.31029907 -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522468
 -2.0265424   0.7289737   1.60512664  2.83077099 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 12:23:08 2024]  Iteration number: 0 with current cost as 0.08455690777284282 and parameters 
[-4.66452267  2.23743464 -2.12427949 -0.11653103  0.553887   -2.77010912
  3.06858498  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578412 -0.54534343 -0.47522485
 -2.02654248  0.72897362  1.60512656  2.83077092 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 12:28:52 2024]  Iteration number: 0 with current cost as 0.08874339146370301 and parameters 
[-4.68039068  2.23743464 -2.12427935 -0.11653088  0.55388715 -2.7701089
  3.06858505  2.18960152  1.18552006 -1.06648308  0.60271532  1.1443246
  1.31029913 -1.87354666  0.72965095  2.88578419 -0.54534335 -0.47522471
 -2.0265424   0.7289737   1.60512671  2.83077107 -1.2645671  -0.25136097
 -2.39279211 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 12:34:39 2024]  Iteration number: 0 with current cost as 0.09587137153921405 and parameters 
[-4.6746318   2.2374348  -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432453
  1.31029907 -1.87354672  0.72965089  2.88578419 -0.54534335 -0.47522477
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136096
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550779 -2.69002202]. 
Training complete taking 1739.0820245742798 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 12:40:26 2024]  Iteration number: 0 with current cost as 0.10822095887456029 and parameters 
[-4.66319339  2.23743464 -2.12427957 -0.11653096  0.55388701 -2.77010904
  3.06858491  2.18960145  1.18552006 -1.06648316  0.60271517  1.14432452
  1.31029899 -1.87354673  0.72965073  2.88578412 -0.54534342 -0.47522478
 -2.02654248  0.72897362  1.60512671  2.830771   -1.26456703 -0.25136098
 -2.39279211 -2.27309767  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 12:46:11 2024]  Iteration number: 0 with current cost as 0.09061534838173133 and parameters 
[-4.6900951   2.23743481 -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432454
  1.31029907 -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522468
 -2.0265424   0.7289737   1.60512664  2.83077099 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 12:51:57 2024]  Iteration number: 0 with current cost as 0.08455690777284282 and parameters 
[-4.66452267  2.23743464 -2.12427949 -0.11653103  0.553887   -2.77010912
  3.06858498  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578412 -0.54534343 -0.47522485
 -2.02654248  0.72897362  1.60512656  2.83077092 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 12:57:38 2024]  Iteration number: 0 with current cost as 0.08874339146370301 and parameters 
[-4.68039068  2.23743464 -2.12427935 -0.11653088  0.55388715 -2.7701089
  3.06858505  2.18960152  1.18552006 -1.06648308  0.60271532  1.1443246
  1.31029913 -1.87354666  0.72965095  2.88578419 -0.54534335 -0.47522471
 -2.0265424   0.7289737   1.60512671  2.83077107 -1.2645671  -0.25136097
 -2.39279211 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 13:03:22 2024]  Iteration number: 0 with current cost as 0.09587137153921405 and parameters 
[-4.6746318   2.2374348  -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432453
  1.31029907 -1.87354672  0.72965089  2.88578419 -0.54534335 -0.47522477
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136096
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550779 -2.69002202]. 
Training complete taking 1720.1484837532043 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 13:09:05 2024]  Iteration number: 0 with current cost as 0.10822095887456029 and parameters 
[-4.66319339  2.23743464 -2.12427957 -0.11653096  0.55388701 -2.77010904
  3.06858491  2.18960145  1.18552006 -1.06648316  0.60271517  1.14432452
  1.31029899 -1.87354673  0.72965073  2.88578412 -0.54534342 -0.47522478
 -2.02654248  0.72897362  1.60512671  2.830771   -1.26456703 -0.25136098
 -2.39279211 -2.27309767  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 13:14:52 2024]  Iteration number: 0 with current cost as 0.09061534838173133 and parameters 
[-4.6900951   2.23743481 -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432454
  1.31029907 -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522468
 -2.0265424   0.7289737   1.60512664  2.83077099 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 13:20:39 2024]  Iteration number: 0 with current cost as 0.08455690777284282 and parameters 
[-4.66452267  2.23743464 -2.12427949 -0.11653103  0.553887   -2.77010912
  3.06858498  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578412 -0.54534343 -0.47522485
 -2.02654248  0.72897362  1.60512656  2.83077092 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 13:26:23 2024]  Iteration number: 0 with current cost as 0.08874339146370301 and parameters 
[-4.68039068  2.23743464 -2.12427935 -0.11653088  0.55388715 -2.7701089
  3.06858505  2.18960152  1.18552006 -1.06648308  0.60271532  1.1443246
  1.31029913 -1.87354666  0.72965095  2.88578419 -0.54534335 -0.47522471
 -2.0265424   0.7289737   1.60512671  2.83077107 -1.2645671  -0.25136097
 -2.39279211 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 13:32:07 2024]  Iteration number: 0 with current cost as 0.09587137153921405 and parameters 
[-4.6746318   2.2374348  -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432453
  1.31029907 -1.87354672  0.72965089  2.88578419 -0.54534335 -0.47522477
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136096
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550779 -2.69002202]. 
Training complete taking 1726.8751769065857 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 13:37:55 2024]  Iteration number: 0 with current cost as 0.10822095887456029 and parameters 
[-4.66319339  2.23743464 -2.12427957 -0.11653096  0.55388701 -2.77010904
  3.06858491  2.18960145  1.18552006 -1.06648316  0.60271517  1.14432452
  1.31029899 -1.87354673  0.72965073  2.88578412 -0.54534342 -0.47522478
 -2.02654248  0.72897362  1.60512671  2.830771   -1.26456703 -0.25136098
 -2.39279211 -2.27309767  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 13:43:36 2024]  Iteration number: 0 with current cost as 0.09061534838173133 and parameters 
[-4.6900951   2.23743481 -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432454
  1.31029907 -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522468
 -2.0265424   0.7289737   1.60512664  2.83077099 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 13:49:19 2024]  Iteration number: 0 with current cost as 0.08455690777284282 and parameters 
[-4.66452267  2.23743464 -2.12427949 -0.11653103  0.553887   -2.77010912
  3.06858498  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578412 -0.54534343 -0.47522485
 -2.02654248  0.72897362  1.60512656  2.83077092 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 13:54:57 2024]  Iteration number: 0 with current cost as 0.08874339146370301 and parameters 
[-4.68039068  2.23743464 -2.12427935 -0.11653088  0.55388715 -2.7701089
  3.06858505  2.18960152  1.18552006 -1.06648308  0.60271532  1.1443246
  1.31029913 -1.87354666  0.72965095  2.88578419 -0.54534335 -0.47522471
 -2.0265424   0.7289737   1.60512671  2.83077107 -1.2645671  -0.25136097
 -2.39279211 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 14:00:39 2024]  Iteration number: 0 with current cost as 0.09587137153921405 and parameters 
[-4.6746318   2.2374348  -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432453
  1.31029907 -1.87354672  0.72965089  2.88578419 -0.54534335 -0.47522477
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136096
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550779 -2.69002202]. 
Training complete taking 1709.7612280845642 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 14:06:22 2024]  Iteration number: 0 with current cost as 0.10822095887456029 and parameters 
[-4.66319339  2.23743464 -2.12427957 -0.11653096  0.55388701 -2.77010904
  3.06858491  2.18960145  1.18552006 -1.06648316  0.60271517  1.14432452
  1.31029899 -1.87354673  0.72965073  2.88578412 -0.54534342 -0.47522478
 -2.02654248  0.72897362  1.60512671  2.830771   -1.26456703 -0.25136098
 -2.39279211 -2.27309767  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 14:12:09 2024]  Iteration number: 0 with current cost as 0.09061534838173133 and parameters 
[-4.6900951   2.23743481 -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432454
  1.31029907 -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522468
 -2.0265424   0.7289737   1.60512664  2.83077099 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 14:17:54 2024]  Iteration number: 0 with current cost as 0.08455690777284282 and parameters 
[-4.66452267  2.23743464 -2.12427949 -0.11653103  0.553887   -2.77010912
  3.06858498  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578412 -0.54534343 -0.47522485
 -2.02654248  0.72897362  1.60512656  2.83077092 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 14:23:39 2024]  Iteration number: 0 with current cost as 0.08874339146370301 and parameters 
[-4.68039068  2.23743464 -2.12427935 -0.11653088  0.55388715 -2.7701089
  3.06858505  2.18960152  1.18552006 -1.06648308  0.60271532  1.1443246
  1.31029913 -1.87354666  0.72965095  2.88578419 -0.54534335 -0.47522471
 -2.0265424   0.7289737   1.60512671  2.83077107 -1.2645671  -0.25136097
 -2.39279211 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 14:29:25 2024]  Iteration number: 0 with current cost as 0.09587137153921405 and parameters 
[-4.6746318   2.2374348  -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432453
  1.31029907 -1.87354672  0.72965089  2.88578419 -0.54534335 -0.47522477
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136096
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550779 -2.69002202]. 
Training complete taking 1728.268797636032 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 14:35:07 2024]  Iteration number: 0 with current cost as 0.10822095887456029 and parameters 
[-4.66319339  2.23743464 -2.12427957 -0.11653096  0.55388701 -2.77010904
  3.06858491  2.18960145  1.18552006 -1.06648316  0.60271517  1.14432452
  1.31029899 -1.87354673  0.72965073  2.88578412 -0.54534342 -0.47522478
 -2.02654248  0.72897362  1.60512671  2.830771   -1.26456703 -0.25136098
 -2.39279211 -2.27309767  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 14:40:51 2024]  Iteration number: 0 with current cost as 0.09061534838173133 and parameters 
[-4.6900951   2.23743481 -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432454
  1.31029907 -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522468
 -2.0265424   0.7289737   1.60512664  2.83077099 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 14:46:34 2024]  Iteration number: 0 with current cost as 0.08455690777284282 and parameters 
[-4.66452267  2.23743464 -2.12427949 -0.11653103  0.553887   -2.77010912
  3.06858498  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578412 -0.54534343 -0.47522485
 -2.02654248  0.72897362  1.60512656  2.83077092 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 14:52:18 2024]  Iteration number: 0 with current cost as 0.08874339146370301 and parameters 
[-4.68039068  2.23743464 -2.12427935 -0.11653088  0.55388715 -2.7701089
  3.06858505  2.18960152  1.18552006 -1.06648308  0.60271532  1.1443246
  1.31029913 -1.87354666  0.72965095  2.88578419 -0.54534335 -0.47522471
 -2.0265424   0.7289737   1.60512671  2.83077107 -1.2645671  -0.25136097
 -2.39279211 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 14:58:05 2024]  Iteration number: 0 with current cost as 0.09587137153921405 and parameters 
[-4.6746318   2.2374348  -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432453
  1.31029907 -1.87354672  0.72965089  2.88578419 -0.54534335 -0.47522477
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136096
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550779 -2.69002202]. 
Training complete taking 1717.6607294082642 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 15:03:45 2024]  Iteration number: 0 with current cost as 0.10822095887456029 and parameters 
[-4.66319339  2.23743464 -2.12427957 -0.11653096  0.55388701 -2.77010904
  3.06858491  2.18960145  1.18552006 -1.06648316  0.60271517  1.14432452
  1.31029899 -1.87354673  0.72965073  2.88578412 -0.54534342 -0.47522478
 -2.02654248  0.72897362  1.60512671  2.830771   -1.26456703 -0.25136098
 -2.39279211 -2.27309767  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 15:09:26 2024]  Iteration number: 0 with current cost as 0.09061534838173133 and parameters 
[-4.6900951   2.23743481 -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432454
  1.31029907 -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522468
 -2.0265424   0.7289737   1.60512664  2.83077099 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 15:15:05 2024]  Iteration number: 0 with current cost as 0.08455690777284282 and parameters 
[-4.66452267  2.23743464 -2.12427949 -0.11653103  0.553887   -2.77010912
  3.06858498  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578412 -0.54534343 -0.47522485
 -2.02654248  0.72897362  1.60512656  2.83077092 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 15:20:42 2024]  Iteration number: 0 with current cost as 0.08874339146370301 and parameters 
[-4.68039068  2.23743464 -2.12427935 -0.11653088  0.55388715 -2.7701089
  3.06858505  2.18960152  1.18552006 -1.06648308  0.60271532  1.1443246
  1.31029913 -1.87354666  0.72965095  2.88578419 -0.54534335 -0.47522471
 -2.0265424   0.7289737   1.60512671  2.83077107 -1.2645671  -0.25136097
 -2.39279211 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 15:26:23 2024]  Iteration number: 0 with current cost as 0.09587137153921405 and parameters 
[-4.6746318   2.2374348  -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432453
  1.31029907 -1.87354672  0.72965089  2.88578419 -0.54534335 -0.47522477
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136096
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550779 -2.69002202]. 
Training complete taking 1697.9593715667725 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 15:32:04 2024]  Iteration number: 0 with current cost as 0.10822095887456029 and parameters 
[-4.66319339  2.23743464 -2.12427957 -0.11653096  0.55388701 -2.77010904
  3.06858491  2.18960145  1.18552006 -1.06648316  0.60271517  1.14432452
  1.31029899 -1.87354673  0.72965073  2.88578412 -0.54534342 -0.47522478
 -2.02654248  0.72897362  1.60512671  2.830771   -1.26456703 -0.25136098
 -2.39279211 -2.27309767  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 15:37:47 2024]  Iteration number: 0 with current cost as 0.09061534838173133 and parameters 
[-4.6900951   2.23743481 -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432454
  1.31029907 -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522468
 -2.0265424   0.7289737   1.60512664  2.83077099 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 15:43:28 2024]  Iteration number: 0 with current cost as 0.08455690777284282 and parameters 
[-4.66452267  2.23743464 -2.12427949 -0.11653103  0.553887   -2.77010912
  3.06858498  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578412 -0.54534343 -0.47522485
 -2.02654248  0.72897362  1.60512656  2.83077092 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 15:49:11 2024]  Iteration number: 0 with current cost as 0.08874339146370301 and parameters 
[-4.68039068  2.23743464 -2.12427935 -0.11653088  0.55388715 -2.7701089
  3.06858505  2.18960152  1.18552006 -1.06648308  0.60271532  1.1443246
  1.31029913 -1.87354666  0.72965095  2.88578419 -0.54534335 -0.47522471
 -2.0265424   0.7289737   1.60512671  2.83077107 -1.2645671  -0.25136097
 -2.39279211 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 15:54:51 2024]  Iteration number: 0 with current cost as 0.09587137153921405 and parameters 
[-4.6746318   2.2374348  -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432453
  1.31029907 -1.87354672  0.72965089  2.88578419 -0.54534335 -0.47522477
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136096
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550779 -2.69002202]. 
Training complete taking 1708.5911605358124 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 16:00:33 2024]  Iteration number: 0 with current cost as 0.10822095887456029 and parameters 
[-4.66319339  2.23743464 -2.12427957 -0.11653096  0.55388701 -2.77010904
  3.06858491  2.18960145  1.18552006 -1.06648316  0.60271517  1.14432452
  1.31029899 -1.87354673  0.72965073  2.88578412 -0.54534342 -0.47522478
 -2.02654248  0.72897362  1.60512671  2.830771   -1.26456703 -0.25136098
 -2.39279211 -2.27309767  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 16:06:13 2024]  Iteration number: 0 with current cost as 0.09061534838173133 and parameters 
[-4.6900951   2.23743481 -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432454
  1.31029907 -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522468
 -2.0265424   0.7289737   1.60512664  2.83077099 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 16:11:56 2024]  Iteration number: 0 with current cost as 0.08455690777284282 and parameters 
[-4.66452267  2.23743464 -2.12427949 -0.11653103  0.553887   -2.77010912
  3.06858498  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578412 -0.54534343 -0.47522485
 -2.02654248  0.72897362  1.60512656  2.83077092 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 16:17:37 2024]  Iteration number: 0 with current cost as 0.08874339146370301 and parameters 
[-4.68039068  2.23743464 -2.12427935 -0.11653088  0.55388715 -2.7701089
  3.06858505  2.18960152  1.18552006 -1.06648308  0.60271532  1.1443246
  1.31029913 -1.87354666  0.72965095  2.88578419 -0.54534335 -0.47522471
 -2.0265424   0.7289737   1.60512671  2.83077107 -1.2645671  -0.25136097
 -2.39279211 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 16:23:19 2024]  Iteration number: 0 with current cost as 0.09587137153921405 and parameters 
[-4.6746318   2.2374348  -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432453
  1.31029907 -1.87354672  0.72965089  2.88578419 -0.54534335 -0.47522477
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136096
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550779 -2.69002202]. 
Training complete taking 1709.1906917095184 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 16:29:04 2024]  Iteration number: 0 with current cost as 0.10822095887456029 and parameters 
[-4.66319339  2.23743464 -2.12427957 -0.11653096  0.55388701 -2.77010904
  3.06858491  2.18960145  1.18552006 -1.06648316  0.60271517  1.14432452
  1.31029899 -1.87354673  0.72965073  2.88578412 -0.54534342 -0.47522478
 -2.02654248  0.72897362  1.60512671  2.830771   -1.26456703 -0.25136098
 -2.39279211 -2.27309767  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 16:34:47 2024]  Iteration number: 0 with current cost as 0.09061534838173133 and parameters 
[-4.6900951   2.23743481 -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432454
  1.31029907 -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522468
 -2.0265424   0.7289737   1.60512664  2.83077099 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 16:40:28 2024]  Iteration number: 0 with current cost as 0.08455690777284282 and parameters 
[-4.66452267  2.23743464 -2.12427949 -0.11653103  0.553887   -2.77010912
  3.06858498  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578412 -0.54534343 -0.47522485
 -2.02654248  0.72897362  1.60512656  2.83077092 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 16:46:05 2024]  Iteration number: 0 with current cost as 0.08874339146370301 and parameters 
[-4.68039068  2.23743464 -2.12427935 -0.11653088  0.55388715 -2.7701089
  3.06858505  2.18960152  1.18552006 -1.06648308  0.60271532  1.1443246
  1.31029913 -1.87354666  0.72965095  2.88578419 -0.54534335 -0.47522471
 -2.0265424   0.7289737   1.60512671  2.83077107 -1.2645671  -0.25136097
 -2.39279211 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 16:51:49 2024]  Iteration number: 0 with current cost as 0.09587137153921405 and parameters 
[-4.6746318   2.2374348  -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432453
  1.31029907 -1.87354672  0.72965089  2.88578419 -0.54534335 -0.47522477
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136096
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550779 -2.69002202]. 
Training complete taking 1709.323414325714 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 16:57:30 2024]  Iteration number: 0 with current cost as 0.10822095887456029 and parameters 
[-4.66319339  2.23743464 -2.12427957 -0.11653096  0.55388701 -2.77010904
  3.06858491  2.18960145  1.18552006 -1.06648316  0.60271517  1.14432452
  1.31029899 -1.87354673  0.72965073  2.88578412 -0.54534342 -0.47522478
 -2.02654248  0.72897362  1.60512671  2.830771   -1.26456703 -0.25136098
 -2.39279211 -2.27309767  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 17:03:19 2024]  Iteration number: 0 with current cost as 0.09061534838173133 and parameters 
[-4.6900951   2.23743481 -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432454
  1.31029907 -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522468
 -2.0265424   0.7289737   1.60512664  2.83077099 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 17:09:11 2024]  Iteration number: 0 with current cost as 0.08455690777284282 and parameters 
[-4.66452267  2.23743464 -2.12427949 -0.11653103  0.553887   -2.77010912
  3.06858498  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578412 -0.54534343 -0.47522485
 -2.02654248  0.72897362  1.60512656  2.83077092 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 17:14:58 2024]  Iteration number: 0 with current cost as 0.08874339146370301 and parameters 
[-4.68039068  2.23743464 -2.12427935 -0.11653088  0.55388715 -2.7701089
  3.06858505  2.18960152  1.18552006 -1.06648308  0.60271532  1.1443246
  1.31029913 -1.87354666  0.72965095  2.88578419 -0.54534335 -0.47522471
 -2.0265424   0.7289737   1.60512671  2.83077107 -1.2645671  -0.25136097
 -2.39279211 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 17:20:47 2024]  Iteration number: 0 with current cost as 0.09587137153921405 and parameters 
[-4.6746318   2.2374348  -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432453
  1.31029907 -1.87354672  0.72965089  2.88578419 -0.54534335 -0.47522477
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136096
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550779 -2.69002202]. 
Training complete taking 1741.1026027202606 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 17:26:41 2024]  Iteration number: 0 with current cost as 0.10822095887456029 and parameters 
[-4.66319339  2.23743464 -2.12427957 -0.11653096  0.55388701 -2.77010904
  3.06858491  2.18960145  1.18552006 -1.06648316  0.60271517  1.14432452
  1.31029899 -1.87354673  0.72965073  2.88578412 -0.54534342 -0.47522478
 -2.02654248  0.72897362  1.60512671  2.830771   -1.26456703 -0.25136098
 -2.39279211 -2.27309767  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 17:32:34 2024]  Iteration number: 0 with current cost as 0.09061534838173133 and parameters 
[-4.6900951   2.23743481 -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432454
  1.31029907 -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522468
 -2.0265424   0.7289737   1.60512664  2.83077099 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 17:38:32 2024]  Iteration number: 0 with current cost as 0.08455690777284282 and parameters 
[-4.66452267  2.23743464 -2.12427949 -0.11653103  0.553887   -2.77010912
  3.06858498  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578412 -0.54534343 -0.47522485
 -2.02654248  0.72897362  1.60512656  2.83077092 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 17:44:25 2024]  Iteration number: 0 with current cost as 0.08874339146370301 and parameters 
[-4.68039068  2.23743464 -2.12427935 -0.11653088  0.55388715 -2.7701089
  3.06858505  2.18960152  1.18552006 -1.06648308  0.60271532  1.1443246
  1.31029913 -1.87354666  0.72965095  2.88578419 -0.54534335 -0.47522471
 -2.0265424   0.7289737   1.60512671  2.83077107 -1.2645671  -0.25136097
 -2.39279211 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 17:50:17 2024]  Iteration number: 0 with current cost as 0.09587137153921405 and parameters 
[-4.6746318   2.2374348  -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432453
  1.31029907 -1.87354672  0.72965089  2.88578419 -0.54534335 -0.47522477
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136096
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550779 -2.69002202]. 
Training complete taking 1769.652575492859 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 17:56:07 2024]  Iteration number: 0 with current cost as 0.10822095887456029 and parameters 
[-4.66319339  2.23743464 -2.12427957 -0.11653096  0.55388701 -2.77010904
  3.06858491  2.18960145  1.18552006 -1.06648316  0.60271517  1.14432452
  1.31029899 -1.87354673  0.72965073  2.88578412 -0.54534342 -0.47522478
 -2.02654248  0.72897362  1.60512671  2.830771   -1.26456703 -0.25136098
 -2.39279211 -2.27309767  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 18:01:57 2024]  Iteration number: 0 with current cost as 0.09061534838173133 and parameters 
[-4.6900951   2.23743481 -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432454
  1.31029907 -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522468
 -2.0265424   0.7289737   1.60512664  2.83077099 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 18:07:47 2024]  Iteration number: 0 with current cost as 0.08455690777284282 and parameters 
[-4.66452267  2.23743464 -2.12427949 -0.11653103  0.553887   -2.77010912
  3.06858498  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578412 -0.54534343 -0.47522485
 -2.02654248  0.72897362  1.60512656  2.83077092 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 18:13:35 2024]  Iteration number: 0 with current cost as 0.08874339146370301 and parameters 
[-4.68039068  2.23743464 -2.12427935 -0.11653088  0.55388715 -2.7701089
  3.06858505  2.18960152  1.18552006 -1.06648308  0.60271532  1.1443246
  1.31029913 -1.87354666  0.72965095  2.88578419 -0.54534335 -0.47522471
 -2.0265424   0.7289737   1.60512671  2.83077107 -1.2645671  -0.25136097
 -2.39279211 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 18:19:24 2024]  Iteration number: 0 with current cost as 0.09587137153921405 and parameters 
[-4.6746318   2.2374348  -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432453
  1.31029907 -1.87354672  0.72965089  2.88578419 -0.54534335 -0.47522477
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136096
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550779 -2.69002202]. 
Training complete taking 1748.1340823173523 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 18:25:19 2024]  Iteration number: 0 with current cost as 0.10822095887456029 and parameters 
[-4.66319339  2.23743464 -2.12427957 -0.11653096  0.55388701 -2.77010904
  3.06858491  2.18960145  1.18552006 -1.06648316  0.60271517  1.14432452
  1.31029899 -1.87354673  0.72965073  2.88578412 -0.54534342 -0.47522478
 -2.02654248  0.72897362  1.60512671  2.830771   -1.26456703 -0.25136098
 -2.39279211 -2.27309767  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 18:31:09 2024]  Iteration number: 0 with current cost as 0.09061534838173133 and parameters 
[-4.6900951   2.23743481 -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432454
  1.31029907 -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522468
 -2.0265424   0.7289737   1.60512664  2.83077099 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 18:36:56 2024]  Iteration number: 0 with current cost as 0.08455690777284282 and parameters 
[-4.66452267  2.23743464 -2.12427949 -0.11653103  0.553887   -2.77010912
  3.06858498  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578412 -0.54534343 -0.47522485
 -2.02654248  0.72897362  1.60512656  2.83077092 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 18:42:45 2024]  Iteration number: 0 with current cost as 0.08874339146370301 and parameters 
[-4.68039068  2.23743464 -2.12427935 -0.11653088  0.55388715 -2.7701089
  3.06858505  2.18960152  1.18552006 -1.06648308  0.60271532  1.1443246
  1.31029913 -1.87354666  0.72965095  2.88578419 -0.54534335 -0.47522471
 -2.0265424   0.7289737   1.60512671  2.83077107 -1.2645671  -0.25136097
 -2.39279211 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 18:48:35 2024]  Iteration number: 0 with current cost as 0.09587137153921405 and parameters 
[-4.6746318   2.2374348  -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432453
  1.31029907 -1.87354672  0.72965089  2.88578419 -0.54534335 -0.47522477
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136096
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550779 -2.69002202]. 
Training complete taking 1746.9764726161957 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 18:54:18 2024]  Iteration number: 0 with current cost as 0.10822095887456029 and parameters 
[-4.66319339  2.23743464 -2.12427957 -0.11653096  0.55388701 -2.77010904
  3.06858491  2.18960145  1.18552006 -1.06648316  0.60271517  1.14432452
  1.31029899 -1.87354673  0.72965073  2.88578412 -0.54534342 -0.47522478
 -2.02654248  0.72897362  1.60512671  2.830771   -1.26456703 -0.25136098
 -2.39279211 -2.27309767  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 19:00:11 2024]  Iteration number: 0 with current cost as 0.09061534838173133 and parameters 
[-4.6900951   2.23743481 -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432454
  1.31029907 -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522468
 -2.0265424   0.7289737   1.60512664  2.83077099 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 19:06:00 2024]  Iteration number: 0 with current cost as 0.08455690777284282 and parameters 
[-4.66452267  2.23743464 -2.12427949 -0.11653103  0.553887   -2.77010912
  3.06858498  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578412 -0.54534343 -0.47522485
 -2.02654248  0.72897362  1.60512656  2.83077092 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 19:11:50 2024]  Iteration number: 0 with current cost as 0.08874339146370301 and parameters 
[-4.68039068  2.23743464 -2.12427935 -0.11653088  0.55388715 -2.7701089
  3.06858505  2.18960152  1.18552006 -1.06648308  0.60271532  1.1443246
  1.31029913 -1.87354666  0.72965095  2.88578419 -0.54534335 -0.47522471
 -2.0265424   0.7289737   1.60512671  2.83077107 -1.2645671  -0.25136097
 -2.39279211 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 19:17:33 2024]  Iteration number: 0 with current cost as 0.09587137153921405 and parameters 
[-4.6746318   2.2374348  -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432453
  1.31029907 -1.87354672  0.72965089  2.88578419 -0.54534335 -0.47522477
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136096
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550779 -2.69002202]. 
Training complete taking 1739.873456478119 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 19:23:20 2024]  Iteration number: 0 with current cost as 0.10822095887456029 and parameters 
[-4.66319339  2.23743464 -2.12427957 -0.11653096  0.55388701 -2.77010904
  3.06858491  2.18960145  1.18552006 -1.06648316  0.60271517  1.14432452
  1.31029899 -1.87354673  0.72965073  2.88578412 -0.54534342 -0.47522478
 -2.02654248  0.72897362  1.60512671  2.830771   -1.26456703 -0.25136098
 -2.39279211 -2.27309767  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 19:29:08 2024]  Iteration number: 0 with current cost as 0.09061534838173133 and parameters 
[-4.6900951   2.23743481 -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432454
  1.31029907 -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522468
 -2.0265424   0.7289737   1.60512664  2.83077099 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 19:34:54 2024]  Iteration number: 0 with current cost as 0.08455690777284282 and parameters 
[-4.66452267  2.23743464 -2.12427949 -0.11653103  0.553887   -2.77010912
  3.06858498  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578412 -0.54534343 -0.47522485
 -2.02654248  0.72897362  1.60512656  2.83077092 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 19:40:46 2024]  Iteration number: 0 with current cost as 0.08874339146370301 and parameters 
[-4.68039068  2.23743464 -2.12427935 -0.11653088  0.55388715 -2.7701089
  3.06858505  2.18960152  1.18552006 -1.06648308  0.60271532  1.1443246
  1.31029913 -1.87354666  0.72965095  2.88578419 -0.54534335 -0.47522471
 -2.0265424   0.7289737   1.60512671  2.83077107 -1.2645671  -0.25136097
 -2.39279211 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 19:46:38 2024]  Iteration number: 0 with current cost as 0.09587137153921405 and parameters 
[-4.6746318   2.2374348  -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432453
  1.31029907 -1.87354672  0.72965089  2.88578419 -0.54534335 -0.47522477
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136096
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550779 -2.69002202]. 
Training complete taking 1746.2119581699371 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 19:52:32 2024]  Iteration number: 0 with current cost as 0.10822095887456029 and parameters 
[-4.66319339  2.23743464 -2.12427957 -0.11653096  0.55388701 -2.77010904
  3.06858491  2.18960145  1.18552006 -1.06648316  0.60271517  1.14432452
  1.31029899 -1.87354673  0.72965073  2.88578412 -0.54534342 -0.47522478
 -2.02654248  0.72897362  1.60512671  2.830771   -1.26456703 -0.25136098
 -2.39279211 -2.27309767  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 19:58:18 2024]  Iteration number: 0 with current cost as 0.09061534838173133 and parameters 
[-4.6900951   2.23743481 -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432454
  1.31029907 -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522468
 -2.0265424   0.7289737   1.60512664  2.83077099 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 20:04:17 2024]  Iteration number: 0 with current cost as 0.08455690777284282 and parameters 
[-4.66452267  2.23743464 -2.12427949 -0.11653103  0.553887   -2.77010912
  3.06858498  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578412 -0.54534343 -0.47522485
 -2.02654248  0.72897362  1.60512656  2.83077092 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 20:10:04 2024]  Iteration number: 0 with current cost as 0.08874339146370301 and parameters 
[-4.68039068  2.23743464 -2.12427935 -0.11653088  0.55388715 -2.7701089
  3.06858505  2.18960152  1.18552006 -1.06648308  0.60271532  1.1443246
  1.31029913 -1.87354666  0.72965095  2.88578419 -0.54534335 -0.47522471
 -2.0265424   0.7289737   1.60512671  2.83077107 -1.2645671  -0.25136097
 -2.39279211 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 20:15:54 2024]  Iteration number: 0 with current cost as 0.09587137153921405 and parameters 
[-4.6746318   2.2374348  -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432453
  1.31029907 -1.87354672  0.72965089  2.88578419 -0.54534335 -0.47522477
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136096
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550779 -2.69002202]. 
Training complete taking 1756.2722477912903 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 20:21:42 2024]  Iteration number: 0 with current cost as 0.10822095887456029 and parameters 
[-4.66319339  2.23743464 -2.12427957 -0.11653096  0.55388701 -2.77010904
  3.06858491  2.18960145  1.18552006 -1.06648316  0.60271517  1.14432452
  1.31029899 -1.87354673  0.72965073  2.88578412 -0.54534342 -0.47522478
 -2.02654248  0.72897362  1.60512671  2.830771   -1.26456703 -0.25136098
 -2.39279211 -2.27309767  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 20:27:28 2024]  Iteration number: 0 with current cost as 0.09061534838173133 and parameters 
[-4.6900951   2.23743481 -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432454
  1.31029907 -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522468
 -2.0265424   0.7289737   1.60512664  2.83077099 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 20:33:14 2024]  Iteration number: 0 with current cost as 0.08455690777284282 and parameters 
[-4.66452267  2.23743464 -2.12427949 -0.11653103  0.553887   -2.77010912
  3.06858498  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578412 -0.54534343 -0.47522485
 -2.02654248  0.72897362  1.60512656  2.83077092 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 20:39:03 2024]  Iteration number: 0 with current cost as 0.08874339146370301 and parameters 
[-4.68039068  2.23743464 -2.12427935 -0.11653088  0.55388715 -2.7701089
  3.06858505  2.18960152  1.18552006 -1.06648308  0.60271532  1.1443246
  1.31029913 -1.87354666  0.72965095  2.88578419 -0.54534335 -0.47522471
 -2.0265424   0.7289737   1.60512671  2.83077107 -1.2645671  -0.25136097
 -2.39279211 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 20:44:46 2024]  Iteration number: 0 with current cost as 0.09587137153921405 and parameters 
[-4.6746318   2.2374348  -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432453
  1.31029907 -1.87354672  0.72965089  2.88578419 -0.54534335 -0.47522477
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136096
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550779 -2.69002202]. 
Training complete taking 1729.1006355285645 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 20:50:30 2024]  Iteration number: 0 with current cost as 0.10822095887456029 and parameters 
[-4.66319339  2.23743464 -2.12427957 -0.11653096  0.55388701 -2.77010904
  3.06858491  2.18960145  1.18552006 -1.06648316  0.60271517  1.14432452
  1.31029899 -1.87354673  0.72965073  2.88578412 -0.54534342 -0.47522478
 -2.02654248  0.72897362  1.60512671  2.830771   -1.26456703 -0.25136098
 -2.39279211 -2.27309767  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 20:56:19 2024]  Iteration number: 0 with current cost as 0.09061534838173133 and parameters 
[-4.6900951   2.23743481 -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432454
  1.31029907 -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522468
 -2.0265424   0.7289737   1.60512664  2.83077099 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 21:02:13 2024]  Iteration number: 0 with current cost as 0.08455690777284282 and parameters 
[-4.66452267  2.23743464 -2.12427949 -0.11653103  0.553887   -2.77010912
  3.06858498  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578412 -0.54534343 -0.47522485
 -2.02654248  0.72897362  1.60512656  2.83077092 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 21:08:07 2024]  Iteration number: 0 with current cost as 0.08874339146370301 and parameters 
[-4.68039068  2.23743464 -2.12427935 -0.11653088  0.55388715 -2.7701089
  3.06858505  2.18960152  1.18552006 -1.06648308  0.60271532  1.1443246
  1.31029913 -1.87354666  0.72965095  2.88578419 -0.54534335 -0.47522471
 -2.0265424   0.7289737   1.60512671  2.83077107 -1.2645671  -0.25136097
 -2.39279211 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 21:13:54 2024]  Iteration number: 0 with current cost as 0.09587137153921405 and parameters 
[-4.6746318   2.2374348  -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432453
  1.31029907 -1.87354672  0.72965089  2.88578419 -0.54534335 -0.47522477
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136096
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550779 -2.69002202]. 
Training complete taking 1749.5454351902008 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 21:19:43 2024]  Iteration number: 0 with current cost as 0.10822095887456029 and parameters 
[-4.66319339  2.23743464 -2.12427957 -0.11653096  0.55388701 -2.77010904
  3.06858491  2.18960145  1.18552006 -1.06648316  0.60271517  1.14432452
  1.31029899 -1.87354673  0.72965073  2.88578412 -0.54534342 -0.47522478
 -2.02654248  0.72897362  1.60512671  2.830771   -1.26456703 -0.25136098
 -2.39279211 -2.27309767  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 21:25:34 2024]  Iteration number: 0 with current cost as 0.09061534838173133 and parameters 
[-4.6900951   2.23743481 -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432454
  1.31029907 -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522468
 -2.0265424   0.7289737   1.60512664  2.83077099 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 21:31:21 2024]  Iteration number: 0 with current cost as 0.08455690777284282 and parameters 
[-4.66452267  2.23743464 -2.12427949 -0.11653103  0.553887   -2.77010912
  3.06858498  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578412 -0.54534343 -0.47522485
 -2.02654248  0.72897362  1.60512656  2.83077092 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 21:37:15 2024]  Iteration number: 0 with current cost as 0.08874339146370301 and parameters 
[-4.68039068  2.23743464 -2.12427935 -0.11653088  0.55388715 -2.7701089
  3.06858505  2.18960152  1.18552006 -1.06648308  0.60271532  1.1443246
  1.31029913 -1.87354666  0.72965095  2.88578419 -0.54534335 -0.47522471
 -2.0265424   0.7289737   1.60512671  2.83077107 -1.2645671  -0.25136097
 -2.39279211 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 21:43:10 2024]  Iteration number: 0 with current cost as 0.09587137153921405 and parameters 
[-4.6746318   2.2374348  -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432453
  1.31029907 -1.87354672  0.72965089  2.88578419 -0.54534335 -0.47522477
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136096
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550779 -2.69002202]. 
Training complete taking 1760.9564788341522 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 21:49:12 2024]  Iteration number: 0 with current cost as 0.10822095887456029 and parameters 
[-4.66319339  2.23743464 -2.12427957 -0.11653096  0.55388701 -2.77010904
  3.06858491  2.18960145  1.18552006 -1.06648316  0.60271517  1.14432452
  1.31029899 -1.87354673  0.72965073  2.88578412 -0.54534342 -0.47522478
 -2.02654248  0.72897362  1.60512671  2.830771   -1.26456703 -0.25136098
 -2.39279211 -2.27309767  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 21:55:09 2024]  Iteration number: 0 with current cost as 0.09061534838173133 and parameters 
[-4.6900951   2.23743481 -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432454
  1.31029907 -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522468
 -2.0265424   0.7289737   1.60512664  2.83077099 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 22:00:55 2024]  Iteration number: 0 with current cost as 0.08455690777284282 and parameters 
[-4.66452267  2.23743464 -2.12427949 -0.11653103  0.553887   -2.77010912
  3.06858498  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578412 -0.54534343 -0.47522485
 -2.02654248  0.72897362  1.60512656  2.83077092 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 22:06:45 2024]  Iteration number: 0 with current cost as 0.08874339146370301 and parameters 
[-4.68039068  2.23743464 -2.12427935 -0.11653088  0.55388715 -2.7701089
  3.06858505  2.18960152  1.18552006 -1.06648308  0.60271532  1.1443246
  1.31029913 -1.87354666  0.72965095  2.88578419 -0.54534335 -0.47522471
 -2.0265424   0.7289737   1.60512671  2.83077107 -1.2645671  -0.25136097
 -2.39279211 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 22:12:37 2024]  Iteration number: 0 with current cost as 0.09587137153921405 and parameters 
[-4.6746318   2.2374348  -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432453
  1.31029907 -1.87354672  0.72965089  2.88578419 -0.54534335 -0.47522477
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136096
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550779 -2.69002202]. 
Training complete taking 1762.4557890892029 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 22:18:26 2024]  Iteration number: 0 with current cost as 0.10822095887456029 and parameters 
[-4.66319339  2.23743464 -2.12427957 -0.11653096  0.55388701 -2.77010904
  3.06858491  2.18960145  1.18552006 -1.06648316  0.60271517  1.14432452
  1.31029899 -1.87354673  0.72965073  2.88578412 -0.54534342 -0.47522478
 -2.02654248  0.72897362  1.60512671  2.830771   -1.26456703 -0.25136098
 -2.39279211 -2.27309767  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 22:24:20 2024]  Iteration number: 0 with current cost as 0.09061534838173133 and parameters 
[-4.6900951   2.23743481 -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432454
  1.31029907 -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522468
 -2.0265424   0.7289737   1.60512664  2.83077099 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 22:30:10 2024]  Iteration number: 0 with current cost as 0.08455690777284282 and parameters 
[-4.66452267  2.23743464 -2.12427949 -0.11653103  0.553887   -2.77010912
  3.06858498  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578412 -0.54534343 -0.47522485
 -2.02654248  0.72897362  1.60512656  2.83077092 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 22:36:01 2024]  Iteration number: 0 with current cost as 0.08874339146370301 and parameters 
[-4.68039068  2.23743464 -2.12427935 -0.11653088  0.55388715 -2.7701089
  3.06858505  2.18960152  1.18552006 -1.06648308  0.60271532  1.1443246
  1.31029913 -1.87354666  0.72965095  2.88578419 -0.54534335 -0.47522471
 -2.0265424   0.7289737   1.60512671  2.83077107 -1.2645671  -0.25136097
 -2.39279211 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 22:41:54 2024]  Iteration number: 0 with current cost as 0.09587137153921405 and parameters 
[-4.6746318   2.2374348  -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432453
  1.31029907 -1.87354672  0.72965089  2.88578419 -0.54534335 -0.47522477
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136096
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550779 -2.69002202]. 
Training complete taking 1758.5147087574005 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 22:47:49 2024]  Iteration number: 0 with current cost as 0.10822095887456029 and parameters 
[-4.66319339  2.23743464 -2.12427957 -0.11653096  0.55388701 -2.77010904
  3.06858491  2.18960145  1.18552006 -1.06648316  0.60271517  1.14432452
  1.31029899 -1.87354673  0.72965073  2.88578412 -0.54534342 -0.47522478
 -2.02654248  0.72897362  1.60512671  2.830771   -1.26456703 -0.25136098
 -2.39279211 -2.27309767  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 22:53:49 2024]  Iteration number: 0 with current cost as 0.09061534838173133 and parameters 
[-4.6900951   2.23743481 -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432454
  1.31029907 -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522468
 -2.0265424   0.7289737   1.60512664  2.83077099 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 22:59:44 2024]  Iteration number: 0 with current cost as 0.08455690777284282 and parameters 
[-4.66452267  2.23743464 -2.12427949 -0.11653103  0.553887   -2.77010912
  3.06858498  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578412 -0.54534343 -0.47522485
 -2.02654248  0.72897362  1.60512656  2.83077092 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 23:05:40 2024]  Iteration number: 0 with current cost as 0.08874339146370301 and parameters 
[-4.68039068  2.23743464 -2.12427935 -0.11653088  0.55388715 -2.7701089
  3.06858505  2.18960152  1.18552006 -1.06648308  0.60271532  1.1443246
  1.31029913 -1.87354666  0.72965095  2.88578419 -0.54534335 -0.47522471
 -2.0265424   0.7289737   1.60512671  2.83077107 -1.2645671  -0.25136097
 -2.39279211 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 23:11:25 2024]  Iteration number: 0 with current cost as 0.09587137153921405 and parameters 
[-4.6746318   2.2374348  -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432453
  1.31029907 -1.87354672  0.72965089  2.88578419 -0.54534335 -0.47522477
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136096
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550779 -2.69002202]. 
Training complete taking 1766.9395945072174 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 23:17:06 2024]  Iteration number: 0 with current cost as 0.10822095887456029 and parameters 
[-4.66319339  2.23743464 -2.12427957 -0.11653096  0.55388701 -2.77010904
  3.06858491  2.18960145  1.18552006 -1.06648316  0.60271517  1.14432452
  1.31029899 -1.87354673  0.72965073  2.88578412 -0.54534342 -0.47522478
 -2.02654248  0.72897362  1.60512671  2.830771   -1.26456703 -0.25136098
 -2.39279211 -2.27309767  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 23:22:55 2024]  Iteration number: 0 with current cost as 0.09061534838173133 and parameters 
[-4.6900951   2.23743481 -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432454
  1.31029907 -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522468
 -2.0265424   0.7289737   1.60512664  2.83077099 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 23:28:44 2024]  Iteration number: 0 with current cost as 0.08455690777284282 and parameters 
[-4.66452267  2.23743464 -2.12427949 -0.11653103  0.553887   -2.77010912
  3.06858498  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578412 -0.54534343 -0.47522485
 -2.02654248  0.72897362  1.60512656  2.83077092 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 23:34:32 2024]  Iteration number: 0 with current cost as 0.08874339146370301 and parameters 
[-4.68039068  2.23743464 -2.12427935 -0.11653088  0.55388715 -2.7701089
  3.06858505  2.18960152  1.18552006 -1.06648308  0.60271532  1.1443246
  1.31029913 -1.87354666  0.72965095  2.88578419 -0.54534335 -0.47522471
 -2.0265424   0.7289737   1.60512671  2.83077107 -1.2645671  -0.25136097
 -2.39279211 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 23:40:14 2024]  Iteration number: 0 with current cost as 0.09587137153921405 and parameters 
[-4.6746318   2.2374348  -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432453
  1.31029907 -1.87354672  0.72965089  2.88578419 -0.54534335 -0.47522477
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136096
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550779 -2.69002202]. 
Training complete taking 1731.3887856006622 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 23:45:57 2024]  Iteration number: 0 with current cost as 0.10822095887456029 and parameters 
[-4.66319339  2.23743464 -2.12427957 -0.11653096  0.55388701 -2.77010904
  3.06858491  2.18960145  1.18552006 -1.06648316  0.60271517  1.14432452
  1.31029899 -1.87354673  0.72965073  2.88578412 -0.54534342 -0.47522478
 -2.02654248  0.72897362  1.60512671  2.830771   -1.26456703 -0.25136098
 -2.39279211 -2.27309767  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 23:51:42 2024]  Iteration number: 0 with current cost as 0.09061534838173133 and parameters 
[-4.6900951   2.23743481 -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432454
  1.31029907 -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522468
 -2.0265424   0.7289737   1.60512664  2.83077099 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 23:57:27 2024]  Iteration number: 0 with current cost as 0.08455690777284282 and parameters 
[-4.66452267  2.23743464 -2.12427949 -0.11653103  0.553887   -2.77010912
  3.06858498  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578412 -0.54534343 -0.47522485
 -2.02654248  0.72897362  1.60512656  2.83077092 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 00:03:12 2024]  Iteration number: 0 with current cost as 0.08874339146370301 and parameters 
[-4.68039068  2.23743464 -2.12427935 -0.11653088  0.55388715 -2.7701089
  3.06858505  2.18960152  1.18552006 -1.06648308  0.60271532  1.1443246
  1.31029913 -1.87354666  0.72965095  2.88578419 -0.54534335 -0.47522471
 -2.0265424   0.7289737   1.60512671  2.83077107 -1.2645671  -0.25136097
 -2.39279211 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 00:08:54 2024]  Iteration number: 0 with current cost as 0.09587137153921405 and parameters 
[-4.6746318   2.2374348  -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432453
  1.31029907 -1.87354672  0.72965089  2.88578419 -0.54534335 -0.47522477
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136096
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550779 -2.69002202]. 
Training complete taking 1718.4216604232788 seconds. 
Discarding model... 

Training complete taking 43392.408213377 total seconds. 
Now scoring model... 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Scoring complete taking 2.0549778938293457 seconds. 
Saved predicted values as M-A1-CZ_Full-CRX_predicted_values.csv
Model scores: {'MSE_train': (58.35083788403513,), 'R2_train': 0.7173097739218985, 'MAE_train': 5.013475892245843, 'MSE_test': 77.63284334844319, 'R2_test': 0.5320436669882785, 'MAE_test': 5.13789221227575}. 
Saved model results as M-A1-CZ_Full-CRX_results.json. 
