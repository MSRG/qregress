/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/circuits/Encoders.py:12: RuntimeWarning: invalid value encountered in arcsin
  if np.isnan(np.arcsin(features[feature_idx])) or np.isnan(np.arccos(features[feature_idx])):
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Sun Mar 24 17:35:01 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 17:37:28 2024]  Iteration number: 0 with current cost as 0.44731676974652385 and parameters 
[-1.26342713  2.23743432 -2.12427979 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551967 -1.0664834   0.6027151   1.14432414
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.7289737   1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279218 -2.2730979   3.13337139  2.54856927 -0.67550803 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 17:47:16 2024]  Iteration number: 0 with current cost as 0.23225510178558495 and parameters 
[ 1.31234093  2.23743464 -2.12427943 -0.11653123  0.55388688 -2.77010917
  3.06858478  2.18960165  1.18551978 -1.06648329  0.6027153   1.14432465
  1.31029878 -1.8735466   0.729651    2.88578439 -0.54534355 -0.47522485
 -2.02654261  0.7289739   1.60512684  2.83077087 -1.2645673  -0.25136125
 -2.39279238 -2.27309774  3.13337135  2.54856938 -0.67550807 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 17:51:31 2024]  Iteration number: 0 with current cost as 0.4080225459531984 and parameters 
[-1.00216457  2.23743451 -2.12427964 -0.11653103  0.55388708 -2.77010922
  3.06858473  2.18960133  1.18551998 -1.06648346  0.6027151   1.14432433
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856946 -0.67550787 -2.69002214]. 
Working on 0.8 fold... 
[Sun Mar 24 17:56:58 2024]  Iteration number: 0 with current cost as 0.46742629522734175 and parameters 
[-0.30763464  2.23743464 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.7296508   2.88578451 -0.54534335 -0.47522454
 -2.0265424   0.72897385  1.60512664  2.83077107 -1.26456678 -0.25136089
 -2.39279187 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 18:01:50 2024]  Iteration number: 0 with current cost as 0.48975815155432034 and parameters 
[ 0.0757259   2.23743432 -2.12427964 -0.11653134  0.55388692 -2.77010929
  3.06858451  2.18960114  1.18551983 -1.0664834   0.6027151   1.14432445
  1.31029883 -1.8735468   0.7296508   2.88578419 -0.54534351 -0.47522485
 -2.02654256  0.7289737   1.60512632  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856927 -0.67550787 -2.69002202]. 
Training complete taking 1898.082347869873 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 18:09:07 2024]  Iteration number: 0 with current cost as 0.44731676974652385 and parameters 
[-1.26342713  2.23743432 -2.12427979 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551967 -1.0664834   0.6027151   1.14432414
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.7289737   1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279218 -2.2730979   3.13337139  2.54856927 -0.67550803 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 18:18:45 2024]  Iteration number: 0 with current cost as 0.23225510178558495 and parameters 
[ 1.31234093  2.23743464 -2.12427943 -0.11653123  0.55388688 -2.77010917
  3.06858478  2.18960165  1.18551978 -1.06648329  0.6027153   1.14432465
  1.31029878 -1.8735466   0.729651    2.88578439 -0.54534355 -0.47522485
 -2.02654261  0.7289739   1.60512684  2.83077087 -1.2645673  -0.25136125
 -2.39279238 -2.27309774  3.13337135  2.54856938 -0.67550807 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 18:22:58 2024]  Iteration number: 0 with current cost as 0.4080225459531984 and parameters 
[-1.00216457  2.23743451 -2.12427964 -0.11653103  0.55388708 -2.77010922
  3.06858473  2.18960133  1.18551998 -1.06648346  0.6027151   1.14432433
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856946 -0.67550787 -2.69002214]. 
Working on 0.8 fold... 
[Sun Mar 24 18:28:26 2024]  Iteration number: 0 with current cost as 0.46742629522734175 and parameters 
[-0.30763464  2.23743464 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.7296508   2.88578451 -0.54534335 -0.47522454
 -2.0265424   0.72897385  1.60512664  2.83077107 -1.26456678 -0.25136089
 -2.39279187 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 18:33:15 2024]  Iteration number: 0 with current cost as 0.48975815155432034 and parameters 
[ 0.0757259   2.23743432 -2.12427964 -0.11653134  0.55388692 -2.77010929
  3.06858451  2.18960114  1.18551983 -1.0664834   0.6027151   1.14432445
  1.31029883 -1.8735468   0.7296508   2.88578419 -0.54534351 -0.47522485
 -2.02654256  0.7289737   1.60512632  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856927 -0.67550787 -2.69002202]. 
Training complete taking 1885.944445848465 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 18:40:32 2024]  Iteration number: 0 with current cost as 0.44731676974652385 and parameters 
[-1.26342713  2.23743432 -2.12427979 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551967 -1.0664834   0.6027151   1.14432414
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.7289737   1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279218 -2.2730979   3.13337139  2.54856927 -0.67550803 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 18:50:17 2024]  Iteration number: 0 with current cost as 0.23225510178558495 and parameters 
[ 1.31234093  2.23743464 -2.12427943 -0.11653123  0.55388688 -2.77010917
  3.06858478  2.18960165  1.18551978 -1.06648329  0.6027153   1.14432465
  1.31029878 -1.8735466   0.729651    2.88578439 -0.54534355 -0.47522485
 -2.02654261  0.7289739   1.60512684  2.83077087 -1.2645673  -0.25136125
 -2.39279238 -2.27309774  3.13337135  2.54856938 -0.67550807 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 18:54:30 2024]  Iteration number: 0 with current cost as 0.4080225459531984 and parameters 
[-1.00216457  2.23743451 -2.12427964 -0.11653103  0.55388708 -2.77010922
  3.06858473  2.18960133  1.18551998 -1.06648346  0.6027151   1.14432433
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856946 -0.67550787 -2.69002214]. 
Working on 0.8 fold... 
[Sun Mar 24 18:59:56 2024]  Iteration number: 0 with current cost as 0.46742629522734175 and parameters 
[-0.30763464  2.23743464 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.7296508   2.88578451 -0.54534335 -0.47522454
 -2.0265424   0.72897385  1.60512664  2.83077107 -1.26456678 -0.25136089
 -2.39279187 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 19:04:45 2024]  Iteration number: 0 with current cost as 0.48975815155432034 and parameters 
[ 0.0757259   2.23743432 -2.12427964 -0.11653134  0.55388692 -2.77010929
  3.06858451  2.18960114  1.18551983 -1.0664834   0.6027151   1.14432445
  1.31029883 -1.8735468   0.7296508   2.88578419 -0.54534351 -0.47522485
 -2.02654256  0.7289737   1.60512632  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856927 -0.67550787 -2.69002202]. 
Training complete taking 1888.2352349758148 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 19:11:58 2024]  Iteration number: 0 with current cost as 0.44731676974652385 and parameters 
[-1.26342713  2.23743432 -2.12427979 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551967 -1.0664834   0.6027151   1.14432414
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.7289737   1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279218 -2.2730979   3.13337139  2.54856927 -0.67550803 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 19:21:32 2024]  Iteration number: 0 with current cost as 0.23225510178558495 and parameters 
[ 1.31234093  2.23743464 -2.12427943 -0.11653123  0.55388688 -2.77010917
  3.06858478  2.18960165  1.18551978 -1.06648329  0.6027153   1.14432465
  1.31029878 -1.8735466   0.729651    2.88578439 -0.54534355 -0.47522485
 -2.02654261  0.7289739   1.60512684  2.83077087 -1.2645673  -0.25136125
 -2.39279238 -2.27309774  3.13337135  2.54856938 -0.67550807 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 19:25:42 2024]  Iteration number: 0 with current cost as 0.4080225459531984 and parameters 
[-1.00216457  2.23743451 -2.12427964 -0.11653103  0.55388708 -2.77010922
  3.06858473  2.18960133  1.18551998 -1.06648346  0.6027151   1.14432433
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856946 -0.67550787 -2.69002214]. 
Working on 0.8 fold... 
[Sun Mar 24 19:31:13 2024]  Iteration number: 0 with current cost as 0.46742629522734175 and parameters 
[-0.30763464  2.23743464 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.7296508   2.88578451 -0.54534335 -0.47522454
 -2.0265424   0.72897385  1.60512664  2.83077107 -1.26456678 -0.25136089
 -2.39279187 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 19:36:02 2024]  Iteration number: 0 with current cost as 0.48975815155432034 and parameters 
[ 0.0757259   2.23743432 -2.12427964 -0.11653134  0.55388692 -2.77010929
  3.06858451  2.18960114  1.18551983 -1.0664834   0.6027151   1.14432445
  1.31029883 -1.8735468   0.7296508   2.88578419 -0.54534351 -0.47522485
 -2.02654256  0.7289737   1.60512632  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856927 -0.67550787 -2.69002202]. 
Training complete taking 1882.7580692768097 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 19:43:23 2024]  Iteration number: 0 with current cost as 0.44731676974652385 and parameters 
[-1.26342713  2.23743432 -2.12427979 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551967 -1.0664834   0.6027151   1.14432414
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.7289737   1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279218 -2.2730979   3.13337139  2.54856927 -0.67550803 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 19:52:59 2024]  Iteration number: 0 with current cost as 0.23225510178558495 and parameters 
[ 1.31234093  2.23743464 -2.12427943 -0.11653123  0.55388688 -2.77010917
  3.06858478  2.18960165  1.18551978 -1.06648329  0.6027153   1.14432465
  1.31029878 -1.8735466   0.729651    2.88578439 -0.54534355 -0.47522485
 -2.02654261  0.7289739   1.60512684  2.83077087 -1.2645673  -0.25136125
 -2.39279238 -2.27309774  3.13337135  2.54856938 -0.67550807 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 19:57:15 2024]  Iteration number: 0 with current cost as 0.4080225459531984 and parameters 
[-1.00216457  2.23743451 -2.12427964 -0.11653103  0.55388708 -2.77010922
  3.06858473  2.18960133  1.18551998 -1.06648346  0.6027151   1.14432433
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856946 -0.67550787 -2.69002214]. 
Working on 0.8 fold... 
[Sun Mar 24 20:02:48 2024]  Iteration number: 0 with current cost as 0.46742629522734175 and parameters 
[-0.30763464  2.23743464 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.7296508   2.88578451 -0.54534335 -0.47522454
 -2.0265424   0.72897385  1.60512664  2.83077107 -1.26456678 -0.25136089
 -2.39279187 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 20:07:37 2024]  Iteration number: 0 with current cost as 0.48975815155432034 and parameters 
[ 0.0757259   2.23743432 -2.12427964 -0.11653134  0.55388692 -2.77010929
  3.06858451  2.18960114  1.18551983 -1.0664834   0.6027151   1.14432445
  1.31029883 -1.8735468   0.7296508   2.88578419 -0.54534351 -0.47522485
 -2.02654256  0.7289737   1.60512632  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856927 -0.67550787 -2.69002202]. 
Training complete taking 1890.1325635910034 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 20:14:51 2024]  Iteration number: 0 with current cost as 0.44731676974652385 and parameters 
[-1.26342713  2.23743432 -2.12427979 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551967 -1.0664834   0.6027151   1.14432414
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.7289737   1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279218 -2.2730979   3.13337139  2.54856927 -0.67550803 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 20:24:28 2024]  Iteration number: 0 with current cost as 0.23225510178558495 and parameters 
[ 1.31234093  2.23743464 -2.12427943 -0.11653123  0.55388688 -2.77010917
  3.06858478  2.18960165  1.18551978 -1.06648329  0.6027153   1.14432465
  1.31029878 -1.8735466   0.729651    2.88578439 -0.54534355 -0.47522485
 -2.02654261  0.7289739   1.60512684  2.83077087 -1.2645673  -0.25136125
 -2.39279238 -2.27309774  3.13337135  2.54856938 -0.67550807 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 20:28:44 2024]  Iteration number: 0 with current cost as 0.4080225459531984 and parameters 
[-1.00216457  2.23743451 -2.12427964 -0.11653103  0.55388708 -2.77010922
  3.06858473  2.18960133  1.18551998 -1.06648346  0.6027151   1.14432433
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856946 -0.67550787 -2.69002214]. 
Working on 0.8 fold... 
[Sun Mar 24 20:34:09 2024]  Iteration number: 0 with current cost as 0.46742629522734175 and parameters 
[-0.30763464  2.23743464 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.7296508   2.88578451 -0.54534335 -0.47522454
 -2.0265424   0.72897385  1.60512664  2.83077107 -1.26456678 -0.25136089
 -2.39279187 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 20:39:08 2024]  Iteration number: 0 with current cost as 0.48975815155432034 and parameters 
[ 0.0757259   2.23743432 -2.12427964 -0.11653134  0.55388692 -2.77010929
  3.06858451  2.18960114  1.18551983 -1.0664834   0.6027151   1.14432445
  1.31029883 -1.8735468   0.7296508   2.88578419 -0.54534351 -0.47522485
 -2.02654256  0.7289737   1.60512632  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856927 -0.67550787 -2.69002202]. 
Training complete taking 1896.0869925022125 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 20:46:30 2024]  Iteration number: 0 with current cost as 0.44731676974652385 and parameters 
[-1.26342713  2.23743432 -2.12427979 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551967 -1.0664834   0.6027151   1.14432414
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.7289737   1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279218 -2.2730979   3.13337139  2.54856927 -0.67550803 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 20:56:14 2024]  Iteration number: 0 with current cost as 0.23225510178558495 and parameters 
[ 1.31234093  2.23743464 -2.12427943 -0.11653123  0.55388688 -2.77010917
  3.06858478  2.18960165  1.18551978 -1.06648329  0.6027153   1.14432465
  1.31029878 -1.8735466   0.729651    2.88578439 -0.54534355 -0.47522485
 -2.02654261  0.7289739   1.60512684  2.83077087 -1.2645673  -0.25136125
 -2.39279238 -2.27309774  3.13337135  2.54856938 -0.67550807 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 21:00:28 2024]  Iteration number: 0 with current cost as 0.4080225459531984 and parameters 
[-1.00216457  2.23743451 -2.12427964 -0.11653103  0.55388708 -2.77010922
  3.06858473  2.18960133  1.18551998 -1.06648346  0.6027151   1.14432433
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856946 -0.67550787 -2.69002214]. 
Working on 0.8 fold... 
[Sun Mar 24 21:05:54 2024]  Iteration number: 0 with current cost as 0.46742629522734175 and parameters 
[-0.30763464  2.23743464 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.7296508   2.88578451 -0.54534335 -0.47522454
 -2.0265424   0.72897385  1.60512664  2.83077107 -1.26456678 -0.25136089
 -2.39279187 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 21:10:49 2024]  Iteration number: 0 with current cost as 0.48975815155432034 and parameters 
[ 0.0757259   2.23743432 -2.12427964 -0.11653134  0.55388692 -2.77010929
  3.06858451  2.18960114  1.18551983 -1.0664834   0.6027151   1.14432445
  1.31029883 -1.8735468   0.7296508   2.88578419 -0.54534351 -0.47522485
 -2.02654256  0.7289737   1.60512632  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856927 -0.67550787 -2.69002202]. 
Training complete taking 1894.2559685707092 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 21:18:09 2024]  Iteration number: 0 with current cost as 0.44731676974652385 and parameters 
[-1.26342713  2.23743432 -2.12427979 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551967 -1.0664834   0.6027151   1.14432414
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.7289737   1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279218 -2.2730979   3.13337139  2.54856927 -0.67550803 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 21:27:50 2024]  Iteration number: 0 with current cost as 0.23225510178558495 and parameters 
[ 1.31234093  2.23743464 -2.12427943 -0.11653123  0.55388688 -2.77010917
  3.06858478  2.18960165  1.18551978 -1.06648329  0.6027153   1.14432465
  1.31029878 -1.8735466   0.729651    2.88578439 -0.54534355 -0.47522485
 -2.02654261  0.7289739   1.60512684  2.83077087 -1.2645673  -0.25136125
 -2.39279238 -2.27309774  3.13337135  2.54856938 -0.67550807 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 21:32:05 2024]  Iteration number: 0 with current cost as 0.4080225459531984 and parameters 
[-1.00216457  2.23743451 -2.12427964 -0.11653103  0.55388708 -2.77010922
  3.06858473  2.18960133  1.18551998 -1.06648346  0.6027151   1.14432433
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856946 -0.67550787 -2.69002214]. 
Working on 0.8 fold... 
[Sun Mar 24 21:37:32 2024]  Iteration number: 0 with current cost as 0.46742629522734175 and parameters 
[-0.30763464  2.23743464 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.7296508   2.88578451 -0.54534335 -0.47522454
 -2.0265424   0.72897385  1.60512664  2.83077107 -1.26456678 -0.25136089
 -2.39279187 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 21:42:29 2024]  Iteration number: 0 with current cost as 0.48975815155432034 and parameters 
[ 0.0757259   2.23743432 -2.12427964 -0.11653134  0.55388692 -2.77010929
  3.06858451  2.18960114  1.18551983 -1.0664834   0.6027151   1.14432445
  1.31029883 -1.8735468   0.7296508   2.88578419 -0.54534351 -0.47522485
 -2.02654256  0.7289737   1.60512632  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856927 -0.67550787 -2.69002202]. 
Training complete taking 1897.5198383331299 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 21:49:40 2024]  Iteration number: 0 with current cost as 0.44731676974652385 and parameters 
[-1.26342713  2.23743432 -2.12427979 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551967 -1.0664834   0.6027151   1.14432414
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.7289737   1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279218 -2.2730979   3.13337139  2.54856927 -0.67550803 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 21:59:25 2024]  Iteration number: 0 with current cost as 0.23225510178558495 and parameters 
[ 1.31234093  2.23743464 -2.12427943 -0.11653123  0.55388688 -2.77010917
  3.06858478  2.18960165  1.18551978 -1.06648329  0.6027153   1.14432465
  1.31029878 -1.8735466   0.729651    2.88578439 -0.54534355 -0.47522485
 -2.02654261  0.7289739   1.60512684  2.83077087 -1.2645673  -0.25136125
 -2.39279238 -2.27309774  3.13337135  2.54856938 -0.67550807 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 22:03:35 2024]  Iteration number: 0 with current cost as 0.4080225459531984 and parameters 
[-1.00216457  2.23743451 -2.12427964 -0.11653103  0.55388708 -2.77010922
  3.06858473  2.18960133  1.18551998 -1.06648346  0.6027151   1.14432433
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856946 -0.67550787 -2.69002214]. 
Working on 0.8 fold... 
[Sun Mar 24 22:09:00 2024]  Iteration number: 0 with current cost as 0.46742629522734175 and parameters 
[-0.30763464  2.23743464 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.7296508   2.88578451 -0.54534335 -0.47522454
 -2.0265424   0.72897385  1.60512664  2.83077107 -1.26456678 -0.25136089
 -2.39279187 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 22:13:51 2024]  Iteration number: 0 with current cost as 0.48975815155432034 and parameters 
[ 0.0757259   2.23743432 -2.12427964 -0.11653134  0.55388692 -2.77010929
  3.06858451  2.18960114  1.18551983 -1.0664834   0.6027151   1.14432445
  1.31029883 -1.8735468   0.7296508   2.88578419 -0.54534351 -0.47522485
 -2.02654256  0.7289737   1.60512632  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856927 -0.67550787 -2.69002202]. 
Training complete taking 1890.538812160492 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 22:21:12 2024]  Iteration number: 0 with current cost as 0.44731676974652385 and parameters 
[-1.26342713  2.23743432 -2.12427979 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551967 -1.0664834   0.6027151   1.14432414
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.7289737   1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279218 -2.2730979   3.13337139  2.54856927 -0.67550803 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 22:31:19 2024]  Iteration number: 0 with current cost as 0.23225510178558495 and parameters 
[ 1.31234093  2.23743464 -2.12427943 -0.11653123  0.55388688 -2.77010917
  3.06858478  2.18960165  1.18551978 -1.06648329  0.6027153   1.14432465
  1.31029878 -1.8735466   0.729651    2.88578439 -0.54534355 -0.47522485
 -2.02654261  0.7289739   1.60512684  2.83077087 -1.2645673  -0.25136125
 -2.39279238 -2.27309774  3.13337135  2.54856938 -0.67550807 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 22:35:38 2024]  Iteration number: 0 with current cost as 0.4080225459531984 and parameters 
[-1.00216457  2.23743451 -2.12427964 -0.11653103  0.55388708 -2.77010922
  3.06858473  2.18960133  1.18551998 -1.06648346  0.6027151   1.14432433
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856946 -0.67550787 -2.69002214]. 
Working on 0.8 fold... 
[Sun Mar 24 22:41:07 2024]  Iteration number: 0 with current cost as 0.46742629522734175 and parameters 
[-0.30763464  2.23743464 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.7296508   2.88578451 -0.54534335 -0.47522454
 -2.0265424   0.72897385  1.60512664  2.83077107 -1.26456678 -0.25136089
 -2.39279187 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 22:45:58 2024]  Iteration number: 0 with current cost as 0.48975815155432034 and parameters 
[ 0.0757259   2.23743432 -2.12427964 -0.11653134  0.55388692 -2.77010929
  3.06858451  2.18960114  1.18551983 -1.0664834   0.6027151   1.14432445
  1.31029883 -1.8735468   0.7296508   2.88578419 -0.54534351 -0.47522485
 -2.02654256  0.7289737   1.60512632  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856927 -0.67550787 -2.69002202]. 
Training complete taking 1931.9252212047577 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 22:53:23 2024]  Iteration number: 0 with current cost as 0.44731676974652385 and parameters 
[-1.26342713  2.23743432 -2.12427979 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551967 -1.0664834   0.6027151   1.14432414
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.7289737   1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279218 -2.2730979   3.13337139  2.54856927 -0.67550803 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 23:03:03 2024]  Iteration number: 0 with current cost as 0.23225510178558495 and parameters 
[ 1.31234093  2.23743464 -2.12427943 -0.11653123  0.55388688 -2.77010917
  3.06858478  2.18960165  1.18551978 -1.06648329  0.6027153   1.14432465
  1.31029878 -1.8735466   0.729651    2.88578439 -0.54534355 -0.47522485
 -2.02654261  0.7289739   1.60512684  2.83077087 -1.2645673  -0.25136125
 -2.39279238 -2.27309774  3.13337135  2.54856938 -0.67550807 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 23:07:18 2024]  Iteration number: 0 with current cost as 0.4080225459531984 and parameters 
[-1.00216457  2.23743451 -2.12427964 -0.11653103  0.55388708 -2.77010922
  3.06858473  2.18960133  1.18551998 -1.06648346  0.6027151   1.14432433
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856946 -0.67550787 -2.69002214]. 
Working on 0.8 fold... 
[Sun Mar 24 23:12:46 2024]  Iteration number: 0 with current cost as 0.46742629522734175 and parameters 
[-0.30763464  2.23743464 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.7296508   2.88578451 -0.54534335 -0.47522454
 -2.0265424   0.72897385  1.60512664  2.83077107 -1.26456678 -0.25136089
 -2.39279187 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 23:17:42 2024]  Iteration number: 0 with current cost as 0.48975815155432034 and parameters 
[ 0.0757259   2.23743432 -2.12427964 -0.11653134  0.55388692 -2.77010929
  3.06858451  2.18960114  1.18551983 -1.0664834   0.6027151   1.14432445
  1.31029883 -1.8735468   0.7296508   2.88578419 -0.54534351 -0.47522485
 -2.02654256  0.7289737   1.60512632  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856927 -0.67550787 -2.69002202]. 
Training complete taking 1900.5977125167847 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 23:25:03 2024]  Iteration number: 0 with current cost as 0.44731676974652385 and parameters 
[-1.26342713  2.23743432 -2.12427979 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551967 -1.0664834   0.6027151   1.14432414
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.7289737   1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279218 -2.2730979   3.13337139  2.54856927 -0.67550803 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 23:34:43 2024]  Iteration number: 0 with current cost as 0.23225510178558495 and parameters 
[ 1.31234093  2.23743464 -2.12427943 -0.11653123  0.55388688 -2.77010917
  3.06858478  2.18960165  1.18551978 -1.06648329  0.6027153   1.14432465
  1.31029878 -1.8735466   0.729651    2.88578439 -0.54534355 -0.47522485
 -2.02654261  0.7289739   1.60512684  2.83077087 -1.2645673  -0.25136125
 -2.39279238 -2.27309774  3.13337135  2.54856938 -0.67550807 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 23:38:58 2024]  Iteration number: 0 with current cost as 0.4080225459531984 and parameters 
[-1.00216457  2.23743451 -2.12427964 -0.11653103  0.55388708 -2.77010922
  3.06858473  2.18960133  1.18551998 -1.06648346  0.6027151   1.14432433
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856946 -0.67550787 -2.69002214]. 
Working on 0.8 fold... 
[Sun Mar 24 23:44:28 2024]  Iteration number: 0 with current cost as 0.46742629522734175 and parameters 
[-0.30763464  2.23743464 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.7296508   2.88578451 -0.54534335 -0.47522454
 -2.0265424   0.72897385  1.60512664  2.83077107 -1.26456678 -0.25136089
 -2.39279187 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 23:49:15 2024]  Iteration number: 0 with current cost as 0.48975815155432034 and parameters 
[ 0.0757259   2.23743432 -2.12427964 -0.11653134  0.55388692 -2.77010929
  3.06858451  2.18960114  1.18551983 -1.0664834   0.6027151   1.14432445
  1.31029883 -1.8735468   0.7296508   2.88578419 -0.54534351 -0.47522485
 -2.02654256  0.7289737   1.60512632  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856927 -0.67550787 -2.69002202]. 
Training complete taking 1886.6890840530396 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 23:56:29 2024]  Iteration number: 0 with current cost as 0.44731676974652385 and parameters 
[-1.26342713  2.23743432 -2.12427979 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551967 -1.0664834   0.6027151   1.14432414
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.7289737   1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279218 -2.2730979   3.13337139  2.54856927 -0.67550803 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 00:06:08 2024]  Iteration number: 0 with current cost as 0.23225510178558495 and parameters 
[ 1.31234093  2.23743464 -2.12427943 -0.11653123  0.55388688 -2.77010917
  3.06858478  2.18960165  1.18551978 -1.06648329  0.6027153   1.14432465
  1.31029878 -1.8735466   0.729651    2.88578439 -0.54534355 -0.47522485
 -2.02654261  0.7289739   1.60512684  2.83077087 -1.2645673  -0.25136125
 -2.39279238 -2.27309774  3.13337135  2.54856938 -0.67550807 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 00:10:28 2024]  Iteration number: 0 with current cost as 0.4080225459531984 and parameters 
[-1.00216457  2.23743451 -2.12427964 -0.11653103  0.55388708 -2.77010922
  3.06858473  2.18960133  1.18551998 -1.06648346  0.6027151   1.14432433
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856946 -0.67550787 -2.69002214]. 
Working on 0.8 fold... 
[Mon Mar 25 00:16:02 2024]  Iteration number: 0 with current cost as 0.46742629522734175 and parameters 
[-0.30763464  2.23743464 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.7296508   2.88578451 -0.54534335 -0.47522454
 -2.0265424   0.72897385  1.60512664  2.83077107 -1.26456678 -0.25136089
 -2.39279187 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 00:20:59 2024]  Iteration number: 0 with current cost as 0.48975815155432034 and parameters 
[ 0.0757259   2.23743432 -2.12427964 -0.11653134  0.55388692 -2.77010929
  3.06858451  2.18960114  1.18551983 -1.0664834   0.6027151   1.14432445
  1.31029883 -1.8735468   0.7296508   2.88578419 -0.54534351 -0.47522485
 -2.02654256  0.7289737   1.60512632  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856927 -0.67550787 -2.69002202]. 
Training complete taking 1916.6910517215729 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 00:28:30 2024]  Iteration number: 0 with current cost as 0.44731676974652385 and parameters 
[-1.26342713  2.23743432 -2.12427979 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551967 -1.0664834   0.6027151   1.14432414
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.7289737   1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279218 -2.2730979   3.13337139  2.54856927 -0.67550803 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 00:38:31 2024]  Iteration number: 0 with current cost as 0.23225510178558495 and parameters 
[ 1.31234093  2.23743464 -2.12427943 -0.11653123  0.55388688 -2.77010917
  3.06858478  2.18960165  1.18551978 -1.06648329  0.6027153   1.14432465
  1.31029878 -1.8735466   0.729651    2.88578439 -0.54534355 -0.47522485
 -2.02654261  0.7289739   1.60512684  2.83077087 -1.2645673  -0.25136125
 -2.39279238 -2.27309774  3.13337135  2.54856938 -0.67550807 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 00:43:01 2024]  Iteration number: 0 with current cost as 0.4080225459531984 and parameters 
[-1.00216457  2.23743451 -2.12427964 -0.11653103  0.55388708 -2.77010922
  3.06858473  2.18960133  1.18551998 -1.06648346  0.6027151   1.14432433
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856946 -0.67550787 -2.69002214]. 
Working on 0.8 fold... 
[Mon Mar 25 00:48:37 2024]  Iteration number: 0 with current cost as 0.46742629522734175 and parameters 
[-0.30763464  2.23743464 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.7296508   2.88578451 -0.54534335 -0.47522454
 -2.0265424   0.72897385  1.60512664  2.83077107 -1.26456678 -0.25136089
 -2.39279187 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 00:53:35 2024]  Iteration number: 0 with current cost as 0.48975815155432034 and parameters 
[ 0.0757259   2.23743432 -2.12427964 -0.11653134  0.55388692 -2.77010929
  3.06858451  2.18960114  1.18551983 -1.0664834   0.6027151   1.14432445
  1.31029883 -1.8735468   0.7296508   2.88578419 -0.54534351 -0.47522485
 -2.02654256  0.7289737   1.60512632  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856927 -0.67550787 -2.69002202]. 
Training complete taking 1945.1488955020905 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 01:00:50 2024]  Iteration number: 0 with current cost as 0.44731676974652385 and parameters 
[-1.26342713  2.23743432 -2.12427979 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551967 -1.0664834   0.6027151   1.14432414
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.7289737   1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279218 -2.2730979   3.13337139  2.54856927 -0.67550803 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 01:10:44 2024]  Iteration number: 0 with current cost as 0.23225510178558495 and parameters 
[ 1.31234093  2.23743464 -2.12427943 -0.11653123  0.55388688 -2.77010917
  3.06858478  2.18960165  1.18551978 -1.06648329  0.6027153   1.14432465
  1.31029878 -1.8735466   0.729651    2.88578439 -0.54534355 -0.47522485
 -2.02654261  0.7289739   1.60512684  2.83077087 -1.2645673  -0.25136125
 -2.39279238 -2.27309774  3.13337135  2.54856938 -0.67550807 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 01:15:05 2024]  Iteration number: 0 with current cost as 0.4080225459531984 and parameters 
[-1.00216457  2.23743451 -2.12427964 -0.11653103  0.55388708 -2.77010922
  3.06858473  2.18960133  1.18551998 -1.06648346  0.6027151   1.14432433
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856946 -0.67550787 -2.69002214]. 
Working on 0.8 fold... 
[Mon Mar 25 01:20:33 2024]  Iteration number: 0 with current cost as 0.46742629522734175 and parameters 
[-0.30763464  2.23743464 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.7296508   2.88578451 -0.54534335 -0.47522454
 -2.0265424   0.72897385  1.60512664  2.83077107 -1.26456678 -0.25136089
 -2.39279187 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 01:25:33 2024]  Iteration number: 0 with current cost as 0.48975815155432034 and parameters 
[ 0.0757259   2.23743432 -2.12427964 -0.11653134  0.55388692 -2.77010929
  3.06858451  2.18960114  1.18551983 -1.0664834   0.6027151   1.14432445
  1.31029883 -1.8735468   0.7296508   2.88578419 -0.54534351 -0.47522485
 -2.02654256  0.7289737   1.60512632  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856927 -0.67550787 -2.69002202]. 
Training complete taking 1921.737848997116 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 01:32:58 2024]  Iteration number: 0 with current cost as 0.44731676974652385 and parameters 
[-1.26342713  2.23743432 -2.12427979 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551967 -1.0664834   0.6027151   1.14432414
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.7289737   1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279218 -2.2730979   3.13337139  2.54856927 -0.67550803 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 01:42:55 2024]  Iteration number: 0 with current cost as 0.23225510178558495 and parameters 
[ 1.31234093  2.23743464 -2.12427943 -0.11653123  0.55388688 -2.77010917
  3.06858478  2.18960165  1.18551978 -1.06648329  0.6027153   1.14432465
  1.31029878 -1.8735466   0.729651    2.88578439 -0.54534355 -0.47522485
 -2.02654261  0.7289739   1.60512684  2.83077087 -1.2645673  -0.25136125
 -2.39279238 -2.27309774  3.13337135  2.54856938 -0.67550807 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 01:47:16 2024]  Iteration number: 0 with current cost as 0.4080225459531984 and parameters 
[-1.00216457  2.23743451 -2.12427964 -0.11653103  0.55388708 -2.77010922
  3.06858473  2.18960133  1.18551998 -1.06648346  0.6027151   1.14432433
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856946 -0.67550787 -2.69002214]. 
Working on 0.8 fold... 
[Mon Mar 25 01:52:59 2024]  Iteration number: 0 with current cost as 0.46742629522734175 and parameters 
[-0.30763464  2.23743464 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.7296508   2.88578451 -0.54534335 -0.47522454
 -2.0265424   0.72897385  1.60512664  2.83077107 -1.26456678 -0.25136089
 -2.39279187 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 01:58:05 2024]  Iteration number: 0 with current cost as 0.48975815155432034 and parameters 
[ 0.0757259   2.23743432 -2.12427964 -0.11653134  0.55388692 -2.77010929
  3.06858451  2.18960114  1.18551983 -1.0664834   0.6027151   1.14432445
  1.31029883 -1.8735468   0.7296508   2.88578419 -0.54534351 -0.47522485
 -2.02654256  0.7289737   1.60512632  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856927 -0.67550787 -2.69002202]. 
Training complete taking 1961.5493218898773 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 02:05:38 2024]  Iteration number: 0 with current cost as 0.44731676974652385 and parameters 
[-1.26342713  2.23743432 -2.12427979 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551967 -1.0664834   0.6027151   1.14432414
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.7289737   1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279218 -2.2730979   3.13337139  2.54856927 -0.67550803 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 02:15:41 2024]  Iteration number: 0 with current cost as 0.23225510178558495 and parameters 
[ 1.31234093  2.23743464 -2.12427943 -0.11653123  0.55388688 -2.77010917
  3.06858478  2.18960165  1.18551978 -1.06648329  0.6027153   1.14432465
  1.31029878 -1.8735466   0.729651    2.88578439 -0.54534355 -0.47522485
 -2.02654261  0.7289739   1.60512684  2.83077087 -1.2645673  -0.25136125
 -2.39279238 -2.27309774  3.13337135  2.54856938 -0.67550807 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 02:20:05 2024]  Iteration number: 0 with current cost as 0.4080225459531984 and parameters 
[-1.00216457  2.23743451 -2.12427964 -0.11653103  0.55388708 -2.77010922
  3.06858473  2.18960133  1.18551998 -1.06648346  0.6027151   1.14432433
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856946 -0.67550787 -2.69002214]. 
Working on 0.8 fold... 
[Mon Mar 25 02:25:37 2024]  Iteration number: 0 with current cost as 0.46742629522734175 and parameters 
[-0.30763464  2.23743464 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.7296508   2.88578451 -0.54534335 -0.47522454
 -2.0265424   0.72897385  1.60512664  2.83077107 -1.26456678 -0.25136089
 -2.39279187 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 02:30:37 2024]  Iteration number: 0 with current cost as 0.48975815155432034 and parameters 
[ 0.0757259   2.23743432 -2.12427964 -0.11653134  0.55388692 -2.77010929
  3.06858451  2.18960114  1.18551983 -1.0664834   0.6027151   1.14432445
  1.31029883 -1.8735468   0.7296508   2.88578419 -0.54534351 -0.47522485
 -2.02654256  0.7289737   1.60512632  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856927 -0.67550787 -2.69002202]. 
Training complete taking 1944.2216539382935 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 02:38:09 2024]  Iteration number: 0 with current cost as 0.44731676974652385 and parameters 
[-1.26342713  2.23743432 -2.12427979 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551967 -1.0664834   0.6027151   1.14432414
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.7289737   1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279218 -2.2730979   3.13337139  2.54856927 -0.67550803 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 02:48:18 2024]  Iteration number: 0 with current cost as 0.23225510178558495 and parameters 
[ 1.31234093  2.23743464 -2.12427943 -0.11653123  0.55388688 -2.77010917
  3.06858478  2.18960165  1.18551978 -1.06648329  0.6027153   1.14432465
  1.31029878 -1.8735466   0.729651    2.88578439 -0.54534355 -0.47522485
 -2.02654261  0.7289739   1.60512684  2.83077087 -1.2645673  -0.25136125
 -2.39279238 -2.27309774  3.13337135  2.54856938 -0.67550807 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 02:52:49 2024]  Iteration number: 0 with current cost as 0.4080225459531984 and parameters 
[-1.00216457  2.23743451 -2.12427964 -0.11653103  0.55388708 -2.77010922
  3.06858473  2.18960133  1.18551998 -1.06648346  0.6027151   1.14432433
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856946 -0.67550787 -2.69002214]. 
Working on 0.8 fold... 
[Mon Mar 25 02:58:23 2024]  Iteration number: 0 with current cost as 0.46742629522734175 and parameters 
[-0.30763464  2.23743464 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.7296508   2.88578451 -0.54534335 -0.47522454
 -2.0265424   0.72897385  1.60512664  2.83077107 -1.26456678 -0.25136089
 -2.39279187 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 03:03:24 2024]  Iteration number: 0 with current cost as 0.48975815155432034 and parameters 
[ 0.0757259   2.23743432 -2.12427964 -0.11653134  0.55388692 -2.77010929
  3.06858451  2.18960114  1.18551983 -1.0664834   0.6027151   1.14432445
  1.31029883 -1.8735468   0.7296508   2.88578419 -0.54534351 -0.47522485
 -2.02654256  0.7289737   1.60512632  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856927 -0.67550787 -2.69002202]. 
Training complete taking 1979.0002720355988 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 03:11:01 2024]  Iteration number: 0 with current cost as 0.44731676974652385 and parameters 
[-1.26342713  2.23743432 -2.12427979 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551967 -1.0664834   0.6027151   1.14432414
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.7289737   1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279218 -2.2730979   3.13337139  2.54856927 -0.67550803 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 03:21:04 2024]  Iteration number: 0 with current cost as 0.23225510178558495 and parameters 
[ 1.31234093  2.23743464 -2.12427943 -0.11653123  0.55388688 -2.77010917
  3.06858478  2.18960165  1.18551978 -1.06648329  0.6027153   1.14432465
  1.31029878 -1.8735466   0.729651    2.88578439 -0.54534355 -0.47522485
 -2.02654261  0.7289739   1.60512684  2.83077087 -1.2645673  -0.25136125
 -2.39279238 -2.27309774  3.13337135  2.54856938 -0.67550807 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 03:25:29 2024]  Iteration number: 0 with current cost as 0.4080225459531984 and parameters 
[-1.00216457  2.23743451 -2.12427964 -0.11653103  0.55388708 -2.77010922
  3.06858473  2.18960133  1.18551998 -1.06648346  0.6027151   1.14432433
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856946 -0.67550787 -2.69002214]. 
Working on 0.8 fold... 
[Mon Mar 25 03:31:03 2024]  Iteration number: 0 with current cost as 0.46742629522734175 and parameters 
[-0.30763464  2.23743464 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.7296508   2.88578451 -0.54534335 -0.47522454
 -2.0265424   0.72897385  1.60512664  2.83077107 -1.26456678 -0.25136089
 -2.39279187 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 03:36:06 2024]  Iteration number: 0 with current cost as 0.48975815155432034 and parameters 
[ 0.0757259   2.23743432 -2.12427964 -0.11653134  0.55388692 -2.77010929
  3.06858451  2.18960114  1.18551983 -1.0664834   0.6027151   1.14432445
  1.31029883 -1.8735468   0.7296508   2.88578419 -0.54534351 -0.47522485
 -2.02654256  0.7289737   1.60512632  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856927 -0.67550787 -2.69002202]. 
Training complete taking 1950.3809158802032 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 03:43:36 2024]  Iteration number: 0 with current cost as 0.44731676974652385 and parameters 
[-1.26342713  2.23743432 -2.12427979 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551967 -1.0664834   0.6027151   1.14432414
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.7289737   1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279218 -2.2730979   3.13337139  2.54856927 -0.67550803 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 03:53:50 2024]  Iteration number: 0 with current cost as 0.23225510178558495 and parameters 
[ 1.31234093  2.23743464 -2.12427943 -0.11653123  0.55388688 -2.77010917
  3.06858478  2.18960165  1.18551978 -1.06648329  0.6027153   1.14432465
  1.31029878 -1.8735466   0.729651    2.88578439 -0.54534355 -0.47522485
 -2.02654261  0.7289739   1.60512684  2.83077087 -1.2645673  -0.25136125
 -2.39279238 -2.27309774  3.13337135  2.54856938 -0.67550807 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 03:58:12 2024]  Iteration number: 0 with current cost as 0.4080225459531984 and parameters 
[-1.00216457  2.23743451 -2.12427964 -0.11653103  0.55388708 -2.77010922
  3.06858473  2.18960133  1.18551998 -1.06648346  0.6027151   1.14432433
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856946 -0.67550787 -2.69002214]. 
Working on 0.8 fold... 
[Mon Mar 25 04:03:59 2024]  Iteration number: 0 with current cost as 0.46742629522734175 and parameters 
[-0.30763464  2.23743464 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.7296508   2.88578451 -0.54534335 -0.47522454
 -2.0265424   0.72897385  1.60512664  2.83077107 -1.26456678 -0.25136089
 -2.39279187 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 04:09:00 2024]  Iteration number: 0 with current cost as 0.48975815155432034 and parameters 
[ 0.0757259   2.23743432 -2.12427964 -0.11653134  0.55388692 -2.77010929
  3.06858451  2.18960114  1.18551983 -1.0664834   0.6027151   1.14432445
  1.31029883 -1.8735468   0.7296508   2.88578419 -0.54534351 -0.47522485
 -2.02654256  0.7289737   1.60512632  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856927 -0.67550787 -2.69002202]. 
Training complete taking 1980.416908979416 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 04:16:33 2024]  Iteration number: 0 with current cost as 0.44731676974652385 and parameters 
[-1.26342713  2.23743432 -2.12427979 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551967 -1.0664834   0.6027151   1.14432414
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.7289737   1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279218 -2.2730979   3.13337139  2.54856927 -0.67550803 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 04:26:32 2024]  Iteration number: 0 with current cost as 0.23225510178558495 and parameters 
[ 1.31234093  2.23743464 -2.12427943 -0.11653123  0.55388688 -2.77010917
  3.06858478  2.18960165  1.18551978 -1.06648329  0.6027153   1.14432465
  1.31029878 -1.8735466   0.729651    2.88578439 -0.54534355 -0.47522485
 -2.02654261  0.7289739   1.60512684  2.83077087 -1.2645673  -0.25136125
 -2.39279238 -2.27309774  3.13337135  2.54856938 -0.67550807 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 04:30:55 2024]  Iteration number: 0 with current cost as 0.4080225459531984 and parameters 
[-1.00216457  2.23743451 -2.12427964 -0.11653103  0.55388708 -2.77010922
  3.06858473  2.18960133  1.18551998 -1.06648346  0.6027151   1.14432433
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856946 -0.67550787 -2.69002214]. 
Working on 0.8 fold... 
[Mon Mar 25 04:36:35 2024]  Iteration number: 0 with current cost as 0.46742629522734175 and parameters 
[-0.30763464  2.23743464 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.7296508   2.88578451 -0.54534335 -0.47522454
 -2.0265424   0.72897385  1.60512664  2.83077107 -1.26456678 -0.25136089
 -2.39279187 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 04:41:43 2024]  Iteration number: 0 with current cost as 0.48975815155432034 and parameters 
[ 0.0757259   2.23743432 -2.12427964 -0.11653134  0.55388692 -2.77010929
  3.06858451  2.18960114  1.18551983 -1.0664834   0.6027151   1.14432445
  1.31029883 -1.8735468   0.7296508   2.88578419 -0.54534351 -0.47522485
 -2.02654256  0.7289737   1.60512632  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856927 -0.67550787 -2.69002202]. 
Training complete taking 1960.0759885311127 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 04:49:17 2024]  Iteration number: 0 with current cost as 0.44731676974652385 and parameters 
[-1.26342713  2.23743432 -2.12427979 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551967 -1.0664834   0.6027151   1.14432414
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.7289737   1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279218 -2.2730979   3.13337139  2.54856927 -0.67550803 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 04:59:23 2024]  Iteration number: 0 with current cost as 0.23225510178558495 and parameters 
[ 1.31234093  2.23743464 -2.12427943 -0.11653123  0.55388688 -2.77010917
  3.06858478  2.18960165  1.18551978 -1.06648329  0.6027153   1.14432465
  1.31029878 -1.8735466   0.729651    2.88578439 -0.54534355 -0.47522485
 -2.02654261  0.7289739   1.60512684  2.83077087 -1.2645673  -0.25136125
 -2.39279238 -2.27309774  3.13337135  2.54856938 -0.67550807 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 05:03:50 2024]  Iteration number: 0 with current cost as 0.4080225459531984 and parameters 
[-1.00216457  2.23743451 -2.12427964 -0.11653103  0.55388708 -2.77010922
  3.06858473  2.18960133  1.18551998 -1.06648346  0.6027151   1.14432433
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856946 -0.67550787 -2.69002214]. 
Working on 0.8 fold... 
[Mon Mar 25 05:09:29 2024]  Iteration number: 0 with current cost as 0.46742629522734175 and parameters 
[-0.30763464  2.23743464 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.7296508   2.88578451 -0.54534335 -0.47522454
 -2.0265424   0.72897385  1.60512664  2.83077107 -1.26456678 -0.25136089
 -2.39279187 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 05:14:40 2024]  Iteration number: 0 with current cost as 0.48975815155432034 and parameters 
[ 0.0757259   2.23743432 -2.12427964 -0.11653134  0.55388692 -2.77010929
  3.06858451  2.18960114  1.18551983 -1.0664834   0.6027151   1.14432445
  1.31029883 -1.8735468   0.7296508   2.88578419 -0.54534351 -0.47522485
 -2.02654256  0.7289737   1.60512632  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856927 -0.67550787 -2.69002202]. 
Training complete taking 1985.7982342243195 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 05:22:19 2024]  Iteration number: 0 with current cost as 0.44731676974652385 and parameters 
[-1.26342713  2.23743432 -2.12427979 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551967 -1.0664834   0.6027151   1.14432414
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.7289737   1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279218 -2.2730979   3.13337139  2.54856927 -0.67550803 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 05:32:30 2024]  Iteration number: 0 with current cost as 0.23225510178558495 and parameters 
[ 1.31234093  2.23743464 -2.12427943 -0.11653123  0.55388688 -2.77010917
  3.06858478  2.18960165  1.18551978 -1.06648329  0.6027153   1.14432465
  1.31029878 -1.8735466   0.729651    2.88578439 -0.54534355 -0.47522485
 -2.02654261  0.7289739   1.60512684  2.83077087 -1.2645673  -0.25136125
 -2.39279238 -2.27309774  3.13337135  2.54856938 -0.67550807 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 05:36:59 2024]  Iteration number: 0 with current cost as 0.4080225459531984 and parameters 
[-1.00216457  2.23743451 -2.12427964 -0.11653103  0.55388708 -2.77010922
  3.06858473  2.18960133  1.18551998 -1.06648346  0.6027151   1.14432433
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856946 -0.67550787 -2.69002214]. 
Working on 0.8 fold... 
[Mon Mar 25 05:42:53 2024]  Iteration number: 0 with current cost as 0.46742629522734175 and parameters 
[-0.30763464  2.23743464 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.7296508   2.88578451 -0.54534335 -0.47522454
 -2.0265424   0.72897385  1.60512664  2.83077107 -1.26456678 -0.25136089
 -2.39279187 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 05:48:04 2024]  Iteration number: 0 with current cost as 0.48975815155432034 and parameters 
[ 0.0757259   2.23743432 -2.12427964 -0.11653134  0.55388692 -2.77010929
  3.06858451  2.18960114  1.18551983 -1.0664834   0.6027151   1.14432445
  1.31029883 -1.8735468   0.7296508   2.88578419 -0.54534351 -0.47522485
 -2.02654256  0.7289737   1.60512632  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856927 -0.67550787 -2.69002202]. 
Training complete taking 2007.3494336605072 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 05:55:54 2024]  Iteration number: 0 with current cost as 0.44731676974652385 and parameters 
[-1.26342713  2.23743432 -2.12427979 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551967 -1.0664834   0.6027151   1.14432414
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.7289737   1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279218 -2.2730979   3.13337139  2.54856927 -0.67550803 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 06:06:18 2024]  Iteration number: 0 with current cost as 0.23225510178558495 and parameters 
[ 1.31234093  2.23743464 -2.12427943 -0.11653123  0.55388688 -2.77010917
  3.06858478  2.18960165  1.18551978 -1.06648329  0.6027153   1.14432465
  1.31029878 -1.8735466   0.729651    2.88578439 -0.54534355 -0.47522485
 -2.02654261  0.7289739   1.60512684  2.83077087 -1.2645673  -0.25136125
 -2.39279238 -2.27309774  3.13337135  2.54856938 -0.67550807 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 06:10:55 2024]  Iteration number: 0 with current cost as 0.4080225459531984 and parameters 
[-1.00216457  2.23743451 -2.12427964 -0.11653103  0.55388708 -2.77010922
  3.06858473  2.18960133  1.18551998 -1.06648346  0.6027151   1.14432433
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856946 -0.67550787 -2.69002214]. 
Working on 0.8 fold... 
[Mon Mar 25 06:16:32 2024]  Iteration number: 0 with current cost as 0.46742629522734175 and parameters 
[-0.30763464  2.23743464 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.7296508   2.88578451 -0.54534335 -0.47522454
 -2.0265424   0.72897385  1.60512664  2.83077107 -1.26456678 -0.25136089
 -2.39279187 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 06:21:40 2024]  Iteration number: 0 with current cost as 0.48975815155432034 and parameters 
[ 0.0757259   2.23743432 -2.12427964 -0.11653134  0.55388692 -2.77010929
  3.06858451  2.18960114  1.18551983 -1.0664834   0.6027151   1.14432445
  1.31029883 -1.8735468   0.7296508   2.88578419 -0.54534351 -0.47522485
 -2.02654256  0.7289737   1.60512632  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856927 -0.67550787 -2.69002202]. 
Training complete taking 2013.2108821868896 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 06:29:26 2024]  Iteration number: 0 with current cost as 0.44731676974652385 and parameters 
[-1.26342713  2.23743432 -2.12427979 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551967 -1.0664834   0.6027151   1.14432414
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.7289737   1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279218 -2.2730979   3.13337139  2.54856927 -0.67550803 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 06:39:50 2024]  Iteration number: 0 with current cost as 0.23225510178558495 and parameters 
[ 1.31234093  2.23743464 -2.12427943 -0.11653123  0.55388688 -2.77010917
  3.06858478  2.18960165  1.18551978 -1.06648329  0.6027153   1.14432465
  1.31029878 -1.8735466   0.729651    2.88578439 -0.54534355 -0.47522485
 -2.02654261  0.7289739   1.60512684  2.83077087 -1.2645673  -0.25136125
 -2.39279238 -2.27309774  3.13337135  2.54856938 -0.67550807 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 06:44:20 2024]  Iteration number: 0 with current cost as 0.4080225459531984 and parameters 
[-1.00216457  2.23743451 -2.12427964 -0.11653103  0.55388708 -2.77010922
  3.06858473  2.18960133  1.18551998 -1.06648346  0.6027151   1.14432433
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856946 -0.67550787 -2.69002214]. 
Working on 0.8 fold... 
[Mon Mar 25 06:50:05 2024]  Iteration number: 0 with current cost as 0.46742629522734175 and parameters 
[-0.30763464  2.23743464 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.7296508   2.88578451 -0.54534335 -0.47522454
 -2.0265424   0.72897385  1.60512664  2.83077107 -1.26456678 -0.25136089
 -2.39279187 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 06:55:18 2024]  Iteration number: 0 with current cost as 0.48975815155432034 and parameters 
[ 0.0757259   2.23743432 -2.12427964 -0.11653134  0.55388692 -2.77010929
  3.06858451  2.18960114  1.18551983 -1.0664834   0.6027151   1.14432445
  1.31029883 -1.8735468   0.7296508   2.88578419 -0.54534351 -0.47522485
 -2.02654256  0.7289737   1.60512632  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856927 -0.67550787 -2.69002202]. 
Training complete taking 2016.2077040672302 seconds. 
Discarding model... 

Training complete taking 48324.55598902702 total seconds. 
Now scoring model... 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Scoring complete taking 2.0676088333129883 seconds. 
Saved predicted values as M-A2-CNOT_Full-CRX_predicted_values.csv
Model scores: {'MSE_train': (147.9083184558395,), 'R2_train': 0.2834338374676022, 'MAE_train': 10.976254797333095, 'MSE_test': 167.36739707502417, 'R2_test': -0.008859524176060285, 'MAE_test': 10.950798118296555}. 
Saved model results as M-A2-CNOT_Full-CRX_results.json. 
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/circuits/Encoders.py:12: RuntimeWarning: invalid value encountered in arcsin
  if np.isnan(np.arcsin(features[feature_idx])) or np.isnan(np.arccos(features[feature_idx])):
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Thu Apr  4 12:08:28 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 12:12:17 2024]  Iteration number: 0 with current cost as 0.44731676974652385 and parameters 
[-1.26342713  2.23743432 -2.12427979 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551967 -1.0664834   0.6027151   1.14432414
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.7289737   1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279218 -2.2730979   3.13337139  2.54856927 -0.67550803 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 12:27:20 2024]  Iteration number: 0 with current cost as 0.23225510178558495 and parameters 
[ 1.31234093  2.23743464 -2.12427943 -0.11653123  0.55388688 -2.77010917
  3.06858478  2.18960165  1.18551978 -1.06648329  0.6027153   1.14432465
  1.31029878 -1.8735466   0.729651    2.88578439 -0.54534355 -0.47522485
 -2.02654261  0.7289739   1.60512684  2.83077087 -1.2645673  -0.25136125
 -2.39279238 -2.27309774  3.13337135  2.54856938 -0.67550807 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 12:33:44 2024]  Iteration number: 0 with current cost as 0.4080225459531984 and parameters 
[-1.00216457  2.23743451 -2.12427964 -0.11653103  0.55388708 -2.77010922
  3.06858473  2.18960133  1.18551998 -1.06648346  0.6027151   1.14432433
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856946 -0.67550787 -2.69002214]. 
Working on 0.8 fold... 
[Thu Apr  4 12:41:48 2024]  Iteration number: 0 with current cost as 0.46742629522734175 and parameters 
[-0.30763464  2.23743464 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.7296508   2.88578451 -0.54534335 -0.47522454
 -2.0265424   0.72897385  1.60512664  2.83077107 -1.26456678 -0.25136089
 -2.39279187 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 12:49:17 2024]  Iteration number: 0 with current cost as 0.48975815155432034 and parameters 
[ 0.0757259   2.23743432 -2.12427964 -0.11653134  0.55388692 -2.77010929
  3.06858451  2.18960114  1.18551983 -1.0664834   0.6027151   1.14432445
  1.31029883 -1.8735468   0.7296508   2.88578419 -0.54534351 -0.47522485
 -2.02654256  0.7289737   1.60512632  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856927 -0.67550787 -2.69002202]. 
Training complete taking 2889.590618133545 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 13:00:16 2024]  Iteration number: 0 with current cost as 0.44731676974652385 and parameters 
[-1.26342713  2.23743432 -2.12427979 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551967 -1.0664834   0.6027151   1.14432414
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.7289737   1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279218 -2.2730979   3.13337139  2.54856927 -0.67550803 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 13:15:10 2024]  Iteration number: 0 with current cost as 0.23225510178558495 and parameters 
[ 1.31234093  2.23743464 -2.12427943 -0.11653123  0.55388688 -2.77010917
  3.06858478  2.18960165  1.18551978 -1.06648329  0.6027153   1.14432465
  1.31029878 -1.8735466   0.729651    2.88578439 -0.54534355 -0.47522485
 -2.02654261  0.7289739   1.60512684  2.83077087 -1.2645673  -0.25136125
 -2.39279238 -2.27309774  3.13337135  2.54856938 -0.67550807 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 13:21:32 2024]  Iteration number: 0 with current cost as 0.4080225459531984 and parameters 
[-1.00216457  2.23743451 -2.12427964 -0.11653103  0.55388708 -2.77010922
  3.06858473  2.18960133  1.18551998 -1.06648346  0.6027151   1.14432433
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856946 -0.67550787 -2.69002214]. 
Working on 0.8 fold... 
[Thu Apr  4 13:29:52 2024]  Iteration number: 0 with current cost as 0.46742629522734175 and parameters 
[-0.30763464  2.23743464 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.7296508   2.88578451 -0.54534335 -0.47522454
 -2.0265424   0.72897385  1.60512664  2.83077107 -1.26456678 -0.25136089
 -2.39279187 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 13:37:13 2024]  Iteration number: 0 with current cost as 0.48975815155432034 and parameters 
[ 0.0757259   2.23743432 -2.12427964 -0.11653134  0.55388692 -2.77010929
  3.06858451  2.18960114  1.18551983 -1.0664834   0.6027151   1.14432445
  1.31029883 -1.8735468   0.7296508   2.88578419 -0.54534351 -0.47522485
 -2.02654256  0.7289737   1.60512632  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856927 -0.67550787 -2.69002202]. 
Training complete taking 2889.669011592865 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 13:48:36 2024]  Iteration number: 0 with current cost as 0.44731676974652385 and parameters 
[-1.26342713  2.23743432 -2.12427979 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551967 -1.0664834   0.6027151   1.14432414
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.7289737   1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279218 -2.2730979   3.13337139  2.54856927 -0.67550803 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 14:03:30 2024]  Iteration number: 0 with current cost as 0.23225510178558495 and parameters 
[ 1.31234093  2.23743464 -2.12427943 -0.11653123  0.55388688 -2.77010917
  3.06858478  2.18960165  1.18551978 -1.06648329  0.6027153   1.14432465
  1.31029878 -1.8735466   0.729651    2.88578439 -0.54534355 -0.47522485
 -2.02654261  0.7289739   1.60512684  2.83077087 -1.2645673  -0.25136125
 -2.39279238 -2.27309774  3.13337135  2.54856938 -0.67550807 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 14:10:01 2024]  Iteration number: 0 with current cost as 0.4080225459531984 and parameters 
[-1.00216457  2.23743451 -2.12427964 -0.11653103  0.55388708 -2.77010922
  3.06858473  2.18960133  1.18551998 -1.06648346  0.6027151   1.14432433
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856946 -0.67550787 -2.69002214]. 
Working on 0.8 fold... 
[Thu Apr  4 14:18:29 2024]  Iteration number: 0 with current cost as 0.46742629522734175 and parameters 
[-0.30763464  2.23743464 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.7296508   2.88578451 -0.54534335 -0.47522454
 -2.0265424   0.72897385  1.60512664  2.83077107 -1.26456678 -0.25136089
 -2.39279187 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 14:25:57 2024]  Iteration number: 0 with current cost as 0.48975815155432034 and parameters 
[ 0.0757259   2.23743432 -2.12427964 -0.11653134  0.55388692 -2.77010929
  3.06858451  2.18960114  1.18551983 -1.0664834   0.6027151   1.14432445
  1.31029883 -1.8735468   0.7296508   2.88578419 -0.54534351 -0.47522485
 -2.02654256  0.7289737   1.60512632  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856927 -0.67550787 -2.69002202]. 
Training complete taking 2922.0950038433075 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 14:37:14 2024]  Iteration number: 0 with current cost as 0.44731676974652385 and parameters 
[-1.26342713  2.23743432 -2.12427979 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551967 -1.0664834   0.6027151   1.14432414
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.7289737   1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279218 -2.2730979   3.13337139  2.54856927 -0.67550803 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 14:52:04 2024]  Iteration number: 0 with current cost as 0.23225510178558495 and parameters 
[ 1.31234093  2.23743464 -2.12427943 -0.11653123  0.55388688 -2.77010917
  3.06858478  2.18960165  1.18551978 -1.06648329  0.6027153   1.14432465
  1.31029878 -1.8735466   0.729651    2.88578439 -0.54534355 -0.47522485
 -2.02654261  0.7289739   1.60512684  2.83077087 -1.2645673  -0.25136125
 -2.39279238 -2.27309774  3.13337135  2.54856938 -0.67550807 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 14:58:37 2024]  Iteration number: 0 with current cost as 0.4080225459531984 and parameters 
[-1.00216457  2.23743451 -2.12427964 -0.11653103  0.55388708 -2.77010922
  3.06858473  2.18960133  1.18551998 -1.06648346  0.6027151   1.14432433
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856946 -0.67550787 -2.69002214]. 
Working on 0.8 fold... 
[Thu Apr  4 15:07:04 2024]  Iteration number: 0 with current cost as 0.46742629522734175 and parameters 
[-0.30763464  2.23743464 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.7296508   2.88578451 -0.54534335 -0.47522454
 -2.0265424   0.72897385  1.60512664  2.83077107 -1.26456678 -0.25136089
 -2.39279187 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 15:14:43 2024]  Iteration number: 0 with current cost as 0.48975815155432034 and parameters 
[ 0.0757259   2.23743432 -2.12427964 -0.11653134  0.55388692 -2.77010929
  3.06858451  2.18960114  1.18551983 -1.0664834   0.6027151   1.14432445
  1.31029883 -1.8735468   0.7296508   2.88578419 -0.54534351 -0.47522485
 -2.02654256  0.7289737   1.60512632  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856927 -0.67550787 -2.69002202]. 
Training complete taking 2925.446920633316 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 15:26:07 2024]  Iteration number: 0 with current cost as 0.44731676974652385 and parameters 
[-1.26342713  2.23743432 -2.12427979 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551967 -1.0664834   0.6027151   1.14432414
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.7289737   1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279218 -2.2730979   3.13337139  2.54856927 -0.67550803 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 15:41:31 2024]  Iteration number: 0 with current cost as 0.23225510178558495 and parameters 
[ 1.31234093  2.23743464 -2.12427943 -0.11653123  0.55388688 -2.77010917
  3.06858478  2.18960165  1.18551978 -1.06648329  0.6027153   1.14432465
  1.31029878 -1.8735466   0.729651    2.88578439 -0.54534355 -0.47522485
 -2.02654261  0.7289739   1.60512684  2.83077087 -1.2645673  -0.25136125
 -2.39279238 -2.27309774  3.13337135  2.54856938 -0.67550807 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 15:48:11 2024]  Iteration number: 0 with current cost as 0.4080225459531984 and parameters 
[-1.00216457  2.23743451 -2.12427964 -0.11653103  0.55388708 -2.77010922
  3.06858473  2.18960133  1.18551998 -1.06648346  0.6027151   1.14432433
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856946 -0.67550787 -2.69002214]. 
Working on 0.8 fold... 
[Thu Apr  4 15:56:39 2024]  Iteration number: 0 with current cost as 0.46742629522734175 and parameters 
[-0.30763464  2.23743464 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.7296508   2.88578451 -0.54534335 -0.47522454
 -2.0265424   0.72897385  1.60512664  2.83077107 -1.26456678 -0.25136089
 -2.39279187 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 16:04:16 2024]  Iteration number: 0 with current cost as 0.48975815155432034 and parameters 
[ 0.0757259   2.23743432 -2.12427964 -0.11653134  0.55388692 -2.77010929
  3.06858451  2.18960114  1.18551983 -1.0664834   0.6027151   1.14432445
  1.31029883 -1.8735468   0.7296508   2.88578419 -0.54534351 -0.47522485
 -2.02654256  0.7289737   1.60512632  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856927 -0.67550787 -2.69002202]. 
Training complete taking 2980.0175161361694 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 16:15:38 2024]  Iteration number: 0 with current cost as 0.44731676974652385 and parameters 
[-1.26342713  2.23743432 -2.12427979 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551967 -1.0664834   0.6027151   1.14432414
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.7289737   1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279218 -2.2730979   3.13337139  2.54856927 -0.67550803 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 16:30:18 2024]  Iteration number: 0 with current cost as 0.23225510178558495 and parameters 
[ 1.31234093  2.23743464 -2.12427943 -0.11653123  0.55388688 -2.77010917
  3.06858478  2.18960165  1.18551978 -1.06648329  0.6027153   1.14432465
  1.31029878 -1.8735466   0.729651    2.88578439 -0.54534355 -0.47522485
 -2.02654261  0.7289739   1.60512684  2.83077087 -1.2645673  -0.25136125
 -2.39279238 -2.27309774  3.13337135  2.54856938 -0.67550807 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 16:36:42 2024]  Iteration number: 0 with current cost as 0.4080225459531984 and parameters 
[-1.00216457  2.23743451 -2.12427964 -0.11653103  0.55388708 -2.77010922
  3.06858473  2.18960133  1.18551998 -1.06648346  0.6027151   1.14432433
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856946 -0.67550787 -2.69002214]. 
Working on 0.8 fold... 
[Thu Apr  4 16:44:57 2024]  Iteration number: 0 with current cost as 0.46742629522734175 and parameters 
[-0.30763464  2.23743464 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.7296508   2.88578451 -0.54534335 -0.47522454
 -2.0265424   0.72897385  1.60512664  2.83077107 -1.26456678 -0.25136089
 -2.39279187 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 16:52:30 2024]  Iteration number: 0 with current cost as 0.48975815155432034 and parameters 
[ 0.0757259   2.23743432 -2.12427964 -0.11653134  0.55388692 -2.77010929
  3.06858451  2.18960114  1.18551983 -1.0664834   0.6027151   1.14432445
  1.31029883 -1.8735468   0.7296508   2.88578419 -0.54534351 -0.47522485
 -2.02654256  0.7289737   1.60512632  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856927 -0.67550787 -2.69002202]. 
Training complete taking 2881.388509750366 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 17:03:38 2024]  Iteration number: 0 with current cost as 0.44731676974652385 and parameters 
[-1.26342713  2.23743432 -2.12427979 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551967 -1.0664834   0.6027151   1.14432414
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.7289737   1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279218 -2.2730979   3.13337139  2.54856927 -0.67550803 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 17:18:16 2024]  Iteration number: 0 with current cost as 0.23225510178558495 and parameters 
[ 1.31234093  2.23743464 -2.12427943 -0.11653123  0.55388688 -2.77010917
  3.06858478  2.18960165  1.18551978 -1.06648329  0.6027153   1.14432465
  1.31029878 -1.8735466   0.729651    2.88578439 -0.54534355 -0.47522485
 -2.02654261  0.7289739   1.60512684  2.83077087 -1.2645673  -0.25136125
 -2.39279238 -2.27309774  3.13337135  2.54856938 -0.67550807 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 17:24:41 2024]  Iteration number: 0 with current cost as 0.4080225459531984 and parameters 
[-1.00216457  2.23743451 -2.12427964 -0.11653103  0.55388708 -2.77010922
  3.06858473  2.18960133  1.18551998 -1.06648346  0.6027151   1.14432433
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856946 -0.67550787 -2.69002214]. 
Working on 0.8 fold... 
[Thu Apr  4 17:33:19 2024]  Iteration number: 0 with current cost as 0.46742629522734175 and parameters 
[-0.30763464  2.23743464 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.7296508   2.88578451 -0.54534335 -0.47522454
 -2.0265424   0.72897385  1.60512664  2.83077107 -1.26456678 -0.25136089
 -2.39279187 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 17:40:38 2024]  Iteration number: 0 with current cost as 0.48975815155432034 and parameters 
[ 0.0757259   2.23743432 -2.12427964 -0.11653134  0.55388692 -2.77010929
  3.06858451  2.18960114  1.18551983 -1.0664834   0.6027151   1.14432445
  1.31029883 -1.8735468   0.7296508   2.88578419 -0.54534351 -0.47522485
 -2.02654256  0.7289737   1.60512632  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856927 -0.67550787 -2.69002202]. 
Training complete taking 2881.604794025421 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 17:51:47 2024]  Iteration number: 0 with current cost as 0.44731676974652385 and parameters 
[-1.26342713  2.23743432 -2.12427979 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551967 -1.0664834   0.6027151   1.14432414
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.7289737   1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279218 -2.2730979   3.13337139  2.54856927 -0.67550803 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 18:07:01 2024]  Iteration number: 0 with current cost as 0.23225510178558495 and parameters 
[ 1.31234093  2.23743464 -2.12427943 -0.11653123  0.55388688 -2.77010917
  3.06858478  2.18960165  1.18551978 -1.06648329  0.6027153   1.14432465
  1.31029878 -1.8735466   0.729651    2.88578439 -0.54534355 -0.47522485
 -2.02654261  0.7289739   1.60512684  2.83077087 -1.2645673  -0.25136125
 -2.39279238 -2.27309774  3.13337135  2.54856938 -0.67550807 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 18:13:40 2024]  Iteration number: 0 with current cost as 0.4080225459531984 and parameters 
[-1.00216457  2.23743451 -2.12427964 -0.11653103  0.55388708 -2.77010922
  3.06858473  2.18960133  1.18551998 -1.06648346  0.6027151   1.14432433
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856946 -0.67550787 -2.69002214]. 
Working on 0.8 fold... 
[Thu Apr  4 18:22:14 2024]  Iteration number: 0 with current cost as 0.46742629522734175 and parameters 
[-0.30763464  2.23743464 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.7296508   2.88578451 -0.54534335 -0.47522454
 -2.0265424   0.72897385  1.60512664  2.83077107 -1.26456678 -0.25136089
 -2.39279187 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 18:29:39 2024]  Iteration number: 0 with current cost as 0.48975815155432034 and parameters 
[ 0.0757259   2.23743432 -2.12427964 -0.11653134  0.55388692 -2.77010929
  3.06858451  2.18960114  1.18551983 -1.0664834   0.6027151   1.14432445
  1.31029883 -1.8735468   0.7296508   2.88578419 -0.54534351 -0.47522485
 -2.02654256  0.7289737   1.60512632  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856927 -0.67550787 -2.69002202]. 
Training complete taking 2947.777136325836 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 18:40:56 2024]  Iteration number: 0 with current cost as 0.44731676974652385 and parameters 
[-1.26342713  2.23743432 -2.12427979 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551967 -1.0664834   0.6027151   1.14432414
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.7289737   1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279218 -2.2730979   3.13337139  2.54856927 -0.67550803 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 18:56:04 2024]  Iteration number: 0 with current cost as 0.23225510178558495 and parameters 
[ 1.31234093  2.23743464 -2.12427943 -0.11653123  0.55388688 -2.77010917
  3.06858478  2.18960165  1.18551978 -1.06648329  0.6027153   1.14432465
  1.31029878 -1.8735466   0.729651    2.88578439 -0.54534355 -0.47522485
 -2.02654261  0.7289739   1.60512684  2.83077087 -1.2645673  -0.25136125
 -2.39279238 -2.27309774  3.13337135  2.54856938 -0.67550807 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 19:02:36 2024]  Iteration number: 0 with current cost as 0.4080225459531984 and parameters 
[-1.00216457  2.23743451 -2.12427964 -0.11653103  0.55388708 -2.77010922
  3.06858473  2.18960133  1.18551998 -1.06648346  0.6027151   1.14432433
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856946 -0.67550787 -2.69002214]. 
Working on 0.8 fold... 
[Thu Apr  4 19:10:58 2024]  Iteration number: 0 with current cost as 0.46742629522734175 and parameters 
[-0.30763464  2.23743464 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.7296508   2.88578451 -0.54534335 -0.47522454
 -2.0265424   0.72897385  1.60512664  2.83077107 -1.26456678 -0.25136089
 -2.39279187 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 19:18:21 2024]  Iteration number: 0 with current cost as 0.48975815155432034 and parameters 
[ 0.0757259   2.23743432 -2.12427964 -0.11653134  0.55388692 -2.77010929
  3.06858451  2.18960114  1.18551983 -1.0664834   0.6027151   1.14432445
  1.31029883 -1.8735468   0.7296508   2.88578419 -0.54534351 -0.47522485
 -2.02654256  0.7289737   1.60512632  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856927 -0.67550787 -2.69002202]. 
Training complete taking 2914.486299753189 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 19:29:22 2024]  Iteration number: 0 with current cost as 0.44731676974652385 and parameters 
[-1.26342713  2.23743432 -2.12427979 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551967 -1.0664834   0.6027151   1.14432414
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.7289737   1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279218 -2.2730979   3.13337139  2.54856927 -0.67550803 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 19:44:49 2024]  Iteration number: 0 with current cost as 0.23225510178558495 and parameters 
[ 1.31234093  2.23743464 -2.12427943 -0.11653123  0.55388688 -2.77010917
  3.06858478  2.18960165  1.18551978 -1.06648329  0.6027153   1.14432465
  1.31029878 -1.8735466   0.729651    2.88578439 -0.54534355 -0.47522485
 -2.02654261  0.7289739   1.60512684  2.83077087 -1.2645673  -0.25136125
 -2.39279238 -2.27309774  3.13337135  2.54856938 -0.67550807 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 19:51:51 2024]  Iteration number: 0 with current cost as 0.4080225459531984 and parameters 
[-1.00216457  2.23743451 -2.12427964 -0.11653103  0.55388708 -2.77010922
  3.06858473  2.18960133  1.18551998 -1.06648346  0.6027151   1.14432433
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856946 -0.67550787 -2.69002214]. 
Working on 0.8 fold... 
[Thu Apr  4 20:00:16 2024]  Iteration number: 0 with current cost as 0.46742629522734175 and parameters 
[-0.30763464  2.23743464 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.7296508   2.88578451 -0.54534335 -0.47522454
 -2.0265424   0.72897385  1.60512664  2.83077107 -1.26456678 -0.25136089
 -2.39279187 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 20:08:10 2024]  Iteration number: 0 with current cost as 0.48975815155432034 and parameters 
[ 0.0757259   2.23743432 -2.12427964 -0.11653134  0.55388692 -2.77010929
  3.06858451  2.18960114  1.18551983 -1.0664834   0.6027151   1.14432445
  1.31029883 -1.8735468   0.7296508   2.88578419 -0.54534351 -0.47522485
 -2.02654256  0.7289737   1.60512632  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856927 -0.67550787 -2.69002202]. 
Training complete taking 3028.155692100525 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 20:20:16 2024]  Iteration number: 0 with current cost as 0.44731676974652385 and parameters 
[-1.26342713  2.23743432 -2.12427979 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551967 -1.0664834   0.6027151   1.14432414
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.7289737   1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279218 -2.2730979   3.13337139  2.54856927 -0.67550803 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 20:36:14 2024]  Iteration number: 0 with current cost as 0.23225510178558495 and parameters 
[ 1.31234093  2.23743464 -2.12427943 -0.11653123  0.55388688 -2.77010917
  3.06858478  2.18960165  1.18551978 -1.06648329  0.6027153   1.14432465
  1.31029878 -1.8735466   0.729651    2.88578439 -0.54534355 -0.47522485
 -2.02654261  0.7289739   1.60512684  2.83077087 -1.2645673  -0.25136125
 -2.39279238 -2.27309774  3.13337135  2.54856938 -0.67550807 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 20:43:03 2024]  Iteration number: 0 with current cost as 0.4080225459531984 and parameters 
[-1.00216457  2.23743451 -2.12427964 -0.11653103  0.55388708 -2.77010922
  3.06858473  2.18960133  1.18551998 -1.06648346  0.6027151   1.14432433
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856946 -0.67550787 -2.69002214]. 
Working on 0.8 fold... 
[Thu Apr  4 20:51:46 2024]  Iteration number: 0 with current cost as 0.46742629522734175 and parameters 
[-0.30763464  2.23743464 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.7296508   2.88578451 -0.54534335 -0.47522454
 -2.0265424   0.72897385  1.60512664  2.83077107 -1.26456678 -0.25136089
 -2.39279187 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 20:59:50 2024]  Iteration number: 0 with current cost as 0.48975815155432034 and parameters 
[ 0.0757259   2.23743432 -2.12427964 -0.11653134  0.55388692 -2.77010929
  3.06858451  2.18960114  1.18551983 -1.0664834   0.6027151   1.14432445
  1.31029883 -1.8735468   0.7296508   2.88578419 -0.54534351 -0.47522485
 -2.02654256  0.7289737   1.60512632  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856927 -0.67550787 -2.69002202]. 
Training complete taking 3079.3819551467896 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 21:11:22 2024]  Iteration number: 0 with current cost as 0.44731676974652385 and parameters 
[-1.26342713  2.23743432 -2.12427979 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551967 -1.0664834   0.6027151   1.14432414
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.7289737   1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279218 -2.2730979   3.13337139  2.54856927 -0.67550803 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 21:27:28 2024]  Iteration number: 0 with current cost as 0.23225510178558495 and parameters 
[ 1.31234093  2.23743464 -2.12427943 -0.11653123  0.55388688 -2.77010917
  3.06858478  2.18960165  1.18551978 -1.06648329  0.6027153   1.14432465
  1.31029878 -1.8735466   0.729651    2.88578439 -0.54534355 -0.47522485
 -2.02654261  0.7289739   1.60512684  2.83077087 -1.2645673  -0.25136125
 -2.39279238 -2.27309774  3.13337135  2.54856938 -0.67550807 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 21:34:21 2024]  Iteration number: 0 with current cost as 0.4080225459531984 and parameters 
[-1.00216457  2.23743451 -2.12427964 -0.11653103  0.55388708 -2.77010922
  3.06858473  2.18960133  1.18551998 -1.06648346  0.6027151   1.14432433
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856946 -0.67550787 -2.69002214]. 
Working on 0.8 fold... 
[Thu Apr  4 21:43:34 2024]  Iteration number: 0 with current cost as 0.46742629522734175 and parameters 
[-0.30763464  2.23743464 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.7296508   2.88578451 -0.54534335 -0.47522454
 -2.0265424   0.72897385  1.60512664  2.83077107 -1.26456678 -0.25136089
 -2.39279187 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 21:51:50 2024]  Iteration number: 0 with current cost as 0.48975815155432034 and parameters 
[ 0.0757259   2.23743432 -2.12427964 -0.11653134  0.55388692 -2.77010929
  3.06858451  2.18960114  1.18551983 -1.0664834   0.6027151   1.14432445
  1.31029883 -1.8735468   0.7296508   2.88578419 -0.54534351 -0.47522485
 -2.02654256  0.7289737   1.60512632  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856927 -0.67550787 -2.69002202]. 
Training complete taking 3136.9770278930664 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 22:03:53 2024]  Iteration number: 0 with current cost as 0.44731676974652385 and parameters 
[-1.26342713  2.23743432 -2.12427979 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551967 -1.0664834   0.6027151   1.14432414
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.7289737   1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279218 -2.2730979   3.13337139  2.54856927 -0.67550803 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 22:20:51 2024]  Iteration number: 0 with current cost as 0.23225510178558495 and parameters 
[ 1.31234093  2.23743464 -2.12427943 -0.11653123  0.55388688 -2.77010917
  3.06858478  2.18960165  1.18551978 -1.06648329  0.6027153   1.14432465
  1.31029878 -1.8735466   0.729651    2.88578439 -0.54534355 -0.47522485
 -2.02654261  0.7289739   1.60512684  2.83077087 -1.2645673  -0.25136125
 -2.39279238 -2.27309774  3.13337135  2.54856938 -0.67550807 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 22:28:06 2024]  Iteration number: 0 with current cost as 0.4080225459531984 and parameters 
[-1.00216457  2.23743451 -2.12427964 -0.11653103  0.55388708 -2.77010922
  3.06858473  2.18960133  1.18551998 -1.06648346  0.6027151   1.14432433
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856946 -0.67550787 -2.69002214]. 
Working on 0.8 fold... 
[Thu Apr  4 22:36:32 2024]  Iteration number: 0 with current cost as 0.46742629522734175 and parameters 
[-0.30763464  2.23743464 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.7296508   2.88578451 -0.54534335 -0.47522454
 -2.0265424   0.72897385  1.60512664  2.83077107 -1.26456678 -0.25136089
 -2.39279187 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 22:44:03 2024]  Iteration number: 0 with current cost as 0.48975815155432034 and parameters 
[ 0.0757259   2.23743432 -2.12427964 -0.11653134  0.55388692 -2.77010929
  3.06858451  2.18960114  1.18551983 -1.0664834   0.6027151   1.14432445
  1.31029883 -1.8735468   0.7296508   2.88578419 -0.54534351 -0.47522485
 -2.02654256  0.7289737   1.60512632  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856927 -0.67550787 -2.69002202]. 
Training complete taking 3105.2309308052063 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 22:55:17 2024]  Iteration number: 0 with current cost as 0.44731676974652385 and parameters 
[-1.26342713  2.23743432 -2.12427979 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551967 -1.0664834   0.6027151   1.14432414
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.7289737   1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279218 -2.2730979   3.13337139  2.54856927 -0.67550803 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 23:10:09 2024]  Iteration number: 0 with current cost as 0.23225510178558495 and parameters 
[ 1.31234093  2.23743464 -2.12427943 -0.11653123  0.55388688 -2.77010917
  3.06858478  2.18960165  1.18551978 -1.06648329  0.6027153   1.14432465
  1.31029878 -1.8735466   0.729651    2.88578439 -0.54534355 -0.47522485
 -2.02654261  0.7289739   1.60512684  2.83077087 -1.2645673  -0.25136125
 -2.39279238 -2.27309774  3.13337135  2.54856938 -0.67550807 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 23:16:44 2024]  Iteration number: 0 with current cost as 0.4080225459531984 and parameters 
[-1.00216457  2.23743451 -2.12427964 -0.11653103  0.55388708 -2.77010922
  3.06858473  2.18960133  1.18551998 -1.06648346  0.6027151   1.14432433
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856946 -0.67550787 -2.69002214]. 
Working on 0.8 fold... 
[Thu Apr  4 23:25:12 2024]  Iteration number: 0 with current cost as 0.46742629522734175 and parameters 
[-0.30763464  2.23743464 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.7296508   2.88578451 -0.54534335 -0.47522454
 -2.0265424   0.72897385  1.60512664  2.83077107 -1.26456678 -0.25136089
 -2.39279187 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 23:33:22 2024]  Iteration number: 0 with current cost as 0.48975815155432034 and parameters 
[ 0.0757259   2.23743432 -2.12427964 -0.11653134  0.55388692 -2.77010929
  3.06858451  2.18960114  1.18551983 -1.0664834   0.6027151   1.14432445
  1.31029883 -1.8735468   0.7296508   2.88578419 -0.54534351 -0.47522485
 -2.02654256  0.7289737   1.60512632  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856927 -0.67550787 -2.69002202]. 
Training complete taking 3000.7200639247894 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 23:45:38 2024]  Iteration number: 0 with current cost as 0.44731676974652385 and parameters 
[-1.26342713  2.23743432 -2.12427979 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551967 -1.0664834   0.6027151   1.14432414
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.7289737   1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279218 -2.2730979   3.13337139  2.54856927 -0.67550803 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 00:01:06 2024]  Iteration number: 0 with current cost as 0.23225510178558495 and parameters 
[ 1.31234093  2.23743464 -2.12427943 -0.11653123  0.55388688 -2.77010917
  3.06858478  2.18960165  1.18551978 -1.06648329  0.6027153   1.14432465
  1.31029878 -1.8735466   0.729651    2.88578439 -0.54534355 -0.47522485
 -2.02654261  0.7289739   1.60512684  2.83077087 -1.2645673  -0.25136125
 -2.39279238 -2.27309774  3.13337135  2.54856938 -0.67550807 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 00:07:30 2024]  Iteration number: 0 with current cost as 0.4080225459531984 and parameters 
[-1.00216457  2.23743451 -2.12427964 -0.11653103  0.55388708 -2.77010922
  3.06858473  2.18960133  1.18551998 -1.06648346  0.6027151   1.14432433
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856946 -0.67550787 -2.69002214]. 
Working on 0.8 fold... 
[Fri Apr  5 00:16:07 2024]  Iteration number: 0 with current cost as 0.46742629522734175 and parameters 
[-0.30763464  2.23743464 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.7296508   2.88578451 -0.54534335 -0.47522454
 -2.0265424   0.72897385  1.60512664  2.83077107 -1.26456678 -0.25136089
 -2.39279187 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 00:23:28 2024]  Iteration number: 0 with current cost as 0.48975815155432034 and parameters 
[ 0.0757259   2.23743432 -2.12427964 -0.11653134  0.55388692 -2.77010929
  3.06858451  2.18960114  1.18551983 -1.0664834   0.6027151   1.14432445
  1.31029883 -1.8735468   0.7296508   2.88578419 -0.54534351 -0.47522485
 -2.02654256  0.7289737   1.60512632  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856927 -0.67550787 -2.69002202]. 
Training complete taking 2971.250730752945 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 00:34:50 2024]  Iteration number: 0 with current cost as 0.44731676974652385 and parameters 
[-1.26342713  2.23743432 -2.12427979 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551967 -1.0664834   0.6027151   1.14432414
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.7289737   1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279218 -2.2730979   3.13337139  2.54856927 -0.67550803 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 00:50:20 2024]  Iteration number: 0 with current cost as 0.23225510178558495 and parameters 
[ 1.31234093  2.23743464 -2.12427943 -0.11653123  0.55388688 -2.77010917
  3.06858478  2.18960165  1.18551978 -1.06648329  0.6027153   1.14432465
  1.31029878 -1.8735466   0.729651    2.88578439 -0.54534355 -0.47522485
 -2.02654261  0.7289739   1.60512684  2.83077087 -1.2645673  -0.25136125
 -2.39279238 -2.27309774  3.13337135  2.54856938 -0.67550807 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 00:57:05 2024]  Iteration number: 0 with current cost as 0.4080225459531984 and parameters 
[-1.00216457  2.23743451 -2.12427964 -0.11653103  0.55388708 -2.77010922
  3.06858473  2.18960133  1.18551998 -1.06648346  0.6027151   1.14432433
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856946 -0.67550787 -2.69002214]. 
Working on 0.8 fold... 
[Fri Apr  5 01:05:41 2024]  Iteration number: 0 with current cost as 0.46742629522734175 and parameters 
[-0.30763464  2.23743464 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.7296508   2.88578451 -0.54534335 -0.47522454
 -2.0265424   0.72897385  1.60512664  2.83077107 -1.26456678 -0.25136089
 -2.39279187 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 01:13:14 2024]  Iteration number: 0 with current cost as 0.48975815155432034 and parameters 
[ 0.0757259   2.23743432 -2.12427964 -0.11653134  0.55388692 -2.77010929
  3.06858451  2.18960114  1.18551983 -1.0664834   0.6027151   1.14432445
  1.31029883 -1.8735468   0.7296508   2.88578419 -0.54534351 -0.47522485
 -2.02654256  0.7289737   1.60512632  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856927 -0.67550787 -2.69002202]. 
Training complete taking 2981.12922000885 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 01:24:19 2024]  Iteration number: 0 with current cost as 0.44731676974652385 and parameters 
[-1.26342713  2.23743432 -2.12427979 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551967 -1.0664834   0.6027151   1.14432414
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.7289737   1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279218 -2.2730979   3.13337139  2.54856927 -0.67550803 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 01:39:44 2024]  Iteration number: 0 with current cost as 0.23225510178558495 and parameters 
[ 1.31234093  2.23743464 -2.12427943 -0.11653123  0.55388688 -2.77010917
  3.06858478  2.18960165  1.18551978 -1.06648329  0.6027153   1.14432465
  1.31029878 -1.8735466   0.729651    2.88578439 -0.54534355 -0.47522485
 -2.02654261  0.7289739   1.60512684  2.83077087 -1.2645673  -0.25136125
 -2.39279238 -2.27309774  3.13337135  2.54856938 -0.67550807 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 01:47:03 2024]  Iteration number: 0 with current cost as 0.4080225459531984 and parameters 
[-1.00216457  2.23743451 -2.12427964 -0.11653103  0.55388708 -2.77010922
  3.06858473  2.18960133  1.18551998 -1.06648346  0.6027151   1.14432433
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856946 -0.67550787 -2.69002214]. 
Working on 0.8 fold... 
[Fri Apr  5 01:56:26 2024]  Iteration number: 0 with current cost as 0.46742629522734175 and parameters 
[-0.30763464  2.23743464 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.7296508   2.88578451 -0.54534335 -0.47522454
 -2.0265424   0.72897385  1.60512664  2.83077107 -1.26456678 -0.25136089
 -2.39279187 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 02:04:40 2024]  Iteration number: 0 with current cost as 0.48975815155432034 and parameters 
[ 0.0757259   2.23743432 -2.12427964 -0.11653134  0.55388692 -2.77010929
  3.06858451  2.18960114  1.18551983 -1.0664834   0.6027151   1.14432445
  1.31029883 -1.8735468   0.7296508   2.88578419 -0.54534351 -0.47522485
 -2.02654256  0.7289737   1.60512632  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856927 -0.67550787 -2.69002202]. 
Training complete taking 3133.603944540024 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 02:16:55 2024]  Iteration number: 0 with current cost as 0.44731676974652385 and parameters 
[-1.26342713  2.23743432 -2.12427979 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551967 -1.0664834   0.6027151   1.14432414
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.7289737   1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279218 -2.2730979   3.13337139  2.54856927 -0.67550803 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 02:33:14 2024]  Iteration number: 0 with current cost as 0.23225510178558495 and parameters 
[ 1.31234093  2.23743464 -2.12427943 -0.11653123  0.55388688 -2.77010917
  3.06858478  2.18960165  1.18551978 -1.06648329  0.6027153   1.14432465
  1.31029878 -1.8735466   0.729651    2.88578439 -0.54534355 -0.47522485
 -2.02654261  0.7289739   1.60512684  2.83077087 -1.2645673  -0.25136125
 -2.39279238 -2.27309774  3.13337135  2.54856938 -0.67550807 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 02:40:33 2024]  Iteration number: 0 with current cost as 0.4080225459531984 and parameters 
[-1.00216457  2.23743451 -2.12427964 -0.11653103  0.55388708 -2.77010922
  3.06858473  2.18960133  1.18551998 -1.06648346  0.6027151   1.14432433
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856946 -0.67550787 -2.69002214]. 
Working on 0.8 fold... 
[Fri Apr  5 02:49:47 2024]  Iteration number: 0 with current cost as 0.46742629522734175 and parameters 
[-0.30763464  2.23743464 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.7296508   2.88578451 -0.54534335 -0.47522454
 -2.0265424   0.72897385  1.60512664  2.83077107 -1.26456678 -0.25136089
 -2.39279187 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 02:58:12 2024]  Iteration number: 0 with current cost as 0.48975815155432034 and parameters 
[ 0.0757259   2.23743432 -2.12427964 -0.11653134  0.55388692 -2.77010929
  3.06858451  2.18960114  1.18551983 -1.0664834   0.6027151   1.14432445
  1.31029883 -1.8735468   0.7296508   2.88578419 -0.54534351 -0.47522485
 -2.02654256  0.7289737   1.60512632  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856927 -0.67550787 -2.69002202]. 
Training complete taking 3220.2720465660095 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 03:10:37 2024]  Iteration number: 0 with current cost as 0.44731676974652385 and parameters 
[-1.26342713  2.23743432 -2.12427979 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551967 -1.0664834   0.6027151   1.14432414
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.7289737   1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279218 -2.2730979   3.13337139  2.54856927 -0.67550803 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 03:27:16 2024]  Iteration number: 0 with current cost as 0.23225510178558495 and parameters 
[ 1.31234093  2.23743464 -2.12427943 -0.11653123  0.55388688 -2.77010917
  3.06858478  2.18960165  1.18551978 -1.06648329  0.6027153   1.14432465
  1.31029878 -1.8735466   0.729651    2.88578439 -0.54534355 -0.47522485
 -2.02654261  0.7289739   1.60512684  2.83077087 -1.2645673  -0.25136125
 -2.39279238 -2.27309774  3.13337135  2.54856938 -0.67550807 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 03:34:21 2024]  Iteration number: 0 with current cost as 0.4080225459531984 and parameters 
[-1.00216457  2.23743451 -2.12427964 -0.11653103  0.55388708 -2.77010922
  3.06858473  2.18960133  1.18551998 -1.06648346  0.6027151   1.14432433
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856946 -0.67550787 -2.69002214]. 
Working on 0.8 fold... 
[Fri Apr  5 03:43:54 2024]  Iteration number: 0 with current cost as 0.46742629522734175 and parameters 
[-0.30763464  2.23743464 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.7296508   2.88578451 -0.54534335 -0.47522454
 -2.0265424   0.72897385  1.60512664  2.83077107 -1.26456678 -0.25136089
 -2.39279187 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 03:52:00 2024]  Iteration number: 0 with current cost as 0.48975815155432034 and parameters 
[ 0.0757259   2.23743432 -2.12427964 -0.11653134  0.55388692 -2.77010929
  3.06858451  2.18960114  1.18551983 -1.0664834   0.6027151   1.14432445
  1.31029883 -1.8735468   0.7296508   2.88578419 -0.54534351 -0.47522485
 -2.02654256  0.7289737   1.60512632  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856927 -0.67550787 -2.69002202]. 
Training complete taking 3215.724506855011 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 04:04:21 2024]  Iteration number: 0 with current cost as 0.44731676974652385 and parameters 
[-1.26342713  2.23743432 -2.12427979 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551967 -1.0664834   0.6027151   1.14432414
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.7289737   1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279218 -2.2730979   3.13337139  2.54856927 -0.67550803 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 04:20:40 2024]  Iteration number: 0 with current cost as 0.23225510178558495 and parameters 
[ 1.31234093  2.23743464 -2.12427943 -0.11653123  0.55388688 -2.77010917
  3.06858478  2.18960165  1.18551978 -1.06648329  0.6027153   1.14432465
  1.31029878 -1.8735466   0.729651    2.88578439 -0.54534355 -0.47522485
 -2.02654261  0.7289739   1.60512684  2.83077087 -1.2645673  -0.25136125
 -2.39279238 -2.27309774  3.13337135  2.54856938 -0.67550807 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 04:27:46 2024]  Iteration number: 0 with current cost as 0.4080225459531984 and parameters 
[-1.00216457  2.23743451 -2.12427964 -0.11653103  0.55388708 -2.77010922
  3.06858473  2.18960133  1.18551998 -1.06648346  0.6027151   1.14432433
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856946 -0.67550787 -2.69002214]. 
Working on 0.8 fold... 
[Fri Apr  5 04:36:46 2024]  Iteration number: 0 with current cost as 0.46742629522734175 and parameters 
[-0.30763464  2.23743464 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.7296508   2.88578451 -0.54534335 -0.47522454
 -2.0265424   0.72897385  1.60512664  2.83077107 -1.26456678 -0.25136089
 -2.39279187 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 04:45:03 2024]  Iteration number: 0 with current cost as 0.48975815155432034 and parameters 
[ 0.0757259   2.23743432 -2.12427964 -0.11653134  0.55388692 -2.77010929
  3.06858451  2.18960114  1.18551983 -1.0664834   0.6027151   1.14432445
  1.31029883 -1.8735468   0.7296508   2.88578419 -0.54534351 -0.47522485
 -2.02654256  0.7289737   1.60512632  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856927 -0.67550787 -2.69002202]. 
Training complete taking 3187.9003512859344 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 04:57:33 2024]  Iteration number: 0 with current cost as 0.44731676974652385 and parameters 
[-1.26342713  2.23743432 -2.12427979 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551967 -1.0664834   0.6027151   1.14432414
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.7289737   1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279218 -2.2730979   3.13337139  2.54856927 -0.67550803 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 05:12:39 2024]  Iteration number: 0 with current cost as 0.23225510178558495 and parameters 
[ 1.31234093  2.23743464 -2.12427943 -0.11653123  0.55388688 -2.77010917
  3.06858478  2.18960165  1.18551978 -1.06648329  0.6027153   1.14432465
  1.31029878 -1.8735466   0.729651    2.88578439 -0.54534355 -0.47522485
 -2.02654261  0.7289739   1.60512684  2.83077087 -1.2645673  -0.25136125
 -2.39279238 -2.27309774  3.13337135  2.54856938 -0.67550807 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 05:19:13 2024]  Iteration number: 0 with current cost as 0.4080225459531984 and parameters 
[-1.00216457  2.23743451 -2.12427964 -0.11653103  0.55388708 -2.77010922
  3.06858473  2.18960133  1.18551998 -1.06648346  0.6027151   1.14432433
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856946 -0.67550787 -2.69002214]. 
Working on 0.8 fold... 
[Fri Apr  5 05:27:37 2024]  Iteration number: 0 with current cost as 0.46742629522734175 and parameters 
[-0.30763464  2.23743464 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.7296508   2.88578451 -0.54534335 -0.47522454
 -2.0265424   0.72897385  1.60512664  2.83077107 -1.26456678 -0.25136089
 -2.39279187 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 05:35:12 2024]  Iteration number: 0 with current cost as 0.48975815155432034 and parameters 
[ 0.0757259   2.23743432 -2.12427964 -0.11653134  0.55388692 -2.77010929
  3.06858451  2.18960114  1.18551983 -1.0664834   0.6027151   1.14432445
  1.31029883 -1.8735468   0.7296508   2.88578419 -0.54534351 -0.47522485
 -2.02654256  0.7289737   1.60512632  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856927 -0.67550787 -2.69002202]. 
Training complete taking 2961.852048397064 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 05:46:23 2024]  Iteration number: 0 with current cost as 0.44731676974652385 and parameters 
[-1.26342713  2.23743432 -2.12427979 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551967 -1.0664834   0.6027151   1.14432414
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.7289737   1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279218 -2.2730979   3.13337139  2.54856927 -0.67550803 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 06:01:10 2024]  Iteration number: 0 with current cost as 0.23225510178558495 and parameters 
[ 1.31234093  2.23743464 -2.12427943 -0.11653123  0.55388688 -2.77010917
  3.06858478  2.18960165  1.18551978 -1.06648329  0.6027153   1.14432465
  1.31029878 -1.8735466   0.729651    2.88578439 -0.54534355 -0.47522485
 -2.02654261  0.7289739   1.60512684  2.83077087 -1.2645673  -0.25136125
 -2.39279238 -2.27309774  3.13337135  2.54856938 -0.67550807 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 06:07:44 2024]  Iteration number: 0 with current cost as 0.4080225459531984 and parameters 
[-1.00216457  2.23743451 -2.12427964 -0.11653103  0.55388708 -2.77010922
  3.06858473  2.18960133  1.18551998 -1.06648346  0.6027151   1.14432433
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856946 -0.67550787 -2.69002214]. 
Working on 0.8 fold... 
[Fri Apr  5 06:16:08 2024]  Iteration number: 0 with current cost as 0.46742629522734175 and parameters 
[-0.30763464  2.23743464 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.7296508   2.88578451 -0.54534335 -0.47522454
 -2.0265424   0.72897385  1.60512664  2.83077107 -1.26456678 -0.25136089
 -2.39279187 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 06:23:36 2024]  Iteration number: 0 with current cost as 0.48975815155432034 and parameters 
[ 0.0757259   2.23743432 -2.12427964 -0.11653134  0.55388692 -2.77010929
  3.06858451  2.18960114  1.18551983 -1.0664834   0.6027151   1.14432445
  1.31029883 -1.8735468   0.7296508   2.88578419 -0.54534351 -0.47522485
 -2.02654256  0.7289737   1.60512632  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856927 -0.67550787 -2.69002202]. 
Training complete taking 2903.995922803879 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 06:35:07 2024]  Iteration number: 0 with current cost as 0.44731676974652385 and parameters 
[-1.26342713  2.23743432 -2.12427979 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551967 -1.0664834   0.6027151   1.14432414
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.7289737   1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279218 -2.2730979   3.13337139  2.54856927 -0.67550803 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 06:51:19 2024]  Iteration number: 0 with current cost as 0.23225510178558495 and parameters 
[ 1.31234093  2.23743464 -2.12427943 -0.11653123  0.55388688 -2.77010917
  3.06858478  2.18960165  1.18551978 -1.06648329  0.6027153   1.14432465
  1.31029878 -1.8735466   0.729651    2.88578439 -0.54534355 -0.47522485
 -2.02654261  0.7289739   1.60512684  2.83077087 -1.2645673  -0.25136125
 -2.39279238 -2.27309774  3.13337135  2.54856938 -0.67550807 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 06:58:34 2024]  Iteration number: 0 with current cost as 0.4080225459531984 and parameters 
[-1.00216457  2.23743451 -2.12427964 -0.11653103  0.55388708 -2.77010922
  3.06858473  2.18960133  1.18551998 -1.06648346  0.6027151   1.14432433
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856946 -0.67550787 -2.69002214]. 
Working on 0.8 fold... 
[Fri Apr  5 07:07:46 2024]  Iteration number: 0 with current cost as 0.46742629522734175 and parameters 
[-0.30763464  2.23743464 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.7296508   2.88578451 -0.54534335 -0.47522454
 -2.0265424   0.72897385  1.60512664  2.83077107 -1.26456678 -0.25136089
 -2.39279187 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 07:15:57 2024]  Iteration number: 0 with current cost as 0.48975815155432034 and parameters 
[ 0.0757259   2.23743432 -2.12427964 -0.11653134  0.55388692 -2.77010929
  3.06858451  2.18960114  1.18551983 -1.0664834   0.6027151   1.14432445
  1.31029883 -1.8735468   0.7296508   2.88578419 -0.54534351 -0.47522485
 -2.02654256  0.7289737   1.60512632  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856927 -0.67550787 -2.69002202]. 
Training complete taking 3162.78315615654 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 07:27:48 2024]  Iteration number: 0 with current cost as 0.44731676974652385 and parameters 
[-1.26342713  2.23743432 -2.12427979 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551967 -1.0664834   0.6027151   1.14432414
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.7289737   1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279218 -2.2730979   3.13337139  2.54856927 -0.67550803 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 07:43:56 2024]  Iteration number: 0 with current cost as 0.23225510178558495 and parameters 
[ 1.31234093  2.23743464 -2.12427943 -0.11653123  0.55388688 -2.77010917
  3.06858478  2.18960165  1.18551978 -1.06648329  0.6027153   1.14432465
  1.31029878 -1.8735466   0.729651    2.88578439 -0.54534355 -0.47522485
 -2.02654261  0.7289739   1.60512684  2.83077087 -1.2645673  -0.25136125
 -2.39279238 -2.27309774  3.13337135  2.54856938 -0.67550807 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 07:51:25 2024]  Iteration number: 0 with current cost as 0.4080225459531984 and parameters 
[-1.00216457  2.23743451 -2.12427964 -0.11653103  0.55388708 -2.77010922
  3.06858473  2.18960133  1.18551998 -1.06648346  0.6027151   1.14432433
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856946 -0.67550787 -2.69002214]. 
Working on 0.8 fold... 
[Fri Apr  5 08:00:53 2024]  Iteration number: 0 with current cost as 0.46742629522734175 and parameters 
[-0.30763464  2.23743464 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.7296508   2.88578451 -0.54534335 -0.47522454
 -2.0265424   0.72897385  1.60512664  2.83077107 -1.26456678 -0.25136089
 -2.39279187 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 08:09:21 2024]  Iteration number: 0 with current cost as 0.48975815155432034 and parameters 
[ 0.0757259   2.23743432 -2.12427964 -0.11653134  0.55388692 -2.77010929
  3.06858451  2.18960114  1.18551983 -1.0664834   0.6027151   1.14432445
  1.31029883 -1.8735468   0.7296508   2.88578419 -0.54534351 -0.47522485
 -2.02654256  0.7289737   1.60512632  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856927 -0.67550787 -2.69002202]. 
Training complete taking 3203.3058586120605 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 08:21:22 2024]  Iteration number: 0 with current cost as 0.44731676974652385 and parameters 
[-1.26342713  2.23743432 -2.12427979 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551967 -1.0664834   0.6027151   1.14432414
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.7289737   1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279218 -2.2730979   3.13337139  2.54856927 -0.67550803 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 08:37:12 2024]  Iteration number: 0 with current cost as 0.23225510178558495 and parameters 
[ 1.31234093  2.23743464 -2.12427943 -0.11653123  0.55388688 -2.77010917
  3.06858478  2.18960165  1.18551978 -1.06648329  0.6027153   1.14432465
  1.31029878 -1.8735466   0.729651    2.88578439 -0.54534355 -0.47522485
 -2.02654261  0.7289739   1.60512684  2.83077087 -1.2645673  -0.25136125
 -2.39279238 -2.27309774  3.13337135  2.54856938 -0.67550807 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 08:44:04 2024]  Iteration number: 0 with current cost as 0.4080225459531984 and parameters 
[-1.00216457  2.23743451 -2.12427964 -0.11653103  0.55388708 -2.77010922
  3.06858473  2.18960133  1.18551998 -1.06648346  0.6027151   1.14432433
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856946 -0.67550787 -2.69002214]. 
Working on 0.8 fold... 
[Fri Apr  5 08:53:14 2024]  Iteration number: 0 with current cost as 0.46742629522734175 and parameters 
[-0.30763464  2.23743464 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.7296508   2.88578451 -0.54534335 -0.47522454
 -2.0265424   0.72897385  1.60512664  2.83077107 -1.26456678 -0.25136089
 -2.39279187 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 09:01:26 2024]  Iteration number: 0 with current cost as 0.48975815155432034 and parameters 
[ 0.0757259   2.23743432 -2.12427964 -0.11653134  0.55388692 -2.77010929
  3.06858451  2.18960114  1.18551983 -1.0664834   0.6027151   1.14432445
  1.31029883 -1.8735468   0.7296508   2.88578419 -0.54534351 -0.47522485
 -2.02654256  0.7289737   1.60512632  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856927 -0.67550787 -2.69002202]. 
Training complete taking 3167.687712430954 seconds. 
Discarding model... 

Training complete taking 75692.04767394066 total seconds. 
Now scoring model... 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Scoring complete taking 2.837893486022949 seconds. 
Saved predicted values as M-A2-CNOT_Full-CRX_predicted_values.csv
Model scores: {'MSE_train': (147.9083184558395,), 'R2_train': 0.2834338374676022, 'MAE_train': 10.976254797333095, 'MSE_test': 167.36739707502417, 'R2_test': -0.008859524176060285, 'MAE_test': 10.950798118296555}. 
Saved model results as M-A2-CNOT_Full-CRX_results.json. 
