/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/circuits/Encoders.py:12: RuntimeWarning: invalid value encountered in arcsin
  if np.isnan(np.arcsin(features[feature_idx])) or np.isnan(np.arccos(features[feature_idx])):
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Sun Mar 24 17:35:03 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 17:35:31 2024]  Iteration number: 0 with current cost as 0.44731675517515807 and parameters 
[-1.26342681  2.23743432 -2.12427964 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.60271479  1.14432429
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 17:37:22 2024]  Iteration number: 0 with current cost as 0.23225510177981423 and parameters 
[ 1.31234095  2.23743443 -2.12427943 -0.11653103  0.55388688 -2.77010938
  3.06858478  2.18960165  1.18552019 -1.06648329  0.6027149   1.14432445
  1.31029878 -1.8735466   0.7296504   2.88578399 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 17:38:10 2024]  Iteration number: 0 with current cost as 0.40802254751369477 and parameters 
[-1.00216455  2.23743464 -2.12427951 -0.1165309   0.55388708 -2.7701091
  3.06858486  2.18960145  1.18552011 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354667  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 17:39:12 2024]  Iteration number: 0 with current cost as 0.4674262952273419 and parameters 
[-0.30763464  2.23743432 -2.12427964 -0.11653118  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.60271479  1.14432445
  1.31029883 -1.8735468   0.72965049  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 17:40:06 2024]  Iteration number: 0 with current cost as 0.4897581184010037 and parameters 
[ 0.07572621  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858483  2.18960145  1.18552014 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354649  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 358.5345809459686 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 17:41:29 2024]  Iteration number: 0 with current cost as 0.44731675517515807 and parameters 
[-1.26342681  2.23743432 -2.12427964 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.60271479  1.14432429
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 17:43:21 2024]  Iteration number: 0 with current cost as 0.23225510177981423 and parameters 
[ 1.31234095  2.23743443 -2.12427943 -0.11653103  0.55388688 -2.77010938
  3.06858478  2.18960165  1.18552019 -1.06648329  0.6027149   1.14432445
  1.31029878 -1.8735466   0.7296504   2.88578399 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 17:44:10 2024]  Iteration number: 0 with current cost as 0.40802254751369477 and parameters 
[-1.00216455  2.23743464 -2.12427951 -0.1165309   0.55388708 -2.7701091
  3.06858486  2.18960145  1.18552011 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354667  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 17:45:12 2024]  Iteration number: 0 with current cost as 0.4674262952273419 and parameters 
[-0.30763464  2.23743432 -2.12427964 -0.11653118  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.60271479  1.14432445
  1.31029883 -1.8735468   0.72965049  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 17:46:07 2024]  Iteration number: 0 with current cost as 0.4897581184010037 and parameters 
[ 0.07572621  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858483  2.18960145  1.18552014 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354649  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 362.2266061306 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 17:47:31 2024]  Iteration number: 0 with current cost as 0.44731675517515807 and parameters 
[-1.26342681  2.23743432 -2.12427964 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.60271479  1.14432429
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 17:49:21 2024]  Iteration number: 0 with current cost as 0.23225510177981423 and parameters 
[ 1.31234095  2.23743443 -2.12427943 -0.11653103  0.55388688 -2.77010938
  3.06858478  2.18960165  1.18552019 -1.06648329  0.6027149   1.14432445
  1.31029878 -1.8735466   0.7296504   2.88578399 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 17:50:09 2024]  Iteration number: 0 with current cost as 0.40802254751369477 and parameters 
[-1.00216455  2.23743464 -2.12427951 -0.1165309   0.55388708 -2.7701091
  3.06858486  2.18960145  1.18552011 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354667  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 17:51:12 2024]  Iteration number: 0 with current cost as 0.4674262952273419 and parameters 
[-0.30763464  2.23743432 -2.12427964 -0.11653118  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.60271479  1.14432445
  1.31029883 -1.8735468   0.72965049  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 17:52:07 2024]  Iteration number: 0 with current cost as 0.4897581184010037 and parameters 
[ 0.07572621  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858483  2.18960145  1.18552014 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354649  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 358.73306369781494 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 17:53:31 2024]  Iteration number: 0 with current cost as 0.44731675517515807 and parameters 
[-1.26342681  2.23743432 -2.12427964 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.60271479  1.14432429
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 17:55:23 2024]  Iteration number: 0 with current cost as 0.23225510177981423 and parameters 
[ 1.31234095  2.23743443 -2.12427943 -0.11653103  0.55388688 -2.77010938
  3.06858478  2.18960165  1.18552019 -1.06648329  0.6027149   1.14432445
  1.31029878 -1.8735466   0.7296504   2.88578399 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 17:56:12 2024]  Iteration number: 0 with current cost as 0.40802254751369477 and parameters 
[-1.00216455  2.23743464 -2.12427951 -0.1165309   0.55388708 -2.7701091
  3.06858486  2.18960145  1.18552011 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354667  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 17:57:14 2024]  Iteration number: 0 with current cost as 0.4674262952273419 and parameters 
[-0.30763464  2.23743432 -2.12427964 -0.11653118  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.60271479  1.14432445
  1.31029883 -1.8735468   0.72965049  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 17:58:08 2024]  Iteration number: 0 with current cost as 0.4897581184010037 and parameters 
[ 0.07572621  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858483  2.18960145  1.18552014 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354649  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 362.3905420303345 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 17:59:32 2024]  Iteration number: 0 with current cost as 0.44731675517515807 and parameters 
[-1.26342681  2.23743432 -2.12427964 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.60271479  1.14432429
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:01:25 2024]  Iteration number: 0 with current cost as 0.23225510177981423 and parameters 
[ 1.31234095  2.23743443 -2.12427943 -0.11653103  0.55388688 -2.77010938
  3.06858478  2.18960165  1.18552019 -1.06648329  0.6027149   1.14432445
  1.31029878 -1.8735466   0.7296504   2.88578399 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:02:14 2024]  Iteration number: 0 with current cost as 0.40802254751369477 and parameters 
[-1.00216455  2.23743464 -2.12427951 -0.1165309   0.55388708 -2.7701091
  3.06858486  2.18960145  1.18552011 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354667  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:03:17 2024]  Iteration number: 0 with current cost as 0.4674262952273419 and parameters 
[-0.30763464  2.23743432 -2.12427964 -0.11653118  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.60271479  1.14432445
  1.31029883 -1.8735468   0.72965049  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:04:11 2024]  Iteration number: 0 with current cost as 0.4897581184010037 and parameters 
[ 0.07572621  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858483  2.18960145  1.18552014 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354649  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 362.3639407157898 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 18:05:35 2024]  Iteration number: 0 with current cost as 0.44731675517515807 and parameters 
[-1.26342681  2.23743432 -2.12427964 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.60271479  1.14432429
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:07:27 2024]  Iteration number: 0 with current cost as 0.23225510177981423 and parameters 
[ 1.31234095  2.23743443 -2.12427943 -0.11653103  0.55388688 -2.77010938
  3.06858478  2.18960165  1.18552019 -1.06648329  0.6027149   1.14432445
  1.31029878 -1.8735466   0.7296504   2.88578399 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:08:16 2024]  Iteration number: 0 with current cost as 0.40802254751369477 and parameters 
[-1.00216455  2.23743464 -2.12427951 -0.1165309   0.55388708 -2.7701091
  3.06858486  2.18960145  1.18552011 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354667  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:09:18 2024]  Iteration number: 0 with current cost as 0.4674262952273419 and parameters 
[-0.30763464  2.23743432 -2.12427964 -0.11653118  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.60271479  1.14432445
  1.31029883 -1.8735468   0.72965049  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:10:14 2024]  Iteration number: 0 with current cost as 0.4897581184010037 and parameters 
[ 0.07572621  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858483  2.18960145  1.18552014 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354649  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 362.1480858325958 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 18:11:38 2024]  Iteration number: 0 with current cost as 0.44731675517515807 and parameters 
[-1.26342681  2.23743432 -2.12427964 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.60271479  1.14432429
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:13:30 2024]  Iteration number: 0 with current cost as 0.23225510177981423 and parameters 
[ 1.31234095  2.23743443 -2.12427943 -0.11653103  0.55388688 -2.77010938
  3.06858478  2.18960165  1.18552019 -1.06648329  0.6027149   1.14432445
  1.31029878 -1.8735466   0.7296504   2.88578399 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:14:20 2024]  Iteration number: 0 with current cost as 0.40802254751369477 and parameters 
[-1.00216455  2.23743464 -2.12427951 -0.1165309   0.55388708 -2.7701091
  3.06858486  2.18960145  1.18552011 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354667  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:15:24 2024]  Iteration number: 0 with current cost as 0.4674262952273419 and parameters 
[-0.30763464  2.23743432 -2.12427964 -0.11653118  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.60271479  1.14432445
  1.31029883 -1.8735468   0.72965049  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:16:22 2024]  Iteration number: 0 with current cost as 0.4897581184010037 and parameters 
[ 0.07572621  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858483  2.18960145  1.18552014 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354649  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 367.6932680606842 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 18:17:46 2024]  Iteration number: 0 with current cost as 0.44731675517515807 and parameters 
[-1.26342681  2.23743432 -2.12427964 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.60271479  1.14432429
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:19:37 2024]  Iteration number: 0 with current cost as 0.23225510177981423 and parameters 
[ 1.31234095  2.23743443 -2.12427943 -0.11653103  0.55388688 -2.77010938
  3.06858478  2.18960165  1.18552019 -1.06648329  0.6027149   1.14432445
  1.31029878 -1.8735466   0.7296504   2.88578399 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:20:27 2024]  Iteration number: 0 with current cost as 0.40802254751369477 and parameters 
[-1.00216455  2.23743464 -2.12427951 -0.1165309   0.55388708 -2.7701091
  3.06858486  2.18960145  1.18552011 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354667  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:21:30 2024]  Iteration number: 0 with current cost as 0.4674262952273419 and parameters 
[-0.30763464  2.23743432 -2.12427964 -0.11653118  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.60271479  1.14432445
  1.31029883 -1.8735468   0.72965049  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:22:27 2024]  Iteration number: 0 with current cost as 0.4897581184010037 and parameters 
[ 0.07572621  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858483  2.18960145  1.18552014 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354649  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 365.8974850177765 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 18:23:51 2024]  Iteration number: 0 with current cost as 0.44731675517515807 and parameters 
[-1.26342681  2.23743432 -2.12427964 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.60271479  1.14432429
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:25:41 2024]  Iteration number: 0 with current cost as 0.23225510177981423 and parameters 
[ 1.31234095  2.23743443 -2.12427943 -0.11653103  0.55388688 -2.77010938
  3.06858478  2.18960165  1.18552019 -1.06648329  0.6027149   1.14432445
  1.31029878 -1.8735466   0.7296504   2.88578399 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:26:31 2024]  Iteration number: 0 with current cost as 0.40802254751369477 and parameters 
[-1.00216455  2.23743464 -2.12427951 -0.1165309   0.55388708 -2.7701091
  3.06858486  2.18960145  1.18552011 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354667  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:27:33 2024]  Iteration number: 0 with current cost as 0.4674262952273419 and parameters 
[-0.30763464  2.23743432 -2.12427964 -0.11653118  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.60271479  1.14432445
  1.31029883 -1.8735468   0.72965049  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:28:29 2024]  Iteration number: 0 with current cost as 0.4897581184010037 and parameters 
[ 0.07572621  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858483  2.18960145  1.18552014 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354649  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 361.6444981098175 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 18:29:52 2024]  Iteration number: 0 with current cost as 0.44731675517515807 and parameters 
[-1.26342681  2.23743432 -2.12427964 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.60271479  1.14432429
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:31:46 2024]  Iteration number: 0 with current cost as 0.23225510177981423 and parameters 
[ 1.31234095  2.23743443 -2.12427943 -0.11653103  0.55388688 -2.77010938
  3.06858478  2.18960165  1.18552019 -1.06648329  0.6027149   1.14432445
  1.31029878 -1.8735466   0.7296504   2.88578399 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:32:35 2024]  Iteration number: 0 with current cost as 0.40802254751369477 and parameters 
[-1.00216455  2.23743464 -2.12427951 -0.1165309   0.55388708 -2.7701091
  3.06858486  2.18960145  1.18552011 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354667  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:33:37 2024]  Iteration number: 0 with current cost as 0.4674262952273419 and parameters 
[-0.30763464  2.23743432 -2.12427964 -0.11653118  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.60271479  1.14432445
  1.31029883 -1.8735468   0.72965049  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:34:34 2024]  Iteration number: 0 with current cost as 0.4897581184010037 and parameters 
[ 0.07572621  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858483  2.18960145  1.18552014 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354649  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 365.44602489471436 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 18:35:57 2024]  Iteration number: 0 with current cost as 0.44731675517515807 and parameters 
[-1.26342681  2.23743432 -2.12427964 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.60271479  1.14432429
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:37:49 2024]  Iteration number: 0 with current cost as 0.23225510177981423 and parameters 
[ 1.31234095  2.23743443 -2.12427943 -0.11653103  0.55388688 -2.77010938
  3.06858478  2.18960165  1.18552019 -1.06648329  0.6027149   1.14432445
  1.31029878 -1.8735466   0.7296504   2.88578399 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:38:38 2024]  Iteration number: 0 with current cost as 0.40802254751369477 and parameters 
[-1.00216455  2.23743464 -2.12427951 -0.1165309   0.55388708 -2.7701091
  3.06858486  2.18960145  1.18552011 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354667  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:39:41 2024]  Iteration number: 0 with current cost as 0.4674262952273419 and parameters 
[-0.30763464  2.23743432 -2.12427964 -0.11653118  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.60271479  1.14432445
  1.31029883 -1.8735468   0.72965049  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:40:39 2024]  Iteration number: 0 with current cost as 0.4897581184010037 and parameters 
[ 0.07572621  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858483  2.18960145  1.18552014 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354649  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 366.7965774536133 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 18:42:05 2024]  Iteration number: 0 with current cost as 0.44731675517515807 and parameters 
[-1.26342681  2.23743432 -2.12427964 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.60271479  1.14432429
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:43:57 2024]  Iteration number: 0 with current cost as 0.23225510177981423 and parameters 
[ 1.31234095  2.23743443 -2.12427943 -0.11653103  0.55388688 -2.77010938
  3.06858478  2.18960165  1.18552019 -1.06648329  0.6027149   1.14432445
  1.31029878 -1.8735466   0.7296504   2.88578399 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:44:47 2024]  Iteration number: 0 with current cost as 0.40802254751369477 and parameters 
[-1.00216455  2.23743464 -2.12427951 -0.1165309   0.55388708 -2.7701091
  3.06858486  2.18960145  1.18552011 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354667  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:45:50 2024]  Iteration number: 0 with current cost as 0.4674262952273419 and parameters 
[-0.30763464  2.23743432 -2.12427964 -0.11653118  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.60271479  1.14432445
  1.31029883 -1.8735468   0.72965049  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:46:45 2024]  Iteration number: 0 with current cost as 0.4897581184010037 and parameters 
[ 0.07572621  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858483  2.18960145  1.18552014 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354649  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 363.1996204853058 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 18:48:08 2024]  Iteration number: 0 with current cost as 0.44731675517515807 and parameters 
[-1.26342681  2.23743432 -2.12427964 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.60271479  1.14432429
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:49:59 2024]  Iteration number: 0 with current cost as 0.23225510177981423 and parameters 
[ 1.31234095  2.23743443 -2.12427943 -0.11653103  0.55388688 -2.77010938
  3.06858478  2.18960165  1.18552019 -1.06648329  0.6027149   1.14432445
  1.31029878 -1.8735466   0.7296504   2.88578399 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:50:51 2024]  Iteration number: 0 with current cost as 0.40802254751369477 and parameters 
[-1.00216455  2.23743464 -2.12427951 -0.1165309   0.55388708 -2.7701091
  3.06858486  2.18960145  1.18552011 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354667  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:51:56 2024]  Iteration number: 0 with current cost as 0.4674262952273419 and parameters 
[-0.30763464  2.23743432 -2.12427964 -0.11653118  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.60271479  1.14432445
  1.31029883 -1.8735468   0.72965049  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:52:52 2024]  Iteration number: 0 with current cost as 0.4897581184010037 and parameters 
[ 0.07572621  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858483  2.18960145  1.18552014 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354649  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 366.3640820980072 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 18:54:13 2024]  Iteration number: 0 with current cost as 0.44731675517515807 and parameters 
[-1.26342681  2.23743432 -2.12427964 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.60271479  1.14432429
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:56:02 2024]  Iteration number: 0 with current cost as 0.23225510177981423 and parameters 
[ 1.31234095  2.23743443 -2.12427943 -0.11653103  0.55388688 -2.77010938
  3.06858478  2.18960165  1.18552019 -1.06648329  0.6027149   1.14432445
  1.31029878 -1.8735466   0.7296504   2.88578399 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:56:50 2024]  Iteration number: 0 with current cost as 0.40802254751369477 and parameters 
[-1.00216455  2.23743464 -2.12427951 -0.1165309   0.55388708 -2.7701091
  3.06858486  2.18960145  1.18552011 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354667  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:57:51 2024]  Iteration number: 0 with current cost as 0.4674262952273419 and parameters 
[-0.30763464  2.23743432 -2.12427964 -0.11653118  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.60271479  1.14432445
  1.31029883 -1.8735468   0.72965049  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:58:46 2024]  Iteration number: 0 with current cost as 0.4897581184010037 and parameters 
[ 0.07572621  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858483  2.18960145  1.18552014 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354649  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 354.42508578300476 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 19:00:08 2024]  Iteration number: 0 with current cost as 0.44731675517515807 and parameters 
[-1.26342681  2.23743432 -2.12427964 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.60271479  1.14432429
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:02:01 2024]  Iteration number: 0 with current cost as 0.23225510177981423 and parameters 
[ 1.31234095  2.23743443 -2.12427943 -0.11653103  0.55388688 -2.77010938
  3.06858478  2.18960165  1.18552019 -1.06648329  0.6027149   1.14432445
  1.31029878 -1.8735466   0.7296504   2.88578399 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:02:50 2024]  Iteration number: 0 with current cost as 0.40802254751369477 and parameters 
[-1.00216455  2.23743464 -2.12427951 -0.1165309   0.55388708 -2.7701091
  3.06858486  2.18960145  1.18552011 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354667  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:03:51 2024]  Iteration number: 0 with current cost as 0.4674262952273419 and parameters 
[-0.30763464  2.23743432 -2.12427964 -0.11653118  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.60271479  1.14432445
  1.31029883 -1.8735468   0.72965049  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:04:46 2024]  Iteration number: 0 with current cost as 0.4897581184010037 and parameters 
[ 0.07572621  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858483  2.18960145  1.18552014 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354649  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 359.05241441726685 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 19:06:08 2024]  Iteration number: 0 with current cost as 0.44731675517515807 and parameters 
[-1.26342681  2.23743432 -2.12427964 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.60271479  1.14432429
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:07:58 2024]  Iteration number: 0 with current cost as 0.23225510177981423 and parameters 
[ 1.31234095  2.23743443 -2.12427943 -0.11653103  0.55388688 -2.77010938
  3.06858478  2.18960165  1.18552019 -1.06648329  0.6027149   1.14432445
  1.31029878 -1.8735466   0.7296504   2.88578399 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:08:46 2024]  Iteration number: 0 with current cost as 0.40802254751369477 and parameters 
[-1.00216455  2.23743464 -2.12427951 -0.1165309   0.55388708 -2.7701091
  3.06858486  2.18960145  1.18552011 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354667  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:09:48 2024]  Iteration number: 0 with current cost as 0.4674262952273419 and parameters 
[-0.30763464  2.23743432 -2.12427964 -0.11653118  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.60271479  1.14432445
  1.31029883 -1.8735468   0.72965049  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:10:42 2024]  Iteration number: 0 with current cost as 0.4897581184010037 and parameters 
[ 0.07572621  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858483  2.18960145  1.18552014 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354649  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 358.0770444869995 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 19:12:05 2024]  Iteration number: 0 with current cost as 0.44731675517515807 and parameters 
[-1.26342681  2.23743432 -2.12427964 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.60271479  1.14432429
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:13:56 2024]  Iteration number: 0 with current cost as 0.23225510177981423 and parameters 
[ 1.31234095  2.23743443 -2.12427943 -0.11653103  0.55388688 -2.77010938
  3.06858478  2.18960165  1.18552019 -1.06648329  0.6027149   1.14432445
  1.31029878 -1.8735466   0.7296504   2.88578399 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:14:43 2024]  Iteration number: 0 with current cost as 0.40802254751369477 and parameters 
[-1.00216455  2.23743464 -2.12427951 -0.1165309   0.55388708 -2.7701091
  3.06858486  2.18960145  1.18552011 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354667  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:15:46 2024]  Iteration number: 0 with current cost as 0.4674262952273419 and parameters 
[-0.30763464  2.23743432 -2.12427964 -0.11653118  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.60271479  1.14432445
  1.31029883 -1.8735468   0.72965049  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:16:41 2024]  Iteration number: 0 with current cost as 0.4897581184010037 and parameters 
[ 0.07572621  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858483  2.18960145  1.18552014 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354649  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 358.7467942237854 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 19:18:04 2024]  Iteration number: 0 with current cost as 0.44731675517515807 and parameters 
[-1.26342681  2.23743432 -2.12427964 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.60271479  1.14432429
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:19:55 2024]  Iteration number: 0 with current cost as 0.23225510177981423 and parameters 
[ 1.31234095  2.23743443 -2.12427943 -0.11653103  0.55388688 -2.77010938
  3.06858478  2.18960165  1.18552019 -1.06648329  0.6027149   1.14432445
  1.31029878 -1.8735466   0.7296504   2.88578399 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:20:42 2024]  Iteration number: 0 with current cost as 0.40802254751369477 and parameters 
[-1.00216455  2.23743464 -2.12427951 -0.1165309   0.55388708 -2.7701091
  3.06858486  2.18960145  1.18552011 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354667  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:21:47 2024]  Iteration number: 0 with current cost as 0.4674262952273419 and parameters 
[-0.30763464  2.23743432 -2.12427964 -0.11653118  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.60271479  1.14432445
  1.31029883 -1.8735468   0.72965049  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:22:42 2024]  Iteration number: 0 with current cost as 0.4897581184010037 and parameters 
[ 0.07572621  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858483  2.18960145  1.18552014 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354649  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 359.43423080444336 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 19:24:03 2024]  Iteration number: 0 with current cost as 0.44731675517515807 and parameters 
[-1.26342681  2.23743432 -2.12427964 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.60271479  1.14432429
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:25:52 2024]  Iteration number: 0 with current cost as 0.23225510177981423 and parameters 
[ 1.31234095  2.23743443 -2.12427943 -0.11653103  0.55388688 -2.77010938
  3.06858478  2.18960165  1.18552019 -1.06648329  0.6027149   1.14432445
  1.31029878 -1.8735466   0.7296504   2.88578399 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:26:39 2024]  Iteration number: 0 with current cost as 0.40802254751369477 and parameters 
[-1.00216455  2.23743464 -2.12427951 -0.1165309   0.55388708 -2.7701091
  3.06858486  2.18960145  1.18552011 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354667  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:27:41 2024]  Iteration number: 0 with current cost as 0.4674262952273419 and parameters 
[-0.30763464  2.23743432 -2.12427964 -0.11653118  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.60271479  1.14432445
  1.31029883 -1.8735468   0.72965049  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:28:36 2024]  Iteration number: 0 with current cost as 0.4897581184010037 and parameters 
[ 0.07572621  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858483  2.18960145  1.18552014 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354649  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 355.8958623409271 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 19:29:59 2024]  Iteration number: 0 with current cost as 0.44731675517515807 and parameters 
[-1.26342681  2.23743432 -2.12427964 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.60271479  1.14432429
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:31:50 2024]  Iteration number: 0 with current cost as 0.23225510177981423 and parameters 
[ 1.31234095  2.23743443 -2.12427943 -0.11653103  0.55388688 -2.77010938
  3.06858478  2.18960165  1.18552019 -1.06648329  0.6027149   1.14432445
  1.31029878 -1.8735466   0.7296504   2.88578399 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:32:38 2024]  Iteration number: 0 with current cost as 0.40802254751369477 and parameters 
[-1.00216455  2.23743464 -2.12427951 -0.1165309   0.55388708 -2.7701091
  3.06858486  2.18960145  1.18552011 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354667  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:33:41 2024]  Iteration number: 0 with current cost as 0.4674262952273419 and parameters 
[-0.30763464  2.23743432 -2.12427964 -0.11653118  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.60271479  1.14432445
  1.31029883 -1.8735468   0.72965049  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:34:37 2024]  Iteration number: 0 with current cost as 0.4897581184010037 and parameters 
[ 0.07572621  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858483  2.18960145  1.18552014 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354649  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 361.35468435287476 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 19:36:02 2024]  Iteration number: 0 with current cost as 0.44731675517515807 and parameters 
[-1.26342681  2.23743432 -2.12427964 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.60271479  1.14432429
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:37:53 2024]  Iteration number: 0 with current cost as 0.23225510177981423 and parameters 
[ 1.31234095  2.23743443 -2.12427943 -0.11653103  0.55388688 -2.77010938
  3.06858478  2.18960165  1.18552019 -1.06648329  0.6027149   1.14432445
  1.31029878 -1.8735466   0.7296504   2.88578399 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:38:41 2024]  Iteration number: 0 with current cost as 0.40802254751369477 and parameters 
[-1.00216455  2.23743464 -2.12427951 -0.1165309   0.55388708 -2.7701091
  3.06858486  2.18960145  1.18552011 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354667  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:39:42 2024]  Iteration number: 0 with current cost as 0.4674262952273419 and parameters 
[-0.30763464  2.23743432 -2.12427964 -0.11653118  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.60271479  1.14432445
  1.31029883 -1.8735468   0.72965049  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:40:37 2024]  Iteration number: 0 with current cost as 0.4897581184010037 and parameters 
[ 0.07572621  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858483  2.18960145  1.18552014 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354649  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 359.06744289398193 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 19:42:00 2024]  Iteration number: 0 with current cost as 0.44731675517515807 and parameters 
[-1.26342681  2.23743432 -2.12427964 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.60271479  1.14432429
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:43:50 2024]  Iteration number: 0 with current cost as 0.23225510177981423 and parameters 
[ 1.31234095  2.23743443 -2.12427943 -0.11653103  0.55388688 -2.77010938
  3.06858478  2.18960165  1.18552019 -1.06648329  0.6027149   1.14432445
  1.31029878 -1.8735466   0.7296504   2.88578399 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:44:37 2024]  Iteration number: 0 with current cost as 0.40802254751369477 and parameters 
[-1.00216455  2.23743464 -2.12427951 -0.1165309   0.55388708 -2.7701091
  3.06858486  2.18960145  1.18552011 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354667  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:45:38 2024]  Iteration number: 0 with current cost as 0.4674262952273419 and parameters 
[-0.30763464  2.23743432 -2.12427964 -0.11653118  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.60271479  1.14432445
  1.31029883 -1.8735468   0.72965049  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:46:33 2024]  Iteration number: 0 with current cost as 0.4897581184010037 and parameters 
[ 0.07572621  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858483  2.18960145  1.18552014 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354649  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 356.4581971168518 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 19:47:56 2024]  Iteration number: 0 with current cost as 0.44731675517515807 and parameters 
[-1.26342681  2.23743432 -2.12427964 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.60271479  1.14432429
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:49:45 2024]  Iteration number: 0 with current cost as 0.23225510177981423 and parameters 
[ 1.31234095  2.23743443 -2.12427943 -0.11653103  0.55388688 -2.77010938
  3.06858478  2.18960165  1.18552019 -1.06648329  0.6027149   1.14432445
  1.31029878 -1.8735466   0.7296504   2.88578399 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:50:34 2024]  Iteration number: 0 with current cost as 0.40802254751369477 and parameters 
[-1.00216455  2.23743464 -2.12427951 -0.1165309   0.55388708 -2.7701091
  3.06858486  2.18960145  1.18552011 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354667  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:51:37 2024]  Iteration number: 0 with current cost as 0.4674262952273419 and parameters 
[-0.30763464  2.23743432 -2.12427964 -0.11653118  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.60271479  1.14432445
  1.31029883 -1.8735468   0.72965049  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:52:33 2024]  Iteration number: 0 with current cost as 0.4897581184010037 and parameters 
[ 0.07572621  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858483  2.18960145  1.18552014 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354649  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 358.41599202156067 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 19:53:54 2024]  Iteration number: 0 with current cost as 0.44731675517515807 and parameters 
[-1.26342681  2.23743432 -2.12427964 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.60271479  1.14432429
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:55:45 2024]  Iteration number: 0 with current cost as 0.23225510177981423 and parameters 
[ 1.31234095  2.23743443 -2.12427943 -0.11653103  0.55388688 -2.77010938
  3.06858478  2.18960165  1.18552019 -1.06648329  0.6027149   1.14432445
  1.31029878 -1.8735466   0.7296504   2.88578399 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:56:34 2024]  Iteration number: 0 with current cost as 0.40802254751369477 and parameters 
[-1.00216455  2.23743464 -2.12427951 -0.1165309   0.55388708 -2.7701091
  3.06858486  2.18960145  1.18552011 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354667  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:57:36 2024]  Iteration number: 0 with current cost as 0.4674262952273419 and parameters 
[-0.30763464  2.23743432 -2.12427964 -0.11653118  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.60271479  1.14432445
  1.31029883 -1.8735468   0.72965049  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:58:31 2024]  Iteration number: 0 with current cost as 0.4897581184010037 and parameters 
[ 0.07572621  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858483  2.18960145  1.18552014 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354649  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 359.3758029937744 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 19:59:54 2024]  Iteration number: 0 with current cost as 0.44731675517515807 and parameters 
[-1.26342681  2.23743432 -2.12427964 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.60271479  1.14432429
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 20:01:44 2024]  Iteration number: 0 with current cost as 0.23225510177981423 and parameters 
[ 1.31234095  2.23743443 -2.12427943 -0.11653103  0.55388688 -2.77010938
  3.06858478  2.18960165  1.18552019 -1.06648329  0.6027149   1.14432445
  1.31029878 -1.8735466   0.7296504   2.88578399 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 20:02:33 2024]  Iteration number: 0 with current cost as 0.40802254751369477 and parameters 
[-1.00216455  2.23743464 -2.12427951 -0.1165309   0.55388708 -2.7701091
  3.06858486  2.18960145  1.18552011 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354667  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:03:36 2024]  Iteration number: 0 with current cost as 0.4674262952273419 and parameters 
[-0.30763464  2.23743432 -2.12427964 -0.11653118  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.60271479  1.14432445
  1.31029883 -1.8735468   0.72965049  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 20:04:31 2024]  Iteration number: 0 with current cost as 0.4897581184010037 and parameters 
[ 0.07572621  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858483  2.18960145  1.18552014 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354649  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 359.7261519432068 seconds. 
Discarding model... 

Training complete taking 9023.468923330307 total seconds. 
Now scoring model... 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Scoring complete taking 0.9326620101928711 seconds. 
Saved predicted values as M-A2-CNOT_Modified-Pauli-CRZ_predicted_values.csv
Model scores: {'MSE_train': (147.9083184644662,), 'R2_train': 0.28343383742580874, 'MAE_train': 10.976254785918496, 'MSE_test': 167.3673969668236, 'R2_test': -0.008859523523847557, 'MAE_test': 10.95079813178329}. 
Saved model results as M-A2-CNOT_Modified-Pauli-CRZ_results.json. 
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/circuits/Encoders.py:12: RuntimeWarning: invalid value encountered in arcsin
  if np.isnan(np.arcsin(features[feature_idx])) or np.isnan(np.arccos(features[feature_idx])):
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Thu Apr  4 12:11:16 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 12:11:49 2024]  Iteration number: 0 with current cost as 0.44731675517515807 and parameters 
[-1.26342681  2.23743432 -2.12427964 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.60271479  1.14432429
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 12:13:58 2024]  Iteration number: 0 with current cost as 0.23225510177981423 and parameters 
[ 1.31234095  2.23743443 -2.12427943 -0.11653103  0.55388688 -2.77010938
  3.06858478  2.18960165  1.18552019 -1.06648329  0.6027149   1.14432445
  1.31029878 -1.8735466   0.7296504   2.88578399 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 12:14:55 2024]  Iteration number: 0 with current cost as 0.40802254751369477 and parameters 
[-1.00216455  2.23743464 -2.12427951 -0.1165309   0.55388708 -2.7701091
  3.06858486  2.18960145  1.18552011 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354667  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:16:09 2024]  Iteration number: 0 with current cost as 0.4674262952273419 and parameters 
[-0.30763464  2.23743432 -2.12427964 -0.11653118  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.60271479  1.14432445
  1.31029883 -1.8735468   0.72965049  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:17:13 2024]  Iteration number: 0 with current cost as 0.4897581184010037 and parameters 
[ 0.07572621  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858483  2.18960145  1.18552014 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354649  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 423.27114748954773 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 12:18:52 2024]  Iteration number: 0 with current cost as 0.44731675517515807 and parameters 
[-1.26342681  2.23743432 -2.12427964 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.60271479  1.14432429
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 12:21:00 2024]  Iteration number: 0 with current cost as 0.23225510177981423 and parameters 
[ 1.31234095  2.23743443 -2.12427943 -0.11653103  0.55388688 -2.77010938
  3.06858478  2.18960165  1.18552019 -1.06648329  0.6027149   1.14432445
  1.31029878 -1.8735466   0.7296504   2.88578399 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 12:21:57 2024]  Iteration number: 0 with current cost as 0.40802254751369477 and parameters 
[-1.00216455  2.23743464 -2.12427951 -0.1165309   0.55388708 -2.7701091
  3.06858486  2.18960145  1.18552011 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354667  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:23:09 2024]  Iteration number: 0 with current cost as 0.4674262952273419 and parameters 
[-0.30763464  2.23743432 -2.12427964 -0.11653118  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.60271479  1.14432445
  1.31029883 -1.8735468   0.72965049  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:24:14 2024]  Iteration number: 0 with current cost as 0.4897581184010037 and parameters 
[ 0.07572621  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858483  2.18960145  1.18552014 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354649  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 421.43884778022766 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 12:25:53 2024]  Iteration number: 0 with current cost as 0.44731675517515807 and parameters 
[-1.26342681  2.23743432 -2.12427964 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.60271479  1.14432429
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 12:28:03 2024]  Iteration number: 0 with current cost as 0.23225510177981423 and parameters 
[ 1.31234095  2.23743443 -2.12427943 -0.11653103  0.55388688 -2.77010938
  3.06858478  2.18960165  1.18552019 -1.06648329  0.6027149   1.14432445
  1.31029878 -1.8735466   0.7296504   2.88578399 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 12:29:01 2024]  Iteration number: 0 with current cost as 0.40802254751369477 and parameters 
[-1.00216455  2.23743464 -2.12427951 -0.1165309   0.55388708 -2.7701091
  3.06858486  2.18960145  1.18552011 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354667  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:30:13 2024]  Iteration number: 0 with current cost as 0.4674262952273419 and parameters 
[-0.30763464  2.23743432 -2.12427964 -0.11653118  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.60271479  1.14432445
  1.31029883 -1.8735468   0.72965049  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:31:20 2024]  Iteration number: 0 with current cost as 0.4897581184010037 and parameters 
[ 0.07572621  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858483  2.18960145  1.18552014 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354649  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 424.77776288986206 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 12:32:58 2024]  Iteration number: 0 with current cost as 0.44731675517515807 and parameters 
[-1.26342681  2.23743432 -2.12427964 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.60271479  1.14432429
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 12:35:13 2024]  Iteration number: 0 with current cost as 0.23225510177981423 and parameters 
[ 1.31234095  2.23743443 -2.12427943 -0.11653103  0.55388688 -2.77010938
  3.06858478  2.18960165  1.18552019 -1.06648329  0.6027149   1.14432445
  1.31029878 -1.8735466   0.7296504   2.88578399 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 12:36:10 2024]  Iteration number: 0 with current cost as 0.40802254751369477 and parameters 
[-1.00216455  2.23743464 -2.12427951 -0.1165309   0.55388708 -2.7701091
  3.06858486  2.18960145  1.18552011 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354667  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:37:24 2024]  Iteration number: 0 with current cost as 0.4674262952273419 and parameters 
[-0.30763464  2.23743432 -2.12427964 -0.11653118  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.60271479  1.14432445
  1.31029883 -1.8735468   0.72965049  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:38:28 2024]  Iteration number: 0 with current cost as 0.4897581184010037 and parameters 
[ 0.07572621  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858483  2.18960145  1.18552014 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354649  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 427.3533892631531 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 12:40:05 2024]  Iteration number: 0 with current cost as 0.44731675517515807 and parameters 
[-1.26342681  2.23743432 -2.12427964 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.60271479  1.14432429
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 12:42:16 2024]  Iteration number: 0 with current cost as 0.23225510177981423 and parameters 
[ 1.31234095  2.23743443 -2.12427943 -0.11653103  0.55388688 -2.77010938
  3.06858478  2.18960165  1.18552019 -1.06648329  0.6027149   1.14432445
  1.31029878 -1.8735466   0.7296504   2.88578399 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 12:43:19 2024]  Iteration number: 0 with current cost as 0.40802254751369477 and parameters 
[-1.00216455  2.23743464 -2.12427951 -0.1165309   0.55388708 -2.7701091
  3.06858486  2.18960145  1.18552011 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354667  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:44:34 2024]  Iteration number: 0 with current cost as 0.4674262952273419 and parameters 
[-0.30763464  2.23743432 -2.12427964 -0.11653118  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.60271479  1.14432445
  1.31029883 -1.8735468   0.72965049  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:45:39 2024]  Iteration number: 0 with current cost as 0.4897581184010037 and parameters 
[ 0.07572621  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858483  2.18960145  1.18552014 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354649  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 432.6350018978119 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 12:47:21 2024]  Iteration number: 0 with current cost as 0.44731675517515807 and parameters 
[-1.26342681  2.23743432 -2.12427964 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.60271479  1.14432429
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 12:49:32 2024]  Iteration number: 0 with current cost as 0.23225510177981423 and parameters 
[ 1.31234095  2.23743443 -2.12427943 -0.11653103  0.55388688 -2.77010938
  3.06858478  2.18960165  1.18552019 -1.06648329  0.6027149   1.14432445
  1.31029878 -1.8735466   0.7296504   2.88578399 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 12:50:28 2024]  Iteration number: 0 with current cost as 0.40802254751369477 and parameters 
[-1.00216455  2.23743464 -2.12427951 -0.1165309   0.55388708 -2.7701091
  3.06858486  2.18960145  1.18552011 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354667  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:51:41 2024]  Iteration number: 0 with current cost as 0.4674262952273419 and parameters 
[-0.30763464  2.23743432 -2.12427964 -0.11653118  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.60271479  1.14432445
  1.31029883 -1.8735468   0.72965049  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:52:45 2024]  Iteration number: 0 with current cost as 0.4897581184010037 and parameters 
[ 0.07572621  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858483  2.18960145  1.18552014 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354649  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 426.8915114402771 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 12:54:25 2024]  Iteration number: 0 with current cost as 0.44731675517515807 and parameters 
[-1.26342681  2.23743432 -2.12427964 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.60271479  1.14432429
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 12:56:34 2024]  Iteration number: 0 with current cost as 0.23225510177981423 and parameters 
[ 1.31234095  2.23743443 -2.12427943 -0.11653103  0.55388688 -2.77010938
  3.06858478  2.18960165  1.18552019 -1.06648329  0.6027149   1.14432445
  1.31029878 -1.8735466   0.7296504   2.88578399 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 12:57:31 2024]  Iteration number: 0 with current cost as 0.40802254751369477 and parameters 
[-1.00216455  2.23743464 -2.12427951 -0.1165309   0.55388708 -2.7701091
  3.06858486  2.18960145  1.18552011 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354667  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:58:43 2024]  Iteration number: 0 with current cost as 0.4674262952273419 and parameters 
[-0.30763464  2.23743432 -2.12427964 -0.11653118  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.60271479  1.14432445
  1.31029883 -1.8735468   0.72965049  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:59:49 2024]  Iteration number: 0 with current cost as 0.4897581184010037 and parameters 
[ 0.07572621  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858483  2.18960145  1.18552014 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354649  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 420.3594796657562 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 13:01:25 2024]  Iteration number: 0 with current cost as 0.44731675517515807 and parameters 
[-1.26342681  2.23743432 -2.12427964 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.60271479  1.14432429
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 13:03:35 2024]  Iteration number: 0 with current cost as 0.23225510177981423 and parameters 
[ 1.31234095  2.23743443 -2.12427943 -0.11653103  0.55388688 -2.77010938
  3.06858478  2.18960165  1.18552019 -1.06648329  0.6027149   1.14432445
  1.31029878 -1.8735466   0.7296504   2.88578399 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 13:04:32 2024]  Iteration number: 0 with current cost as 0.40802254751369477 and parameters 
[-1.00216455  2.23743464 -2.12427951 -0.1165309   0.55388708 -2.7701091
  3.06858486  2.18960145  1.18552011 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354667  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:05:44 2024]  Iteration number: 0 with current cost as 0.4674262952273419 and parameters 
[-0.30763464  2.23743432 -2.12427964 -0.11653118  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.60271479  1.14432445
  1.31029883 -1.8735468   0.72965049  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:06:52 2024]  Iteration number: 0 with current cost as 0.4897581184010037 and parameters 
[ 0.07572621  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858483  2.18960145  1.18552014 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354649  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 438.7498400211334 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 13:08:44 2024]  Iteration number: 0 with current cost as 0.44731675517515807 and parameters 
[-1.26342681  2.23743432 -2.12427964 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.60271479  1.14432429
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 13:10:55 2024]  Iteration number: 0 with current cost as 0.23225510177981423 and parameters 
[ 1.31234095  2.23743443 -2.12427943 -0.11653103  0.55388688 -2.77010938
  3.06858478  2.18960165  1.18552019 -1.06648329  0.6027149   1.14432445
  1.31029878 -1.8735466   0.7296504   2.88578399 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 13:12:04 2024]  Iteration number: 0 with current cost as 0.40802254751369477 and parameters 
[-1.00216455  2.23743464 -2.12427951 -0.1165309   0.55388708 -2.7701091
  3.06858486  2.18960145  1.18552011 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354667  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:13:20 2024]  Iteration number: 0 with current cost as 0.4674262952273419 and parameters 
[-0.30763464  2.23743432 -2.12427964 -0.11653118  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.60271479  1.14432445
  1.31029883 -1.8735468   0.72965049  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:14:26 2024]  Iteration number: 0 with current cost as 0.4897581184010037 and parameters 
[ 0.07572621  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858483  2.18960145  1.18552014 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354649  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 438.40830874443054 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 13:16:02 2024]  Iteration number: 0 with current cost as 0.44731675517515807 and parameters 
[-1.26342681  2.23743432 -2.12427964 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.60271479  1.14432429
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 13:18:12 2024]  Iteration number: 0 with current cost as 0.23225510177981423 and parameters 
[ 1.31234095  2.23743443 -2.12427943 -0.11653103  0.55388688 -2.77010938
  3.06858478  2.18960165  1.18552019 -1.06648329  0.6027149   1.14432445
  1.31029878 -1.8735466   0.7296504   2.88578399 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 13:19:10 2024]  Iteration number: 0 with current cost as 0.40802254751369477 and parameters 
[-1.00216455  2.23743464 -2.12427951 -0.1165309   0.55388708 -2.7701091
  3.06858486  2.18960145  1.18552011 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354667  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:20:26 2024]  Iteration number: 0 with current cost as 0.4674262952273419 and parameters 
[-0.30763464  2.23743432 -2.12427964 -0.11653118  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.60271479  1.14432445
  1.31029883 -1.8735468   0.72965049  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:21:31 2024]  Iteration number: 0 with current cost as 0.4897581184010037 and parameters 
[ 0.07572621  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858483  2.18960145  1.18552014 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354649  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 426.1341619491577 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 13:23:08 2024]  Iteration number: 0 with current cost as 0.44731675517515807 and parameters 
[-1.26342681  2.23743432 -2.12427964 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.60271479  1.14432429
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 13:25:23 2024]  Iteration number: 0 with current cost as 0.23225510177981423 and parameters 
[ 1.31234095  2.23743443 -2.12427943 -0.11653103  0.55388688 -2.77010938
  3.06858478  2.18960165  1.18552019 -1.06648329  0.6027149   1.14432445
  1.31029878 -1.8735466   0.7296504   2.88578399 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 13:26:18 2024]  Iteration number: 0 with current cost as 0.40802254751369477 and parameters 
[-1.00216455  2.23743464 -2.12427951 -0.1165309   0.55388708 -2.7701091
  3.06858486  2.18960145  1.18552011 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354667  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:27:33 2024]  Iteration number: 0 with current cost as 0.4674262952273419 and parameters 
[-0.30763464  2.23743432 -2.12427964 -0.11653118  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.60271479  1.14432445
  1.31029883 -1.8735468   0.72965049  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:28:38 2024]  Iteration number: 0 with current cost as 0.4897581184010037 and parameters 
[ 0.07572621  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858483  2.18960145  1.18552014 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354649  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 425.93620109558105 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 13:30:14 2024]  Iteration number: 0 with current cost as 0.44731675517515807 and parameters 
[-1.26342681  2.23743432 -2.12427964 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.60271479  1.14432429
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 13:32:28 2024]  Iteration number: 0 with current cost as 0.23225510177981423 and parameters 
[ 1.31234095  2.23743443 -2.12427943 -0.11653103  0.55388688 -2.77010938
  3.06858478  2.18960165  1.18552019 -1.06648329  0.6027149   1.14432445
  1.31029878 -1.8735466   0.7296504   2.88578399 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 13:33:25 2024]  Iteration number: 0 with current cost as 0.40802254751369477 and parameters 
[-1.00216455  2.23743464 -2.12427951 -0.1165309   0.55388708 -2.7701091
  3.06858486  2.18960145  1.18552011 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354667  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:34:36 2024]  Iteration number: 0 with current cost as 0.4674262952273419 and parameters 
[-0.30763464  2.23743432 -2.12427964 -0.11653118  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.60271479  1.14432445
  1.31029883 -1.8735468   0.72965049  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:35:44 2024]  Iteration number: 0 with current cost as 0.4897581184010037 and parameters 
[ 0.07572621  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858483  2.18960145  1.18552014 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354649  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 426.25132846832275 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 13:37:22 2024]  Iteration number: 0 with current cost as 0.44731675517515807 and parameters 
[-1.26342681  2.23743432 -2.12427964 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.60271479  1.14432429
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 13:39:34 2024]  Iteration number: 0 with current cost as 0.23225510177981423 and parameters 
[ 1.31234095  2.23743443 -2.12427943 -0.11653103  0.55388688 -2.77010938
  3.06858478  2.18960165  1.18552019 -1.06648329  0.6027149   1.14432445
  1.31029878 -1.8735466   0.7296504   2.88578399 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 13:40:31 2024]  Iteration number: 0 with current cost as 0.40802254751369477 and parameters 
[-1.00216455  2.23743464 -2.12427951 -0.1165309   0.55388708 -2.7701091
  3.06858486  2.18960145  1.18552011 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354667  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:41:56 2024]  Iteration number: 0 with current cost as 0.4674262952273419 and parameters 
[-0.30763464  2.23743432 -2.12427964 -0.11653118  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.60271479  1.14432445
  1.31029883 -1.8735468   0.72965049  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:43:04 2024]  Iteration number: 0 with current cost as 0.4897581184010037 and parameters 
[ 0.07572621  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858483  2.18960145  1.18552014 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354649  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 440.69385981559753 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 13:44:41 2024]  Iteration number: 0 with current cost as 0.44731675517515807 and parameters 
[-1.26342681  2.23743432 -2.12427964 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.60271479  1.14432429
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 13:46:49 2024]  Iteration number: 0 with current cost as 0.23225510177981423 and parameters 
[ 1.31234095  2.23743443 -2.12427943 -0.11653103  0.55388688 -2.77010938
  3.06858478  2.18960165  1.18552019 -1.06648329  0.6027149   1.14432445
  1.31029878 -1.8735466   0.7296504   2.88578399 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 13:48:10 2024]  Iteration number: 0 with current cost as 0.40802254751369477 and parameters 
[-1.00216455  2.23743464 -2.12427951 -0.1165309   0.55388708 -2.7701091
  3.06858486  2.18960145  1.18552011 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354667  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:49:24 2024]  Iteration number: 0 with current cost as 0.4674262952273419 and parameters 
[-0.30763464  2.23743432 -2.12427964 -0.11653118  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.60271479  1.14432445
  1.31029883 -1.8735468   0.72965049  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:50:29 2024]  Iteration number: 0 with current cost as 0.4897581184010037 and parameters 
[ 0.07572621  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858483  2.18960145  1.18552014 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354649  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 444.1736173629761 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 13:52:06 2024]  Iteration number: 0 with current cost as 0.44731675517515807 and parameters 
[-1.26342681  2.23743432 -2.12427964 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.60271479  1.14432429
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 13:54:24 2024]  Iteration number: 0 with current cost as 0.23225510177981423 and parameters 
[ 1.31234095  2.23743443 -2.12427943 -0.11653103  0.55388688 -2.77010938
  3.06858478  2.18960165  1.18552019 -1.06648329  0.6027149   1.14432445
  1.31029878 -1.8735466   0.7296504   2.88578399 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 13:55:19 2024]  Iteration number: 0 with current cost as 0.40802254751369477 and parameters 
[-1.00216455  2.23743464 -2.12427951 -0.1165309   0.55388708 -2.7701091
  3.06858486  2.18960145  1.18552011 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354667  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:56:33 2024]  Iteration number: 0 with current cost as 0.4674262952273419 and parameters 
[-0.30763464  2.23743432 -2.12427964 -0.11653118  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.60271479  1.14432445
  1.31029883 -1.8735468   0.72965049  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:57:37 2024]  Iteration number: 0 with current cost as 0.4897581184010037 and parameters 
[ 0.07572621  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858483  2.18960145  1.18552014 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354649  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 428.2443747520447 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 13:59:14 2024]  Iteration number: 0 with current cost as 0.44731675517515807 and parameters 
[-1.26342681  2.23743432 -2.12427964 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.60271479  1.14432429
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 14:01:25 2024]  Iteration number: 0 with current cost as 0.23225510177981423 and parameters 
[ 1.31234095  2.23743443 -2.12427943 -0.11653103  0.55388688 -2.77010938
  3.06858478  2.18960165  1.18552019 -1.06648329  0.6027149   1.14432445
  1.31029878 -1.8735466   0.7296504   2.88578399 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 14:02:22 2024]  Iteration number: 0 with current cost as 0.40802254751369477 and parameters 
[-1.00216455  2.23743464 -2.12427951 -0.1165309   0.55388708 -2.7701091
  3.06858486  2.18960145  1.18552011 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354667  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 14:03:35 2024]  Iteration number: 0 with current cost as 0.4674262952273419 and parameters 
[-0.30763464  2.23743432 -2.12427964 -0.11653118  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.60271479  1.14432445
  1.31029883 -1.8735468   0.72965049  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 14:04:40 2024]  Iteration number: 0 with current cost as 0.4897581184010037 and parameters 
[ 0.07572621  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858483  2.18960145  1.18552014 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354649  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 429.6017224788666 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 14:06:24 2024]  Iteration number: 0 with current cost as 0.44731675517515807 and parameters 
[-1.26342681  2.23743432 -2.12427964 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.60271479  1.14432429
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 14:08:33 2024]  Iteration number: 0 with current cost as 0.23225510177981423 and parameters 
[ 1.31234095  2.23743443 -2.12427943 -0.11653103  0.55388688 -2.77010938
  3.06858478  2.18960165  1.18552019 -1.06648329  0.6027149   1.14432445
  1.31029878 -1.8735466   0.7296504   2.88578399 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 14:09:30 2024]  Iteration number: 0 with current cost as 0.40802254751369477 and parameters 
[-1.00216455  2.23743464 -2.12427951 -0.1165309   0.55388708 -2.7701091
  3.06858486  2.18960145  1.18552011 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354667  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 14:10:42 2024]  Iteration number: 0 with current cost as 0.4674262952273419 and parameters 
[-0.30763464  2.23743432 -2.12427964 -0.11653118  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.60271479  1.14432445
  1.31029883 -1.8735468   0.72965049  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 14:11:52 2024]  Iteration number: 0 with current cost as 0.4897581184010037 and parameters 
[ 0.07572621  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858483  2.18960145  1.18552014 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354649  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 427.3454704284668 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 14:13:31 2024]  Iteration number: 0 with current cost as 0.44731675517515807 and parameters 
[-1.26342681  2.23743432 -2.12427964 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.60271479  1.14432429
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 14:15:41 2024]  Iteration number: 0 with current cost as 0.23225510177981423 and parameters 
[ 1.31234095  2.23743443 -2.12427943 -0.11653103  0.55388688 -2.77010938
  3.06858478  2.18960165  1.18552019 -1.06648329  0.6027149   1.14432445
  1.31029878 -1.8735466   0.7296504   2.88578399 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 14:16:38 2024]  Iteration number: 0 with current cost as 0.40802254751369477 and parameters 
[-1.00216455  2.23743464 -2.12427951 -0.1165309   0.55388708 -2.7701091
  3.06858486  2.18960145  1.18552011 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354667  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 14:17:50 2024]  Iteration number: 0 with current cost as 0.4674262952273419 and parameters 
[-0.30763464  2.23743432 -2.12427964 -0.11653118  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.60271479  1.14432445
  1.31029883 -1.8735468   0.72965049  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 14:18:58 2024]  Iteration number: 0 with current cost as 0.4897581184010037 and parameters 
[ 0.07572621  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858483  2.18960145  1.18552014 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354649  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 423.8729021549225 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 14:20:34 2024]  Iteration number: 0 with current cost as 0.44731675517515807 and parameters 
[-1.26342681  2.23743432 -2.12427964 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.60271479  1.14432429
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 14:23:01 2024]  Iteration number: 0 with current cost as 0.23225510177981423 and parameters 
[ 1.31234095  2.23743443 -2.12427943 -0.11653103  0.55388688 -2.77010938
  3.06858478  2.18960165  1.18552019 -1.06648329  0.6027149   1.14432445
  1.31029878 -1.8735466   0.7296504   2.88578399 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 14:23:58 2024]  Iteration number: 0 with current cost as 0.40802254751369477 and parameters 
[-1.00216455  2.23743464 -2.12427951 -0.1165309   0.55388708 -2.7701091
  3.06858486  2.18960145  1.18552011 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354667  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 14:25:11 2024]  Iteration number: 0 with current cost as 0.4674262952273419 and parameters 
[-0.30763464  2.23743432 -2.12427964 -0.11653118  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.60271479  1.14432445
  1.31029883 -1.8735468   0.72965049  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 14:26:15 2024]  Iteration number: 0 with current cost as 0.4897581184010037 and parameters 
[ 0.07572621  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858483  2.18960145  1.18552014 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354649  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 437.1732132434845 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 14:27:51 2024]  Iteration number: 0 with current cost as 0.44731675517515807 and parameters 
[-1.26342681  2.23743432 -2.12427964 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.60271479  1.14432429
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 14:30:01 2024]  Iteration number: 0 with current cost as 0.23225510177981423 and parameters 
[ 1.31234095  2.23743443 -2.12427943 -0.11653103  0.55388688 -2.77010938
  3.06858478  2.18960165  1.18552019 -1.06648329  0.6027149   1.14432445
  1.31029878 -1.8735466   0.7296504   2.88578399 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 14:30:57 2024]  Iteration number: 0 with current cost as 0.40802254751369477 and parameters 
[-1.00216455  2.23743464 -2.12427951 -0.1165309   0.55388708 -2.7701091
  3.06858486  2.18960145  1.18552011 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354667  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 14:32:15 2024]  Iteration number: 0 with current cost as 0.4674262952273419 and parameters 
[-0.30763464  2.23743432 -2.12427964 -0.11653118  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.60271479  1.14432445
  1.31029883 -1.8735468   0.72965049  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 14:33:20 2024]  Iteration number: 0 with current cost as 0.4897581184010037 and parameters 
[ 0.07572621  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858483  2.18960145  1.18552014 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354649  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 425.5300009250641 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 14:34:57 2024]  Iteration number: 0 with current cost as 0.44731675517515807 and parameters 
[-1.26342681  2.23743432 -2.12427964 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.60271479  1.14432429
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 14:37:08 2024]  Iteration number: 0 with current cost as 0.23225510177981423 and parameters 
[ 1.31234095  2.23743443 -2.12427943 -0.11653103  0.55388688 -2.77010938
  3.06858478  2.18960165  1.18552019 -1.06648329  0.6027149   1.14432445
  1.31029878 -1.8735466   0.7296504   2.88578399 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 14:38:04 2024]  Iteration number: 0 with current cost as 0.40802254751369477 and parameters 
[-1.00216455  2.23743464 -2.12427951 -0.1165309   0.55388708 -2.7701091
  3.06858486  2.18960145  1.18552011 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354667  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 14:39:16 2024]  Iteration number: 0 with current cost as 0.4674262952273419 and parameters 
[-0.30763464  2.23743432 -2.12427964 -0.11653118  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.60271479  1.14432445
  1.31029883 -1.8735468   0.72965049  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 14:40:20 2024]  Iteration number: 0 with current cost as 0.4897581184010037 and parameters 
[ 0.07572621  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858483  2.18960145  1.18552014 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354649  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 421.0292241573334 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 14:41:58 2024]  Iteration number: 0 with current cost as 0.44731675517515807 and parameters 
[-1.26342681  2.23743432 -2.12427964 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.60271479  1.14432429
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 14:44:09 2024]  Iteration number: 0 with current cost as 0.23225510177981423 and parameters 
[ 1.31234095  2.23743443 -2.12427943 -0.11653103  0.55388688 -2.77010938
  3.06858478  2.18960165  1.18552019 -1.06648329  0.6027149   1.14432445
  1.31029878 -1.8735466   0.7296504   2.88578399 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 14:45:05 2024]  Iteration number: 0 with current cost as 0.40802254751369477 and parameters 
[-1.00216455  2.23743464 -2.12427951 -0.1165309   0.55388708 -2.7701091
  3.06858486  2.18960145  1.18552011 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354667  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 14:46:16 2024]  Iteration number: 0 with current cost as 0.4674262952273419 and parameters 
[-0.30763464  2.23743432 -2.12427964 -0.11653118  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.60271479  1.14432445
  1.31029883 -1.8735468   0.72965049  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 14:47:21 2024]  Iteration number: 0 with current cost as 0.4897581184010037 and parameters 
[ 0.07572621  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858483  2.18960145  1.18552014 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354649  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 420.94179677963257 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 14:48:59 2024]  Iteration number: 0 with current cost as 0.44731675517515807 and parameters 
[-1.26342681  2.23743432 -2.12427964 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.60271479  1.14432429
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 14:51:07 2024]  Iteration number: 0 with current cost as 0.23225510177981423 and parameters 
[ 1.31234095  2.23743443 -2.12427943 -0.11653103  0.55388688 -2.77010938
  3.06858478  2.18960165  1.18552019 -1.06648329  0.6027149   1.14432445
  1.31029878 -1.8735466   0.7296504   2.88578399 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 14:52:05 2024]  Iteration number: 0 with current cost as 0.40802254751369477 and parameters 
[-1.00216455  2.23743464 -2.12427951 -0.1165309   0.55388708 -2.7701091
  3.06858486  2.18960145  1.18552011 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354667  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 14:53:17 2024]  Iteration number: 0 with current cost as 0.4674262952273419 and parameters 
[-0.30763464  2.23743432 -2.12427964 -0.11653118  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.60271479  1.14432445
  1.31029883 -1.8735468   0.72965049  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 14:54:22 2024]  Iteration number: 0 with current cost as 0.4897581184010037 and parameters 
[ 0.07572621  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858483  2.18960145  1.18552014 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354649  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 420.15470361709595 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 14:55:59 2024]  Iteration number: 0 with current cost as 0.44731675517515807 and parameters 
[-1.26342681  2.23743432 -2.12427964 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.60271479  1.14432429
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 14:58:08 2024]  Iteration number: 0 with current cost as 0.23225510177981423 and parameters 
[ 1.31234095  2.23743443 -2.12427943 -0.11653103  0.55388688 -2.77010938
  3.06858478  2.18960165  1.18552019 -1.06648329  0.6027149   1.14432445
  1.31029878 -1.8735466   0.7296504   2.88578399 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 14:59:26 2024]  Iteration number: 0 with current cost as 0.40802254751369477 and parameters 
[-1.00216455  2.23743464 -2.12427951 -0.1165309   0.55388708 -2.7701091
  3.06858486  2.18960145  1.18552011 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354667  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 15:00:39 2024]  Iteration number: 0 with current cost as 0.4674262952273419 and parameters 
[-0.30763464  2.23743432 -2.12427964 -0.11653118  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.60271479  1.14432445
  1.31029883 -1.8735468   0.72965049  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 15:01:44 2024]  Iteration number: 0 with current cost as 0.4897581184010037 and parameters 
[ 0.07572621  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858483  2.18960145  1.18552014 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354649  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 441.14592123031616 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 15:03:21 2024]  Iteration number: 0 with current cost as 0.44731675517515807 and parameters 
[-1.26342681  2.23743432 -2.12427964 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.60271479  1.14432429
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 15:05:30 2024]  Iteration number: 0 with current cost as 0.23225510177981423 and parameters 
[ 1.31234095  2.23743443 -2.12427943 -0.11653103  0.55388688 -2.77010938
  3.06858478  2.18960165  1.18552019 -1.06648329  0.6027149   1.14432445
  1.31029878 -1.8735466   0.7296504   2.88578399 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 15:06:25 2024]  Iteration number: 0 with current cost as 0.40802254751369477 and parameters 
[-1.00216455  2.23743464 -2.12427951 -0.1165309   0.55388708 -2.7701091
  3.06858486  2.18960145  1.18552011 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354667  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 15:07:37 2024]  Iteration number: 0 with current cost as 0.4674262952273419 and parameters 
[-0.30763464  2.23743432 -2.12427964 -0.11653118  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.60271479  1.14432445
  1.31029883 -1.8735468   0.72965049  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 15:08:42 2024]  Iteration number: 0 with current cost as 0.4897581184010037 and parameters 
[ 0.07572621  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858483  2.18960145  1.18552014 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354649  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 417.4109697341919 seconds. 
Discarding model... 

Training complete taking 10709.526207447052 total seconds. 
Now scoring model... 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Scoring complete taking 1.1354131698608398 seconds. 
Saved predicted values as M-A2-CNOT_Modified-Pauli-CRZ_predicted_values.csv
Model scores: {'MSE_train': (147.9083184644662,), 'R2_train': 0.28343383742580874, 'MAE_train': 10.976254785918496, 'MSE_test': 167.3673969668236, 'R2_test': -0.008859523523847557, 'MAE_test': 10.95079813178329}. 
Saved model results as M-A2-CNOT_Modified-Pauli-CRZ_results.json. 
