/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /scratch/j/jacobsen/gjones/quad5qubits/quadratic_train.bin... 
Successfully loaded /scratch/j/jacobsen/gjones/quad5qubits/quadratic_train.bin into X and y data. 
Loading dataset from /scratch/j/jacobsen/gjones/quad5qubits/quadratic_test.bin... 
Successfully loaded /scratch/j/jacobsen/gjones/quad5qubits/quadratic_test.bin into X and y data. 
Training model with dataset /scratch/j/jacobsen/gjones/quad5qubits/quadratic_train.bin 
 at time Mon Apr  1 15:44:10 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Apr  1 15:44:30 2024]  Iteration number: 0 with current cost as 0.20997205648300027 and parameters 
[-2.45608949  2.44048661 -1.94579965 -0.11653103  0.55388707 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.08165539  1.14432445
  0.62064795 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522486
 -2.02654241  0.7289737   1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Apr  1 15:50:03 2024]  Iteration number: 0 with current cost as 0.19057582790054778 and parameters 
[-2.49492041  2.42610584 -1.96232674 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.12910906  1.14432446
  0.68105881 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.6 fold... 
[Mon Apr  1 15:55:34 2024]  Iteration number: 0 with current cost as 0.18599250431699874 and parameters 
[-2.45457795  2.43407796 -1.94599601 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.0664831   0.08134682  1.14432444
  0.61995518 -1.87354681  0.72965079  2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.72897368  1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Apr  1 16:02:16 2024]  Iteration number: 0 with current cost as 0.13303285288306724 and parameters 
[-2.45492236  2.45062465 -1.94698423 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.08593753  1.14432445
  0.62093747 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Apr  1 16:09:09 2024]  Iteration number: 0 with current cost as 0.14210197933036975 and parameters 
[-2.48229862  2.45274794 -1.95861667 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648309  0.12152981  1.14432445
  0.66362901 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 1860.0719423294067 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Apr  1 16:15:31 2024]  Iteration number: 0 with current cost as 0.20997205648300027 and parameters 
[-2.45608949  2.44048661 -1.94579965 -0.11653103  0.55388707 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.08165539  1.14432445
  0.62064795 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522486
 -2.02654241  0.7289737   1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Apr  1 16:21:25 2024]  Iteration number: 0 with current cost as 0.19057582790054778 and parameters 
[-2.49492041  2.42610584 -1.96232674 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.12910906  1.14432446
  0.68105881 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.6 fold... 
[Mon Apr  1 16:26:52 2024]  Iteration number: 0 with current cost as 0.18599250431699874 and parameters 
[-2.45457795  2.43407796 -1.94599601 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.0664831   0.08134682  1.14432444
  0.61995518 -1.87354681  0.72965079  2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.72897368  1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Apr  1 16:33:32 2024]  Iteration number: 0 with current cost as 0.13303285288306724 and parameters 
[-2.45492236  2.45062465 -1.94698423 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.08593753  1.14432445
  0.62093747 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Apr  1 16:40:29 2024]  Iteration number: 0 with current cost as 0.14210197933036975 and parameters 
[-2.48229862  2.45274794 -1.95861667 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648309  0.12152981  1.14432445
  0.66362901 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 1881.959273815155 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Apr  1 16:46:52 2024]  Iteration number: 0 with current cost as 0.20997205648300027 and parameters 
[-2.45608949  2.44048661 -1.94579965 -0.11653103  0.55388707 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.08165539  1.14432445
  0.62064795 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522486
 -2.02654241  0.7289737   1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Apr  1 16:53:02 2024]  Iteration number: 0 with current cost as 0.19057582790054778 and parameters 
[-2.49492041  2.42610584 -1.96232674 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.12910906  1.14432446
  0.68105881 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.6 fold... 
[Mon Apr  1 16:59:07 2024]  Iteration number: 0 with current cost as 0.18599250431699874 and parameters 
[-2.45457795  2.43407796 -1.94599601 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.0664831   0.08134682  1.14432444
  0.61995518 -1.87354681  0.72965079  2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.72897368  1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Apr  1 17:05:41 2024]  Iteration number: 0 with current cost as 0.13303285288306724 and parameters 
[-2.45492236  2.45062465 -1.94698423 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.08593753  1.14432445
  0.62093747 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Apr  1 17:12:27 2024]  Iteration number: 0 with current cost as 0.14210197933036975 and parameters 
[-2.48229862  2.45274794 -1.95861667 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648309  0.12152981  1.14432445
  0.66362901 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 1909.1769819259644 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Mon Apr  1 17:18:42 2024]  Iteration number: 0 with current cost as 0.20997205648300027 and parameters 
[-2.45608949  2.44048661 -1.94579965 -0.11653103  0.55388707 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.08165539  1.14432445
  0.62064795 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522486
 -2.02654241  0.7289737   1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Apr  1 17:24:30 2024]  Iteration number: 0 with current cost as 0.19057582790054778 and parameters 
[-2.49492041  2.42610584 -1.96232674 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.12910906  1.14432446
  0.68105881 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.6 fold... 
[Mon Apr  1 17:29:50 2024]  Iteration number: 0 with current cost as 0.18599250431699874 and parameters 
[-2.45457795  2.43407796 -1.94599601 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.0664831   0.08134682  1.14432444
  0.61995518 -1.87354681  0.72965079  2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.72897368  1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Apr  1 17:36:37 2024]  Iteration number: 0 with current cost as 0.13303285288306724 and parameters 
[-2.45492236  2.45062465 -1.94698423 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.08593753  1.14432445
  0.62093747 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Apr  1 17:43:11 2024]  Iteration number: 0 with current cost as 0.14210197933036975 and parameters 
[-2.48229862  2.45274794 -1.95861667 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648309  0.12152981  1.14432445
  0.66362901 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 1826.8856568336487 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Mon Apr  1 17:49:08 2024]  Iteration number: 0 with current cost as 0.20997205648300027 and parameters 
[-2.45608949  2.44048661 -1.94579965 -0.11653103  0.55388707 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.08165539  1.14432445
  0.62064795 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522486
 -2.02654241  0.7289737   1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Apr  1 17:54:37 2024]  Iteration number: 0 with current cost as 0.19057582790054778 and parameters 
[-2.49492041  2.42610584 -1.96232674 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.12910906  1.14432446
  0.68105881 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.6 fold... 
[Mon Apr  1 17:59:59 2024]  Iteration number: 0 with current cost as 0.18599250431699874 and parameters 
[-2.45457795  2.43407796 -1.94599601 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.0664831   0.08134682  1.14432444
  0.61995518 -1.87354681  0.72965079  2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.72897368  1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Apr  1 18:06:38 2024]  Iteration number: 0 with current cost as 0.13303285288306724 and parameters 
[-2.45492236  2.45062465 -1.94698423 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.08593753  1.14432445
  0.62093747 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Apr  1 18:13:25 2024]  Iteration number: 0 with current cost as 0.14210197933036975 and parameters 
[-2.48229862  2.45274794 -1.95861667 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648309  0.12152981  1.14432445
  0.66362901 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 1825.8265843391418 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Apr  1 18:19:35 2024]  Iteration number: 0 with current cost as 0.20997205648300027 and parameters 
[-2.45608949  2.44048661 -1.94579965 -0.11653103  0.55388707 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.08165539  1.14432445
  0.62064795 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522486
 -2.02654241  0.7289737   1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Apr  1 18:25:25 2024]  Iteration number: 0 with current cost as 0.19057582790054778 and parameters 
[-2.49492041  2.42610584 -1.96232674 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.12910906  1.14432446
  0.68105881 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.6 fold... 
[Mon Apr  1 18:30:51 2024]  Iteration number: 0 with current cost as 0.18599250431699874 and parameters 
[-2.45457795  2.43407796 -1.94599601 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.0664831   0.08134682  1.14432444
  0.61995518 -1.87354681  0.72965079  2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.72897368  1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Apr  1 18:37:13 2024]  Iteration number: 0 with current cost as 0.13303285288306724 and parameters 
[-2.45492236  2.45062465 -1.94698423 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.08593753  1.14432445
  0.62093747 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Apr  1 18:44:01 2024]  Iteration number: 0 with current cost as 0.14210197933036975 and parameters 
[-2.48229862  2.45274794 -1.95861667 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648309  0.12152981  1.14432445
  0.66362901 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 1843.069334745407 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Apr  1 18:50:17 2024]  Iteration number: 0 with current cost as 0.20997205648300027 and parameters 
[-2.45608949  2.44048661 -1.94579965 -0.11653103  0.55388707 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.08165539  1.14432445
  0.62064795 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522486
 -2.02654241  0.7289737   1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Apr  1 18:56:00 2024]  Iteration number: 0 with current cost as 0.19057582790054778 and parameters 
[-2.49492041  2.42610584 -1.96232674 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.12910906  1.14432446
  0.68105881 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.6 fold... 
[Mon Apr  1 19:01:18 2024]  Iteration number: 0 with current cost as 0.18599250431699874 and parameters 
[-2.45457795  2.43407796 -1.94599601 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.0664831   0.08134682  1.14432444
  0.61995518 -1.87354681  0.72965079  2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.72897368  1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Apr  1 19:07:49 2024]  Iteration number: 0 with current cost as 0.13303285288306724 and parameters 
[-2.45492236  2.45062465 -1.94698423 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.08593753  1.14432445
  0.62093747 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Apr  1 19:14:19 2024]  Iteration number: 0 with current cost as 0.14210197933036975 and parameters 
[-2.48229862  2.45274794 -1.95861667 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648309  0.12152981  1.14432445
  0.66362901 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 1799.6916139125824 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Apr  1 19:20:16 2024]  Iteration number: 0 with current cost as 0.20997205648300027 and parameters 
[-2.45608949  2.44048661 -1.94579965 -0.11653103  0.55388707 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.08165539  1.14432445
  0.62064795 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522486
 -2.02654241  0.7289737   1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Apr  1 19:25:54 2024]  Iteration number: 0 with current cost as 0.19057582790054778 and parameters 
[-2.49492041  2.42610584 -1.96232674 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.12910906  1.14432446
  0.68105881 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.6 fold... 
[Mon Apr  1 19:31:15 2024]  Iteration number: 0 with current cost as 0.18599250431699874 and parameters 
[-2.45457795  2.43407796 -1.94599601 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.0664831   0.08134682  1.14432444
  0.61995518 -1.87354681  0.72965079  2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.72897368  1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Apr  1 19:37:42 2024]  Iteration number: 0 with current cost as 0.13303285288306724 and parameters 
[-2.45492236  2.45062465 -1.94698423 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.08593753  1.14432445
  0.62093747 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Apr  1 19:44:21 2024]  Iteration number: 0 with current cost as 0.14210197933036975 and parameters 
[-2.48229862  2.45274794 -1.95861667 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648309  0.12152981  1.14432445
  0.66362901 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 1807.6217875480652 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Mon Apr  1 19:50:23 2024]  Iteration number: 0 with current cost as 0.20997205648300027 and parameters 
[-2.45608949  2.44048661 -1.94579965 -0.11653103  0.55388707 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.08165539  1.14432445
  0.62064795 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522486
 -2.02654241  0.7289737   1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Apr  1 19:55:59 2024]  Iteration number: 0 with current cost as 0.19057582790054778 and parameters 
[-2.49492041  2.42610584 -1.96232674 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.12910906  1.14432446
  0.68105881 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.6 fold... 
[Mon Apr  1 20:01:11 2024]  Iteration number: 0 with current cost as 0.18599250431699874 and parameters 
[-2.45457795  2.43407796 -1.94599601 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.0664831   0.08134682  1.14432444
  0.61995518 -1.87354681  0.72965079  2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.72897368  1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Apr  1 20:07:45 2024]  Iteration number: 0 with current cost as 0.13303285288306724 and parameters 
[-2.45492236  2.45062465 -1.94698423 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.08593753  1.14432445
  0.62093747 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Apr  1 20:14:28 2024]  Iteration number: 0 with current cost as 0.14210197933036975 and parameters 
[-2.48229862  2.45274794 -1.95861667 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648309  0.12152981  1.14432445
  0.66362901 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 1816.4155445098877 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Mon Apr  1 20:20:41 2024]  Iteration number: 0 with current cost as 0.20997205648300027 and parameters 
[-2.45608949  2.44048661 -1.94579965 -0.11653103  0.55388707 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.08165539  1.14432445
  0.62064795 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522486
 -2.02654241  0.7289737   1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Apr  1 20:26:25 2024]  Iteration number: 0 with current cost as 0.19057582790054778 and parameters 
[-2.49492041  2.42610584 -1.96232674 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.12910906  1.14432446
  0.68105881 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.6 fold... 
[Mon Apr  1 20:32:00 2024]  Iteration number: 0 with current cost as 0.18599250431699874 and parameters 
[-2.45457795  2.43407796 -1.94599601 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.0664831   0.08134682  1.14432444
  0.61995518 -1.87354681  0.72965079  2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.72897368  1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Apr  1 20:38:29 2024]  Iteration number: 0 with current cost as 0.13303285288306724 and parameters 
[-2.45492236  2.45062465 -1.94698423 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.08593753  1.14432445
  0.62093747 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Apr  1 20:45:06 2024]  Iteration number: 0 with current cost as 0.14210197933036975 and parameters 
[-2.48229862  2.45274794 -1.95861667 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648309  0.12152981  1.14432445
  0.66362901 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 1850.0391449928284 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Apr  1 20:51:31 2024]  Iteration number: 0 with current cost as 0.20997205648300027 and parameters 
[-2.45608949  2.44048661 -1.94579965 -0.11653103  0.55388707 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.08165539  1.14432445
  0.62064795 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522486
 -2.02654241  0.7289737   1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Apr  1 20:57:13 2024]  Iteration number: 0 with current cost as 0.19057582790054778 and parameters 
[-2.49492041  2.42610584 -1.96232674 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.12910906  1.14432446
  0.68105881 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.6 fold... 
[Mon Apr  1 21:02:33 2024]  Iteration number: 0 with current cost as 0.18599250431699874 and parameters 
[-2.45457795  2.43407796 -1.94599601 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.0664831   0.08134682  1.14432444
  0.61995518 -1.87354681  0.72965079  2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.72897368  1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Apr  1 21:09:18 2024]  Iteration number: 0 with current cost as 0.13303285288306724 and parameters 
[-2.45492236  2.45062465 -1.94698423 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.08593753  1.14432445
  0.62093747 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Apr  1 21:15:54 2024]  Iteration number: 0 with current cost as 0.14210197933036975 and parameters 
[-2.48229862  2.45274794 -1.95861667 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648309  0.12152981  1.14432445
  0.66362901 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 1840.2578382492065 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Apr  1 21:22:11 2024]  Iteration number: 0 with current cost as 0.20997205648300027 and parameters 
[-2.45608949  2.44048661 -1.94579965 -0.11653103  0.55388707 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.08165539  1.14432445
  0.62064795 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522486
 -2.02654241  0.7289737   1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Apr  1 21:28:06 2024]  Iteration number: 0 with current cost as 0.19057582790054778 and parameters 
[-2.49492041  2.42610584 -1.96232674 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.12910906  1.14432446
  0.68105881 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.6 fold... 
[Mon Apr  1 21:33:25 2024]  Iteration number: 0 with current cost as 0.18599250431699874 and parameters 
[-2.45457795  2.43407796 -1.94599601 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.0664831   0.08134682  1.14432444
  0.61995518 -1.87354681  0.72965079  2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.72897368  1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Apr  1 21:39:47 2024]  Iteration number: 0 with current cost as 0.13303285288306724 and parameters 
[-2.45492236  2.45062465 -1.94698423 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.08593753  1.14432445
  0.62093747 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Apr  1 21:46:43 2024]  Iteration number: 0 with current cost as 0.14210197933036975 and parameters 
[-2.48229862  2.45274794 -1.95861667 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648309  0.12152981  1.14432445
  0.66362901 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 1844.1143834590912 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Apr  1 21:52:55 2024]  Iteration number: 0 with current cost as 0.20997205648300027 and parameters 
[-2.45608949  2.44048661 -1.94579965 -0.11653103  0.55388707 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.08165539  1.14432445
  0.62064795 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522486
 -2.02654241  0.7289737   1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Apr  1 21:58:42 2024]  Iteration number: 0 with current cost as 0.19057582790054778 and parameters 
[-2.49492041  2.42610584 -1.96232674 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.12910906  1.14432446
  0.68105881 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.6 fold... 
[Mon Apr  1 22:04:17 2024]  Iteration number: 0 with current cost as 0.18599250431699874 and parameters 
[-2.45457795  2.43407796 -1.94599601 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.0664831   0.08134682  1.14432444
  0.61995518 -1.87354681  0.72965079  2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.72897368  1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Apr  1 22:10:57 2024]  Iteration number: 0 with current cost as 0.13303285288306724 and parameters 
[-2.45492236  2.45062465 -1.94698423 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.08593753  1.14432445
  0.62093747 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Apr  1 22:17:41 2024]  Iteration number: 0 with current cost as 0.14210197933036975 and parameters 
[-2.48229862  2.45274794 -1.95861667 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648309  0.12152981  1.14432445
  0.66362901 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 1869.3742554187775 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Apr  1 22:24:05 2024]  Iteration number: 0 with current cost as 0.20997205648300027 and parameters 
[-2.45608949  2.44048661 -1.94579965 -0.11653103  0.55388707 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.08165539  1.14432445
  0.62064795 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522486
 -2.02654241  0.7289737   1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Apr  1 22:29:35 2024]  Iteration number: 0 with current cost as 0.19057582790054778 and parameters 
[-2.49492041  2.42610584 -1.96232674 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.12910906  1.14432446
  0.68105881 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.6 fold... 
[Mon Apr  1 22:34:57 2024]  Iteration number: 0 with current cost as 0.18599250431699874 and parameters 
[-2.45457795  2.43407796 -1.94599601 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.0664831   0.08134682  1.14432444
  0.61995518 -1.87354681  0.72965079  2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.72897368  1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Apr  1 22:41:32 2024]  Iteration number: 0 with current cost as 0.13303285288306724 and parameters 
[-2.45492236  2.45062465 -1.94698423 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.08593753  1.14432445
  0.62093747 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Apr  1 22:48:21 2024]  Iteration number: 0 with current cost as 0.14210197933036975 and parameters 
[-2.48229862  2.45274794 -1.95861667 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648309  0.12152981  1.14432445
  0.66362901 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 1830.1313734054565 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Apr  1 22:54:36 2024]  Iteration number: 0 with current cost as 0.20997205648300027 and parameters 
[-2.45608949  2.44048661 -1.94579965 -0.11653103  0.55388707 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.08165539  1.14432445
  0.62064795 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522486
 -2.02654241  0.7289737   1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Apr  1 23:00:10 2024]  Iteration number: 0 with current cost as 0.19057582790054778 and parameters 
[-2.49492041  2.42610584 -1.96232674 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.12910906  1.14432446
  0.68105881 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.6 fold... 
[Mon Apr  1 23:05:29 2024]  Iteration number: 0 with current cost as 0.18599250431699874 and parameters 
[-2.45457795  2.43407796 -1.94599601 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.0664831   0.08134682  1.14432444
  0.61995518 -1.87354681  0.72965079  2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.72897368  1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Apr  1 23:12:01 2024]  Iteration number: 0 with current cost as 0.13303285288306724 and parameters 
[-2.45492236  2.45062465 -1.94698423 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.08593753  1.14432445
  0.62093747 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Apr  1 23:18:40 2024]  Iteration number: 0 with current cost as 0.14210197933036975 and parameters 
[-2.48229862  2.45274794 -1.95861667 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648309  0.12152981  1.14432445
  0.66362901 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 1822.7076964378357 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Apr  1 23:25:02 2024]  Iteration number: 0 with current cost as 0.20997205648300027 and parameters 
[-2.45608949  2.44048661 -1.94579965 -0.11653103  0.55388707 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.08165539  1.14432445
  0.62064795 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522486
 -2.02654241  0.7289737   1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Apr  1 23:31:05 2024]  Iteration number: 0 with current cost as 0.19057582790054778 and parameters 
[-2.49492041  2.42610584 -1.96232674 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.12910906  1.14432446
  0.68105881 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.6 fold... 
[Mon Apr  1 23:36:27 2024]  Iteration number: 0 with current cost as 0.18599250431699874 and parameters 
[-2.45457795  2.43407796 -1.94599601 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.0664831   0.08134682  1.14432444
  0.61995518 -1.87354681  0.72965079  2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.72897368  1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Apr  1 23:43:04 2024]  Iteration number: 0 with current cost as 0.13303285288306724 and parameters 
[-2.45492236  2.45062465 -1.94698423 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.08593753  1.14432445
  0.62093747 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Apr  1 23:49:48 2024]  Iteration number: 0 with current cost as 0.14210197933036975 and parameters 
[-2.48229862  2.45274794 -1.95861667 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648309  0.12152981  1.14432445
  0.66362901 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 1863.0196583271027 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Apr  1 23:56:01 2024]  Iteration number: 0 with current cost as 0.20997205648300027 and parameters 
[-2.45608949  2.44048661 -1.94579965 -0.11653103  0.55388707 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.08165539  1.14432445
  0.62064795 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522486
 -2.02654241  0.7289737   1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Tue Apr  2 00:01:30 2024]  Iteration number: 0 with current cost as 0.19057582790054778 and parameters 
[-2.49492041  2.42610584 -1.96232674 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.12910906  1.14432446
  0.68105881 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.6 fold... 
[Tue Apr  2 00:06:57 2024]  Iteration number: 0 with current cost as 0.18599250431699874 and parameters 
[-2.45457795  2.43407796 -1.94599601 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.0664831   0.08134682  1.14432444
  0.61995518 -1.87354681  0.72965079  2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.72897368  1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Tue Apr  2 00:14:00 2024]  Iteration number: 0 with current cost as 0.13303285288306724 and parameters 
[-2.45492236  2.45062465 -1.94698423 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.08593753  1.14432445
  0.62093747 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Tue Apr  2 00:21:32 2024]  Iteration number: 0 with current cost as 0.14210197933036975 and parameters 
[-2.48229862  2.45274794 -1.95861667 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648309  0.12152981  1.14432445
  0.66362901 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 1943.2179605960846 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Tue Apr  2 00:28:26 2024]  Iteration number: 0 with current cost as 0.20997205648300027 and parameters 
[-2.45608949  2.44048661 -1.94579965 -0.11653103  0.55388707 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.08165539  1.14432445
  0.62064795 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522486
 -2.02654241  0.7289737   1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Tue Apr  2 00:34:16 2024]  Iteration number: 0 with current cost as 0.19057582790054778 and parameters 
[-2.49492041  2.42610584 -1.96232674 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.12910906  1.14432446
  0.68105881 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.6 fold... 
[Tue Apr  2 00:39:53 2024]  Iteration number: 0 with current cost as 0.18599250431699874 and parameters 
[-2.45457795  2.43407796 -1.94599601 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.0664831   0.08134682  1.14432444
  0.61995518 -1.87354681  0.72965079  2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.72897368  1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Tue Apr  2 00:46:58 2024]  Iteration number: 0 with current cost as 0.13303285288306724 and parameters 
[-2.45492236  2.45062465 -1.94698423 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.08593753  1.14432445
  0.62093747 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Tue Apr  2 00:54:04 2024]  Iteration number: 0 with current cost as 0.14210197933036975 and parameters 
[-2.48229862  2.45274794 -1.95861667 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648309  0.12152981  1.14432445
  0.66362901 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 1941.0751538276672 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Tue Apr  2 01:00:49 2024]  Iteration number: 0 with current cost as 0.20997205648300027 and parameters 
[-2.45608949  2.44048661 -1.94579965 -0.11653103  0.55388707 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.08165539  1.14432445
  0.62064795 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522486
 -2.02654241  0.7289737   1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Tue Apr  2 01:07:06 2024]  Iteration number: 0 with current cost as 0.19057582790054778 and parameters 
[-2.49492041  2.42610584 -1.96232674 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.12910906  1.14432446
  0.68105881 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.6 fold... 
[Tue Apr  2 01:12:48 2024]  Iteration number: 0 with current cost as 0.18599250431699874 and parameters 
[-2.45457795  2.43407796 -1.94599601 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.0664831   0.08134682  1.14432444
  0.61995518 -1.87354681  0.72965079  2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.72897368  1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Tue Apr  2 01:19:32 2024]  Iteration number: 0 with current cost as 0.13303285288306724 and parameters 
[-2.45492236  2.45062465 -1.94698423 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.08593753  1.14432445
  0.62093747 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Tue Apr  2 01:26:40 2024]  Iteration number: 0 with current cost as 0.14210197933036975 and parameters 
[-2.48229862  2.45274794 -1.95861667 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648309  0.12152981  1.14432445
  0.66362901 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 1954.356769323349 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Tue Apr  2 01:33:22 2024]  Iteration number: 0 with current cost as 0.20997205648300027 and parameters 
[-2.45608949  2.44048661 -1.94579965 -0.11653103  0.55388707 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.08165539  1.14432445
  0.62064795 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522486
 -2.02654241  0.7289737   1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Tue Apr  2 01:39:31 2024]  Iteration number: 0 with current cost as 0.19057582790054778 and parameters 
[-2.49492041  2.42610584 -1.96232674 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.12910906  1.14432446
  0.68105881 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.6 fold... 
[Tue Apr  2 01:45:41 2024]  Iteration number: 0 with current cost as 0.18599250431699874 and parameters 
[-2.45457795  2.43407796 -1.94599601 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.0664831   0.08134682  1.14432444
  0.61995518 -1.87354681  0.72965079  2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.72897368  1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Tue Apr  2 01:52:37 2024]  Iteration number: 0 with current cost as 0.13303285288306724 and parameters 
[-2.45492236  2.45062465 -1.94698423 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.08593753  1.14432445
  0.62093747 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Tue Apr  2 01:59:42 2024]  Iteration number: 0 with current cost as 0.14210197933036975 and parameters 
[-2.48229862  2.45274794 -1.95861667 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648309  0.12152981  1.14432445
  0.66362901 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 1990.0344505310059 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Tue Apr  2 02:06:30 2024]  Iteration number: 0 with current cost as 0.20997205648300027 and parameters 
[-2.45608949  2.44048661 -1.94579965 -0.11653103  0.55388707 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.08165539  1.14432445
  0.62064795 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522486
 -2.02654241  0.7289737   1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Tue Apr  2 02:12:43 2024]  Iteration number: 0 with current cost as 0.19057582790054778 and parameters 
[-2.49492041  2.42610584 -1.96232674 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.12910906  1.14432446
  0.68105881 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.6 fold... 
[Tue Apr  2 02:18:31 2024]  Iteration number: 0 with current cost as 0.18599250431699874 and parameters 
[-2.45457795  2.43407796 -1.94599601 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.0664831   0.08134682  1.14432444
  0.61995518 -1.87354681  0.72965079  2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.72897368  1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Tue Apr  2 02:25:18 2024]  Iteration number: 0 with current cost as 0.13303285288306724 and parameters 
[-2.45492236  2.45062465 -1.94698423 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.08593753  1.14432445
  0.62093747 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Tue Apr  2 02:32:27 2024]  Iteration number: 0 with current cost as 0.14210197933036975 and parameters 
[-2.48229862  2.45274794 -1.95861667 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648309  0.12152981  1.14432445
  0.66362901 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 1956.4211640357971 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Tue Apr  2 02:39:06 2024]  Iteration number: 0 with current cost as 0.20997205648300027 and parameters 
[-2.45608949  2.44048661 -1.94579965 -0.11653103  0.55388707 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.08165539  1.14432445
  0.62064795 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522486
 -2.02654241  0.7289737   1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Tue Apr  2 02:45:05 2024]  Iteration number: 0 with current cost as 0.19057582790054778 and parameters 
[-2.49492041  2.42610584 -1.96232674 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.12910906  1.14432446
  0.68105881 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.6 fold... 
[Tue Apr  2 02:51:00 2024]  Iteration number: 0 with current cost as 0.18599250431699874 and parameters 
[-2.45457795  2.43407796 -1.94599601 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.0664831   0.08134682  1.14432444
  0.61995518 -1.87354681  0.72965079  2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.72897368  1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Tue Apr  2 02:58:06 2024]  Iteration number: 0 with current cost as 0.13303285288306724 and parameters 
[-2.45492236  2.45062465 -1.94698423 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.08593753  1.14432445
  0.62093747 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Tue Apr  2 03:05:24 2024]  Iteration number: 0 with current cost as 0.14210197933036975 and parameters 
[-2.48229862  2.45274794 -1.95861667 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648309  0.12152981  1.14432445
  0.66362901 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 1976.7145535945892 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Tue Apr  2 03:12:03 2024]  Iteration number: 0 with current cost as 0.20997205648300027 and parameters 
[-2.45608949  2.44048661 -1.94579965 -0.11653103  0.55388707 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.08165539  1.14432445
  0.62064795 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522486
 -2.02654241  0.7289737   1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Tue Apr  2 03:18:06 2024]  Iteration number: 0 with current cost as 0.19057582790054778 and parameters 
[-2.49492041  2.42610584 -1.96232674 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.12910906  1.14432446
  0.68105881 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.6 fold... 
[Tue Apr  2 03:23:44 2024]  Iteration number: 0 with current cost as 0.18599250431699874 and parameters 
[-2.45457795  2.43407796 -1.94599601 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.0664831   0.08134682  1.14432444
  0.61995518 -1.87354681  0.72965079  2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.72897368  1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Tue Apr  2 03:30:40 2024]  Iteration number: 0 with current cost as 0.13303285288306724 and parameters 
[-2.45492236  2.45062465 -1.94698423 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.08593753  1.14432445
  0.62093747 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Tue Apr  2 03:37:38 2024]  Iteration number: 0 with current cost as 0.14210197933036975 and parameters 
[-2.48229862  2.45274794 -1.95861667 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648309  0.12152981  1.14432445
  0.66362901 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 1923.2606806755066 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Tue Apr  2 03:44:07 2024]  Iteration number: 0 with current cost as 0.20997205648300027 and parameters 
[-2.45608949  2.44048661 -1.94579965 -0.11653103  0.55388707 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.08165539  1.14432445
  0.62064795 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522486
 -2.02654241  0.7289737   1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Tue Apr  2 03:49:56 2024]  Iteration number: 0 with current cost as 0.19057582790054778 and parameters 
[-2.49492041  2.42610584 -1.96232674 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.12910906  1.14432446
  0.68105881 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.6 fold... 
[Tue Apr  2 03:55:33 2024]  Iteration number: 0 with current cost as 0.18599250431699874 and parameters 
[-2.45457795  2.43407796 -1.94599601 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.0664831   0.08134682  1.14432444
  0.61995518 -1.87354681  0.72965079  2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.72897368  1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Tue Apr  2 04:02:33 2024]  Iteration number: 0 with current cost as 0.13303285288306724 and parameters 
[-2.45492236  2.45062465 -1.94698423 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.08593753  1.14432445
  0.62093747 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Tue Apr  2 04:09:35 2024]  Iteration number: 0 with current cost as 0.14210197933036975 and parameters 
[-2.48229862  2.45274794 -1.95861667 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648309  0.12152981  1.14432445
  0.66362901 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 1918.9504845142365 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Tue Apr  2 04:16:04 2024]  Iteration number: 0 with current cost as 0.20997205648300027 and parameters 
[-2.45608949  2.44048661 -1.94579965 -0.11653103  0.55388707 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.08165539  1.14432445
  0.62064795 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522486
 -2.02654241  0.7289737   1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Tue Apr  2 04:22:00 2024]  Iteration number: 0 with current cost as 0.19057582790054778 and parameters 
[-2.49492041  2.42610584 -1.96232674 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.12910906  1.14432446
  0.68105881 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.6 fold... 
[Tue Apr  2 04:27:50 2024]  Iteration number: 0 with current cost as 0.18599250431699874 and parameters 
[-2.45457795  2.43407796 -1.94599601 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.0664831   0.08134682  1.14432444
  0.61995518 -1.87354681  0.72965079  2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.72897368  1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Tue Apr  2 04:34:52 2024]  Iteration number: 0 with current cost as 0.13303285288306724 and parameters 
[-2.45492236  2.45062465 -1.94698423 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.08593753  1.14432445
  0.62093747 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Tue Apr  2 04:41:55 2024]  Iteration number: 0 with current cost as 0.14210197933036975 and parameters 
[-2.48229862  2.45274794 -1.95861667 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648309  0.12152981  1.14432445
  0.66362901 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 1971.0356812477112 seconds. 
Discarding model... 

Training complete taking 47065.4312517643 total seconds. 
Now scoring model... 
Scoring complete taking 0.9128131866455078 seconds. 
Saved predicted values as A1-A1-CNOT_Full-Pauli-CRX_predicted_values.csv
Model scores: {'MSE_train': (3.262307463894884,), 'R2_train': 0.5520606272449788, 'MAE_train': 1.209076438597266, 'MSE_test': 1.8154298508265296, 'R2_test': 0.8190186031489999, 'MAE_test': 1.1288825483783025}. 
Saved model results as A1-A1-CNOT_Full-Pauli-CRX_results.json. 
