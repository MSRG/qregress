/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/circuits/Encoders.py:12: RuntimeWarning: invalid value encountered in arcsin
  if np.isnan(np.arcsin(features[feature_idx])) or np.isnan(np.arccos(features[feature_idx])):
Loading dataset from /scratch/j/jacobsen/gjones/quad5qubits/quadratic_train.bin... 
Successfully loaded /scratch/j/jacobsen/gjones/quad5qubits/quadratic_train.bin into X and y data. 
Loading dataset from /scratch/j/jacobsen/gjones/quad5qubits/quadratic_test.bin... 
Successfully loaded /scratch/j/jacobsen/gjones/quad5qubits/quadratic_test.bin into X and y data. 
Training model with dataset /scratch/j/jacobsen/gjones/quad5qubits/quadratic_train.bin 
 at time Mon Apr  1 16:02:03 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Apr  1 16:06:56 2024]  Iteration number: 0 with current cost as 0.20767318926165448 and parameters 
[-2.40376861  2.23743465 -2.1242796  -0.11653101  0.55388708 -2.77010897
  3.068585    2.18960145  1.18552002 -1.0664831   0.60271512  1.14432448
  1.31029899 -1.8735468   0.72965079  2.88578421 -0.54534335 -0.47522484
 -2.02654242  0.72897368  1.60512664  2.83077106 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550786 -2.69002203]. 
Working on 0.4 fold... 
[Mon Apr  1 16:15:24 2024]  Iteration number: 0 with current cost as 0.19813386690871543 and parameters 
[-2.39265684  2.23743464 -2.12427961 -0.116531    0.55388709 -2.77010896
  3.06858498  2.18960145  1.18552001 -1.0664831   0.60271513  1.14432448
  1.31029897 -1.87354679  0.7296508   2.88578421 -0.54534334 -0.47522485
 -2.02654243  0.72897367  1.60512664  2.83077107 -1.26456708 -0.25136103
 -2.39279216 -2.27309774  3.13337152  2.54856955 -0.67550789 -2.69002202]. 
Working on 0.6 fold... 
[Mon Apr  1 16:24:06 2024]  Iteration number: 0 with current cost as 0.19068530213583346 and parameters 
[-2.45716183  2.23743464 -2.12427961 -0.116531    0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552001 -1.06648308  0.6027151   1.14432448
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337153  2.54856958 -0.67550786 -2.69002202]. 
Working on 0.8 fold... 
[Mon Apr  1 16:32:37 2024]  Iteration number: 0 with current cost as 0.1455065000565824 and parameters 
[-2.51316813  2.23743464 -2.12427962 -0.11653103  0.55388708 -2.77010897
  3.068585    2.18960147  1.18552002 -1.06648307  0.60271513  1.14432447
  1.310299   -1.87354678  0.7296508   2.88578421 -0.54534335 -0.47522484
 -2.02654242  0.72897368  1.60512665  2.83077109 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.5485696  -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Apr  1 16:41:19 2024]  Iteration number: 0 with current cost as 0.1535315600554532 and parameters 
[-2.4813466   2.23743465 -2.1242796  -0.11653103  0.55388706 -2.77010899
  3.068585    2.18960143  1.18552    -1.0664831   0.60271512  1.14432447
  1.31029897 -1.87354682  0.72965079  2.88578419 -0.54534335 -0.47522483
 -2.02654242  0.72897368  1.60512662  2.83077107 -1.26456711 -0.25136105
 -2.3927922  -2.27309774  3.13337153  2.54856957 -0.67550787 -2.69002203]. 
Training complete taking 2610.0516545772552 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Apr  1 16:50:30 2024]  Iteration number: 0 with current cost as 0.20767318926165448 and parameters 
[-2.40376861  2.23743465 -2.1242796  -0.11653101  0.55388708 -2.77010897
  3.068585    2.18960145  1.18552002 -1.0664831   0.60271512  1.14432448
  1.31029899 -1.8735468   0.72965079  2.88578421 -0.54534335 -0.47522484
 -2.02654242  0.72897368  1.60512664  2.83077106 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550786 -2.69002203]. 
Working on 0.4 fold... 
[Mon Apr  1 16:59:26 2024]  Iteration number: 0 with current cost as 0.19813386690871543 and parameters 
[-2.39265684  2.23743464 -2.12427961 -0.116531    0.55388709 -2.77010896
  3.06858498  2.18960145  1.18552001 -1.0664831   0.60271513  1.14432448
  1.31029897 -1.87354679  0.7296508   2.88578421 -0.54534334 -0.47522485
 -2.02654243  0.72897367  1.60512664  2.83077107 -1.26456708 -0.25136103
 -2.39279216 -2.27309774  3.13337152  2.54856955 -0.67550789 -2.69002202]. 
Working on 0.6 fold... 
[Mon Apr  1 17:08:26 2024]  Iteration number: 0 with current cost as 0.19068530213583346 and parameters 
[-2.45716183  2.23743464 -2.12427961 -0.116531    0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552001 -1.06648308  0.6027151   1.14432448
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337153  2.54856958 -0.67550786 -2.69002202]. 
Working on 0.8 fold... 
[Mon Apr  1 17:17:09 2024]  Iteration number: 0 with current cost as 0.1455065000565824 and parameters 
[-2.51316813  2.23743464 -2.12427962 -0.11653103  0.55388708 -2.77010897
  3.068585    2.18960147  1.18552002 -1.06648307  0.60271513  1.14432447
  1.310299   -1.87354678  0.7296508   2.88578421 -0.54534335 -0.47522484
 -2.02654242  0.72897368  1.60512665  2.83077109 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.5485696  -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Apr  1 17:26:26 2024]  Iteration number: 0 with current cost as 0.1535315600554532 and parameters 
[-2.4813466   2.23743465 -2.1242796  -0.11653103  0.55388706 -2.77010899
  3.068585    2.18960143  1.18552    -1.0664831   0.60271512  1.14432447
  1.31029897 -1.87354682  0.72965079  2.88578419 -0.54534335 -0.47522483
 -2.02654242  0.72897368  1.60512662  2.83077107 -1.26456711 -0.25136105
 -2.3927922  -2.27309774  3.13337153  2.54856957 -0.67550787 -2.69002203]. 
Training complete taking 2698.5829565525055 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Apr  1 17:35:31 2024]  Iteration number: 0 with current cost as 0.20767318926165448 and parameters 
[-2.40376861  2.23743465 -2.1242796  -0.11653101  0.55388708 -2.77010897
  3.068585    2.18960145  1.18552002 -1.0664831   0.60271512  1.14432448
  1.31029899 -1.8735468   0.72965079  2.88578421 -0.54534335 -0.47522484
 -2.02654242  0.72897368  1.60512664  2.83077106 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550786 -2.69002203]. 
Working on 0.4 fold... 
[Mon Apr  1 17:44:21 2024]  Iteration number: 0 with current cost as 0.19813386690871543 and parameters 
[-2.39265684  2.23743464 -2.12427961 -0.116531    0.55388709 -2.77010896
  3.06858498  2.18960145  1.18552001 -1.0664831   0.60271513  1.14432448
  1.31029897 -1.87354679  0.7296508   2.88578421 -0.54534334 -0.47522485
 -2.02654243  0.72897367  1.60512664  2.83077107 -1.26456708 -0.25136103
 -2.39279216 -2.27309774  3.13337152  2.54856955 -0.67550789 -2.69002202]. 
Working on 0.6 fold... 
[Mon Apr  1 17:53:14 2024]  Iteration number: 0 with current cost as 0.19068530213583346 and parameters 
[-2.45716183  2.23743464 -2.12427961 -0.116531    0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552001 -1.06648308  0.6027151   1.14432448
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337153  2.54856958 -0.67550786 -2.69002202]. 
Working on 0.8 fold... 
[Mon Apr  1 18:01:49 2024]  Iteration number: 0 with current cost as 0.1455065000565824 and parameters 
[-2.51316813  2.23743464 -2.12427962 -0.11653103  0.55388708 -2.77010897
  3.068585    2.18960147  1.18552002 -1.06648307  0.60271513  1.14432447
  1.310299   -1.87354678  0.7296508   2.88578421 -0.54534335 -0.47522484
 -2.02654242  0.72897368  1.60512665  2.83077109 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.5485696  -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Apr  1 18:11:18 2024]  Iteration number: 0 with current cost as 0.1535315600554532 and parameters 
[-2.4813466   2.23743465 -2.1242796  -0.11653103  0.55388706 -2.77010899
  3.068585    2.18960143  1.18552    -1.0664831   0.60271512  1.14432447
  1.31029897 -1.87354682  0.72965079  2.88578419 -0.54534335 -0.47522483
 -2.02654242  0.72897368  1.60512662  2.83077107 -1.26456711 -0.25136105
 -2.3927922  -2.27309774  3.13337153  2.54856957 -0.67550787 -2.69002203]. 
Training complete taking 2694.687312602997 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Mon Apr  1 18:20:23 2024]  Iteration number: 0 with current cost as 0.20767318926165448 and parameters 
[-2.40376861  2.23743465 -2.1242796  -0.11653101  0.55388708 -2.77010897
  3.068585    2.18960145  1.18552002 -1.0664831   0.60271512  1.14432448
  1.31029899 -1.8735468   0.72965079  2.88578421 -0.54534335 -0.47522484
 -2.02654242  0.72897368  1.60512664  2.83077106 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550786 -2.69002203]. 
Working on 0.4 fold... 
[Mon Apr  1 18:29:21 2024]  Iteration number: 0 with current cost as 0.19813386690871543 and parameters 
[-2.39265684  2.23743464 -2.12427961 -0.116531    0.55388709 -2.77010896
  3.06858498  2.18960145  1.18552001 -1.0664831   0.60271513  1.14432448
  1.31029897 -1.87354679  0.7296508   2.88578421 -0.54534334 -0.47522485
 -2.02654243  0.72897367  1.60512664  2.83077107 -1.26456708 -0.25136103
 -2.39279216 -2.27309774  3.13337152  2.54856955 -0.67550789 -2.69002202]. 
Working on 0.6 fold... 
[Mon Apr  1 18:38:08 2024]  Iteration number: 0 with current cost as 0.19068530213583346 and parameters 
[-2.45716183  2.23743464 -2.12427961 -0.116531    0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552001 -1.06648308  0.6027151   1.14432448
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337153  2.54856958 -0.67550786 -2.69002202]. 
Working on 0.8 fold... 
[Mon Apr  1 18:46:56 2024]  Iteration number: 0 with current cost as 0.1455065000565824 and parameters 
[-2.51316813  2.23743464 -2.12427962 -0.11653103  0.55388708 -2.77010897
  3.068585    2.18960147  1.18552002 -1.06648307  0.60271513  1.14432447
  1.310299   -1.87354678  0.7296508   2.88578421 -0.54534335 -0.47522484
 -2.02654242  0.72897368  1.60512665  2.83077109 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.5485696  -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Apr  1 18:56:08 2024]  Iteration number: 0 with current cost as 0.1535315600554532 and parameters 
[-2.4813466   2.23743465 -2.1242796  -0.11653103  0.55388706 -2.77010899
  3.068585    2.18960143  1.18552    -1.0664831   0.60271512  1.14432447
  1.31029897 -1.87354682  0.72965079  2.88578419 -0.54534335 -0.47522483
 -2.02654242  0.72897368  1.60512662  2.83077107 -1.26456711 -0.25136105
 -2.3927922  -2.27309774  3.13337153  2.54856957 -0.67550787 -2.69002203]. 
Training complete taking 2679.723630666733 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Mon Apr  1 19:05:08 2024]  Iteration number: 0 with current cost as 0.20767318926165448 and parameters 
[-2.40376861  2.23743465 -2.1242796  -0.11653101  0.55388708 -2.77010897
  3.068585    2.18960145  1.18552002 -1.0664831   0.60271512  1.14432448
  1.31029899 -1.8735468   0.72965079  2.88578421 -0.54534335 -0.47522484
 -2.02654242  0.72897368  1.60512664  2.83077106 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550786 -2.69002203]. 
Working on 0.4 fold... 
[Mon Apr  1 19:14:09 2024]  Iteration number: 0 with current cost as 0.19813386690871543 and parameters 
[-2.39265684  2.23743464 -2.12427961 -0.116531    0.55388709 -2.77010896
  3.06858498  2.18960145  1.18552001 -1.0664831   0.60271513  1.14432448
  1.31029897 -1.87354679  0.7296508   2.88578421 -0.54534334 -0.47522485
 -2.02654243  0.72897367  1.60512664  2.83077107 -1.26456708 -0.25136103
 -2.39279216 -2.27309774  3.13337152  2.54856955 -0.67550789 -2.69002202]. 
Working on 0.6 fold... 
[Mon Apr  1 19:22:53 2024]  Iteration number: 0 with current cost as 0.19068530213583346 and parameters 
[-2.45716183  2.23743464 -2.12427961 -0.116531    0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552001 -1.06648308  0.6027151   1.14432448
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337153  2.54856958 -0.67550786 -2.69002202]. 
Working on 0.8 fold... 
[Mon Apr  1 19:32:05 2024]  Iteration number: 0 with current cost as 0.1455065000565824 and parameters 
[-2.51316813  2.23743464 -2.12427962 -0.11653103  0.55388708 -2.77010897
  3.068585    2.18960147  1.18552002 -1.06648307  0.60271513  1.14432447
  1.310299   -1.87354678  0.7296508   2.88578421 -0.54534335 -0.47522484
 -2.02654242  0.72897368  1.60512665  2.83077109 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.5485696  -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Apr  1 19:40:41 2024]  Iteration number: 0 with current cost as 0.1535315600554532 and parameters 
[-2.4813466   2.23743465 -2.1242796  -0.11653103  0.55388706 -2.77010899
  3.068585    2.18960143  1.18552    -1.0664831   0.60271512  1.14432447
  1.31029897 -1.87354682  0.72965079  2.88578419 -0.54534335 -0.47522483
 -2.02654242  0.72897368  1.60512662  2.83077107 -1.26456711 -0.25136105
 -2.3927922  -2.27309774  3.13337153  2.54856957 -0.67550787 -2.69002203]. 
Training complete taking 2663.0299928188324 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Apr  1 19:49:45 2024]  Iteration number: 0 with current cost as 0.20767318926165448 and parameters 
[-2.40376861  2.23743465 -2.1242796  -0.11653101  0.55388708 -2.77010897
  3.068585    2.18960145  1.18552002 -1.0664831   0.60271512  1.14432448
  1.31029899 -1.8735468   0.72965079  2.88578421 -0.54534335 -0.47522484
 -2.02654242  0.72897368  1.60512664  2.83077106 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550786 -2.69002203]. 
Working on 0.4 fold... 
[Mon Apr  1 19:58:31 2024]  Iteration number: 0 with current cost as 0.19813386690871543 and parameters 
[-2.39265684  2.23743464 -2.12427961 -0.116531    0.55388709 -2.77010896
  3.06858498  2.18960145  1.18552001 -1.0664831   0.60271513  1.14432448
  1.31029897 -1.87354679  0.7296508   2.88578421 -0.54534334 -0.47522485
 -2.02654243  0.72897367  1.60512664  2.83077107 -1.26456708 -0.25136103
 -2.39279216 -2.27309774  3.13337152  2.54856955 -0.67550789 -2.69002202]. 
Working on 0.6 fold... 
[Mon Apr  1 20:07:01 2024]  Iteration number: 0 with current cost as 0.19068530213583346 and parameters 
[-2.45716183  2.23743464 -2.12427961 -0.116531    0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552001 -1.06648308  0.6027151   1.14432448
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337153  2.54856958 -0.67550786 -2.69002202]. 
Working on 0.8 fold... 
[Mon Apr  1 20:15:39 2024]  Iteration number: 0 with current cost as 0.1455065000565824 and parameters 
[-2.51316813  2.23743464 -2.12427962 -0.11653103  0.55388708 -2.77010897
  3.068585    2.18960147  1.18552002 -1.06648307  0.60271513  1.14432447
  1.310299   -1.87354678  0.7296508   2.88578421 -0.54534335 -0.47522484
 -2.02654242  0.72897368  1.60512665  2.83077109 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.5485696  -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Apr  1 20:24:31 2024]  Iteration number: 0 with current cost as 0.1535315600554532 and parameters 
[-2.4813466   2.23743465 -2.1242796  -0.11653103  0.55388706 -2.77010899
  3.068585    2.18960143  1.18552    -1.0664831   0.60271512  1.14432447
  1.31029897 -1.87354682  0.72965079  2.88578419 -0.54534335 -0.47522483
 -2.02654242  0.72897368  1.60512662  2.83077107 -1.26456711 -0.25136105
 -2.3927922  -2.27309774  3.13337153  2.54856957 -0.67550787 -2.69002203]. 
Training complete taking 2636.1088650226593 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Apr  1 20:33:02 2024]  Iteration number: 0 with current cost as 0.20767318926165448 and parameters 
[-2.40376861  2.23743465 -2.1242796  -0.11653101  0.55388708 -2.77010897
  3.068585    2.18960145  1.18552002 -1.0664831   0.60271512  1.14432448
  1.31029899 -1.8735468   0.72965079  2.88578421 -0.54534335 -0.47522484
 -2.02654242  0.72897368  1.60512664  2.83077106 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550786 -2.69002203]. 
Working on 0.4 fold... 
[Mon Apr  1 20:41:32 2024]  Iteration number: 0 with current cost as 0.19813386690871543 and parameters 
[-2.39265684  2.23743464 -2.12427961 -0.116531    0.55388709 -2.77010896
  3.06858498  2.18960145  1.18552001 -1.0664831   0.60271513  1.14432448
  1.31029897 -1.87354679  0.7296508   2.88578421 -0.54534334 -0.47522485
 -2.02654243  0.72897367  1.60512664  2.83077107 -1.26456708 -0.25136103
 -2.39279216 -2.27309774  3.13337152  2.54856955 -0.67550789 -2.69002202]. 
Working on 0.6 fold... 
[Mon Apr  1 20:49:42 2024]  Iteration number: 0 with current cost as 0.19068530213583346 and parameters 
[-2.45716183  2.23743464 -2.12427961 -0.116531    0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552001 -1.06648308  0.6027151   1.14432448
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337153  2.54856958 -0.67550786 -2.69002202]. 
Working on 0.8 fold... 
[Mon Apr  1 20:58:09 2024]  Iteration number: 0 with current cost as 0.1455065000565824 and parameters 
[-2.51316813  2.23743464 -2.12427962 -0.11653103  0.55388708 -2.77010897
  3.068585    2.18960147  1.18552002 -1.06648307  0.60271513  1.14432447
  1.310299   -1.87354678  0.7296508   2.88578421 -0.54534335 -0.47522484
 -2.02654242  0.72897368  1.60512665  2.83077109 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.5485696  -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Apr  1 21:07:04 2024]  Iteration number: 0 with current cost as 0.1535315600554532 and parameters 
[-2.4813466   2.23743465 -2.1242796  -0.11653103  0.55388706 -2.77010899
  3.068585    2.18960143  1.18552    -1.0664831   0.60271512  1.14432447
  1.31029897 -1.87354682  0.72965079  2.88578419 -0.54534335 -0.47522483
 -2.02654242  0.72897368  1.60512662  2.83077107 -1.26456711 -0.25136105
 -2.3927922  -2.27309774  3.13337153  2.54856957 -0.67550787 -2.69002203]. 
Training complete taking 2551.187716960907 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Apr  1 21:15:47 2024]  Iteration number: 0 with current cost as 0.20767318926165448 and parameters 
[-2.40376861  2.23743465 -2.1242796  -0.11653101  0.55388708 -2.77010897
  3.068585    2.18960145  1.18552002 -1.0664831   0.60271512  1.14432448
  1.31029899 -1.8735468   0.72965079  2.88578421 -0.54534335 -0.47522484
 -2.02654242  0.72897368  1.60512664  2.83077106 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550786 -2.69002203]. 
Working on 0.4 fold... 
[Mon Apr  1 21:24:30 2024]  Iteration number: 0 with current cost as 0.19813386690871543 and parameters 
[-2.39265684  2.23743464 -2.12427961 -0.116531    0.55388709 -2.77010896
  3.06858498  2.18960145  1.18552001 -1.0664831   0.60271513  1.14432448
  1.31029897 -1.87354679  0.7296508   2.88578421 -0.54534334 -0.47522485
 -2.02654243  0.72897367  1.60512664  2.83077107 -1.26456708 -0.25136103
 -2.39279216 -2.27309774  3.13337152  2.54856955 -0.67550789 -2.69002202]. 
Working on 0.6 fold... 
[Mon Apr  1 21:33:12 2024]  Iteration number: 0 with current cost as 0.19068530213583346 and parameters 
[-2.45716183  2.23743464 -2.12427961 -0.116531    0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552001 -1.06648308  0.6027151   1.14432448
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337153  2.54856958 -0.67550786 -2.69002202]. 
Working on 0.8 fold... 
[Mon Apr  1 21:41:52 2024]  Iteration number: 0 with current cost as 0.1455065000565824 and parameters 
[-2.51316813  2.23743464 -2.12427962 -0.11653103  0.55388708 -2.77010897
  3.068585    2.18960147  1.18552002 -1.06648307  0.60271513  1.14432447
  1.310299   -1.87354678  0.7296508   2.88578421 -0.54534335 -0.47522484
 -2.02654242  0.72897368  1.60512665  2.83077109 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.5485696  -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Apr  1 21:50:29 2024]  Iteration number: 0 with current cost as 0.1535315600554532 and parameters 
[-2.4813466   2.23743465 -2.1242796  -0.11653103  0.55388706 -2.77010899
  3.068585    2.18960143  1.18552    -1.0664831   0.60271512  1.14432447
  1.31029897 -1.87354682  0.72965079  2.88578419 -0.54534335 -0.47522483
 -2.02654242  0.72897368  1.60512662  2.83077107 -1.26456711 -0.25136105
 -2.3927922  -2.27309774  3.13337153  2.54856957 -0.67550787 -2.69002203]. 
Training complete taking 2600.0431339740753 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Mon Apr  1 21:58:53 2024]  Iteration number: 0 with current cost as 0.20767318926165448 and parameters 
[-2.40376861  2.23743465 -2.1242796  -0.11653101  0.55388708 -2.77010897
  3.068585    2.18960145  1.18552002 -1.0664831   0.60271512  1.14432448
  1.31029899 -1.8735468   0.72965079  2.88578421 -0.54534335 -0.47522484
 -2.02654242  0.72897368  1.60512664  2.83077106 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550786 -2.69002203]. 
Working on 0.4 fold... 
[Mon Apr  1 22:07:23 2024]  Iteration number: 0 with current cost as 0.19813386690871543 and parameters 
[-2.39265684  2.23743464 -2.12427961 -0.116531    0.55388709 -2.77010896
  3.06858498  2.18960145  1.18552001 -1.0664831   0.60271513  1.14432448
  1.31029897 -1.87354679  0.7296508   2.88578421 -0.54534334 -0.47522485
 -2.02654243  0.72897367  1.60512664  2.83077107 -1.26456708 -0.25136103
 -2.39279216 -2.27309774  3.13337152  2.54856955 -0.67550789 -2.69002202]. 
Working on 0.6 fold... 
[Mon Apr  1 22:15:54 2024]  Iteration number: 0 with current cost as 0.19068530213583346 and parameters 
[-2.45716183  2.23743464 -2.12427961 -0.116531    0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552001 -1.06648308  0.6027151   1.14432448
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337153  2.54856958 -0.67550786 -2.69002202]. 
Working on 0.8 fold... 
[Mon Apr  1 22:24:38 2024]  Iteration number: 0 with current cost as 0.1455065000565824 and parameters 
[-2.51316813  2.23743464 -2.12427962 -0.11653103  0.55388708 -2.77010897
  3.068585    2.18960147  1.18552002 -1.06648307  0.60271513  1.14432447
  1.310299   -1.87354678  0.7296508   2.88578421 -0.54534335 -0.47522484
 -2.02654242  0.72897368  1.60512665  2.83077109 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.5485696  -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Apr  1 22:33:14 2024]  Iteration number: 0 with current cost as 0.1535315600554532 and parameters 
[-2.4813466   2.23743465 -2.1242796  -0.11653103  0.55388706 -2.77010899
  3.068585    2.18960143  1.18552    -1.0664831   0.60271512  1.14432447
  1.31029897 -1.87354682  0.72965079  2.88578419 -0.54534335 -0.47522483
 -2.02654242  0.72897368  1.60512662  2.83077107 -1.26456711 -0.25136105
 -2.3927922  -2.27309774  3.13337153  2.54856957 -0.67550787 -2.69002203]. 
Training complete taking 2570.6722292900085 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Mon Apr  1 22:41:40 2024]  Iteration number: 0 with current cost as 0.20767318926165448 and parameters 
[-2.40376861  2.23743465 -2.1242796  -0.11653101  0.55388708 -2.77010897
  3.068585    2.18960145  1.18552002 -1.0664831   0.60271512  1.14432448
  1.31029899 -1.8735468   0.72965079  2.88578421 -0.54534335 -0.47522484
 -2.02654242  0.72897368  1.60512664  2.83077106 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550786 -2.69002203]. 
Working on 0.4 fold... 
[Mon Apr  1 22:50:11 2024]  Iteration number: 0 with current cost as 0.19813386690871543 and parameters 
[-2.39265684  2.23743464 -2.12427961 -0.116531    0.55388709 -2.77010896
  3.06858498  2.18960145  1.18552001 -1.0664831   0.60271513  1.14432448
  1.31029897 -1.87354679  0.7296508   2.88578421 -0.54534334 -0.47522485
 -2.02654243  0.72897367  1.60512664  2.83077107 -1.26456708 -0.25136103
 -2.39279216 -2.27309774  3.13337152  2.54856955 -0.67550789 -2.69002202]. 
Working on 0.6 fold... 
[Mon Apr  1 22:58:33 2024]  Iteration number: 0 with current cost as 0.19068530213583346 and parameters 
[-2.45716183  2.23743464 -2.12427961 -0.116531    0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552001 -1.06648308  0.6027151   1.14432448
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337153  2.54856958 -0.67550786 -2.69002202]. 
Working on 0.8 fold... 
[Mon Apr  1 23:07:14 2024]  Iteration number: 0 with current cost as 0.1455065000565824 and parameters 
[-2.51316813  2.23743464 -2.12427962 -0.11653103  0.55388708 -2.77010897
  3.068585    2.18960147  1.18552002 -1.06648307  0.60271513  1.14432447
  1.310299   -1.87354678  0.7296508   2.88578421 -0.54534335 -0.47522484
 -2.02654242  0.72897368  1.60512665  2.83077109 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.5485696  -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Apr  1 23:15:31 2024]  Iteration number: 0 with current cost as 0.1535315600554532 and parameters 
[-2.4813466   2.23743465 -2.1242796  -0.11653103  0.55388706 -2.77010899
  3.068585    2.18960143  1.18552    -1.0664831   0.60271512  1.14432447
  1.31029897 -1.87354682  0.72965079  2.88578419 -0.54534335 -0.47522483
 -2.02654242  0.72897368  1.60512662  2.83077107 -1.26456711 -0.25136105
 -2.3927922  -2.27309774  3.13337153  2.54856957 -0.67550787 -2.69002203]. 
Training complete taking 2524.9281034469604 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Apr  1 23:23:54 2024]  Iteration number: 0 with current cost as 0.20767318926165448 and parameters 
[-2.40376861  2.23743465 -2.1242796  -0.11653101  0.55388708 -2.77010897
  3.068585    2.18960145  1.18552002 -1.0664831   0.60271512  1.14432448
  1.31029899 -1.8735468   0.72965079  2.88578421 -0.54534335 -0.47522484
 -2.02654242  0.72897368  1.60512664  2.83077106 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550786 -2.69002203]. 
Working on 0.4 fold... 
[Mon Apr  1 23:32:18 2024]  Iteration number: 0 with current cost as 0.19813386690871543 and parameters 
[-2.39265684  2.23743464 -2.12427961 -0.116531    0.55388709 -2.77010896
  3.06858498  2.18960145  1.18552001 -1.0664831   0.60271513  1.14432448
  1.31029897 -1.87354679  0.7296508   2.88578421 -0.54534334 -0.47522485
 -2.02654243  0.72897367  1.60512664  2.83077107 -1.26456708 -0.25136103
 -2.39279216 -2.27309774  3.13337152  2.54856955 -0.67550789 -2.69002202]. 
Working on 0.6 fold... 
[Mon Apr  1 23:40:40 2024]  Iteration number: 0 with current cost as 0.19068530213583346 and parameters 
[-2.45716183  2.23743464 -2.12427961 -0.116531    0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552001 -1.06648308  0.6027151   1.14432448
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337153  2.54856958 -0.67550786 -2.69002202]. 
Working on 0.8 fold... 
[Mon Apr  1 23:48:55 2024]  Iteration number: 0 with current cost as 0.1455065000565824 and parameters 
[-2.51316813  2.23743464 -2.12427962 -0.11653103  0.55388708 -2.77010897
  3.068585    2.18960147  1.18552002 -1.06648307  0.60271513  1.14432447
  1.310299   -1.87354678  0.7296508   2.88578421 -0.54534335 -0.47522484
 -2.02654242  0.72897368  1.60512665  2.83077109 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.5485696  -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Apr  1 23:57:18 2024]  Iteration number: 0 with current cost as 0.1535315600554532 and parameters 
[-2.4813466   2.23743465 -2.1242796  -0.11653103  0.55388706 -2.77010899
  3.068585    2.18960143  1.18552    -1.0664831   0.60271512  1.14432447
  1.31029897 -1.87354682  0.72965079  2.88578419 -0.54534335 -0.47522483
 -2.02654242  0.72897368  1.60512662  2.83077107 -1.26456711 -0.25136105
 -2.3927922  -2.27309774  3.13337153  2.54856957 -0.67550787 -2.69002203]. 
Training complete taking 2509.7124190330505 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Tue Apr  2 00:05:43 2024]  Iteration number: 0 with current cost as 0.20767318926165448 and parameters 
[-2.40376861  2.23743465 -2.1242796  -0.11653101  0.55388708 -2.77010897
  3.068585    2.18960145  1.18552002 -1.0664831   0.60271512  1.14432448
  1.31029899 -1.8735468   0.72965079  2.88578421 -0.54534335 -0.47522484
 -2.02654242  0.72897368  1.60512664  2.83077106 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550786 -2.69002203]. 
Working on 0.4 fold... 
[Tue Apr  2 00:14:18 2024]  Iteration number: 0 with current cost as 0.19813386690871543 and parameters 
[-2.39265684  2.23743464 -2.12427961 -0.116531    0.55388709 -2.77010896
  3.06858498  2.18960145  1.18552001 -1.0664831   0.60271513  1.14432448
  1.31029897 -1.87354679  0.7296508   2.88578421 -0.54534334 -0.47522485
 -2.02654243  0.72897367  1.60512664  2.83077107 -1.26456708 -0.25136103
 -2.39279216 -2.27309774  3.13337152  2.54856955 -0.67550789 -2.69002202]. 
Working on 0.6 fold... 
[Tue Apr  2 00:22:31 2024]  Iteration number: 0 with current cost as 0.19068530213583346 and parameters 
[-2.45716183  2.23743464 -2.12427961 -0.116531    0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552001 -1.06648308  0.6027151   1.14432448
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337153  2.54856958 -0.67550786 -2.69002202]. 
Working on 0.8 fold... 
[Tue Apr  2 00:30:55 2024]  Iteration number: 0 with current cost as 0.1455065000565824 and parameters 
[-2.51316813  2.23743464 -2.12427962 -0.11653103  0.55388708 -2.77010897
  3.068585    2.18960147  1.18552002 -1.06648307  0.60271513  1.14432447
  1.310299   -1.87354678  0.7296508   2.88578421 -0.54534335 -0.47522484
 -2.02654242  0.72897368  1.60512665  2.83077109 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.5485696  -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Tue Apr  2 00:39:15 2024]  Iteration number: 0 with current cost as 0.1535315600554532 and parameters 
[-2.4813466   2.23743465 -2.1242796  -0.11653103  0.55388706 -2.77010899
  3.068585    2.18960143  1.18552    -1.0664831   0.60271512  1.14432447
  1.31029897 -1.87354682  0.72965079  2.88578419 -0.54534335 -0.47522483
 -2.02654242  0.72897368  1.60512662  2.83077107 -1.26456711 -0.25136105
 -2.3927922  -2.27309774  3.13337153  2.54856957 -0.67550787 -2.69002203]. 
Training complete taking 2516.248626470566 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Tue Apr  2 00:47:40 2024]  Iteration number: 0 with current cost as 0.20767318926165448 and parameters 
[-2.40376861  2.23743465 -2.1242796  -0.11653101  0.55388708 -2.77010897
  3.068585    2.18960145  1.18552002 -1.0664831   0.60271512  1.14432448
  1.31029899 -1.8735468   0.72965079  2.88578421 -0.54534335 -0.47522484
 -2.02654242  0.72897368  1.60512664  2.83077106 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550786 -2.69002203]. 
Working on 0.4 fold... 
[Tue Apr  2 00:56:01 2024]  Iteration number: 0 with current cost as 0.19813386690871543 and parameters 
[-2.39265684  2.23743464 -2.12427961 -0.116531    0.55388709 -2.77010896
  3.06858498  2.18960145  1.18552001 -1.0664831   0.60271513  1.14432448
  1.31029897 -1.87354679  0.7296508   2.88578421 -0.54534334 -0.47522485
 -2.02654243  0.72897367  1.60512664  2.83077107 -1.26456708 -0.25136103
 -2.39279216 -2.27309774  3.13337152  2.54856955 -0.67550789 -2.69002202]. 
Working on 0.6 fold... 
[Tue Apr  2 01:04:29 2024]  Iteration number: 0 with current cost as 0.19068530213583346 and parameters 
[-2.45716183  2.23743464 -2.12427961 -0.116531    0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552001 -1.06648308  0.6027151   1.14432448
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337153  2.54856958 -0.67550786 -2.69002202]. 
Working on 0.8 fold... 
[Tue Apr  2 01:12:52 2024]  Iteration number: 0 with current cost as 0.1455065000565824 and parameters 
[-2.51316813  2.23743464 -2.12427962 -0.11653103  0.55388708 -2.77010897
  3.068585    2.18960147  1.18552002 -1.06648307  0.60271513  1.14432447
  1.310299   -1.87354678  0.7296508   2.88578421 -0.54534335 -0.47522484
 -2.02654242  0.72897368  1.60512665  2.83077109 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.5485696  -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Tue Apr  2 01:21:13 2024]  Iteration number: 0 with current cost as 0.1535315600554532 and parameters 
[-2.4813466   2.23743465 -2.1242796  -0.11653103  0.55388706 -2.77010899
  3.068585    2.18960143  1.18552    -1.0664831   0.60271512  1.14432447
  1.31029897 -1.87354682  0.72965079  2.88578419 -0.54534335 -0.47522483
 -2.02654242  0.72897368  1.60512662  2.83077107 -1.26456711 -0.25136105
 -2.3927922  -2.27309774  3.13337153  2.54856957 -0.67550787 -2.69002203]. 
Training complete taking 2519.6796889305115 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Tue Apr  2 01:29:30 2024]  Iteration number: 0 with current cost as 0.20767318926165448 and parameters 
[-2.40376861  2.23743465 -2.1242796  -0.11653101  0.55388708 -2.77010897
  3.068585    2.18960145  1.18552002 -1.0664831   0.60271512  1.14432448
  1.31029899 -1.8735468   0.72965079  2.88578421 -0.54534335 -0.47522484
 -2.02654242  0.72897368  1.60512664  2.83077106 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550786 -2.69002203]. 
Working on 0.4 fold... 
[Tue Apr  2 01:37:51 2024]  Iteration number: 0 with current cost as 0.19813386690871543 and parameters 
[-2.39265684  2.23743464 -2.12427961 -0.116531    0.55388709 -2.77010896
  3.06858498  2.18960145  1.18552001 -1.0664831   0.60271513  1.14432448
  1.31029897 -1.87354679  0.7296508   2.88578421 -0.54534334 -0.47522485
 -2.02654243  0.72897367  1.60512664  2.83077107 -1.26456708 -0.25136103
 -2.39279216 -2.27309774  3.13337152  2.54856955 -0.67550789 -2.69002202]. 
Working on 0.6 fold... 
[Tue Apr  2 01:46:19 2024]  Iteration number: 0 with current cost as 0.19068530213583346 and parameters 
[-2.45716183  2.23743464 -2.12427961 -0.116531    0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552001 -1.06648308  0.6027151   1.14432448
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337153  2.54856958 -0.67550786 -2.69002202]. 
Working on 0.8 fold... 
[Tue Apr  2 01:54:37 2024]  Iteration number: 0 with current cost as 0.1455065000565824 and parameters 
[-2.51316813  2.23743464 -2.12427962 -0.11653103  0.55388708 -2.77010897
  3.068585    2.18960147  1.18552002 -1.06648307  0.60271513  1.14432447
  1.310299   -1.87354678  0.7296508   2.88578421 -0.54534335 -0.47522484
 -2.02654242  0.72897368  1.60512665  2.83077109 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.5485696  -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Tue Apr  2 02:03:03 2024]  Iteration number: 0 with current cost as 0.1535315600554532 and parameters 
[-2.4813466   2.23743465 -2.1242796  -0.11653103  0.55388706 -2.77010899
  3.068585    2.18960143  1.18552    -1.0664831   0.60271512  1.14432447
  1.31029897 -1.87354682  0.72965079  2.88578419 -0.54534335 -0.47522483
 -2.02654242  0.72897368  1.60512662  2.83077107 -1.26456711 -0.25136105
 -2.3927922  -2.27309774  3.13337153  2.54856957 -0.67550787 -2.69002203]. 
Training complete taking 2507.4800062179565 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Tue Apr  2 02:11:18 2024]  Iteration number: 0 with current cost as 0.20767318926165448 and parameters 
[-2.40376861  2.23743465 -2.1242796  -0.11653101  0.55388708 -2.77010897
  3.068585    2.18960145  1.18552002 -1.0664831   0.60271512  1.14432448
  1.31029899 -1.8735468   0.72965079  2.88578421 -0.54534335 -0.47522484
 -2.02654242  0.72897368  1.60512664  2.83077106 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550786 -2.69002203]. 
Working on 0.4 fold... 
[Tue Apr  2 02:19:29 2024]  Iteration number: 0 with current cost as 0.19813386690871543 and parameters 
[-2.39265684  2.23743464 -2.12427961 -0.116531    0.55388709 -2.77010896
  3.06858498  2.18960145  1.18552001 -1.0664831   0.60271513  1.14432448
  1.31029897 -1.87354679  0.7296508   2.88578421 -0.54534334 -0.47522485
 -2.02654243  0.72897367  1.60512664  2.83077107 -1.26456708 -0.25136103
 -2.39279216 -2.27309774  3.13337152  2.54856955 -0.67550789 -2.69002202]. 
Working on 0.6 fold... 
[Tue Apr  2 02:27:35 2024]  Iteration number: 0 with current cost as 0.19068530213583346 and parameters 
[-2.45716183  2.23743464 -2.12427961 -0.116531    0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552001 -1.06648308  0.6027151   1.14432448
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337153  2.54856958 -0.67550786 -2.69002202]. 
Working on 0.8 fold... 
[Tue Apr  2 02:36:25 2024]  Iteration number: 0 with current cost as 0.1455065000565824 and parameters 
[-2.51316813  2.23743464 -2.12427962 -0.11653103  0.55388708 -2.77010897
  3.068585    2.18960147  1.18552002 -1.06648307  0.60271513  1.14432447
  1.310299   -1.87354678  0.7296508   2.88578421 -0.54534335 -0.47522484
 -2.02654242  0.72897368  1.60512665  2.83077109 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.5485696  -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Tue Apr  2 02:44:59 2024]  Iteration number: 0 with current cost as 0.1535315600554532 and parameters 
[-2.4813466   2.23743465 -2.1242796  -0.11653103  0.55388706 -2.77010899
  3.068585    2.18960143  1.18552    -1.0664831   0.60271512  1.14432447
  1.31029897 -1.87354682  0.72965079  2.88578419 -0.54534335 -0.47522483
 -2.02654242  0.72897368  1.60512662  2.83077107 -1.26456711 -0.25136105
 -2.3927922  -2.27309774  3.13337153  2.54856957 -0.67550787 -2.69002203]. 
Training complete taking 2517.074046611786 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Tue Apr  2 02:53:35 2024]  Iteration number: 0 with current cost as 0.20767318926165448 and parameters 
[-2.40376861  2.23743465 -2.1242796  -0.11653101  0.55388708 -2.77010897
  3.068585    2.18960145  1.18552002 -1.0664831   0.60271512  1.14432448
  1.31029899 -1.8735468   0.72965079  2.88578421 -0.54534335 -0.47522484
 -2.02654242  0.72897368  1.60512664  2.83077106 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550786 -2.69002203]. 
Working on 0.4 fold... 
[Tue Apr  2 03:02:31 2024]  Iteration number: 0 with current cost as 0.19813386690871543 and parameters 
[-2.39265684  2.23743464 -2.12427961 -0.116531    0.55388709 -2.77010896
  3.06858498  2.18960145  1.18552001 -1.0664831   0.60271513  1.14432448
  1.31029897 -1.87354679  0.7296508   2.88578421 -0.54534334 -0.47522485
 -2.02654243  0.72897367  1.60512664  2.83077107 -1.26456708 -0.25136103
 -2.39279216 -2.27309774  3.13337152  2.54856955 -0.67550789 -2.69002202]. 
Working on 0.6 fold... 
[Tue Apr  2 03:11:30 2024]  Iteration number: 0 with current cost as 0.19068530213583346 and parameters 
[-2.45716183  2.23743464 -2.12427961 -0.116531    0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552001 -1.06648308  0.6027151   1.14432448
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337153  2.54856958 -0.67550786 -2.69002202]. 
Working on 0.8 fold... 
[Tue Apr  2 03:19:52 2024]  Iteration number: 0 with current cost as 0.1455065000565824 and parameters 
[-2.51316813  2.23743464 -2.12427962 -0.11653103  0.55388708 -2.77010897
  3.068585    2.18960147  1.18552002 -1.06648307  0.60271513  1.14432447
  1.310299   -1.87354678  0.7296508   2.88578421 -0.54534335 -0.47522484
 -2.02654242  0.72897368  1.60512665  2.83077109 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.5485696  -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Tue Apr  2 03:28:07 2024]  Iteration number: 0 with current cost as 0.1535315600554532 and parameters 
[-2.4813466   2.23743465 -2.1242796  -0.11653103  0.55388706 -2.77010899
  3.068585    2.18960143  1.18552    -1.0664831   0.60271512  1.14432447
  1.31029897 -1.87354682  0.72965079  2.88578419 -0.54534335 -0.47522483
 -2.02654242  0.72897368  1.60512662  2.83077107 -1.26456711 -0.25136105
 -2.3927922  -2.27309774  3.13337153  2.54856957 -0.67550787 -2.69002203]. 
Training complete taking 2592.006982564926 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Tue Apr  2 03:36:31 2024]  Iteration number: 0 with current cost as 0.20767318926165448 and parameters 
[-2.40376861  2.23743465 -2.1242796  -0.11653101  0.55388708 -2.77010897
  3.068585    2.18960145  1.18552002 -1.0664831   0.60271512  1.14432448
  1.31029899 -1.8735468   0.72965079  2.88578421 -0.54534335 -0.47522484
 -2.02654242  0.72897368  1.60512664  2.83077106 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550786 -2.69002203]. 
Working on 0.4 fold... 
[Tue Apr  2 03:45:01 2024]  Iteration number: 0 with current cost as 0.19813386690871543 and parameters 
[-2.39265684  2.23743464 -2.12427961 -0.116531    0.55388709 -2.77010896
  3.06858498  2.18960145  1.18552001 -1.0664831   0.60271513  1.14432448
  1.31029897 -1.87354679  0.7296508   2.88578421 -0.54534334 -0.47522485
 -2.02654243  0.72897367  1.60512664  2.83077107 -1.26456708 -0.25136103
 -2.39279216 -2.27309774  3.13337152  2.54856955 -0.67550789 -2.69002202]. 
Working on 0.6 fold... 
[Tue Apr  2 03:53:49 2024]  Iteration number: 0 with current cost as 0.19068530213583346 and parameters 
[-2.45716183  2.23743464 -2.12427961 -0.116531    0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552001 -1.06648308  0.6027151   1.14432448
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337153  2.54856958 -0.67550786 -2.69002202]. 
Working on 0.8 fold... 
[Tue Apr  2 04:02:30 2024]  Iteration number: 0 with current cost as 0.1455065000565824 and parameters 
[-2.51316813  2.23743464 -2.12427962 -0.11653103  0.55388708 -2.77010897
  3.068585    2.18960147  1.18552002 -1.06648307  0.60271513  1.14432447
  1.310299   -1.87354678  0.7296508   2.88578421 -0.54534335 -0.47522484
 -2.02654242  0.72897368  1.60512665  2.83077109 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.5485696  -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Tue Apr  2 04:10:56 2024]  Iteration number: 0 with current cost as 0.1535315600554532 and parameters 
[-2.4813466   2.23743465 -2.1242796  -0.11653103  0.55388706 -2.77010899
  3.068585    2.18960143  1.18552    -1.0664831   0.60271512  1.14432447
  1.31029897 -1.87354682  0.72965079  2.88578419 -0.54534335 -0.47522483
 -2.02654242  0.72897368  1.60512662  2.83077107 -1.26456711 -0.25136105
 -2.3927922  -2.27309774  3.13337153  2.54856957 -0.67550787 -2.69002203]. 
Training complete taking 2576.860955476761 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Tue Apr  2 04:19:35 2024]  Iteration number: 0 with current cost as 0.20767318926165448 and parameters 
[-2.40376861  2.23743465 -2.1242796  -0.11653101  0.55388708 -2.77010897
  3.068585    2.18960145  1.18552002 -1.0664831   0.60271512  1.14432448
  1.31029899 -1.8735468   0.72965079  2.88578421 -0.54534335 -0.47522484
 -2.02654242  0.72897368  1.60512664  2.83077106 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550786 -2.69002203]. 
Working on 0.4 fold... 
[Tue Apr  2 04:27:51 2024]  Iteration number: 0 with current cost as 0.19813386690871543 and parameters 
[-2.39265684  2.23743464 -2.12427961 -0.116531    0.55388709 -2.77010896
  3.06858498  2.18960145  1.18552001 -1.0664831   0.60271513  1.14432448
  1.31029897 -1.87354679  0.7296508   2.88578421 -0.54534334 -0.47522485
 -2.02654243  0.72897367  1.60512664  2.83077107 -1.26456708 -0.25136103
 -2.39279216 -2.27309774  3.13337152  2.54856955 -0.67550789 -2.69002202]. 
Working on 0.6 fold... 
[Tue Apr  2 04:36:10 2024]  Iteration number: 0 with current cost as 0.19068530213583346 and parameters 
[-2.45716183  2.23743464 -2.12427961 -0.116531    0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552001 -1.06648308  0.6027151   1.14432448
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337153  2.54856958 -0.67550786 -2.69002202]. 
Working on 0.8 fold... 
[Tue Apr  2 04:44:12 2024]  Iteration number: 0 with current cost as 0.1455065000565824 and parameters 
[-2.51316813  2.23743464 -2.12427962 -0.11653103  0.55388708 -2.77010897
  3.068585    2.18960147  1.18552002 -1.06648307  0.60271513  1.14432447
  1.310299   -1.87354678  0.7296508   2.88578421 -0.54534335 -0.47522484
 -2.02654242  0.72897368  1.60512665  2.83077109 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.5485696  -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Tue Apr  2 04:52:54 2024]  Iteration number: 0 with current cost as 0.1535315600554532 and parameters 
[-2.4813466   2.23743465 -2.1242796  -0.11653103  0.55388706 -2.77010899
  3.068585    2.18960143  1.18552    -1.0664831   0.60271512  1.14432447
  1.31029897 -1.87354682  0.72965079  2.88578419 -0.54534335 -0.47522483
 -2.02654242  0.72897368  1.60512662  2.83077107 -1.26456711 -0.25136105
 -2.3927922  -2.27309774  3.13337153  2.54856957 -0.67550787 -2.69002203]. 
Training complete taking 2499.405986070633 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Tue Apr  2 05:01:20 2024]  Iteration number: 0 with current cost as 0.20767318926165448 and parameters 
[-2.40376861  2.23743465 -2.1242796  -0.11653101  0.55388708 -2.77010897
  3.068585    2.18960145  1.18552002 -1.0664831   0.60271512  1.14432448
  1.31029899 -1.8735468   0.72965079  2.88578421 -0.54534335 -0.47522484
 -2.02654242  0.72897368  1.60512664  2.83077106 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550786 -2.69002203]. 
Working on 0.4 fold... 
[Tue Apr  2 05:09:37 2024]  Iteration number: 0 with current cost as 0.19813386690871543 and parameters 
[-2.39265684  2.23743464 -2.12427961 -0.116531    0.55388709 -2.77010896
  3.06858498  2.18960145  1.18552001 -1.0664831   0.60271513  1.14432448
  1.31029897 -1.87354679  0.7296508   2.88578421 -0.54534334 -0.47522485
 -2.02654243  0.72897367  1.60512664  2.83077107 -1.26456708 -0.25136103
 -2.39279216 -2.27309774  3.13337152  2.54856955 -0.67550789 -2.69002202]. 
Working on 0.6 fold... 
[Tue Apr  2 05:17:44 2024]  Iteration number: 0 with current cost as 0.19068530213583346 and parameters 
[-2.45716183  2.23743464 -2.12427961 -0.116531    0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552001 -1.06648308  0.6027151   1.14432448
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337153  2.54856958 -0.67550786 -2.69002202]. 
Working on 0.8 fold... 
[Tue Apr  2 05:26:05 2024]  Iteration number: 0 with current cost as 0.1455065000565824 and parameters 
[-2.51316813  2.23743464 -2.12427962 -0.11653103  0.55388708 -2.77010897
  3.068585    2.18960147  1.18552002 -1.06648307  0.60271513  1.14432447
  1.310299   -1.87354678  0.7296508   2.88578421 -0.54534335 -0.47522484
 -2.02654242  0.72897368  1.60512665  2.83077109 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.5485696  -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Tue Apr  2 05:34:20 2024]  Iteration number: 0 with current cost as 0.1535315600554532 and parameters 
[-2.4813466   2.23743465 -2.1242796  -0.11653103  0.55388706 -2.77010899
  3.068585    2.18960143  1.18552    -1.0664831   0.60271512  1.14432447
  1.31029897 -1.87354682  0.72965079  2.88578419 -0.54534335 -0.47522483
 -2.02654242  0.72897368  1.60512662  2.83077107 -1.26456711 -0.25136105
 -2.3927922  -2.27309774  3.13337153  2.54856957 -0.67550787 -2.69002203]. 
Training complete taking 2490.826481103897 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Tue Apr  2 05:42:46 2024]  Iteration number: 0 with current cost as 0.20767318926165448 and parameters 
[-2.40376861  2.23743465 -2.1242796  -0.11653101  0.55388708 -2.77010897
  3.068585    2.18960145  1.18552002 -1.0664831   0.60271512  1.14432448
  1.31029899 -1.8735468   0.72965079  2.88578421 -0.54534335 -0.47522484
 -2.02654242  0.72897368  1.60512664  2.83077106 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550786 -2.69002203]. 
Working on 0.4 fold... 
[Tue Apr  2 05:50:51 2024]  Iteration number: 0 with current cost as 0.19813386690871543 and parameters 
[-2.39265684  2.23743464 -2.12427961 -0.116531    0.55388709 -2.77010896
  3.06858498  2.18960145  1.18552001 -1.0664831   0.60271513  1.14432448
  1.31029897 -1.87354679  0.7296508   2.88578421 -0.54534334 -0.47522485
 -2.02654243  0.72897367  1.60512664  2.83077107 -1.26456708 -0.25136103
 -2.39279216 -2.27309774  3.13337152  2.54856955 -0.67550789 -2.69002202]. 
Working on 0.6 fold... 
[Tue Apr  2 05:59:09 2024]  Iteration number: 0 with current cost as 0.19068530213583346 and parameters 
[-2.45716183  2.23743464 -2.12427961 -0.116531    0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552001 -1.06648308  0.6027151   1.14432448
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337153  2.54856958 -0.67550786 -2.69002202]. 
Working on 0.8 fold... 
[Tue Apr  2 06:07:18 2024]  Iteration number: 0 with current cost as 0.1455065000565824 and parameters 
[-2.51316813  2.23743464 -2.12427962 -0.11653103  0.55388708 -2.77010897
  3.068585    2.18960147  1.18552002 -1.06648307  0.60271513  1.14432447
  1.310299   -1.87354678  0.7296508   2.88578421 -0.54534335 -0.47522484
 -2.02654242  0.72897368  1.60512665  2.83077109 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.5485696  -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Tue Apr  2 06:16:01 2024]  Iteration number: 0 with current cost as 0.1535315600554532 and parameters 
[-2.4813466   2.23743465 -2.1242796  -0.11653103  0.55388706 -2.77010899
  3.068585    2.18960143  1.18552    -1.0664831   0.60271512  1.14432447
  1.31029897 -1.87354682  0.72965079  2.88578419 -0.54534335 -0.47522483
 -2.02654242  0.72897368  1.60512662  2.83077107 -1.26456711 -0.25136105
 -2.3927922  -2.27309774  3.13337153  2.54856957 -0.67550787 -2.69002203]. 
Training complete taking 2503.193069934845 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Tue Apr  2 06:24:17 2024]  Iteration number: 0 with current cost as 0.20767318926165448 and parameters 
[-2.40376861  2.23743465 -2.1242796  -0.11653101  0.55388708 -2.77010897
  3.068585    2.18960145  1.18552002 -1.0664831   0.60271512  1.14432448
  1.31029899 -1.8735468   0.72965079  2.88578421 -0.54534335 -0.47522484
 -2.02654242  0.72897368  1.60512664  2.83077106 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550786 -2.69002203]. 
Working on 0.4 fold... 
[Tue Apr  2 06:32:38 2024]  Iteration number: 0 with current cost as 0.19813386690871543 and parameters 
[-2.39265684  2.23743464 -2.12427961 -0.116531    0.55388709 -2.77010896
  3.06858498  2.18960145  1.18552001 -1.0664831   0.60271513  1.14432448
  1.31029897 -1.87354679  0.7296508   2.88578421 -0.54534334 -0.47522485
 -2.02654243  0.72897367  1.60512664  2.83077107 -1.26456708 -0.25136103
 -2.39279216 -2.27309774  3.13337152  2.54856955 -0.67550789 -2.69002202]. 
Working on 0.6 fold... 
[Tue Apr  2 06:41:06 2024]  Iteration number: 0 with current cost as 0.19068530213583346 and parameters 
[-2.45716183  2.23743464 -2.12427961 -0.116531    0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552001 -1.06648308  0.6027151   1.14432448
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337153  2.54856958 -0.67550786 -2.69002202]. 
Working on 0.8 fold... 
[Tue Apr  2 06:49:22 2024]  Iteration number: 0 with current cost as 0.1455065000565824 and parameters 
[-2.51316813  2.23743464 -2.12427962 -0.11653103  0.55388708 -2.77010897
  3.068585    2.18960147  1.18552002 -1.06648307  0.60271513  1.14432447
  1.310299   -1.87354678  0.7296508   2.88578421 -0.54534335 -0.47522484
 -2.02654242  0.72897368  1.60512665  2.83077109 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.5485696  -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Tue Apr  2 06:57:38 2024]  Iteration number: 0 with current cost as 0.1535315600554532 and parameters 
[-2.4813466   2.23743465 -2.1242796  -0.11653103  0.55388706 -2.77010899
  3.068585    2.18960143  1.18552    -1.0664831   0.60271512  1.14432447
  1.31029897 -1.87354682  0.72965079  2.88578419 -0.54534335 -0.47522483
 -2.02654242  0.72897368  1.60512662  2.83077107 -1.26456711 -0.25136105
 -2.3927922  -2.27309774  3.13337153  2.54856957 -0.67550787 -2.69002203]. 
Training complete taking 2498.9234886169434 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Tue Apr  2 07:06:09 2024]  Iteration number: 0 with current cost as 0.20767318926165448 and parameters 
[-2.40376861  2.23743465 -2.1242796  -0.11653101  0.55388708 -2.77010897
  3.068585    2.18960145  1.18552002 -1.0664831   0.60271512  1.14432448
  1.31029899 -1.8735468   0.72965079  2.88578421 -0.54534335 -0.47522484
 -2.02654242  0.72897368  1.60512664  2.83077106 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550786 -2.69002203]. 
Working on 0.4 fold... 
[Tue Apr  2 07:14:27 2024]  Iteration number: 0 with current cost as 0.19813386690871543 and parameters 
[-2.39265684  2.23743464 -2.12427961 -0.116531    0.55388709 -2.77010896
  3.06858498  2.18960145  1.18552001 -1.0664831   0.60271513  1.14432448
  1.31029897 -1.87354679  0.7296508   2.88578421 -0.54534334 -0.47522485
 -2.02654243  0.72897367  1.60512664  2.83077107 -1.26456708 -0.25136103
 -2.39279216 -2.27309774  3.13337152  2.54856955 -0.67550789 -2.69002202]. 
Working on 0.6 fold... 
[Tue Apr  2 07:22:40 2024]  Iteration number: 0 with current cost as 0.19068530213583346 and parameters 
[-2.45716183  2.23743464 -2.12427961 -0.116531    0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552001 -1.06648308  0.6027151   1.14432448
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337153  2.54856958 -0.67550786 -2.69002202]. 
Working on 0.8 fold... 
[Tue Apr  2 07:31:20 2024]  Iteration number: 0 with current cost as 0.1455065000565824 and parameters 
[-2.51316813  2.23743464 -2.12427962 -0.11653103  0.55388708 -2.77010897
  3.068585    2.18960147  1.18552002 -1.06648307  0.60271513  1.14432447
  1.310299   -1.87354678  0.7296508   2.88578421 -0.54534335 -0.47522484
 -2.02654242  0.72897368  1.60512665  2.83077109 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.5485696  -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Tue Apr  2 07:39:42 2024]  Iteration number: 0 with current cost as 0.1535315600554532 and parameters 
[-2.4813466   2.23743465 -2.1242796  -0.11653103  0.55388706 -2.77010899
  3.068585    2.18960143  1.18552    -1.0664831   0.60271512  1.14432447
  1.31029897 -1.87354682  0.72965079  2.88578419 -0.54534335 -0.47522483
 -2.02654242  0.72897368  1.60512662  2.83077107 -1.26456711 -0.25136105
 -2.3927922  -2.27309774  3.13337153  2.54856957 -0.67550787 -2.69002203]. 
Training complete taking 2515.742009162903 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Tue Apr  2 07:47:49 2024]  Iteration number: 0 with current cost as 0.20767318926165448 and parameters 
[-2.40376861  2.23743465 -2.1242796  -0.11653101  0.55388708 -2.77010897
  3.068585    2.18960145  1.18552002 -1.0664831   0.60271512  1.14432448
  1.31029899 -1.8735468   0.72965079  2.88578421 -0.54534335 -0.47522484
 -2.02654242  0.72897368  1.60512664  2.83077106 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550786 -2.69002203]. 
Working on 0.4 fold... 
[Tue Apr  2 07:56:10 2024]  Iteration number: 0 with current cost as 0.19813386690871543 and parameters 
[-2.39265684  2.23743464 -2.12427961 -0.116531    0.55388709 -2.77010896
  3.06858498  2.18960145  1.18552001 -1.0664831   0.60271513  1.14432448
  1.31029897 -1.87354679  0.7296508   2.88578421 -0.54534334 -0.47522485
 -2.02654243  0.72897367  1.60512664  2.83077107 -1.26456708 -0.25136103
 -2.39279216 -2.27309774  3.13337152  2.54856955 -0.67550789 -2.69002202]. 
Working on 0.6 fold... 
[Tue Apr  2 08:04:14 2024]  Iteration number: 0 with current cost as 0.19068530213583346 and parameters 
[-2.45716183  2.23743464 -2.12427961 -0.116531    0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552001 -1.06648308  0.6027151   1.14432448
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337153  2.54856958 -0.67550786 -2.69002202]. 
Working on 0.8 fold... 
[Tue Apr  2 08:12:23 2024]  Iteration number: 0 with current cost as 0.1455065000565824 and parameters 
[-2.51316813  2.23743464 -2.12427962 -0.11653103  0.55388708 -2.77010897
  3.068585    2.18960147  1.18552002 -1.06648307  0.60271513  1.14432447
  1.310299   -1.87354678  0.7296508   2.88578421 -0.54534335 -0.47522484
 -2.02654242  0.72897368  1.60512665  2.83077109 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.5485696  -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Tue Apr  2 08:20:36 2024]  Iteration number: 0 with current cost as 0.1535315600554532 and parameters 
[-2.4813466   2.23743465 -2.1242796  -0.11653103  0.55388706 -2.77010899
  3.068585    2.18960143  1.18552    -1.0664831   0.60271512  1.14432447
  1.31029897 -1.87354682  0.72965079  2.88578419 -0.54534335 -0.47522483
 -2.02654242  0.72897368  1.60512662  2.83077107 -1.26456711 -0.25136105
 -2.3927922  -2.27309774  3.13337153  2.54856957 -0.67550787 -2.69002203]. 
Training complete taking 2465.395431995392 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Tue Apr  2 08:29:00 2024]  Iteration number: 0 with current cost as 0.20767318926165448 and parameters 
[-2.40376861  2.23743465 -2.1242796  -0.11653101  0.55388708 -2.77010897
  3.068585    2.18960145  1.18552002 -1.0664831   0.60271512  1.14432448
  1.31029899 -1.8735468   0.72965079  2.88578421 -0.54534335 -0.47522484
 -2.02654242  0.72897368  1.60512664  2.83077106 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550786 -2.69002203]. 
Working on 0.4 fold... 
[Tue Apr  2 08:37:11 2024]  Iteration number: 0 with current cost as 0.19813386690871543 and parameters 
[-2.39265684  2.23743464 -2.12427961 -0.116531    0.55388709 -2.77010896
  3.06858498  2.18960145  1.18552001 -1.0664831   0.60271513  1.14432448
  1.31029897 -1.87354679  0.7296508   2.88578421 -0.54534334 -0.47522485
 -2.02654243  0.72897367  1.60512664  2.83077107 -1.26456708 -0.25136103
 -2.39279216 -2.27309774  3.13337152  2.54856955 -0.67550789 -2.69002202]. 
Working on 0.6 fold... 
[Tue Apr  2 08:45:13 2024]  Iteration number: 0 with current cost as 0.19068530213583346 and parameters 
[-2.45716183  2.23743464 -2.12427961 -0.116531    0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552001 -1.06648308  0.6027151   1.14432448
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337153  2.54856958 -0.67550786 -2.69002202]. 
Working on 0.8 fold... 
[Tue Apr  2 08:53:14 2024]  Iteration number: 0 with current cost as 0.1455065000565824 and parameters 
[-2.51316813  2.23743464 -2.12427962 -0.11653103  0.55388708 -2.77010897
  3.068585    2.18960147  1.18552002 -1.06648307  0.60271513  1.14432447
  1.310299   -1.87354678  0.7296508   2.88578421 -0.54534335 -0.47522484
 -2.02654242  0.72897368  1.60512665  2.83077109 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.5485696  -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Tue Apr  2 09:01:23 2024]  Iteration number: 0 with current cost as 0.1535315600554532 and parameters 
[-2.4813466   2.23743465 -2.1242796  -0.11653103  0.55388706 -2.77010899
  3.068585    2.18960143  1.18552    -1.0664831   0.60271512  1.14432447
  1.31029897 -1.87354682  0.72965079  2.88578419 -0.54534335 -0.47522483
 -2.02654242  0.72897368  1.60512662  2.83077107 -1.26456711 -0.25136105
 -2.3927922  -2.27309774  3.13337153  2.54856957 -0.67550787 -2.69002203]. 
Training complete taking 2458.4774358272552 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Tue Apr  2 09:09:59 2024]  Iteration number: 0 with current cost as 0.20767318926165448 and parameters 
[-2.40376861  2.23743465 -2.1242796  -0.11653101  0.55388708 -2.77010897
  3.068585    2.18960145  1.18552002 -1.0664831   0.60271512  1.14432448
  1.31029899 -1.8735468   0.72965079  2.88578421 -0.54534335 -0.47522484
 -2.02654242  0.72897368  1.60512664  2.83077106 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550786 -2.69002203]. 
Working on 0.4 fold... 
[Tue Apr  2 09:18:00 2024]  Iteration number: 0 with current cost as 0.19813386690871543 and parameters 
[-2.39265684  2.23743464 -2.12427961 -0.116531    0.55388709 -2.77010896
  3.06858498  2.18960145  1.18552001 -1.0664831   0.60271513  1.14432448
  1.31029897 -1.87354679  0.7296508   2.88578421 -0.54534334 -0.47522485
 -2.02654243  0.72897367  1.60512664  2.83077107 -1.26456708 -0.25136103
 -2.39279216 -2.27309774  3.13337152  2.54856955 -0.67550789 -2.69002202]. 
Working on 0.6 fold... 
[Tue Apr  2 09:26:05 2024]  Iteration number: 0 with current cost as 0.19068530213583346 and parameters 
[-2.45716183  2.23743464 -2.12427961 -0.116531    0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552001 -1.06648308  0.6027151   1.14432448
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337153  2.54856958 -0.67550786 -2.69002202]. 
Working on 0.8 fold... 
[Tue Apr  2 09:34:00 2024]  Iteration number: 0 with current cost as 0.1455065000565824 and parameters 
[-2.51316813  2.23743464 -2.12427962 -0.11653103  0.55388708 -2.77010897
  3.068585    2.18960147  1.18552002 -1.06648307  0.60271513  1.14432447
  1.310299   -1.87354678  0.7296508   2.88578421 -0.54534335 -0.47522484
 -2.02654242  0.72897368  1.60512665  2.83077109 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.5485696  -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Tue Apr  2 09:42:16 2024]  Iteration number: 0 with current cost as 0.1535315600554532 and parameters 
[-2.4813466   2.23743465 -2.1242796  -0.11653103  0.55388706 -2.77010899
  3.068585    2.18960143  1.18552    -1.0664831   0.60271512  1.14432447
  1.31029897 -1.87354682  0.72965079  2.88578419 -0.54534335 -0.47522483
 -2.02654242  0.72897368  1.60512662  2.83077107 -1.26456711 -0.25136105
 -2.3927922  -2.27309774  3.13337153  2.54856957 -0.67550787 -2.69002203]. 
Training complete taking 2423.337991952896 seconds. 
Discarding model... 

Training complete taking 63823.38084316254 total seconds. 
Now scoring model... 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Scoring complete taking 3.5729968547821045 seconds. 
Saved predicted values as M-A2-CNOT_Full-CRX_predicted_values.csv
Model scores: {'MSE_train': (3.9415562023217277,), 'R2_train': 0.4587946622177893, 'MAE_train': 1.381709061178546, 'MSE_test': 6.028612064500459, 'R2_test': 0.3990036949049045, 'MAE_test': 1.859773572551869}. 
Saved model results as M-A2-CNOT_Full-CRX_results.json. 
