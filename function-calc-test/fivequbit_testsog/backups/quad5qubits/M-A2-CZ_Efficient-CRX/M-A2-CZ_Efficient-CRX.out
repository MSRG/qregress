/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/circuits/Encoders.py:12: RuntimeWarning: invalid value encountered in arcsin
  if np.isnan(np.arcsin(features[feature_idx])) or np.isnan(np.arccos(features[feature_idx])):
Loading dataset from /scratch/j/jacobsen/gjones/quad5qubits/quadratic_train.bin... 
Successfully loaded /scratch/j/jacobsen/gjones/quad5qubits/quadratic_train.bin into X and y data. 
Loading dataset from /scratch/j/jacobsen/gjones/quad5qubits/quadratic_test.bin... 
Successfully loaded /scratch/j/jacobsen/gjones/quad5qubits/quadratic_test.bin into X and y data. 
Training model with dataset /scratch/j/jacobsen/gjones/quad5qubits/quadratic_train.bin 
 at time Mon Apr  1 16:01:56 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Apr  1 16:04:52 2024]  Iteration number: 0 with current cost as 0.1673453919895405 and parameters 
[-3.83548525  2.23743464 -2.12427928 -0.11653094  0.55388717 -2.77010897
  3.06858489  2.18960154  1.18552007 -1.06648317  0.60271519  1.14432454
  1.31029908 -1.87354653]. 
Working on 0.4 fold... 
[Mon Apr  1 16:09:02 2024]  Iteration number: 0 with current cost as 0.18150727317525248 and parameters 
[-3.85734406  2.23743478 -2.12427906 -0.11653088  0.55388708 -2.77010926
  3.06858469  2.18960145  1.18552027 -1.06648308  0.6027151   1.1443246
  1.31029899 -1.87354637]. 
Working on 0.6 fold... 
[Mon Apr  1 16:12:50 2024]  Iteration number: 0 with current cost as 0.169033987140204 and parameters 
[-3.80294252  2.23743471 -2.12427935 -0.11653088  0.55388708 -2.77010897
  3.06858491  2.1896016   1.18552013 -1.06648308  0.60271517  1.14432459
  1.31029899 -1.87354651]. 
Working on 0.8 fold... 
[Mon Apr  1 16:17:04 2024]  Iteration number: 0 with current cost as 0.18023726350378888 and parameters 
[-2.64172261  2.23743464 -2.12427908 -0.11653103  0.55388652 -2.77010953
  3.06858415  2.18960145  1.18551998 -1.06648364  0.6027151   1.14432445
  1.31029899 -1.87354624]. 
Working on 1.0 fold... 
[Mon Apr  1 16:20:27 2024]  Iteration number: 0 with current cost as 0.1998929084458868 and parameters 
[-2.59250354  2.23743464 -2.12427949 -0.11653088  0.55388694 -2.77010912
  3.06858484  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.87354666]. 
Training complete taking 1236.6514818668365 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Apr  1 16:25:37 2024]  Iteration number: 0 with current cost as 0.1673453919895405 and parameters 
[-3.83548525  2.23743464 -2.12427928 -0.11653094  0.55388717 -2.77010897
  3.06858489  2.18960154  1.18552007 -1.06648317  0.60271519  1.14432454
  1.31029908 -1.87354653]. 
Working on 0.4 fold... 
[Mon Apr  1 16:30:20 2024]  Iteration number: 0 with current cost as 0.18150727317525248 and parameters 
[-3.85734406  2.23743478 -2.12427906 -0.11653088  0.55388708 -2.77010926
  3.06858469  2.18960145  1.18552027 -1.06648308  0.6027151   1.1443246
  1.31029899 -1.87354637]. 
Working on 0.6 fold... 
[Mon Apr  1 16:34:27 2024]  Iteration number: 0 with current cost as 0.169033987140204 and parameters 
[-3.80294252  2.23743471 -2.12427935 -0.11653088  0.55388708 -2.77010897
  3.06858491  2.1896016   1.18552013 -1.06648308  0.60271517  1.14432459
  1.31029899 -1.87354651]. 
Working on 0.8 fold... 
[Mon Apr  1 16:39:01 2024]  Iteration number: 0 with current cost as 0.18023726350378888 and parameters 
[-2.64172261  2.23743464 -2.12427908 -0.11653103  0.55388652 -2.77010953
  3.06858415  2.18960145  1.18551998 -1.06648364  0.6027151   1.14432445
  1.31029899 -1.87354624]. 
Working on 1.0 fold... 
[Mon Apr  1 16:42:36 2024]  Iteration number: 0 with current cost as 0.1998929084458868 and parameters 
[-2.59250354  2.23743464 -2.12427949 -0.11653088  0.55388694 -2.77010912
  3.06858484  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.87354666]. 
Training complete taking 1323.5261776447296 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Apr  1 16:47:41 2024]  Iteration number: 0 with current cost as 0.1673453919895405 and parameters 
[-3.83548525  2.23743464 -2.12427928 -0.11653094  0.55388717 -2.77010897
  3.06858489  2.18960154  1.18552007 -1.06648317  0.60271519  1.14432454
  1.31029908 -1.87354653]. 
Working on 0.4 fold... 
[Mon Apr  1 16:51:58 2024]  Iteration number: 0 with current cost as 0.18150727317525248 and parameters 
[-3.85734406  2.23743478 -2.12427906 -0.11653088  0.55388708 -2.77010926
  3.06858469  2.18960145  1.18552027 -1.06648308  0.6027151   1.1443246
  1.31029899 -1.87354637]. 
Working on 0.6 fold... 
[Mon Apr  1 16:55:37 2024]  Iteration number: 0 with current cost as 0.169033987140204 and parameters 
[-3.80294252  2.23743471 -2.12427935 -0.11653088  0.55388708 -2.77010897
  3.06858491  2.1896016   1.18552013 -1.06648308  0.60271517  1.14432459
  1.31029899 -1.87354651]. 
Working on 0.8 fold... 
[Mon Apr  1 16:59:39 2024]  Iteration number: 0 with current cost as 0.18023726350378888 and parameters 
[-2.64172261  2.23743464 -2.12427908 -0.11653103  0.55388652 -2.77010953
  3.06858415  2.18960145  1.18551998 -1.06648364  0.6027151   1.14432445
  1.31029899 -1.87354624]. 
Working on 1.0 fold... 
[Mon Apr  1 17:02:59 2024]  Iteration number: 0 with current cost as 0.1998929084458868 and parameters 
[-2.59250354  2.23743464 -2.12427949 -0.11653088  0.55388694 -2.77010912
  3.06858484  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.87354666]. 
Training complete taking 1239.8180029392242 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Mon Apr  1 17:08:11 2024]  Iteration number: 0 with current cost as 0.1673453919895405 and parameters 
[-3.83548525  2.23743464 -2.12427928 -0.11653094  0.55388717 -2.77010897
  3.06858489  2.18960154  1.18552007 -1.06648317  0.60271519  1.14432454
  1.31029908 -1.87354653]. 
Working on 0.4 fold... 
[Mon Apr  1 17:12:31 2024]  Iteration number: 0 with current cost as 0.18150727317525248 and parameters 
[-3.85734406  2.23743478 -2.12427906 -0.11653088  0.55388708 -2.77010926
  3.06858469  2.18960145  1.18552027 -1.06648308  0.6027151   1.1443246
  1.31029899 -1.87354637]. 
Working on 0.6 fold... 
[Mon Apr  1 17:16:09 2024]  Iteration number: 0 with current cost as 0.169033987140204 and parameters 
[-3.80294252  2.23743471 -2.12427935 -0.11653088  0.55388708 -2.77010897
  3.06858491  2.1896016   1.18552013 -1.06648308  0.60271517  1.14432459
  1.31029899 -1.87354651]. 
Working on 0.8 fold... 
[Mon Apr  1 17:20:12 2024]  Iteration number: 0 with current cost as 0.18023726350378888 and parameters 
[-2.64172261  2.23743464 -2.12427908 -0.11653103  0.55388652 -2.77010953
  3.06858415  2.18960145  1.18551998 -1.06648364  0.6027151   1.14432445
  1.31029899 -1.87354624]. 
Working on 1.0 fold... 
[Mon Apr  1 17:23:25 2024]  Iteration number: 0 with current cost as 0.1998929084458868 and parameters 
[-2.59250354  2.23743464 -2.12427949 -0.11653088  0.55388694 -2.77010912
  3.06858484  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.87354666]. 
Training complete taking 1208.6830084323883 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Mon Apr  1 17:28:11 2024]  Iteration number: 0 with current cost as 0.1673453919895405 and parameters 
[-3.83548525  2.23743464 -2.12427928 -0.11653094  0.55388717 -2.77010897
  3.06858489  2.18960154  1.18552007 -1.06648317  0.60271519  1.14432454
  1.31029908 -1.87354653]. 
Working on 0.4 fold... 
[Mon Apr  1 17:32:14 2024]  Iteration number: 0 with current cost as 0.18150727317525248 and parameters 
[-3.85734406  2.23743478 -2.12427906 -0.11653088  0.55388708 -2.77010926
  3.06858469  2.18960145  1.18552027 -1.06648308  0.6027151   1.1443246
  1.31029899 -1.87354637]. 
Working on 0.6 fold... 
[Mon Apr  1 17:35:50 2024]  Iteration number: 0 with current cost as 0.169033987140204 and parameters 
[-3.80294252  2.23743471 -2.12427935 -0.11653088  0.55388708 -2.77010897
  3.06858491  2.1896016   1.18552013 -1.06648308  0.60271517  1.14432459
  1.31029899 -1.87354651]. 
Working on 0.8 fold... 
[Mon Apr  1 17:39:50 2024]  Iteration number: 0 with current cost as 0.18023726350378888 and parameters 
[-2.64172261  2.23743464 -2.12427908 -0.11653103  0.55388652 -2.77010953
  3.06858415  2.18960145  1.18551998 -1.06648364  0.6027151   1.14432445
  1.31029899 -1.87354624]. 
Working on 1.0 fold... 
[Mon Apr  1 17:43:08 2024]  Iteration number: 0 with current cost as 0.1998929084458868 and parameters 
[-2.59250354  2.23743464 -2.12427949 -0.11653088  0.55388694 -2.77010912
  3.06858484  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.87354666]. 
Training complete taking 1181.7907507419586 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Apr  1 17:48:00 2024]  Iteration number: 0 with current cost as 0.1673453919895405 and parameters 
[-3.83548525  2.23743464 -2.12427928 -0.11653094  0.55388717 -2.77010897
  3.06858489  2.18960154  1.18552007 -1.06648317  0.60271519  1.14432454
  1.31029908 -1.87354653]. 
Working on 0.4 fold... 
[Mon Apr  1 17:52:05 2024]  Iteration number: 0 with current cost as 0.18150727317525248 and parameters 
[-3.85734406  2.23743478 -2.12427906 -0.11653088  0.55388708 -2.77010926
  3.06858469  2.18960145  1.18552027 -1.06648308  0.6027151   1.1443246
  1.31029899 -1.87354637]. 
Working on 0.6 fold... 
[Mon Apr  1 17:55:51 2024]  Iteration number: 0 with current cost as 0.169033987140204 and parameters 
[-3.80294252  2.23743471 -2.12427935 -0.11653088  0.55388708 -2.77010897
  3.06858491  2.1896016   1.18552013 -1.06648308  0.60271517  1.14432459
  1.31029899 -1.87354651]. 
Working on 0.8 fold... 
[Mon Apr  1 17:59:57 2024]  Iteration number: 0 with current cost as 0.18023726350378888 and parameters 
[-2.64172261  2.23743464 -2.12427908 -0.11653103  0.55388652 -2.77010953
  3.06858415  2.18960145  1.18551998 -1.06648364  0.6027151   1.14432445
  1.31029899 -1.87354624]. 
Working on 1.0 fold... 
[Mon Apr  1 18:03:15 2024]  Iteration number: 0 with current cost as 0.1998929084458868 and parameters 
[-2.59250354  2.23743464 -2.12427949 -0.11653088  0.55388694 -2.77010912
  3.06858484  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.87354666]. 
Training complete taking 1210.5174207687378 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Apr  1 18:08:13 2024]  Iteration number: 0 with current cost as 0.1673453919895405 and parameters 
[-3.83548525  2.23743464 -2.12427928 -0.11653094  0.55388717 -2.77010897
  3.06858489  2.18960154  1.18552007 -1.06648317  0.60271519  1.14432454
  1.31029908 -1.87354653]. 
Working on 0.4 fold... 
[Mon Apr  1 18:12:15 2024]  Iteration number: 0 with current cost as 0.18150727317525248 and parameters 
[-3.85734406  2.23743478 -2.12427906 -0.11653088  0.55388708 -2.77010926
  3.06858469  2.18960145  1.18552027 -1.06648308  0.6027151   1.1443246
  1.31029899 -1.87354637]. 
Working on 0.6 fold... 
[Mon Apr  1 18:15:52 2024]  Iteration number: 0 with current cost as 0.169033987140204 and parameters 
[-3.80294252  2.23743471 -2.12427935 -0.11653088  0.55388708 -2.77010897
  3.06858491  2.1896016   1.18552013 -1.06648308  0.60271517  1.14432459
  1.31029899 -1.87354651]. 
Working on 0.8 fold... 
[Mon Apr  1 18:20:18 2024]  Iteration number: 0 with current cost as 0.18023726350378888 and parameters 
[-2.64172261  2.23743464 -2.12427908 -0.11653103  0.55388652 -2.77010953
  3.06858415  2.18960145  1.18551998 -1.06648364  0.6027151   1.14432445
  1.31029899 -1.87354624]. 
Working on 1.0 fold... 
[Mon Apr  1 18:23:41 2024]  Iteration number: 0 with current cost as 0.1998929084458868 and parameters 
[-2.59250354  2.23743464 -2.12427949 -0.11653088  0.55388694 -2.77010912
  3.06858484  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.87354666]. 
Training complete taking 1225.3385672569275 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Apr  1 18:28:41 2024]  Iteration number: 0 with current cost as 0.1673453919895405 and parameters 
[-3.83548525  2.23743464 -2.12427928 -0.11653094  0.55388717 -2.77010897
  3.06858489  2.18960154  1.18552007 -1.06648317  0.60271519  1.14432454
  1.31029908 -1.87354653]. 
Working on 0.4 fold... 
[Mon Apr  1 18:32:56 2024]  Iteration number: 0 with current cost as 0.18150727317525248 and parameters 
[-3.85734406  2.23743478 -2.12427906 -0.11653088  0.55388708 -2.77010926
  3.06858469  2.18960145  1.18552027 -1.06648308  0.6027151   1.1443246
  1.31029899 -1.87354637]. 
Working on 0.6 fold... 
[Mon Apr  1 18:36:37 2024]  Iteration number: 0 with current cost as 0.169033987140204 and parameters 
[-3.80294252  2.23743471 -2.12427935 -0.11653088  0.55388708 -2.77010897
  3.06858491  2.1896016   1.18552013 -1.06648308  0.60271517  1.14432459
  1.31029899 -1.87354651]. 
Working on 0.8 fold... 
[Mon Apr  1 18:40:36 2024]  Iteration number: 0 with current cost as 0.18023726350378888 and parameters 
[-2.64172261  2.23743464 -2.12427908 -0.11653103  0.55388652 -2.77010953
  3.06858415  2.18960145  1.18551998 -1.06648364  0.6027151   1.14432445
  1.31029899 -1.87354624]. 
Working on 1.0 fold... 
[Mon Apr  1 18:43:56 2024]  Iteration number: 0 with current cost as 0.1998929084458868 and parameters 
[-2.59250354  2.23743464 -2.12427949 -0.11653088  0.55388694 -2.77010912
  3.06858484  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.87354666]. 
Training complete taking 1210.9533298015594 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Mon Apr  1 18:48:41 2024]  Iteration number: 0 with current cost as 0.1673453919895405 and parameters 
[-3.83548525  2.23743464 -2.12427928 -0.11653094  0.55388717 -2.77010897
  3.06858489  2.18960154  1.18552007 -1.06648317  0.60271519  1.14432454
  1.31029908 -1.87354653]. 
Working on 0.4 fold... 
[Mon Apr  1 18:52:40 2024]  Iteration number: 0 with current cost as 0.18150727317525248 and parameters 
[-3.85734406  2.23743478 -2.12427906 -0.11653088  0.55388708 -2.77010926
  3.06858469  2.18960145  1.18552027 -1.06648308  0.6027151   1.1443246
  1.31029899 -1.87354637]. 
Working on 0.6 fold... 
[Mon Apr  1 18:56:22 2024]  Iteration number: 0 with current cost as 0.169033987140204 and parameters 
[-3.80294252  2.23743471 -2.12427935 -0.11653088  0.55388708 -2.77010897
  3.06858491  2.1896016   1.18552013 -1.06648308  0.60271517  1.14432459
  1.31029899 -1.87354651]. 
Working on 0.8 fold... 
[Mon Apr  1 19:00:24 2024]  Iteration number: 0 with current cost as 0.18023726350378888 and parameters 
[-2.64172261  2.23743464 -2.12427908 -0.11653103  0.55388652 -2.77010953
  3.06858415  2.18960145  1.18551998 -1.06648364  0.6027151   1.14432445
  1.31029899 -1.87354624]. 
Working on 1.0 fold... 
[Mon Apr  1 19:03:37 2024]  Iteration number: 0 with current cost as 0.1998929084458868 and parameters 
[-2.59250354  2.23743464 -2.12427949 -0.11653088  0.55388694 -2.77010912
  3.06858484  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.87354666]. 
Training complete taking 1183.2635934352875 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Mon Apr  1 19:08:22 2024]  Iteration number: 0 with current cost as 0.1673453919895405 and parameters 
[-3.83548525  2.23743464 -2.12427928 -0.11653094  0.55388717 -2.77010897
  3.06858489  2.18960154  1.18552007 -1.06648317  0.60271519  1.14432454
  1.31029908 -1.87354653]. 
Working on 0.4 fold... 
[Mon Apr  1 19:12:19 2024]  Iteration number: 0 with current cost as 0.18150727317525248 and parameters 
[-3.85734406  2.23743478 -2.12427906 -0.11653088  0.55388708 -2.77010926
  3.06858469  2.18960145  1.18552027 -1.06648308  0.6027151   1.1443246
  1.31029899 -1.87354637]. 
Working on 0.6 fold... 
[Mon Apr  1 19:16:14 2024]  Iteration number: 0 with current cost as 0.169033987140204 and parameters 
[-3.80294252  2.23743471 -2.12427935 -0.11653088  0.55388708 -2.77010897
  3.06858491  2.1896016   1.18552013 -1.06648308  0.60271517  1.14432459
  1.31029899 -1.87354651]. 
Working on 0.8 fold... 
[Mon Apr  1 19:20:52 2024]  Iteration number: 0 with current cost as 0.18023726350378888 and parameters 
[-2.64172261  2.23743464 -2.12427908 -0.11653103  0.55388652 -2.77010953
  3.06858415  2.18960145  1.18551998 -1.06648364  0.6027151   1.14432445
  1.31029899 -1.87354624]. 
Working on 1.0 fold... 
[Mon Apr  1 19:24:30 2024]  Iteration number: 0 with current cost as 0.1998929084458868 and parameters 
[-2.59250354  2.23743464 -2.12427949 -0.11653088  0.55388694 -2.77010912
  3.06858484  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.87354666]. 
Training complete taking 1274.5619776248932 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Apr  1 19:29:58 2024]  Iteration number: 0 with current cost as 0.1673453919895405 and parameters 
[-3.83548525  2.23743464 -2.12427928 -0.11653094  0.55388717 -2.77010897
  3.06858489  2.18960154  1.18552007 -1.06648317  0.60271519  1.14432454
  1.31029908 -1.87354653]. 
Working on 0.4 fold... 
[Mon Apr  1 19:34:08 2024]  Iteration number: 0 with current cost as 0.18150727317525248 and parameters 
[-3.85734406  2.23743478 -2.12427906 -0.11653088  0.55388708 -2.77010926
  3.06858469  2.18960145  1.18552027 -1.06648308  0.6027151   1.1443246
  1.31029899 -1.87354637]. 
Working on 0.6 fold... 
[Mon Apr  1 19:38:00 2024]  Iteration number: 0 with current cost as 0.169033987140204 and parameters 
[-3.80294252  2.23743471 -2.12427935 -0.11653088  0.55388708 -2.77010897
  3.06858491  2.1896016   1.18552013 -1.06648308  0.60271517  1.14432459
  1.31029899 -1.87354651]. 
Working on 0.8 fold... 
[Mon Apr  1 19:42:18 2024]  Iteration number: 0 with current cost as 0.18023726350378888 and parameters 
[-2.64172261  2.23743464 -2.12427908 -0.11653103  0.55388652 -2.77010953
  3.06858415  2.18960145  1.18551998 -1.06648364  0.6027151   1.14432445
  1.31029899 -1.87354624]. 
Working on 1.0 fold... 
[Mon Apr  1 19:45:51 2024]  Iteration number: 0 with current cost as 0.1998929084458868 and parameters 
[-2.59250354  2.23743464 -2.12427949 -0.11653088  0.55388694 -2.77010912
  3.06858484  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.87354666]. 
Training complete taking 1269.048618555069 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Apr  1 19:50:52 2024]  Iteration number: 0 with current cost as 0.1673453919895405 and parameters 
[-3.83548525  2.23743464 -2.12427928 -0.11653094  0.55388717 -2.77010897
  3.06858489  2.18960154  1.18552007 -1.06648317  0.60271519  1.14432454
  1.31029908 -1.87354653]. 
Working on 0.4 fold... 
[Mon Apr  1 19:55:12 2024]  Iteration number: 0 with current cost as 0.18150727317525248 and parameters 
[-3.85734406  2.23743478 -2.12427906 -0.11653088  0.55388708 -2.77010926
  3.06858469  2.18960145  1.18552027 -1.06648308  0.6027151   1.1443246
  1.31029899 -1.87354637]. 
Working on 0.6 fold... 
[Mon Apr  1 19:59:25 2024]  Iteration number: 0 with current cost as 0.169033987140204 and parameters 
[-3.80294252  2.23743471 -2.12427935 -0.11653088  0.55388708 -2.77010897
  3.06858491  2.1896016   1.18552013 -1.06648308  0.60271517  1.14432459
  1.31029899 -1.87354651]. 
Working on 0.8 fold... 
[Mon Apr  1 20:03:56 2024]  Iteration number: 0 with current cost as 0.18023726350378888 and parameters 
[-2.64172261  2.23743464 -2.12427908 -0.11653103  0.55388652 -2.77010953
  3.06858415  2.18960145  1.18551998 -1.06648364  0.6027151   1.14432445
  1.31029899 -1.87354624]. 
Working on 1.0 fold... 
[Mon Apr  1 20:07:09 2024]  Iteration number: 0 with current cost as 0.1998929084458868 and parameters 
[-2.59250354  2.23743464 -2.12427949 -0.11653088  0.55388694 -2.77010912
  3.06858484  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.87354666]. 
Training complete taking 1272.655342578888 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Apr  1 20:12:11 2024]  Iteration number: 0 with current cost as 0.1673453919895405 and parameters 
[-3.83548525  2.23743464 -2.12427928 -0.11653094  0.55388717 -2.77010897
  3.06858489  2.18960154  1.18552007 -1.06648317  0.60271519  1.14432454
  1.31029908 -1.87354653]. 
Working on 0.4 fold... 
[Mon Apr  1 20:16:29 2024]  Iteration number: 0 with current cost as 0.18150727317525248 and parameters 
[-3.85734406  2.23743478 -2.12427906 -0.11653088  0.55388708 -2.77010926
  3.06858469  2.18960145  1.18552027 -1.06648308  0.6027151   1.1443246
  1.31029899 -1.87354637]. 
Working on 0.6 fold... 
[Mon Apr  1 20:20:28 2024]  Iteration number: 0 with current cost as 0.169033987140204 and parameters 
[-3.80294252  2.23743471 -2.12427935 -0.11653088  0.55388708 -2.77010897
  3.06858491  2.1896016   1.18552013 -1.06648308  0.60271517  1.14432459
  1.31029899 -1.87354651]. 
Working on 0.8 fold... 
[Mon Apr  1 20:24:42 2024]  Iteration number: 0 with current cost as 0.18023726350378888 and parameters 
[-2.64172261  2.23743464 -2.12427908 -0.11653103  0.55388652 -2.77010953
  3.06858415  2.18960145  1.18551998 -1.06648364  0.6027151   1.14432445
  1.31029899 -1.87354624]. 
Working on 1.0 fold... 
[Mon Apr  1 20:28:11 2024]  Iteration number: 0 with current cost as 0.1998929084458868 and parameters 
[-2.59250354  2.23743464 -2.12427949 -0.11653088  0.55388694 -2.77010912
  3.06858484  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.87354666]. 
Training complete taking 1258.7791502475739 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Apr  1 20:33:21 2024]  Iteration number: 0 with current cost as 0.1673453919895405 and parameters 
[-3.83548525  2.23743464 -2.12427928 -0.11653094  0.55388717 -2.77010897
  3.06858489  2.18960154  1.18552007 -1.06648317  0.60271519  1.14432454
  1.31029908 -1.87354653]. 
Working on 0.4 fold... 
[Mon Apr  1 20:37:32 2024]  Iteration number: 0 with current cost as 0.18150727317525248 and parameters 
[-3.85734406  2.23743478 -2.12427906 -0.11653088  0.55388708 -2.77010926
  3.06858469  2.18960145  1.18552027 -1.06648308  0.6027151   1.1443246
  1.31029899 -1.87354637]. 
Working on 0.6 fold... 
[Mon Apr  1 20:41:31 2024]  Iteration number: 0 with current cost as 0.169033987140204 and parameters 
[-3.80294252  2.23743471 -2.12427935 -0.11653088  0.55388708 -2.77010897
  3.06858491  2.1896016   1.18552013 -1.06648308  0.60271517  1.14432459
  1.31029899 -1.87354651]. 
Working on 0.8 fold... 
[Mon Apr  1 20:45:44 2024]  Iteration number: 0 with current cost as 0.18023726350378888 and parameters 
[-2.64172261  2.23743464 -2.12427908 -0.11653103  0.55388652 -2.77010953
  3.06858415  2.18960145  1.18551998 -1.06648364  0.6027151   1.14432445
  1.31029899 -1.87354624]. 
Working on 1.0 fold... 
[Mon Apr  1 20:49:06 2024]  Iteration number: 0 with current cost as 0.1998929084458868 and parameters 
[-2.59250354  2.23743464 -2.12427949 -0.11653088  0.55388694 -2.77010912
  3.06858484  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.87354666]. 
Training complete taking 1264.650931596756 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Apr  1 20:54:13 2024]  Iteration number: 0 with current cost as 0.1673453919895405 and parameters 
[-3.83548525  2.23743464 -2.12427928 -0.11653094  0.55388717 -2.77010897
  3.06858489  2.18960154  1.18552007 -1.06648317  0.60271519  1.14432454
  1.31029908 -1.87354653]. 
Working on 0.4 fold... 
[Mon Apr  1 20:58:22 2024]  Iteration number: 0 with current cost as 0.18150727317525248 and parameters 
[-3.85734406  2.23743478 -2.12427906 -0.11653088  0.55388708 -2.77010926
  3.06858469  2.18960145  1.18552027 -1.06648308  0.6027151   1.1443246
  1.31029899 -1.87354637]. 
Working on 0.6 fold... 
[Mon Apr  1 21:02:10 2024]  Iteration number: 0 with current cost as 0.169033987140204 and parameters 
[-3.80294252  2.23743471 -2.12427935 -0.11653088  0.55388708 -2.77010897
  3.06858491  2.1896016   1.18552013 -1.06648308  0.60271517  1.14432459
  1.31029899 -1.87354651]. 
Working on 0.8 fold... 
[Mon Apr  1 21:06:25 2024]  Iteration number: 0 with current cost as 0.18023726350378888 and parameters 
[-2.64172261  2.23743464 -2.12427908 -0.11653103  0.55388652 -2.77010953
  3.06858415  2.18960145  1.18551998 -1.06648364  0.6027151   1.14432445
  1.31029899 -1.87354624]. 
Working on 1.0 fold... 
[Mon Apr  1 21:09:48 2024]  Iteration number: 0 with current cost as 0.1998929084458868 and parameters 
[-2.59250354  2.23743464 -2.12427949 -0.11653088  0.55388694 -2.77010912
  3.06858484  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.87354666]. 
Training complete taking 1239.1138792037964 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Apr  1 21:14:53 2024]  Iteration number: 0 with current cost as 0.1673453919895405 and parameters 
[-3.83548525  2.23743464 -2.12427928 -0.11653094  0.55388717 -2.77010897
  3.06858489  2.18960154  1.18552007 -1.06648317  0.60271519  1.14432454
  1.31029908 -1.87354653]. 
Working on 0.4 fold... 
[Mon Apr  1 21:19:02 2024]  Iteration number: 0 with current cost as 0.18150727317525248 and parameters 
[-3.85734406  2.23743478 -2.12427906 -0.11653088  0.55388708 -2.77010926
  3.06858469  2.18960145  1.18552027 -1.06648308  0.6027151   1.1443246
  1.31029899 -1.87354637]. 
Working on 0.6 fold... 
[Mon Apr  1 21:22:52 2024]  Iteration number: 0 with current cost as 0.169033987140204 and parameters 
[-3.80294252  2.23743471 -2.12427935 -0.11653088  0.55388708 -2.77010897
  3.06858491  2.1896016   1.18552013 -1.06648308  0.60271517  1.14432459
  1.31029899 -1.87354651]. 
Working on 0.8 fold... 
[Mon Apr  1 21:26:53 2024]  Iteration number: 0 with current cost as 0.18023726350378888 and parameters 
[-2.64172261  2.23743464 -2.12427908 -0.11653103  0.55388652 -2.77010953
  3.06858415  2.18960145  1.18551998 -1.06648364  0.6027151   1.14432445
  1.31029899 -1.87354624]. 
Working on 1.0 fold... 
[Mon Apr  1 21:30:12 2024]  Iteration number: 0 with current cost as 0.1998929084458868 and parameters 
[-2.59250354  2.23743464 -2.12427949 -0.11653088  0.55388694 -2.77010912
  3.06858484  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.87354666]. 
Training complete taking 1220.3559319972992 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Apr  1 21:35:17 2024]  Iteration number: 0 with current cost as 0.1673453919895405 and parameters 
[-3.83548525  2.23743464 -2.12427928 -0.11653094  0.55388717 -2.77010897
  3.06858489  2.18960154  1.18552007 -1.06648317  0.60271519  1.14432454
  1.31029908 -1.87354653]. 
Working on 0.4 fold... 
[Mon Apr  1 21:39:29 2024]  Iteration number: 0 with current cost as 0.18150727317525248 and parameters 
[-3.85734406  2.23743478 -2.12427906 -0.11653088  0.55388708 -2.77010926
  3.06858469  2.18960145  1.18552027 -1.06648308  0.6027151   1.1443246
  1.31029899 -1.87354637]. 
Working on 0.6 fold... 
[Mon Apr  1 21:43:10 2024]  Iteration number: 0 with current cost as 0.169033987140204 and parameters 
[-3.80294252  2.23743471 -2.12427935 -0.11653088  0.55388708 -2.77010897
  3.06858491  2.1896016   1.18552013 -1.06648308  0.60271517  1.14432459
  1.31029899 -1.87354651]. 
Working on 0.8 fold... 
[Mon Apr  1 21:47:24 2024]  Iteration number: 0 with current cost as 0.18023726350378888 and parameters 
[-2.64172261  2.23743464 -2.12427908 -0.11653103  0.55388652 -2.77010953
  3.06858415  2.18960145  1.18551998 -1.06648364  0.6027151   1.14432445
  1.31029899 -1.87354624]. 
Working on 1.0 fold... 
[Mon Apr  1 21:51:05 2024]  Iteration number: 0 with current cost as 0.1998929084458868 and parameters 
[-2.59250354  2.23743464 -2.12427949 -0.11653088  0.55388694 -2.77010912
  3.06858484  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.87354666]. 
Training complete taking 1255.7350130081177 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Apr  1 21:56:17 2024]  Iteration number: 0 with current cost as 0.1673453919895405 and parameters 
[-3.83548525  2.23743464 -2.12427928 -0.11653094  0.55388717 -2.77010897
  3.06858489  2.18960154  1.18552007 -1.06648317  0.60271519  1.14432454
  1.31029908 -1.87354653]. 
Working on 0.4 fold... 
[Mon Apr  1 22:00:37 2024]  Iteration number: 0 with current cost as 0.18150727317525248 and parameters 
[-3.85734406  2.23743478 -2.12427906 -0.11653088  0.55388708 -2.77010926
  3.06858469  2.18960145  1.18552027 -1.06648308  0.6027151   1.1443246
  1.31029899 -1.87354637]. 
Working on 0.6 fold... 
[Mon Apr  1 22:04:35 2024]  Iteration number: 0 with current cost as 0.169033987140204 and parameters 
[-3.80294252  2.23743471 -2.12427935 -0.11653088  0.55388708 -2.77010897
  3.06858491  2.1896016   1.18552013 -1.06648308  0.60271517  1.14432459
  1.31029899 -1.87354651]. 
Working on 0.8 fold... 
[Mon Apr  1 22:08:56 2024]  Iteration number: 0 with current cost as 0.18023726350378888 and parameters 
[-2.64172261  2.23743464 -2.12427908 -0.11653103  0.55388652 -2.77010953
  3.06858415  2.18960145  1.18551998 -1.06648364  0.6027151   1.14432445
  1.31029899 -1.87354624]. 
Working on 1.0 fold... 
[Mon Apr  1 22:12:14 2024]  Iteration number: 0 with current cost as 0.1998929084458868 and parameters 
[-2.59250354  2.23743464 -2.12427949 -0.11653088  0.55388694 -2.77010912
  3.06858484  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.87354666]. 
Training complete taking 1272.9560134410858 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Apr  1 22:17:21 2024]  Iteration number: 0 with current cost as 0.1673453919895405 and parameters 
[-3.83548525  2.23743464 -2.12427928 -0.11653094  0.55388717 -2.77010897
  3.06858489  2.18960154  1.18552007 -1.06648317  0.60271519  1.14432454
  1.31029908 -1.87354653]. 
Working on 0.4 fold... 
[Mon Apr  1 22:21:38 2024]  Iteration number: 0 with current cost as 0.18150727317525248 and parameters 
[-3.85734406  2.23743478 -2.12427906 -0.11653088  0.55388708 -2.77010926
  3.06858469  2.18960145  1.18552027 -1.06648308  0.6027151   1.1443246
  1.31029899 -1.87354637]. 
Working on 0.6 fold... 
[Mon Apr  1 22:25:26 2024]  Iteration number: 0 with current cost as 0.169033987140204 and parameters 
[-3.80294252  2.23743471 -2.12427935 -0.11653088  0.55388708 -2.77010897
  3.06858491  2.1896016   1.18552013 -1.06648308  0.60271517  1.14432459
  1.31029899 -1.87354651]. 
Working on 0.8 fold... 
[Mon Apr  1 22:29:28 2024]  Iteration number: 0 with current cost as 0.18023726350378888 and parameters 
[-2.64172261  2.23743464 -2.12427908 -0.11653103  0.55388652 -2.77010953
  3.06858415  2.18960145  1.18551998 -1.06648364  0.6027151   1.14432445
  1.31029899 -1.87354624]. 
Working on 1.0 fold... 
[Mon Apr  1 22:32:50 2024]  Iteration number: 0 with current cost as 0.1998929084458868 and parameters 
[-2.59250354  2.23743464 -2.12427949 -0.11653088  0.55388694 -2.77010912
  3.06858484  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.87354666]. 
Training complete taking 1238.4589245319366 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Apr  1 22:38:08 2024]  Iteration number: 0 with current cost as 0.1673453919895405 and parameters 
[-3.83548525  2.23743464 -2.12427928 -0.11653094  0.55388717 -2.77010897
  3.06858489  2.18960154  1.18552007 -1.06648317  0.60271519  1.14432454
  1.31029908 -1.87354653]. 
Working on 0.4 fold... 
[Mon Apr  1 22:42:22 2024]  Iteration number: 0 with current cost as 0.18150727317525248 and parameters 
[-3.85734406  2.23743478 -2.12427906 -0.11653088  0.55388708 -2.77010926
  3.06858469  2.18960145  1.18552027 -1.06648308  0.6027151   1.1443246
  1.31029899 -1.87354637]. 
Working on 0.6 fold... 
[Mon Apr  1 22:46:08 2024]  Iteration number: 0 with current cost as 0.169033987140204 and parameters 
[-3.80294252  2.23743471 -2.12427935 -0.11653088  0.55388708 -2.77010897
  3.06858491  2.1896016   1.18552013 -1.06648308  0.60271517  1.14432459
  1.31029899 -1.87354651]. 
Working on 0.8 fold... 
[Mon Apr  1 22:50:10 2024]  Iteration number: 0 with current cost as 0.18023726350378888 and parameters 
[-2.64172261  2.23743464 -2.12427908 -0.11653103  0.55388652 -2.77010953
  3.06858415  2.18960145  1.18551998 -1.06648364  0.6027151   1.14432445
  1.31029899 -1.87354624]. 
Working on 1.0 fold... 
[Mon Apr  1 22:54:02 2024]  Iteration number: 0 with current cost as 0.1998929084458868 and parameters 
[-2.59250354  2.23743464 -2.12427949 -0.11653088  0.55388694 -2.77010912
  3.06858484  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.87354666]. 
Training complete taking 1281.3000450134277 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Apr  1 22:59:39 2024]  Iteration number: 0 with current cost as 0.1673453919895405 and parameters 
[-3.83548525  2.23743464 -2.12427928 -0.11653094  0.55388717 -2.77010897
  3.06858489  2.18960154  1.18552007 -1.06648317  0.60271519  1.14432454
  1.31029908 -1.87354653]. 
Working on 0.4 fold... 
[Mon Apr  1 23:04:09 2024]  Iteration number: 0 with current cost as 0.18150727317525248 and parameters 
[-3.85734406  2.23743478 -2.12427906 -0.11653088  0.55388708 -2.77010926
  3.06858469  2.18960145  1.18552027 -1.06648308  0.6027151   1.1443246
  1.31029899 -1.87354637]. 
Working on 0.6 fold... 
[Mon Apr  1 23:08:10 2024]  Iteration number: 0 with current cost as 0.169033987140204 and parameters 
[-3.80294252  2.23743471 -2.12427935 -0.11653088  0.55388708 -2.77010897
  3.06858491  2.1896016   1.18552013 -1.06648308  0.60271517  1.14432459
  1.31029899 -1.87354651]. 
Working on 0.8 fold... 
[Mon Apr  1 23:12:27 2024]  Iteration number: 0 with current cost as 0.18023726350378888 and parameters 
[-2.64172261  2.23743464 -2.12427908 -0.11653103  0.55388652 -2.77010953
  3.06858415  2.18960145  1.18551998 -1.06648364  0.6027151   1.14432445
  1.31029899 -1.87354624]. 
Working on 1.0 fold... 
[Mon Apr  1 23:15:59 2024]  Iteration number: 0 with current cost as 0.1998929084458868 and parameters 
[-2.59250354  2.23743464 -2.12427949 -0.11653088  0.55388694 -2.77010912
  3.06858484  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.87354666]. 
Training complete taking 1313.6890563964844 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Apr  1 23:21:16 2024]  Iteration number: 0 with current cost as 0.1673453919895405 and parameters 
[-3.83548525  2.23743464 -2.12427928 -0.11653094  0.55388717 -2.77010897
  3.06858489  2.18960154  1.18552007 -1.06648317  0.60271519  1.14432454
  1.31029908 -1.87354653]. 
Working on 0.4 fold... 
[Mon Apr  1 23:25:40 2024]  Iteration number: 0 with current cost as 0.18150727317525248 and parameters 
[-3.85734406  2.23743478 -2.12427906 -0.11653088  0.55388708 -2.77010926
  3.06858469  2.18960145  1.18552027 -1.06648308  0.6027151   1.1443246
  1.31029899 -1.87354637]. 
Working on 0.6 fold... 
[Mon Apr  1 23:29:45 2024]  Iteration number: 0 with current cost as 0.169033987140204 and parameters 
[-3.80294252  2.23743471 -2.12427935 -0.11653088  0.55388708 -2.77010897
  3.06858491  2.1896016   1.18552013 -1.06648308  0.60271517  1.14432459
  1.31029899 -1.87354651]. 
Working on 0.8 fold... 
[Mon Apr  1 23:34:17 2024]  Iteration number: 0 with current cost as 0.18023726350378888 and parameters 
[-2.64172261  2.23743464 -2.12427908 -0.11653103  0.55388652 -2.77010953
  3.06858415  2.18960145  1.18551998 -1.06648364  0.6027151   1.14432445
  1.31029899 -1.87354624]. 
Working on 1.0 fold... 
[Mon Apr  1 23:37:51 2024]  Iteration number: 0 with current cost as 0.1998929084458868 and parameters 
[-2.59250354  2.23743464 -2.12427949 -0.11653088  0.55388694 -2.77010912
  3.06858484  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.87354666]. 
Training complete taking 1309.008901834488 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Apr  1 23:43:09 2024]  Iteration number: 0 with current cost as 0.1673453919895405 and parameters 
[-3.83548525  2.23743464 -2.12427928 -0.11653094  0.55388717 -2.77010897
  3.06858489  2.18960154  1.18552007 -1.06648317  0.60271519  1.14432454
  1.31029908 -1.87354653]. 
Working on 0.4 fold... 
[Mon Apr  1 23:47:26 2024]  Iteration number: 0 with current cost as 0.18150727317525248 and parameters 
[-3.85734406  2.23743478 -2.12427906 -0.11653088  0.55388708 -2.77010926
  3.06858469  2.18960145  1.18552027 -1.06648308  0.6027151   1.1443246
  1.31029899 -1.87354637]. 
Working on 0.6 fold... 
[Mon Apr  1 23:51:15 2024]  Iteration number: 0 with current cost as 0.169033987140204 and parameters 
[-3.80294252  2.23743471 -2.12427935 -0.11653088  0.55388708 -2.77010897
  3.06858491  2.1896016   1.18552013 -1.06648308  0.60271517  1.14432459
  1.31029899 -1.87354651]. 
Working on 0.8 fold... 
[Mon Apr  1 23:55:55 2024]  Iteration number: 0 with current cost as 0.18023726350378888 and parameters 
[-2.64172261  2.23743464 -2.12427908 -0.11653103  0.55388652 -2.77010953
  3.06858415  2.18960145  1.18551998 -1.06648364  0.6027151   1.14432445
  1.31029899 -1.87354624]. 
Working on 1.0 fold... 
[Mon Apr  1 23:59:25 2024]  Iteration number: 0 with current cost as 0.1998929084458868 and parameters 
[-2.59250354  2.23743464 -2.12427949 -0.11653088  0.55388694 -2.77010912
  3.06858484  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.87354666]. 
Training complete taking 1285.7783644199371 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Tue Apr  2 00:04:37 2024]  Iteration number: 0 with current cost as 0.1673453919895405 and parameters 
[-3.83548525  2.23743464 -2.12427928 -0.11653094  0.55388717 -2.77010897
  3.06858489  2.18960154  1.18552007 -1.06648317  0.60271519  1.14432454
  1.31029908 -1.87354653]. 
Working on 0.4 fold... 
[Tue Apr  2 00:09:03 2024]  Iteration number: 0 with current cost as 0.18150727317525248 and parameters 
[-3.85734406  2.23743478 -2.12427906 -0.11653088  0.55388708 -2.77010926
  3.06858469  2.18960145  1.18552027 -1.06648308  0.6027151   1.1443246
  1.31029899 -1.87354637]. 
Working on 0.6 fold... 
[Tue Apr  2 00:13:02 2024]  Iteration number: 0 with current cost as 0.169033987140204 and parameters 
[-3.80294252  2.23743471 -2.12427935 -0.11653088  0.55388708 -2.77010897
  3.06858491  2.1896016   1.18552013 -1.06648308  0.60271517  1.14432459
  1.31029899 -1.87354651]. 
Working on 0.8 fold... 
[Tue Apr  2 00:17:37 2024]  Iteration number: 0 with current cost as 0.18023726350378888 and parameters 
[-2.64172261  2.23743464 -2.12427908 -0.11653103  0.55388652 -2.77010953
  3.06858415  2.18960145  1.18551998 -1.06648364  0.6027151   1.14432445
  1.31029899 -1.87354624]. 
Working on 1.0 fold... 
[Tue Apr  2 00:21:16 2024]  Iteration number: 0 with current cost as 0.1998929084458868 and parameters 
[-2.59250354  2.23743464 -2.12427949 -0.11653088  0.55388694 -2.77010912
  3.06858484  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.87354666]. 
Training complete taking 1312.9329285621643 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Tue Apr  2 00:26:25 2024]  Iteration number: 0 with current cost as 0.1673453919895405 and parameters 
[-3.83548525  2.23743464 -2.12427928 -0.11653094  0.55388717 -2.77010897
  3.06858489  2.18960154  1.18552007 -1.06648317  0.60271519  1.14432454
  1.31029908 -1.87354653]. 
Working on 0.4 fold... 
[Tue Apr  2 00:30:45 2024]  Iteration number: 0 with current cost as 0.18150727317525248 and parameters 
[-3.85734406  2.23743478 -2.12427906 -0.11653088  0.55388708 -2.77010926
  3.06858469  2.18960145  1.18552027 -1.06648308  0.6027151   1.1443246
  1.31029899 -1.87354637]. 
Working on 0.6 fold... 
[Tue Apr  2 00:34:35 2024]  Iteration number: 0 with current cost as 0.169033987140204 and parameters 
[-3.80294252  2.23743471 -2.12427935 -0.11653088  0.55388708 -2.77010897
  3.06858491  2.1896016   1.18552013 -1.06648308  0.60271517  1.14432459
  1.31029899 -1.87354651]. 
Working on 0.8 fold... 
[Tue Apr  2 00:38:48 2024]  Iteration number: 0 with current cost as 0.18023726350378888 and parameters 
[-2.64172261  2.23743464 -2.12427908 -0.11653103  0.55388652 -2.77010953
  3.06858415  2.18960145  1.18551998 -1.06648364  0.6027151   1.14432445
  1.31029899 -1.87354624]. 
Working on 1.0 fold... 
[Tue Apr  2 00:42:20 2024]  Iteration number: 0 with current cost as 0.1998929084458868 and parameters 
[-2.59250354  2.23743464 -2.12427949 -0.11653088  0.55388694 -2.77010912
  3.06858484  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.87354666]. 
Training complete taking 1264.390697479248 seconds. 
Discarding model... 

Training complete taking 31353.95875787735 total seconds. 
Now scoring model... 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Scoring complete taking 3.3553788661956787 seconds. 
Saved predicted values as M-A2-CZ_Efficient-CRX_predicted_values.csv
Model scores: {'MSE_train': (4.6485581445304645,), 'R2_train': 0.36171797339108347, 'MAE_train': 1.549127233119696, 'MSE_test': 12.610030356573166, 'R2_test': -0.25710222690625817, 'MAE_test': 2.5033097941172473}. 
Saved model results as M-A2-CZ_Efficient-CRX_results.json. 
