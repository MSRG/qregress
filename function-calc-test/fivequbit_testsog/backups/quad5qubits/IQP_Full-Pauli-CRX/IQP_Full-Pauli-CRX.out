/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /scratch/j/jacobsen/gjones/quad5qubits/quadratic_train.bin... 
Successfully loaded /scratch/j/jacobsen/gjones/quad5qubits/quadratic_train.bin into X and y data. 
Loading dataset from /scratch/j/jacobsen/gjones/quad5qubits/quadratic_test.bin... 
Successfully loaded /scratch/j/jacobsen/gjones/quad5qubits/quadratic_test.bin into X and y data. 
Training model with dataset /scratch/j/jacobsen/gjones/quad5qubits/quadratic_train.bin 
 at time Mon Apr  1 16:01:44 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Apr  1 16:02:06 2024]  Iteration number: 0 with current cost as 0.5417958949159495 and parameters 
[-2.93703808  3.10543736 -2.13084911 -0.11653101  0.55388711 -2.77010897
  3.06858498  2.18960147  1.18552001 -1.06648308  0.84181283  1.14432447
  1.38455847 -1.87354677  0.7296508   2.88578421 -0.54534333 -0.47522482
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456707 -0.25136102]. 
Working on 0.4 fold... 
[Mon Apr  1 16:07:29 2024]  Iteration number: 0 with current cost as 0.6226460208075915 and parameters 
[-2.92853911  2.98407955 -2.13009767 -0.11653103  0.55388708 -2.77010899
  3.06858497  2.18960144  1.18551998 -1.06648311  0.82256897  1.14432445
  1.37327004 -1.8735468   0.72965079  2.88578419 -0.54534335 -0.47522484
 -2.02654243  0.7289737   1.60512664  2.83077104 -1.2645671  -0.25136102]. 
Working on 0.6 fold... 
[Mon Apr  1 16:12:53 2024]  Iteration number: 0 with current cost as 0.6362248000634001 and parameters 
[-2.92285715  3.07990826 -2.12956821 -0.11653104  0.55388708 -2.77010899
  3.06858497  2.18960145  1.18552    -1.0664831   0.85118343  1.14432445
  1.37725144 -1.87354679  0.7296508   2.88578418 -0.54534336 -0.47522484
 -2.02654243  0.72897368  1.60512662  2.83077104 -1.2645671  -0.25136103]. 
Working on 0.8 fold... 
[Mon Apr  1 16:17:40 2024]  Iteration number: 0 with current cost as 0.576759915419537 and parameters 
[-2.93489062  3.13535076 -2.13068319 -0.11653104  0.55388706 -2.77010899
  3.06858497  2.18960144  1.18551997 -1.0664831   0.85119016  1.14432444
  1.38509735 -1.8735468   0.72965079  2.88578418 -0.54534336 -0.47522484
 -2.02654243  0.72897368  1.60512662  2.83077103 -1.2645671  -0.25136103]. 
Working on 1.0 fold... 
[Mon Apr  1 16:23:47 2024]  Iteration number: 0 with current cost as 0.6729791966725397 and parameters 
[-2.92547499  2.96987324 -2.1298791  -0.11653103  0.55388708 -2.77010899
  3.06858498  2.18960145  1.18551998 -1.06648311  0.8260967   1.14432444
  1.37350105 -1.8735468   0.72965079  2.88578419 -0.54534336 -0.47522485
 -2.02654243  0.7289737   1.60512661  2.83077104 -1.2645671  -0.25136102]. 
Training complete taking 1650.6493391990662 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Apr  1 16:29:37 2024]  Iteration number: 0 with current cost as 0.5417958949159495 and parameters 
[-2.93703808  3.10543736 -2.13084911 -0.11653101  0.55388711 -2.77010897
  3.06858498  2.18960147  1.18552001 -1.06648308  0.84181283  1.14432447
  1.38455847 -1.87354677  0.7296508   2.88578421 -0.54534333 -0.47522482
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456707 -0.25136102]. 
Working on 0.4 fold... 
[Mon Apr  1 16:35:02 2024]  Iteration number: 0 with current cost as 0.6226460208075915 and parameters 
[-2.92853911  2.98407955 -2.13009767 -0.11653103  0.55388708 -2.77010899
  3.06858497  2.18960144  1.18551998 -1.06648311  0.82256897  1.14432445
  1.37327004 -1.8735468   0.72965079  2.88578419 -0.54534335 -0.47522484
 -2.02654243  0.7289737   1.60512664  2.83077104 -1.2645671  -0.25136102]. 
Working on 0.6 fold... 
[Mon Apr  1 16:40:47 2024]  Iteration number: 0 with current cost as 0.6362248000634001 and parameters 
[-2.92285715  3.07990826 -2.12956821 -0.11653104  0.55388708 -2.77010899
  3.06858497  2.18960145  1.18552    -1.0664831   0.85118343  1.14432445
  1.37725144 -1.87354679  0.7296508   2.88578418 -0.54534336 -0.47522484
 -2.02654243  0.72897368  1.60512662  2.83077104 -1.2645671  -0.25136103]. 
Working on 0.8 fold... 
[Mon Apr  1 16:45:52 2024]  Iteration number: 0 with current cost as 0.576759915419537 and parameters 
[-2.93489062  3.13535076 -2.13068319 -0.11653104  0.55388706 -2.77010899
  3.06858497  2.18960144  1.18551997 -1.0664831   0.85119016  1.14432444
  1.38509735 -1.8735468   0.72965079  2.88578418 -0.54534336 -0.47522484
 -2.02654243  0.72897368  1.60512662  2.83077103 -1.2645671  -0.25136103]. 
Working on 1.0 fold... 
[Mon Apr  1 16:52:01 2024]  Iteration number: 0 with current cost as 0.6729791966725397 and parameters 
[-2.92547499  2.96987324 -2.1298791  -0.11653103  0.55388708 -2.77010899
  3.06858498  2.18960145  1.18551998 -1.06648311  0.8260967   1.14432444
  1.37350105 -1.8735468   0.72965079  2.88578419 -0.54534336 -0.47522485
 -2.02654243  0.7289737   1.60512661  2.83077104 -1.2645671  -0.25136102]. 
Training complete taking 1678.7346289157867 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Apr  1 16:57:35 2024]  Iteration number: 0 with current cost as 0.5417958949159495 and parameters 
[-2.93703808  3.10543736 -2.13084911 -0.11653101  0.55388711 -2.77010897
  3.06858498  2.18960147  1.18552001 -1.06648308  0.84181283  1.14432447
  1.38455847 -1.87354677  0.7296508   2.88578421 -0.54534333 -0.47522482
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456707 -0.25136102]. 
Working on 0.4 fold... 
[Mon Apr  1 17:02:58 2024]  Iteration number: 0 with current cost as 0.6226460208075915 and parameters 
[-2.92853911  2.98407955 -2.13009767 -0.11653103  0.55388708 -2.77010899
  3.06858497  2.18960144  1.18551998 -1.06648311  0.82256897  1.14432445
  1.37327004 -1.8735468   0.72965079  2.88578419 -0.54534335 -0.47522484
 -2.02654243  0.7289737   1.60512664  2.83077104 -1.2645671  -0.25136102]. 
Working on 0.6 fold... 
[Mon Apr  1 17:08:22 2024]  Iteration number: 0 with current cost as 0.6362248000634001 and parameters 
[-2.92285715  3.07990826 -2.12956821 -0.11653104  0.55388708 -2.77010899
  3.06858497  2.18960145  1.18552    -1.0664831   0.85118343  1.14432445
  1.37725144 -1.87354679  0.7296508   2.88578418 -0.54534336 -0.47522484
 -2.02654243  0.72897368  1.60512662  2.83077104 -1.2645671  -0.25136103]. 
Working on 0.8 fold... 
[Mon Apr  1 17:13:10 2024]  Iteration number: 0 with current cost as 0.576759915419537 and parameters 
[-2.93489062  3.13535076 -2.13068319 -0.11653104  0.55388706 -2.77010899
  3.06858497  2.18960144  1.18551997 -1.0664831   0.85119016  1.14432444
  1.38509735 -1.8735468   0.72965079  2.88578418 -0.54534336 -0.47522484
 -2.02654243  0.72897368  1.60512662  2.83077103 -1.2645671  -0.25136103]. 
Working on 1.0 fold... 
[Mon Apr  1 17:19:20 2024]  Iteration number: 0 with current cost as 0.6729791966725397 and parameters 
[-2.92547499  2.96987324 -2.1298791  -0.11653103  0.55388708 -2.77010899
  3.06858498  2.18960145  1.18551998 -1.06648311  0.8260967   1.14432444
  1.37350105 -1.8735468   0.72965079  2.88578419 -0.54534336 -0.47522485
 -2.02654243  0.7289737   1.60512661  2.83077104 -1.2645671  -0.25136102]. 
Training complete taking 1635.6523826122284 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Mon Apr  1 17:24:52 2024]  Iteration number: 0 with current cost as 0.5417958949159495 and parameters 
[-2.93703808  3.10543736 -2.13084911 -0.11653101  0.55388711 -2.77010897
  3.06858498  2.18960147  1.18552001 -1.06648308  0.84181283  1.14432447
  1.38455847 -1.87354677  0.7296508   2.88578421 -0.54534333 -0.47522482
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456707 -0.25136102]. 
Working on 0.4 fold... 
[Mon Apr  1 17:30:35 2024]  Iteration number: 0 with current cost as 0.6226460208075915 and parameters 
[-2.92853911  2.98407955 -2.13009767 -0.11653103  0.55388708 -2.77010899
  3.06858497  2.18960144  1.18551998 -1.06648311  0.82256897  1.14432445
  1.37327004 -1.8735468   0.72965079  2.88578419 -0.54534335 -0.47522484
 -2.02654243  0.7289737   1.60512664  2.83077104 -1.2645671  -0.25136102]. 
Working on 0.6 fold... 
[Mon Apr  1 17:35:58 2024]  Iteration number: 0 with current cost as 0.6362248000634001 and parameters 
[-2.92285715  3.07990826 -2.12956821 -0.11653104  0.55388708 -2.77010899
  3.06858497  2.18960145  1.18552    -1.0664831   0.85118343  1.14432445
  1.37725144 -1.87354679  0.7296508   2.88578418 -0.54534336 -0.47522484
 -2.02654243  0.72897368  1.60512662  2.83077104 -1.2645671  -0.25136103]. 
Working on 0.8 fold... 
[Mon Apr  1 17:40:49 2024]  Iteration number: 0 with current cost as 0.576759915419537 and parameters 
[-2.93489062  3.13535076 -2.13068319 -0.11653104  0.55388706 -2.77010899
  3.06858497  2.18960144  1.18551997 -1.0664831   0.85119016  1.14432444
  1.38509735 -1.8735468   0.72965079  2.88578418 -0.54534336 -0.47522484
 -2.02654243  0.72897368  1.60512662  2.83077103 -1.2645671  -0.25136103]. 
Working on 1.0 fold... 
[Mon Apr  1 17:47:02 2024]  Iteration number: 0 with current cost as 0.6729791966725397 and parameters 
[-2.92547499  2.96987324 -2.1298791  -0.11653103  0.55388708 -2.77010899
  3.06858498  2.18960145  1.18551998 -1.06648311  0.8260967   1.14432444
  1.37350105 -1.8735468   0.72965079  2.88578419 -0.54534336 -0.47522485
 -2.02654243  0.7289737   1.60512661  2.83077104 -1.2645671  -0.25136102]. 
Training complete taking 1668.17768907547 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Mon Apr  1 17:52:39 2024]  Iteration number: 0 with current cost as 0.5417958949159495 and parameters 
[-2.93703808  3.10543736 -2.13084911 -0.11653101  0.55388711 -2.77010897
  3.06858498  2.18960147  1.18552001 -1.06648308  0.84181283  1.14432447
  1.38455847 -1.87354677  0.7296508   2.88578421 -0.54534333 -0.47522482
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456707 -0.25136102]. 
Working on 0.4 fold... 
[Mon Apr  1 17:58:19 2024]  Iteration number: 0 with current cost as 0.6226460208075915 and parameters 
[-2.92853911  2.98407955 -2.13009767 -0.11653103  0.55388708 -2.77010899
  3.06858497  2.18960144  1.18551998 -1.06648311  0.82256897  1.14432445
  1.37327004 -1.8735468   0.72965079  2.88578419 -0.54534335 -0.47522484
 -2.02654243  0.7289737   1.60512664  2.83077104 -1.2645671  -0.25136102]. 
Working on 0.6 fold... 
[Mon Apr  1 18:03:42 2024]  Iteration number: 0 with current cost as 0.6362248000634001 and parameters 
[-2.92285715  3.07990826 -2.12956821 -0.11653104  0.55388708 -2.77010899
  3.06858497  2.18960145  1.18552    -1.0664831   0.85118343  1.14432445
  1.37725144 -1.87354679  0.7296508   2.88578418 -0.54534336 -0.47522484
 -2.02654243  0.72897368  1.60512662  2.83077104 -1.2645671  -0.25136103]. 
Working on 0.8 fold... 
[Mon Apr  1 18:08:58 2024]  Iteration number: 0 with current cost as 0.576759915419537 and parameters 
[-2.93489062  3.13535076 -2.13068319 -0.11653104  0.55388706 -2.77010899
  3.06858497  2.18960144  1.18551997 -1.0664831   0.85119016  1.14432444
  1.38509735 -1.8735468   0.72965079  2.88578418 -0.54534336 -0.47522484
 -2.02654243  0.72897368  1.60512662  2.83077103 -1.2645671  -0.25136103]. 
Working on 1.0 fold... 
[Mon Apr  1 18:15:31 2024]  Iteration number: 0 with current cost as 0.6729791966725397 and parameters 
[-2.92547499  2.96987324 -2.1298791  -0.11653103  0.55388708 -2.77010899
  3.06858498  2.18960145  1.18551998 -1.06648311  0.8260967   1.14432444
  1.37350105 -1.8735468   0.72965079  2.88578419 -0.54534336 -0.47522485
 -2.02654243  0.7289737   1.60512661  2.83077104 -1.2645671  -0.25136102]. 
Training complete taking 1715.9470343589783 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Apr  1 18:21:15 2024]  Iteration number: 0 with current cost as 0.5417958949159495 and parameters 
[-2.93703808  3.10543736 -2.13084911 -0.11653101  0.55388711 -2.77010897
  3.06858498  2.18960147  1.18552001 -1.06648308  0.84181283  1.14432447
  1.38455847 -1.87354677  0.7296508   2.88578421 -0.54534333 -0.47522482
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456707 -0.25136102]. 
Working on 0.4 fold... 
[Mon Apr  1 18:26:37 2024]  Iteration number: 0 with current cost as 0.6226460208075915 and parameters 
[-2.92853911  2.98407955 -2.13009767 -0.11653103  0.55388708 -2.77010899
  3.06858497  2.18960144  1.18551998 -1.06648311  0.82256897  1.14432445
  1.37327004 -1.8735468   0.72965079  2.88578419 -0.54534335 -0.47522484
 -2.02654243  0.7289737   1.60512664  2.83077104 -1.2645671  -0.25136102]. 
Working on 0.6 fold... 
[Mon Apr  1 18:32:13 2024]  Iteration number: 0 with current cost as 0.6362248000634001 and parameters 
[-2.92285715  3.07990826 -2.12956821 -0.11653104  0.55388708 -2.77010899
  3.06858497  2.18960145  1.18552    -1.0664831   0.85118343  1.14432445
  1.37725144 -1.87354679  0.7296508   2.88578418 -0.54534336 -0.47522484
 -2.02654243  0.72897368  1.60512662  2.83077104 -1.2645671  -0.25136103]. 
Working on 0.8 fold... 
[Mon Apr  1 18:37:02 2024]  Iteration number: 0 with current cost as 0.576759915419537 and parameters 
[-2.93489062  3.13535076 -2.13068319 -0.11653104  0.55388706 -2.77010899
  3.06858497  2.18960144  1.18551997 -1.0664831   0.85119016  1.14432444
  1.38509735 -1.8735468   0.72965079  2.88578418 -0.54534336 -0.47522484
 -2.02654243  0.72897368  1.60512662  2.83077103 -1.2645671  -0.25136103]. 
Working on 1.0 fold... 
[Mon Apr  1 18:43:09 2024]  Iteration number: 0 with current cost as 0.6729791966725397 and parameters 
[-2.92547499  2.96987324 -2.1298791  -0.11653103  0.55388708 -2.77010899
  3.06858498  2.18960145  1.18551998 -1.06648311  0.8260967   1.14432444
  1.37350105 -1.8735468   0.72965079  2.88578419 -0.54534336 -0.47522485
 -2.02654243  0.7289737   1.60512661  2.83077104 -1.2645671  -0.25136102]. 
Training complete taking 1643.6898744106293 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Apr  1 18:48:40 2024]  Iteration number: 0 with current cost as 0.5417958949159495 and parameters 
[-2.93703808  3.10543736 -2.13084911 -0.11653101  0.55388711 -2.77010897
  3.06858498  2.18960147  1.18552001 -1.06648308  0.84181283  1.14432447
  1.38455847 -1.87354677  0.7296508   2.88578421 -0.54534333 -0.47522482
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456707 -0.25136102]. 
Working on 0.4 fold... 
[Mon Apr  1 18:54:01 2024]  Iteration number: 0 with current cost as 0.6226460208075915 and parameters 
[-2.92853911  2.98407955 -2.13009767 -0.11653103  0.55388708 -2.77010899
  3.06858497  2.18960144  1.18551998 -1.06648311  0.82256897  1.14432445
  1.37327004 -1.8735468   0.72965079  2.88578419 -0.54534335 -0.47522484
 -2.02654243  0.7289737   1.60512664  2.83077104 -1.2645671  -0.25136102]. 
Working on 0.6 fold... 
[Mon Apr  1 18:59:21 2024]  Iteration number: 0 with current cost as 0.6362248000634001 and parameters 
[-2.92285715  3.07990826 -2.12956821 -0.11653104  0.55388708 -2.77010899
  3.06858497  2.18960145  1.18552    -1.0664831   0.85118343  1.14432445
  1.37725144 -1.87354679  0.7296508   2.88578418 -0.54534336 -0.47522484
 -2.02654243  0.72897368  1.60512662  2.83077104 -1.2645671  -0.25136103]. 
Working on 0.8 fold... 
[Mon Apr  1 19:04:14 2024]  Iteration number: 0 with current cost as 0.576759915419537 and parameters 
[-2.93489062  3.13535076 -2.13068319 -0.11653104  0.55388706 -2.77010899
  3.06858497  2.18960144  1.18551997 -1.0664831   0.85119016  1.14432444
  1.38509735 -1.8735468   0.72965079  2.88578418 -0.54534336 -0.47522484
 -2.02654243  0.72897368  1.60512662  2.83077103 -1.2645671  -0.25136103]. 
Working on 1.0 fold... 
[Mon Apr  1 19:10:36 2024]  Iteration number: 0 with current cost as 0.6729791966725397 and parameters 
[-2.92547499  2.96987324 -2.1298791  -0.11653103  0.55388708 -2.77010899
  3.06858498  2.18960145  1.18551998 -1.06648311  0.8260967   1.14432444
  1.37350105 -1.8735468   0.72965079  2.88578419 -0.54534336 -0.47522485
 -2.02654243  0.7289737   1.60512661  2.83077104 -1.2645671  -0.25136102]. 
Training complete taking 1648.4654097557068 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Apr  1 19:16:09 2024]  Iteration number: 0 with current cost as 0.5417958949159495 and parameters 
[-2.93703808  3.10543736 -2.13084911 -0.11653101  0.55388711 -2.77010897
  3.06858498  2.18960147  1.18552001 -1.06648308  0.84181283  1.14432447
  1.38455847 -1.87354677  0.7296508   2.88578421 -0.54534333 -0.47522482
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456707 -0.25136102]. 
Working on 0.4 fold... 
[Mon Apr  1 19:21:33 2024]  Iteration number: 0 with current cost as 0.6226460208075915 and parameters 
[-2.92853911  2.98407955 -2.13009767 -0.11653103  0.55388708 -2.77010899
  3.06858497  2.18960144  1.18551998 -1.06648311  0.82256897  1.14432445
  1.37327004 -1.8735468   0.72965079  2.88578419 -0.54534335 -0.47522484
 -2.02654243  0.7289737   1.60512664  2.83077104 -1.2645671  -0.25136102]. 
Working on 0.6 fold... 
[Mon Apr  1 19:26:57 2024]  Iteration number: 0 with current cost as 0.6362248000634001 and parameters 
[-2.92285715  3.07990826 -2.12956821 -0.11653104  0.55388708 -2.77010899
  3.06858497  2.18960145  1.18552    -1.0664831   0.85118343  1.14432445
  1.37725144 -1.87354679  0.7296508   2.88578418 -0.54534336 -0.47522484
 -2.02654243  0.72897368  1.60512662  2.83077104 -1.2645671  -0.25136103]. 
Working on 0.8 fold... 
[Mon Apr  1 19:32:02 2024]  Iteration number: 0 with current cost as 0.576759915419537 and parameters 
[-2.93489062  3.13535076 -2.13068319 -0.11653104  0.55388706 -2.77010899
  3.06858497  2.18960144  1.18551997 -1.0664831   0.85119016  1.14432444
  1.38509735 -1.8735468   0.72965079  2.88578418 -0.54534336 -0.47522484
 -2.02654243  0.72897368  1.60512662  2.83077103 -1.2645671  -0.25136103]. 
Working on 1.0 fold... 
[Mon Apr  1 19:38:20 2024]  Iteration number: 0 with current cost as 0.6729791966725397 and parameters 
[-2.92547499  2.96987324 -2.1298791  -0.11653103  0.55388708 -2.77010899
  3.06858498  2.18960145  1.18551998 -1.06648311  0.8260967   1.14432444
  1.37350105 -1.8735468   0.72965079  2.88578419 -0.54534336 -0.47522485
 -2.02654243  0.7289737   1.60512661  2.83077104 -1.2645671  -0.25136102]. 
Training complete taking 1666.7196049690247 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Mon Apr  1 19:43:55 2024]  Iteration number: 0 with current cost as 0.5417958949159495 and parameters 
[-2.93703808  3.10543736 -2.13084911 -0.11653101  0.55388711 -2.77010897
  3.06858498  2.18960147  1.18552001 -1.06648308  0.84181283  1.14432447
  1.38455847 -1.87354677  0.7296508   2.88578421 -0.54534333 -0.47522482
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456707 -0.25136102]. 
Working on 0.4 fold... 
[Mon Apr  1 19:49:15 2024]  Iteration number: 0 with current cost as 0.6226460208075915 and parameters 
[-2.92853911  2.98407955 -2.13009767 -0.11653103  0.55388708 -2.77010899
  3.06858497  2.18960144  1.18551998 -1.06648311  0.82256897  1.14432445
  1.37327004 -1.8735468   0.72965079  2.88578419 -0.54534335 -0.47522484
 -2.02654243  0.7289737   1.60512664  2.83077104 -1.2645671  -0.25136102]. 
Working on 0.6 fold... 
[Mon Apr  1 19:54:41 2024]  Iteration number: 0 with current cost as 0.6362248000634001 and parameters 
[-2.92285715  3.07990826 -2.12956821 -0.11653104  0.55388708 -2.77010899
  3.06858497  2.18960145  1.18552    -1.0664831   0.85118343  1.14432445
  1.37725144 -1.87354679  0.7296508   2.88578418 -0.54534336 -0.47522484
 -2.02654243  0.72897368  1.60512662  2.83077104 -1.2645671  -0.25136103]. 
Working on 0.8 fold... 
[Mon Apr  1 19:59:36 2024]  Iteration number: 0 with current cost as 0.576759915419537 and parameters 
[-2.93489062  3.13535076 -2.13068319 -0.11653104  0.55388706 -2.77010899
  3.06858497  2.18960144  1.18551997 -1.0664831   0.85119016  1.14432444
  1.38509735 -1.8735468   0.72965079  2.88578418 -0.54534336 -0.47522484
 -2.02654243  0.72897368  1.60512662  2.83077103 -1.2645671  -0.25136103]. 
Working on 1.0 fold... 
[Mon Apr  1 20:05:41 2024]  Iteration number: 0 with current cost as 0.6729791966725397 and parameters 
[-2.92547499  2.96987324 -2.1298791  -0.11653103  0.55388708 -2.77010899
  3.06858498  2.18960145  1.18551998 -1.06648311  0.8260967   1.14432444
  1.37350105 -1.8735468   0.72965079  2.88578419 -0.54534336 -0.47522485
 -2.02654243  0.7289737   1.60512661  2.83077104 -1.2645671  -0.25136102]. 
Training complete taking 1648.908040523529 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Mon Apr  1 20:11:24 2024]  Iteration number: 0 with current cost as 0.5417958949159495 and parameters 
[-2.93703808  3.10543736 -2.13084911 -0.11653101  0.55388711 -2.77010897
  3.06858498  2.18960147  1.18552001 -1.06648308  0.84181283  1.14432447
  1.38455847 -1.87354677  0.7296508   2.88578421 -0.54534333 -0.47522482
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456707 -0.25136102]. 
Working on 0.4 fold... 
[Mon Apr  1 20:16:48 2024]  Iteration number: 0 with current cost as 0.6226460208075915 and parameters 
[-2.92853911  2.98407955 -2.13009767 -0.11653103  0.55388708 -2.77010899
  3.06858497  2.18960144  1.18551998 -1.06648311  0.82256897  1.14432445
  1.37327004 -1.8735468   0.72965079  2.88578419 -0.54534335 -0.47522484
 -2.02654243  0.7289737   1.60512664  2.83077104 -1.2645671  -0.25136102]. 
Working on 0.6 fold... 
[Mon Apr  1 20:22:18 2024]  Iteration number: 0 with current cost as 0.6362248000634001 and parameters 
[-2.92285715  3.07990826 -2.12956821 -0.11653104  0.55388708 -2.77010899
  3.06858497  2.18960145  1.18552    -1.0664831   0.85118343  1.14432445
  1.37725144 -1.87354679  0.7296508   2.88578418 -0.54534336 -0.47522484
 -2.02654243  0.72897368  1.60512662  2.83077104 -1.2645671  -0.25136103]. 
Working on 0.8 fold... 
[Mon Apr  1 20:27:06 2024]  Iteration number: 0 with current cost as 0.576759915419537 and parameters 
[-2.93489062  3.13535076 -2.13068319 -0.11653104  0.55388706 -2.77010899
  3.06858497  2.18960144  1.18551997 -1.0664831   0.85119016  1.14432444
  1.38509735 -1.8735468   0.72965079  2.88578418 -0.54534336 -0.47522484
 -2.02654243  0.72897368  1.60512662  2.83077103 -1.2645671  -0.25136103]. 
Working on 1.0 fold... 
[Mon Apr  1 20:33:14 2024]  Iteration number: 0 with current cost as 0.6729791966725397 and parameters 
[-2.92547499  2.96987324 -2.1298791  -0.11653103  0.55388708 -2.77010899
  3.06858498  2.18960145  1.18551998 -1.06648311  0.8260967   1.14432444
  1.37350105 -1.8735468   0.72965079  2.88578419 -0.54534336 -0.47522485
 -2.02654243  0.7289737   1.60512661  2.83077104 -1.2645671  -0.25136102]. 
Training complete taking 1649.7385728359222 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Apr  1 20:38:53 2024]  Iteration number: 0 with current cost as 0.5417958949159495 and parameters 
[-2.93703808  3.10543736 -2.13084911 -0.11653101  0.55388711 -2.77010897
  3.06858498  2.18960147  1.18552001 -1.06648308  0.84181283  1.14432447
  1.38455847 -1.87354677  0.7296508   2.88578421 -0.54534333 -0.47522482
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456707 -0.25136102]. 
Working on 0.4 fold... 
[Mon Apr  1 20:44:20 2024]  Iteration number: 0 with current cost as 0.6226460208075915 and parameters 
[-2.92853911  2.98407955 -2.13009767 -0.11653103  0.55388708 -2.77010899
  3.06858497  2.18960144  1.18551998 -1.06648311  0.82256897  1.14432445
  1.37327004 -1.8735468   0.72965079  2.88578419 -0.54534335 -0.47522484
 -2.02654243  0.7289737   1.60512664  2.83077104 -1.2645671  -0.25136102]. 
Working on 0.6 fold... 
[Mon Apr  1 20:50:10 2024]  Iteration number: 0 with current cost as 0.6362248000634001 and parameters 
[-2.92285715  3.07990826 -2.12956821 -0.11653104  0.55388708 -2.77010899
  3.06858497  2.18960145  1.18552    -1.0664831   0.85118343  1.14432445
  1.37725144 -1.87354679  0.7296508   2.88578418 -0.54534336 -0.47522484
 -2.02654243  0.72897368  1.60512662  2.83077104 -1.2645671  -0.25136103]. 
Working on 0.8 fold... 
[Mon Apr  1 20:54:57 2024]  Iteration number: 0 with current cost as 0.576759915419537 and parameters 
[-2.93489062  3.13535076 -2.13068319 -0.11653104  0.55388706 -2.77010899
  3.06858497  2.18960144  1.18551997 -1.0664831   0.85119016  1.14432444
  1.38509735 -1.8735468   0.72965079  2.88578418 -0.54534336 -0.47522484
 -2.02654243  0.72897368  1.60512662  2.83077103 -1.2645671  -0.25136103]. 
Working on 1.0 fold... 
[Mon Apr  1 21:01:04 2024]  Iteration number: 0 with current cost as 0.6729791966725397 and parameters 
[-2.92547499  2.96987324 -2.1298791  -0.11653103  0.55388708 -2.77010899
  3.06858498  2.18960145  1.18551998 -1.06648311  0.8260967   1.14432444
  1.37350105 -1.8735468   0.72965079  2.88578419 -0.54534336 -0.47522485
 -2.02654243  0.7289737   1.60512661  2.83077104 -1.2645671  -0.25136102]. 
Training complete taking 1683.2859437465668 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Apr  1 21:06:56 2024]  Iteration number: 0 with current cost as 0.5417958949159495 and parameters 
[-2.93703808  3.10543736 -2.13084911 -0.11653101  0.55388711 -2.77010897
  3.06858498  2.18960147  1.18552001 -1.06648308  0.84181283  1.14432447
  1.38455847 -1.87354677  0.7296508   2.88578421 -0.54534333 -0.47522482
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456707 -0.25136102]. 
Working on 0.4 fold... 
[Mon Apr  1 21:12:25 2024]  Iteration number: 0 with current cost as 0.6226460208075915 and parameters 
[-2.92853911  2.98407955 -2.13009767 -0.11653103  0.55388708 -2.77010899
  3.06858497  2.18960144  1.18551998 -1.06648311  0.82256897  1.14432445
  1.37327004 -1.8735468   0.72965079  2.88578419 -0.54534335 -0.47522484
 -2.02654243  0.7289737   1.60512664  2.83077104 -1.2645671  -0.25136102]. 
Working on 0.6 fold... 
[Mon Apr  1 21:17:52 2024]  Iteration number: 0 with current cost as 0.6362248000634001 and parameters 
[-2.92285715  3.07990826 -2.12956821 -0.11653104  0.55388708 -2.77010899
  3.06858497  2.18960145  1.18552    -1.0664831   0.85118343  1.14432445
  1.37725144 -1.87354679  0.7296508   2.88578418 -0.54534336 -0.47522484
 -2.02654243  0.72897368  1.60512662  2.83077104 -1.2645671  -0.25136103]. 
Working on 0.8 fold... 
[Mon Apr  1 21:22:40 2024]  Iteration number: 0 with current cost as 0.576759915419537 and parameters 
[-2.93489062  3.13535076 -2.13068319 -0.11653104  0.55388706 -2.77010899
  3.06858497  2.18960144  1.18551997 -1.0664831   0.85119016  1.14432444
  1.38509735 -1.8735468   0.72965079  2.88578418 -0.54534336 -0.47522484
 -2.02654243  0.72897368  1.60512662  2.83077103 -1.2645671  -0.25136103]. 
Working on 1.0 fold... 
[Mon Apr  1 21:28:48 2024]  Iteration number: 0 with current cost as 0.6729791966725397 and parameters 
[-2.92547499  2.96987324 -2.1298791  -0.11653103  0.55388708 -2.77010899
  3.06858498  2.18960145  1.18551998 -1.06648311  0.8260967   1.14432444
  1.37350105 -1.8735468   0.72965079  2.88578419 -0.54534336 -0.47522485
 -2.02654243  0.7289737   1.60512661  2.83077104 -1.2645671  -0.25136102]. 
Training complete taking 1644.497057914734 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Apr  1 21:34:21 2024]  Iteration number: 0 with current cost as 0.5417958949159495 and parameters 
[-2.93703808  3.10543736 -2.13084911 -0.11653101  0.55388711 -2.77010897
  3.06858498  2.18960147  1.18552001 -1.06648308  0.84181283  1.14432447
  1.38455847 -1.87354677  0.7296508   2.88578421 -0.54534333 -0.47522482
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456707 -0.25136102]. 
Working on 0.4 fold... 
[Mon Apr  1 21:39:43 2024]  Iteration number: 0 with current cost as 0.6226460208075915 and parameters 
[-2.92853911  2.98407955 -2.13009767 -0.11653103  0.55388708 -2.77010899
  3.06858497  2.18960144  1.18551998 -1.06648311  0.82256897  1.14432445
  1.37327004 -1.8735468   0.72965079  2.88578419 -0.54534335 -0.47522484
 -2.02654243  0.7289737   1.60512664  2.83077104 -1.2645671  -0.25136102]. 
Working on 0.6 fold... 
[Mon Apr  1 21:45:16 2024]  Iteration number: 0 with current cost as 0.6362248000634001 and parameters 
[-2.92285715  3.07990826 -2.12956821 -0.11653104  0.55388708 -2.77010899
  3.06858497  2.18960145  1.18552    -1.0664831   0.85118343  1.14432445
  1.37725144 -1.87354679  0.7296508   2.88578418 -0.54534336 -0.47522484
 -2.02654243  0.72897368  1.60512662  2.83077104 -1.2645671  -0.25136103]. 
Working on 0.8 fold... 
[Mon Apr  1 21:50:08 2024]  Iteration number: 0 with current cost as 0.576759915419537 and parameters 
[-2.93489062  3.13535076 -2.13068319 -0.11653104  0.55388706 -2.77010899
  3.06858497  2.18960144  1.18551997 -1.0664831   0.85119016  1.14432444
  1.38509735 -1.8735468   0.72965079  2.88578418 -0.54534336 -0.47522484
 -2.02654243  0.72897368  1.60512662  2.83077103 -1.2645671  -0.25136103]. 
Working on 1.0 fold... 
[Mon Apr  1 21:56:21 2024]  Iteration number: 0 with current cost as 0.6729791966725397 and parameters 
[-2.92547499  2.96987324 -2.1298791  -0.11653103  0.55388708 -2.77010899
  3.06858498  2.18960145  1.18551998 -1.06648311  0.8260967   1.14432444
  1.37350105 -1.8735468   0.72965079  2.88578419 -0.54534336 -0.47522485
 -2.02654243  0.7289737   1.60512661  2.83077104 -1.2645671  -0.25136102]. 
Training complete taking 1656.081782579422 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Apr  1 22:02:00 2024]  Iteration number: 0 with current cost as 0.5417958949159495 and parameters 
[-2.93703808  3.10543736 -2.13084911 -0.11653101  0.55388711 -2.77010897
  3.06858498  2.18960147  1.18552001 -1.06648308  0.84181283  1.14432447
  1.38455847 -1.87354677  0.7296508   2.88578421 -0.54534333 -0.47522482
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456707 -0.25136102]. 
Working on 0.4 fold... 
[Mon Apr  1 22:07:28 2024]  Iteration number: 0 with current cost as 0.6226460208075915 and parameters 
[-2.92853911  2.98407955 -2.13009767 -0.11653103  0.55388708 -2.77010899
  3.06858497  2.18960144  1.18551998 -1.06648311  0.82256897  1.14432445
  1.37327004 -1.8735468   0.72965079  2.88578419 -0.54534335 -0.47522484
 -2.02654243  0.7289737   1.60512664  2.83077104 -1.2645671  -0.25136102]. 
Working on 0.6 fold... 
[Mon Apr  1 22:12:47 2024]  Iteration number: 0 with current cost as 0.6362248000634001 and parameters 
[-2.92285715  3.07990826 -2.12956821 -0.11653104  0.55388708 -2.77010899
  3.06858497  2.18960145  1.18552    -1.0664831   0.85118343  1.14432445
  1.37725144 -1.87354679  0.7296508   2.88578418 -0.54534336 -0.47522484
 -2.02654243  0.72897368  1.60512662  2.83077104 -1.2645671  -0.25136103]. 
Working on 0.8 fold... 
[Mon Apr  1 22:17:35 2024]  Iteration number: 0 with current cost as 0.576759915419537 and parameters 
[-2.93489062  3.13535076 -2.13068319 -0.11653104  0.55388706 -2.77010899
  3.06858497  2.18960144  1.18551997 -1.0664831   0.85119016  1.14432444
  1.38509735 -1.8735468   0.72965079  2.88578418 -0.54534336 -0.47522484
 -2.02654243  0.72897368  1.60512662  2.83077103 -1.2645671  -0.25136103]. 
Working on 1.0 fold... 
[Mon Apr  1 22:23:42 2024]  Iteration number: 0 with current cost as 0.6729791966725397 and parameters 
[-2.92547499  2.96987324 -2.1298791  -0.11653103  0.55388708 -2.77010899
  3.06858498  2.18960145  1.18551998 -1.06648311  0.8260967   1.14432444
  1.37350105 -1.8735468   0.72965079  2.88578419 -0.54534336 -0.47522485
 -2.02654243  0.7289737   1.60512661  2.83077104 -1.2645671  -0.25136102]. 
Training complete taking 1638.088597536087 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Apr  1 22:29:16 2024]  Iteration number: 0 with current cost as 0.5417958949159495 and parameters 
[-2.93703808  3.10543736 -2.13084911 -0.11653101  0.55388711 -2.77010897
  3.06858498  2.18960147  1.18552001 -1.06648308  0.84181283  1.14432447
  1.38455847 -1.87354677  0.7296508   2.88578421 -0.54534333 -0.47522482
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456707 -0.25136102]. 
Working on 0.4 fold... 
[Mon Apr  1 22:34:39 2024]  Iteration number: 0 with current cost as 0.6226460208075915 and parameters 
[-2.92853911  2.98407955 -2.13009767 -0.11653103  0.55388708 -2.77010899
  3.06858497  2.18960144  1.18551998 -1.06648311  0.82256897  1.14432445
  1.37327004 -1.8735468   0.72965079  2.88578419 -0.54534335 -0.47522484
 -2.02654243  0.7289737   1.60512664  2.83077104 -1.2645671  -0.25136102]. 
Working on 0.6 fold... 
[Mon Apr  1 22:40:05 2024]  Iteration number: 0 with current cost as 0.6362248000634001 and parameters 
[-2.92285715  3.07990826 -2.12956821 -0.11653104  0.55388708 -2.77010899
  3.06858497  2.18960145  1.18552    -1.0664831   0.85118343  1.14432445
  1.37725144 -1.87354679  0.7296508   2.88578418 -0.54534336 -0.47522484
 -2.02654243  0.72897368  1.60512662  2.83077104 -1.2645671  -0.25136103]. 
Working on 0.8 fold... 
[Mon Apr  1 22:44:51 2024]  Iteration number: 0 with current cost as 0.576759915419537 and parameters 
[-2.93489062  3.13535076 -2.13068319 -0.11653104  0.55388706 -2.77010899
  3.06858497  2.18960144  1.18551997 -1.0664831   0.85119016  1.14432444
  1.38509735 -1.8735468   0.72965079  2.88578418 -0.54534336 -0.47522484
 -2.02654243  0.72897368  1.60512662  2.83077103 -1.2645671  -0.25136103]. 
Working on 1.0 fold... 
[Mon Apr  1 22:50:58 2024]  Iteration number: 0 with current cost as 0.6729791966725397 and parameters 
[-2.92547499  2.96987324 -2.1298791  -0.11653103  0.55388708 -2.77010899
  3.06858498  2.18960145  1.18551998 -1.06648311  0.8260967   1.14432444
  1.37350105 -1.8735468   0.72965079  2.88578419 -0.54534336 -0.47522485
 -2.02654243  0.7289737   1.60512661  2.83077104 -1.2645671  -0.25136102]. 
Training complete taking 1635.955667257309 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Apr  1 22:56:31 2024]  Iteration number: 0 with current cost as 0.5417958949159495 and parameters 
[-2.93703808  3.10543736 -2.13084911 -0.11653101  0.55388711 -2.77010897
  3.06858498  2.18960147  1.18552001 -1.06648308  0.84181283  1.14432447
  1.38455847 -1.87354677  0.7296508   2.88578421 -0.54534333 -0.47522482
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456707 -0.25136102]. 
Working on 0.4 fold... 
[Mon Apr  1 23:01:53 2024]  Iteration number: 0 with current cost as 0.6226460208075915 and parameters 
[-2.92853911  2.98407955 -2.13009767 -0.11653103  0.55388708 -2.77010899
  3.06858497  2.18960144  1.18551998 -1.06648311  0.82256897  1.14432445
  1.37327004 -1.8735468   0.72965079  2.88578419 -0.54534335 -0.47522484
 -2.02654243  0.7289737   1.60512664  2.83077104 -1.2645671  -0.25136102]. 
Working on 0.6 fold... 
[Mon Apr  1 23:07:23 2024]  Iteration number: 0 with current cost as 0.6362248000634001 and parameters 
[-2.92285715  3.07990826 -2.12956821 -0.11653104  0.55388708 -2.77010899
  3.06858497  2.18960145  1.18552    -1.0664831   0.85118343  1.14432445
  1.37725144 -1.87354679  0.7296508   2.88578418 -0.54534336 -0.47522484
 -2.02654243  0.72897368  1.60512662  2.83077104 -1.2645671  -0.25136103]. 
Working on 0.8 fold... 
[Mon Apr  1 23:12:09 2024]  Iteration number: 0 with current cost as 0.576759915419537 and parameters 
[-2.93489062  3.13535076 -2.13068319 -0.11653104  0.55388706 -2.77010899
  3.06858497  2.18960144  1.18551997 -1.0664831   0.85119016  1.14432444
  1.38509735 -1.8735468   0.72965079  2.88578418 -0.54534336 -0.47522484
 -2.02654243  0.72897368  1.60512662  2.83077103 -1.2645671  -0.25136103]. 
Working on 1.0 fold... 
[Mon Apr  1 23:18:43 2024]  Iteration number: 0 with current cost as 0.6729791966725397 and parameters 
[-2.92547499  2.96987324 -2.1298791  -0.11653103  0.55388708 -2.77010899
  3.06858498  2.18960145  1.18551998 -1.06648311  0.8260967   1.14432444
  1.37350105 -1.8735468   0.72965079  2.88578419 -0.54534336 -0.47522485
 -2.02654243  0.7289737   1.60512661  2.83077104 -1.2645671  -0.25136102]. 
Training complete taking 1675.6417016983032 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Apr  1 23:24:30 2024]  Iteration number: 0 with current cost as 0.5417958949159495 and parameters 
[-2.93703808  3.10543736 -2.13084911 -0.11653101  0.55388711 -2.77010897
  3.06858498  2.18960147  1.18552001 -1.06648308  0.84181283  1.14432447
  1.38455847 -1.87354677  0.7296508   2.88578421 -0.54534333 -0.47522482
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456707 -0.25136102]. 
Working on 0.4 fold... 
[Mon Apr  1 23:30:13 2024]  Iteration number: 0 with current cost as 0.6226460208075915 and parameters 
[-2.92853911  2.98407955 -2.13009767 -0.11653103  0.55388708 -2.77010899
  3.06858497  2.18960144  1.18551998 -1.06648311  0.82256897  1.14432445
  1.37327004 -1.8735468   0.72965079  2.88578419 -0.54534335 -0.47522484
 -2.02654243  0.7289737   1.60512664  2.83077104 -1.2645671  -0.25136102]. 
Working on 0.6 fold... 
[Mon Apr  1 23:35:34 2024]  Iteration number: 0 with current cost as 0.6362248000634001 and parameters 
[-2.92285715  3.07990826 -2.12956821 -0.11653104  0.55388708 -2.77010899
  3.06858497  2.18960145  1.18552    -1.0664831   0.85118343  1.14432445
  1.37725144 -1.87354679  0.7296508   2.88578418 -0.54534336 -0.47522484
 -2.02654243  0.72897368  1.60512662  2.83077104 -1.2645671  -0.25136103]. 
Working on 0.8 fold... 
[Mon Apr  1 23:41:21 2024]  Iteration number: 0 with current cost as 0.576759915419537 and parameters 
[-2.93489062  3.13535076 -2.13068319 -0.11653104  0.55388706 -2.77010899
  3.06858497  2.18960144  1.18551997 -1.0664831   0.85119016  1.14432444
  1.38509735 -1.8735468   0.72965079  2.88578418 -0.54534336 -0.47522484
 -2.02654243  0.72897368  1.60512662  2.83077103 -1.2645671  -0.25136103]. 
Working on 1.0 fold... 
[Mon Apr  1 23:47:29 2024]  Iteration number: 0 with current cost as 0.6729791966725397 and parameters 
[-2.92547499  2.96987324 -2.1298791  -0.11653103  0.55388708 -2.77010899
  3.06858498  2.18960145  1.18551998 -1.06648311  0.8260967   1.14432444
  1.37350105 -1.8735468   0.72965079  2.88578419 -0.54534336 -0.47522485
 -2.02654243  0.7289737   1.60512661  2.83077104 -1.2645671  -0.25136102]. 
Training complete taking 1717.1318747997284 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Apr  1 23:53:03 2024]  Iteration number: 0 with current cost as 0.5417958949159495 and parameters 
[-2.93703808  3.10543736 -2.13084911 -0.11653101  0.55388711 -2.77010897
  3.06858498  2.18960147  1.18552001 -1.06648308  0.84181283  1.14432447
  1.38455847 -1.87354677  0.7296508   2.88578421 -0.54534333 -0.47522482
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456707 -0.25136102]. 
Working on 0.4 fold... 
[Mon Apr  1 23:58:27 2024]  Iteration number: 0 with current cost as 0.6226460208075915 and parameters 
[-2.92853911  2.98407955 -2.13009767 -0.11653103  0.55388708 -2.77010899
  3.06858497  2.18960144  1.18551998 -1.06648311  0.82256897  1.14432445
  1.37327004 -1.8735468   0.72965079  2.88578419 -0.54534335 -0.47522484
 -2.02654243  0.7289737   1.60512664  2.83077104 -1.2645671  -0.25136102]. 
Working on 0.6 fold... 
[Tue Apr  2 00:03:49 2024]  Iteration number: 0 with current cost as 0.6362248000634001 and parameters 
[-2.92285715  3.07990826 -2.12956821 -0.11653104  0.55388708 -2.77010899
  3.06858497  2.18960145  1.18552    -1.0664831   0.85118343  1.14432445
  1.37725144 -1.87354679  0.7296508   2.88578418 -0.54534336 -0.47522484
 -2.02654243  0.72897368  1.60512662  2.83077104 -1.2645671  -0.25136103]. 
Working on 0.8 fold... 
[Tue Apr  2 00:08:35 2024]  Iteration number: 0 with current cost as 0.576759915419537 and parameters 
[-2.93489062  3.13535076 -2.13068319 -0.11653104  0.55388706 -2.77010899
  3.06858497  2.18960144  1.18551997 -1.0664831   0.85119016  1.14432444
  1.38509735 -1.8735468   0.72965079  2.88578418 -0.54534336 -0.47522484
 -2.02654243  0.72897368  1.60512662  2.83077103 -1.2645671  -0.25136103]. 
Working on 1.0 fold... 
[Tue Apr  2 00:14:43 2024]  Iteration number: 0 with current cost as 0.6729791966725397 and parameters 
[-2.92547499  2.96987324 -2.1298791  -0.11653103  0.55388708 -2.77010899
  3.06858498  2.18960145  1.18551998 -1.06648311  0.8260967   1.14432444
  1.37350105 -1.8735468   0.72965079  2.88578419 -0.54534336 -0.47522485
 -2.02654243  0.7289737   1.60512661  2.83077104 -1.2645671  -0.25136102]. 
Training complete taking 1629.052330493927 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Tue Apr  2 00:20:14 2024]  Iteration number: 0 with current cost as 0.5417958949159495 and parameters 
[-2.93703808  3.10543736 -2.13084911 -0.11653101  0.55388711 -2.77010897
  3.06858498  2.18960147  1.18552001 -1.06648308  0.84181283  1.14432447
  1.38455847 -1.87354677  0.7296508   2.88578421 -0.54534333 -0.47522482
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456707 -0.25136102]. 
Working on 0.4 fold... 
[Tue Apr  2 00:25:32 2024]  Iteration number: 0 with current cost as 0.6226460208075915 and parameters 
[-2.92853911  2.98407955 -2.13009767 -0.11653103  0.55388708 -2.77010899
  3.06858497  2.18960144  1.18551998 -1.06648311  0.82256897  1.14432445
  1.37327004 -1.8735468   0.72965079  2.88578419 -0.54534335 -0.47522484
 -2.02654243  0.7289737   1.60512664  2.83077104 -1.2645671  -0.25136102]. 
Working on 0.6 fold... 
[Tue Apr  2 00:30:54 2024]  Iteration number: 0 with current cost as 0.6362248000634001 and parameters 
[-2.92285715  3.07990826 -2.12956821 -0.11653104  0.55388708 -2.77010899
  3.06858497  2.18960145  1.18552    -1.0664831   0.85118343  1.14432445
  1.37725144 -1.87354679  0.7296508   2.88578418 -0.54534336 -0.47522484
 -2.02654243  0.72897368  1.60512662  2.83077104 -1.2645671  -0.25136103]. 
Working on 0.8 fold... 
[Tue Apr  2 00:35:46 2024]  Iteration number: 0 with current cost as 0.576759915419537 and parameters 
[-2.93489062  3.13535076 -2.13068319 -0.11653104  0.55388706 -2.77010899
  3.06858497  2.18960144  1.18551997 -1.0664831   0.85119016  1.14432444
  1.38509735 -1.8735468   0.72965079  2.88578418 -0.54534336 -0.47522484
 -2.02654243  0.72897368  1.60512662  2.83077103 -1.2645671  -0.25136103]. 
Working on 1.0 fold... 
[Tue Apr  2 00:42:38 2024]  Iteration number: 0 with current cost as 0.6729791966725397 and parameters 
[-2.92547499  2.96987324 -2.1298791  -0.11653103  0.55388708 -2.77010899
  3.06858498  2.18960145  1.18551998 -1.06648311  0.8260967   1.14432444
  1.37350105 -1.8735468   0.72965079  2.88578419 -0.54534336 -0.47522485
 -2.02654243  0.7289737   1.60512661  2.83077104 -1.2645671  -0.25136102]. 
Training complete taking 1677.8580284118652 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Tue Apr  2 00:48:11 2024]  Iteration number: 0 with current cost as 0.5417958949159495 and parameters 
[-2.93703808  3.10543736 -2.13084911 -0.11653101  0.55388711 -2.77010897
  3.06858498  2.18960147  1.18552001 -1.06648308  0.84181283  1.14432447
  1.38455847 -1.87354677  0.7296508   2.88578421 -0.54534333 -0.47522482
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456707 -0.25136102]. 
Working on 0.4 fold... 
[Tue Apr  2 00:53:50 2024]  Iteration number: 0 with current cost as 0.6226460208075915 and parameters 
[-2.92853911  2.98407955 -2.13009767 -0.11653103  0.55388708 -2.77010899
  3.06858497  2.18960144  1.18551998 -1.06648311  0.82256897  1.14432445
  1.37327004 -1.8735468   0.72965079  2.88578419 -0.54534335 -0.47522484
 -2.02654243  0.7289737   1.60512664  2.83077104 -1.2645671  -0.25136102]. 
Working on 0.6 fold... 
[Tue Apr  2 00:59:11 2024]  Iteration number: 0 with current cost as 0.6362248000634001 and parameters 
[-2.92285715  3.07990826 -2.12956821 -0.11653104  0.55388708 -2.77010899
  3.06858497  2.18960145  1.18552    -1.0664831   0.85118343  1.14432445
  1.37725144 -1.87354679  0.7296508   2.88578418 -0.54534336 -0.47522484
 -2.02654243  0.72897368  1.60512662  2.83077104 -1.2645671  -0.25136103]. 
Working on 0.8 fold... 
[Tue Apr  2 01:03:57 2024]  Iteration number: 0 with current cost as 0.576759915419537 and parameters 
[-2.93489062  3.13535076 -2.13068319 -0.11653104  0.55388706 -2.77010899
  3.06858497  2.18960144  1.18551997 -1.0664831   0.85119016  1.14432444
  1.38509735 -1.8735468   0.72965079  2.88578418 -0.54534336 -0.47522484
 -2.02654243  0.72897368  1.60512662  2.83077103 -1.2645671  -0.25136103]. 
Working on 1.0 fold... 
[Tue Apr  2 01:10:03 2024]  Iteration number: 0 with current cost as 0.6729791966725397 and parameters 
[-2.92547499  2.96987324 -2.1298791  -0.11653103  0.55388708 -2.77010899
  3.06858498  2.18960145  1.18551998 -1.06648311  0.8260967   1.14432444
  1.37350105 -1.8735468   0.72965079  2.88578419 -0.54534336 -0.47522485
 -2.02654243  0.7289737   1.60512661  2.83077104 -1.2645671  -0.25136102]. 
Training complete taking 1646.3045527935028 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Tue Apr  2 01:15:36 2024]  Iteration number: 0 with current cost as 0.5417958949159495 and parameters 
[-2.93703808  3.10543736 -2.13084911 -0.11653101  0.55388711 -2.77010897
  3.06858498  2.18960147  1.18552001 -1.06648308  0.84181283  1.14432447
  1.38455847 -1.87354677  0.7296508   2.88578421 -0.54534333 -0.47522482
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456707 -0.25136102]. 
Working on 0.4 fold... 
[Tue Apr  2 01:20:57 2024]  Iteration number: 0 with current cost as 0.6226460208075915 and parameters 
[-2.92853911  2.98407955 -2.13009767 -0.11653103  0.55388708 -2.77010899
  3.06858497  2.18960144  1.18551998 -1.06648311  0.82256897  1.14432445
  1.37327004 -1.8735468   0.72965079  2.88578419 -0.54534335 -0.47522484
 -2.02654243  0.7289737   1.60512664  2.83077104 -1.2645671  -0.25136102]. 
Working on 0.6 fold... 
[Tue Apr  2 01:26:19 2024]  Iteration number: 0 with current cost as 0.6362248000634001 and parameters 
[-2.92285715  3.07990826 -2.12956821 -0.11653104  0.55388708 -2.77010899
  3.06858497  2.18960145  1.18552    -1.0664831   0.85118343  1.14432445
  1.37725144 -1.87354679  0.7296508   2.88578418 -0.54534336 -0.47522484
 -2.02654243  0.72897368  1.60512662  2.83077104 -1.2645671  -0.25136103]. 
Working on 0.8 fold... 
[Tue Apr  2 01:31:05 2024]  Iteration number: 0 with current cost as 0.576759915419537 and parameters 
[-2.93489062  3.13535076 -2.13068319 -0.11653104  0.55388706 -2.77010899
  3.06858497  2.18960144  1.18551997 -1.0664831   0.85119016  1.14432444
  1.38509735 -1.8735468   0.72965079  2.88578418 -0.54534336 -0.47522484
 -2.02654243  0.72897368  1.60512662  2.83077103 -1.2645671  -0.25136103]. 
Working on 1.0 fold... 
[Tue Apr  2 01:37:16 2024]  Iteration number: 0 with current cost as 0.6729791966725397 and parameters 
[-2.92547499  2.96987324 -2.1298791  -0.11653103  0.55388708 -2.77010899
  3.06858498  2.18960145  1.18551998 -1.06648311  0.8260967   1.14432444
  1.37350105 -1.8735468   0.72965079  2.88578419 -0.54534336 -0.47522485
 -2.02654243  0.7289737   1.60512661  2.83077104 -1.2645671  -0.25136102]. 
Training complete taking 1631.528282880783 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Tue Apr  2 01:42:50 2024]  Iteration number: 0 with current cost as 0.5417958949159495 and parameters 
[-2.93703808  3.10543736 -2.13084911 -0.11653101  0.55388711 -2.77010897
  3.06858498  2.18960147  1.18552001 -1.06648308  0.84181283  1.14432447
  1.38455847 -1.87354677  0.7296508   2.88578421 -0.54534333 -0.47522482
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456707 -0.25136102]. 
Working on 0.4 fold... 
[Tue Apr  2 01:48:12 2024]  Iteration number: 0 with current cost as 0.6226460208075915 and parameters 
[-2.92853911  2.98407955 -2.13009767 -0.11653103  0.55388708 -2.77010899
  3.06858497  2.18960144  1.18551998 -1.06648311  0.82256897  1.14432445
  1.37327004 -1.8735468   0.72965079  2.88578419 -0.54534335 -0.47522484
 -2.02654243  0.7289737   1.60512664  2.83077104 -1.2645671  -0.25136102]. 
Working on 0.6 fold... 
[Tue Apr  2 01:53:35 2024]  Iteration number: 0 with current cost as 0.6362248000634001 and parameters 
[-2.92285715  3.07990826 -2.12956821 -0.11653104  0.55388708 -2.77010899
  3.06858497  2.18960145  1.18552    -1.0664831   0.85118343  1.14432445
  1.37725144 -1.87354679  0.7296508   2.88578418 -0.54534336 -0.47522484
 -2.02654243  0.72897368  1.60512662  2.83077104 -1.2645671  -0.25136103]. 
Working on 0.8 fold... 
[Tue Apr  2 01:58:21 2024]  Iteration number: 0 with current cost as 0.576759915419537 and parameters 
[-2.93489062  3.13535076 -2.13068319 -0.11653104  0.55388706 -2.77010899
  3.06858497  2.18960144  1.18551997 -1.0664831   0.85119016  1.14432444
  1.38509735 -1.8735468   0.72965079  2.88578418 -0.54534336 -0.47522484
 -2.02654243  0.72897368  1.60512662  2.83077103 -1.2645671  -0.25136103]. 
Working on 1.0 fold... 
[Tue Apr  2 02:04:30 2024]  Iteration number: 0 with current cost as 0.6729791966725397 and parameters 
[-2.92547499  2.96987324 -2.1298791  -0.11653103  0.55388708 -2.77010899
  3.06858498  2.18960145  1.18551998 -1.06648311  0.8260967   1.14432444
  1.37350105 -1.8735468   0.72965079  2.88578419 -0.54534336 -0.47522485
 -2.02654243  0.7289737   1.60512661  2.83077104 -1.2645671  -0.25136102]. 
Training complete taking 1631.7953021526337 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Tue Apr  2 02:10:01 2024]  Iteration number: 0 with current cost as 0.5417958949159495 and parameters 
[-2.93703808  3.10543736 -2.13084911 -0.11653101  0.55388711 -2.77010897
  3.06858498  2.18960147  1.18552001 -1.06648308  0.84181283  1.14432447
  1.38455847 -1.87354677  0.7296508   2.88578421 -0.54534333 -0.47522482
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456707 -0.25136102]. 
Working on 0.4 fold... 
[Tue Apr  2 02:15:23 2024]  Iteration number: 0 with current cost as 0.6226460208075915 and parameters 
[-2.92853911  2.98407955 -2.13009767 -0.11653103  0.55388708 -2.77010899
  3.06858497  2.18960144  1.18551998 -1.06648311  0.82256897  1.14432445
  1.37327004 -1.8735468   0.72965079  2.88578419 -0.54534335 -0.47522484
 -2.02654243  0.7289737   1.60512664  2.83077104 -1.2645671  -0.25136102]. 
Working on 0.6 fold... 
[Tue Apr  2 02:20:44 2024]  Iteration number: 0 with current cost as 0.6362248000634001 and parameters 
[-2.92285715  3.07990826 -2.12956821 -0.11653104  0.55388708 -2.77010899
  3.06858497  2.18960145  1.18552    -1.0664831   0.85118343  1.14432445
  1.37725144 -1.87354679  0.7296508   2.88578418 -0.54534336 -0.47522484
 -2.02654243  0.72897368  1.60512662  2.83077104 -1.2645671  -0.25136103]. 
Working on 0.8 fold... 
[Tue Apr  2 02:25:33 2024]  Iteration number: 0 with current cost as 0.576759915419537 and parameters 
[-2.93489062  3.13535076 -2.13068319 -0.11653104  0.55388706 -2.77010899
  3.06858497  2.18960144  1.18551997 -1.0664831   0.85119016  1.14432444
  1.38509735 -1.8735468   0.72965079  2.88578418 -0.54534336 -0.47522484
 -2.02654243  0.72897368  1.60512662  2.83077103 -1.2645671  -0.25136103]. 
Working on 1.0 fold... 
[Tue Apr  2 02:31:51 2024]  Iteration number: 0 with current cost as 0.6729791966725397 and parameters 
[-2.92547499  2.96987324 -2.1298791  -0.11653103  0.55388708 -2.77010899
  3.06858498  2.18960145  1.18551998 -1.06648311  0.8260967   1.14432444
  1.37350105 -1.8735468   0.72965079  2.88578419 -0.54534336 -0.47522485
 -2.02654243  0.7289737   1.60512661  2.83077104 -1.2645671  -0.25136102]. 
Training complete taking 1667.3518180847168 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Tue Apr  2 02:37:49 2024]  Iteration number: 0 with current cost as 0.5417958949159495 and parameters 
[-2.93703808  3.10543736 -2.13084911 -0.11653101  0.55388711 -2.77010897
  3.06858498  2.18960147  1.18552001 -1.06648308  0.84181283  1.14432447
  1.38455847 -1.87354677  0.7296508   2.88578421 -0.54534333 -0.47522482
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456707 -0.25136102]. 
Working on 0.4 fold... 
[Tue Apr  2 02:43:17 2024]  Iteration number: 0 with current cost as 0.6226460208075915 and parameters 
[-2.92853911  2.98407955 -2.13009767 -0.11653103  0.55388708 -2.77010899
  3.06858497  2.18960144  1.18551998 -1.06648311  0.82256897  1.14432445
  1.37327004 -1.8735468   0.72965079  2.88578419 -0.54534335 -0.47522484
 -2.02654243  0.7289737   1.60512664  2.83077104 -1.2645671  -0.25136102]. 
Working on 0.6 fold... 
[Tue Apr  2 02:48:40 2024]  Iteration number: 0 with current cost as 0.6362248000634001 and parameters 
[-2.92285715  3.07990826 -2.12956821 -0.11653104  0.55388708 -2.77010899
  3.06858497  2.18960145  1.18552    -1.0664831   0.85118343  1.14432445
  1.37725144 -1.87354679  0.7296508   2.88578418 -0.54534336 -0.47522484
 -2.02654243  0.72897368  1.60512662  2.83077104 -1.2645671  -0.25136103]. 
Working on 0.8 fold... 
[Tue Apr  2 02:53:28 2024]  Iteration number: 0 with current cost as 0.576759915419537 and parameters 
[-2.93489062  3.13535076 -2.13068319 -0.11653104  0.55388706 -2.77010899
  3.06858497  2.18960144  1.18551997 -1.0664831   0.85119016  1.14432444
  1.38509735 -1.8735468   0.72965079  2.88578418 -0.54534336 -0.47522484
 -2.02654243  0.72897368  1.60512662  2.83077103 -1.2645671  -0.25136103]. 
Working on 1.0 fold... 
[Tue Apr  2 02:59:53 2024]  Iteration number: 0 with current cost as 0.6729791966725397 and parameters 
[-2.92547499  2.96987324 -2.1298791  -0.11653103  0.55388708 -2.77010899
  3.06858498  2.18960145  1.18551998 -1.06648311  0.8260967   1.14432444
  1.37350105 -1.8735468   0.72965079  2.88578419 -0.54534336 -0.47522485
 -2.02654243  0.7289737   1.60512661  2.83077104 -1.2645671  -0.25136102]. 
Training complete taking 1658.5886085033417 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Tue Apr  2 03:05:27 2024]  Iteration number: 0 with current cost as 0.5417958949159495 and parameters 
[-2.93703808  3.10543736 -2.13084911 -0.11653101  0.55388711 -2.77010897
  3.06858498  2.18960147  1.18552001 -1.06648308  0.84181283  1.14432447
  1.38455847 -1.87354677  0.7296508   2.88578421 -0.54534333 -0.47522482
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456707 -0.25136102]. 
Working on 0.4 fold... 
[Tue Apr  2 03:10:55 2024]  Iteration number: 0 with current cost as 0.6226460208075915 and parameters 
[-2.92853911  2.98407955 -2.13009767 -0.11653103  0.55388708 -2.77010899
  3.06858497  2.18960144  1.18551998 -1.06648311  0.82256897  1.14432445
  1.37327004 -1.8735468   0.72965079  2.88578419 -0.54534335 -0.47522484
 -2.02654243  0.7289737   1.60512664  2.83077104 -1.2645671  -0.25136102]. 
Working on 0.6 fold... 
[Tue Apr  2 03:16:16 2024]  Iteration number: 0 with current cost as 0.6362248000634001 and parameters 
[-2.92285715  3.07990826 -2.12956821 -0.11653104  0.55388708 -2.77010899
  3.06858497  2.18960145  1.18552    -1.0664831   0.85118343  1.14432445
  1.37725144 -1.87354679  0.7296508   2.88578418 -0.54534336 -0.47522484
 -2.02654243  0.72897368  1.60512662  2.83077104 -1.2645671  -0.25136103]. 
Working on 0.8 fold... 
[Tue Apr  2 03:21:03 2024]  Iteration number: 0 with current cost as 0.576759915419537 and parameters 
[-2.93489062  3.13535076 -2.13068319 -0.11653104  0.55388706 -2.77010899
  3.06858497  2.18960144  1.18551997 -1.0664831   0.85119016  1.14432444
  1.38509735 -1.8735468   0.72965079  2.88578418 -0.54534336 -0.47522484
 -2.02654243  0.72897368  1.60512662  2.83077103 -1.2645671  -0.25136103]. 
Working on 1.0 fold... 
[Tue Apr  2 03:27:13 2024]  Iteration number: 0 with current cost as 0.6729791966725397 and parameters 
[-2.92547499  2.96987324 -2.1298791  -0.11653103  0.55388708 -2.77010899
  3.06858498  2.18960145  1.18551998 -1.06648311  0.8260967   1.14432444
  1.37350105 -1.8735468   0.72965079  2.88578419 -0.54534336 -0.47522485
 -2.02654243  0.7289737   1.60512661  2.83077104 -1.2645671  -0.25136102]. 
Training complete taking 1639.9396085739136 seconds. 
Discarding model... 

Training complete taking 41439.78547644615 total seconds. 
Now scoring model... 
Scoring complete taking 1.0421550273895264 seconds. 
Saved predicted values as IQP_Full-Pauli-CRX_predicted_values.csv
Model scores: {'MSE_train': (2.0945804671960566,), 'R2_train': 0.712398334294173, 'MAE_train': 1.2128708601263671, 'MSE_test': 3.1393622132782824, 'R2_test': 0.6870349144464734, 'MAE_test': 1.5231960710428187}. 
Saved model results as IQP_Full-Pauli-CRX_results.json. 
