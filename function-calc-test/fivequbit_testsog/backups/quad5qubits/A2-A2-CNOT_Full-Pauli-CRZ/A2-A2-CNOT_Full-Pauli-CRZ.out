/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /scratch/j/jacobsen/gjones/quad5qubits/quadratic_train.bin... 
Successfully loaded /scratch/j/jacobsen/gjones/quad5qubits/quadratic_train.bin into X and y data. 
Loading dataset from /scratch/j/jacobsen/gjones/quad5qubits/quadratic_test.bin... 
Successfully loaded /scratch/j/jacobsen/gjones/quad5qubits/quadratic_test.bin into X and y data. 
Training model with dataset /scratch/j/jacobsen/gjones/quad5qubits/quadratic_train.bin 
 at time Mon Apr  1 15:44:08 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Apr  1 15:44:28 2024]  Iteration number: 0 with current cost as 0.21942668048206151 and parameters 
[-2.6902276   2.29486594 -2.1407811  -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.6346364   1.14432445
  1.06662923 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Apr  1 15:51:13 2024]  Iteration number: 0 with current cost as 0.20707944059971262 and parameters 
[-2.72237926  2.28813301 -2.139446   -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.63044636  1.14432445
  1.10399249 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Apr  1 15:58:09 2024]  Iteration number: 0 with current cost as 0.19978001779527402 and parameters 
[-2.66474662  2.29833623 -2.14040938 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.63688176  1.14432446
  1.03641236 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Apr  1 16:05:20 2024]  Iteration number: 0 with current cost as 0.14572662226547733 and parameters 
[-2.64591727  2.30884203 -2.13972006 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.64125778  1.14432445
  1.00926363 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Apr  1 16:13:33 2024]  Iteration number: 0 with current cost as 0.1570153050659922 and parameters 
[-2.69607348  2.29957248 -2.13656604 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.63460477  1.14432445
  1.06798203 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2209.9720232486725 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Apr  1 16:21:19 2024]  Iteration number: 0 with current cost as 0.21942668048206151 and parameters 
[-2.6902276   2.29486594 -2.1407811  -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.6346364   1.14432445
  1.06662923 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Apr  1 16:28:04 2024]  Iteration number: 0 with current cost as 0.20707944059971262 and parameters 
[-2.72237926  2.28813301 -2.139446   -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.63044636  1.14432445
  1.10399249 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Apr  1 16:34:59 2024]  Iteration number: 0 with current cost as 0.19978001779527402 and parameters 
[-2.66474662  2.29833623 -2.14040938 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.63688176  1.14432446
  1.03641236 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Apr  1 16:42:20 2024]  Iteration number: 0 with current cost as 0.14572662226547733 and parameters 
[-2.64591727  2.30884203 -2.13972006 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.64125778  1.14432445
  1.00926363 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Apr  1 16:50:33 2024]  Iteration number: 0 with current cost as 0.1570153050659922 and parameters 
[-2.69607348  2.29957248 -2.13656604 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.63460477  1.14432445
  1.06798203 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2221.249375104904 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Apr  1 16:58:21 2024]  Iteration number: 0 with current cost as 0.21942668048206151 and parameters 
[-2.6902276   2.29486594 -2.1407811  -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.6346364   1.14432445
  1.06662923 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Apr  1 17:05:05 2024]  Iteration number: 0 with current cost as 0.20707944059971262 and parameters 
[-2.72237926  2.28813301 -2.139446   -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.63044636  1.14432445
  1.10399249 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Apr  1 17:11:59 2024]  Iteration number: 0 with current cost as 0.19978001779527402 and parameters 
[-2.66474662  2.29833623 -2.14040938 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.63688176  1.14432446
  1.03641236 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Apr  1 17:19:17 2024]  Iteration number: 0 with current cost as 0.14572662226547733 and parameters 
[-2.64591727  2.30884203 -2.13972006 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.64125778  1.14432445
  1.00926363 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Apr  1 17:27:30 2024]  Iteration number: 0 with current cost as 0.1570153050659922 and parameters 
[-2.69607348  2.29957248 -2.13656604 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.63460477  1.14432445
  1.06798203 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2216.573578119278 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Mon Apr  1 17:35:17 2024]  Iteration number: 0 with current cost as 0.21942668048206151 and parameters 
[-2.6902276   2.29486594 -2.1407811  -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.6346364   1.14432445
  1.06662923 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Apr  1 17:42:03 2024]  Iteration number: 0 with current cost as 0.20707944059971262 and parameters 
[-2.72237926  2.28813301 -2.139446   -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.63044636  1.14432445
  1.10399249 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Apr  1 17:48:59 2024]  Iteration number: 0 with current cost as 0.19978001779527402 and parameters 
[-2.66474662  2.29833623 -2.14040938 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.63688176  1.14432446
  1.03641236 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Apr  1 17:56:17 2024]  Iteration number: 0 with current cost as 0.14572662226547733 and parameters 
[-2.64591727  2.30884203 -2.13972006 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.64125778  1.14432445
  1.00926363 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Apr  1 18:04:44 2024]  Iteration number: 0 with current cost as 0.1570153050659922 and parameters 
[-2.69607348  2.29957248 -2.13656604 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.63460477  1.14432445
  1.06798203 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2229.512592315674 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Mon Apr  1 18:12:27 2024]  Iteration number: 0 with current cost as 0.21942668048206151 and parameters 
[-2.6902276   2.29486594 -2.1407811  -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.6346364   1.14432445
  1.06662923 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Apr  1 18:19:21 2024]  Iteration number: 0 with current cost as 0.20707944059971262 and parameters 
[-2.72237926  2.28813301 -2.139446   -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.63044636  1.14432445
  1.10399249 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Apr  1 18:26:17 2024]  Iteration number: 0 with current cost as 0.19978001779527402 and parameters 
[-2.66474662  2.29833623 -2.14040938 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.63688176  1.14432446
  1.03641236 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Apr  1 18:33:44 2024]  Iteration number: 0 with current cost as 0.14572662226547733 and parameters 
[-2.64591727  2.30884203 -2.13972006 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.64125778  1.14432445
  1.00926363 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Apr  1 18:42:07 2024]  Iteration number: 0 with current cost as 0.1570153050659922 and parameters 
[-2.69607348  2.29957248 -2.13656604 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.63460477  1.14432445
  1.06798203 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2244.4627418518066 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Apr  1 18:49:51 2024]  Iteration number: 0 with current cost as 0.21942668048206151 and parameters 
[-2.6902276   2.29486594 -2.1407811  -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.6346364   1.14432445
  1.06662923 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Apr  1 18:56:31 2024]  Iteration number: 0 with current cost as 0.20707944059971262 and parameters 
[-2.72237926  2.28813301 -2.139446   -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.63044636  1.14432445
  1.10399249 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Apr  1 19:03:30 2024]  Iteration number: 0 with current cost as 0.19978001779527402 and parameters 
[-2.66474662  2.29833623 -2.14040938 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.63688176  1.14432446
  1.03641236 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Apr  1 19:10:45 2024]  Iteration number: 0 with current cost as 0.14572662226547733 and parameters 
[-2.64591727  2.30884203 -2.13972006 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.64125778  1.14432445
  1.00926363 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Apr  1 19:18:59 2024]  Iteration number: 0 with current cost as 0.1570153050659922 and parameters 
[-2.69607348  2.29957248 -2.13656604 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.63460477  1.14432445
  1.06798203 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2213.885664463043 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Apr  1 19:26:43 2024]  Iteration number: 0 with current cost as 0.21942668048206151 and parameters 
[-2.6902276   2.29486594 -2.1407811  -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.6346364   1.14432445
  1.06662923 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Apr  1 19:33:36 2024]  Iteration number: 0 with current cost as 0.20707944059971262 and parameters 
[-2.72237926  2.28813301 -2.139446   -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.63044636  1.14432445
  1.10399249 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Apr  1 19:40:27 2024]  Iteration number: 0 with current cost as 0.19978001779527402 and parameters 
[-2.66474662  2.29833623 -2.14040938 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.63688176  1.14432446
  1.03641236 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Apr  1 19:47:54 2024]  Iteration number: 0 with current cost as 0.14572662226547733 and parameters 
[-2.64591727  2.30884203 -2.13972006 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.64125778  1.14432445
  1.00926363 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Apr  1 19:56:12 2024]  Iteration number: 0 with current cost as 0.1570153050659922 and parameters 
[-2.69607348  2.29957248 -2.13656604 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.63460477  1.14432445
  1.06798203 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2245.3848066329956 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Apr  1 20:04:08 2024]  Iteration number: 0 with current cost as 0.21942668048206151 and parameters 
[-2.6902276   2.29486594 -2.1407811  -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.6346364   1.14432445
  1.06662923 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Apr  1 20:10:55 2024]  Iteration number: 0 with current cost as 0.20707944059971262 and parameters 
[-2.72237926  2.28813301 -2.139446   -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.63044636  1.14432445
  1.10399249 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Apr  1 20:17:47 2024]  Iteration number: 0 with current cost as 0.19978001779527402 and parameters 
[-2.66474662  2.29833623 -2.14040938 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.63688176  1.14432446
  1.03641236 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Apr  1 20:25:08 2024]  Iteration number: 0 with current cost as 0.14572662226547733 and parameters 
[-2.64591727  2.30884203 -2.13972006 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.64125778  1.14432445
  1.00926363 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Apr  1 20:33:45 2024]  Iteration number: 0 with current cost as 0.1570153050659922 and parameters 
[-2.69607348  2.29957248 -2.13656604 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.63460477  1.14432445
  1.06798203 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2238.069566488266 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Mon Apr  1 20:41:28 2024]  Iteration number: 0 with current cost as 0.21942668048206151 and parameters 
[-2.6902276   2.29486594 -2.1407811  -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.6346364   1.14432445
  1.06662923 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Apr  1 20:48:33 2024]  Iteration number: 0 with current cost as 0.20707944059971262 and parameters 
[-2.72237926  2.28813301 -2.139446   -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.63044636  1.14432445
  1.10399249 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Apr  1 20:55:30 2024]  Iteration number: 0 with current cost as 0.19978001779527402 and parameters 
[-2.66474662  2.29833623 -2.14040938 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.63688176  1.14432446
  1.03641236 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Apr  1 21:02:40 2024]  Iteration number: 0 with current cost as 0.14572662226547733 and parameters 
[-2.64591727  2.30884203 -2.13972006 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.64125778  1.14432445
  1.00926363 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Apr  1 21:11:21 2024]  Iteration number: 0 with current cost as 0.1570153050659922 and parameters 
[-2.69607348  2.29957248 -2.13656604 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.63460477  1.14432445
  1.06798203 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2260.372886657715 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Mon Apr  1 21:19:08 2024]  Iteration number: 0 with current cost as 0.21942668048206151 and parameters 
[-2.6902276   2.29486594 -2.1407811  -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.6346364   1.14432445
  1.06662923 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Apr  1 21:25:50 2024]  Iteration number: 0 with current cost as 0.20707944059971262 and parameters 
[-2.72237926  2.28813301 -2.139446   -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.63044636  1.14432445
  1.10399249 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Apr  1 21:32:44 2024]  Iteration number: 0 with current cost as 0.19978001779527402 and parameters 
[-2.66474662  2.29833623 -2.14040938 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.63688176  1.14432446
  1.03641236 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Apr  1 21:40:01 2024]  Iteration number: 0 with current cost as 0.14572662226547733 and parameters 
[-2.64591727  2.30884203 -2.13972006 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.64125778  1.14432445
  1.00926363 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Apr  1 21:48:28 2024]  Iteration number: 0 with current cost as 0.1570153050659922 and parameters 
[-2.69607348  2.29957248 -2.13656604 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.63460477  1.14432445
  1.06798203 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2221.3701026439667 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Apr  1 21:56:09 2024]  Iteration number: 0 with current cost as 0.21942668048206151 and parameters 
[-2.6902276   2.29486594 -2.1407811  -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.6346364   1.14432445
  1.06662923 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Apr  1 22:02:53 2024]  Iteration number: 0 with current cost as 0.20707944059971262 and parameters 
[-2.72237926  2.28813301 -2.139446   -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.63044636  1.14432445
  1.10399249 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Apr  1 22:09:46 2024]  Iteration number: 0 with current cost as 0.19978001779527402 and parameters 
[-2.66474662  2.29833623 -2.14040938 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.63688176  1.14432446
  1.03641236 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Apr  1 22:17:00 2024]  Iteration number: 0 with current cost as 0.14572662226547733 and parameters 
[-2.64591727  2.30884203 -2.13972006 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.64125778  1.14432445
  1.00926363 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Apr  1 22:25:09 2024]  Iteration number: 0 with current cost as 0.1570153050659922 and parameters 
[-2.69607348  2.29957248 -2.13656604 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.63460477  1.14432445
  1.06798203 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2199.178681373596 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Apr  1 22:32:47 2024]  Iteration number: 0 with current cost as 0.21942668048206151 and parameters 
[-2.6902276   2.29486594 -2.1407811  -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.6346364   1.14432445
  1.06662923 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Apr  1 22:39:31 2024]  Iteration number: 0 with current cost as 0.20707944059971262 and parameters 
[-2.72237926  2.28813301 -2.139446   -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.63044636  1.14432445
  1.10399249 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Apr  1 22:46:23 2024]  Iteration number: 0 with current cost as 0.19978001779527402 and parameters 
[-2.66474662  2.29833623 -2.14040938 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.63688176  1.14432446
  1.03641236 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Apr  1 22:53:37 2024]  Iteration number: 0 with current cost as 0.14572662226547733 and parameters 
[-2.64591727  2.30884203 -2.13972006 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.64125778  1.14432445
  1.00926363 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Apr  1 23:01:49 2024]  Iteration number: 0 with current cost as 0.1570153050659922 and parameters 
[-2.69607348  2.29957248 -2.13656604 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.63460477  1.14432445
  1.06798203 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2202.3811366558075 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Apr  1 23:09:31 2024]  Iteration number: 0 with current cost as 0.21942668048206151 and parameters 
[-2.6902276   2.29486594 -2.1407811  -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.6346364   1.14432445
  1.06662923 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Apr  1 23:16:12 2024]  Iteration number: 0 with current cost as 0.20707944059971262 and parameters 
[-2.72237926  2.28813301 -2.139446   -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.63044636  1.14432445
  1.10399249 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Apr  1 23:23:04 2024]  Iteration number: 0 with current cost as 0.19978001779527402 and parameters 
[-2.66474662  2.29833623 -2.14040938 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.63688176  1.14432446
  1.03641236 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Apr  1 23:30:17 2024]  Iteration number: 0 with current cost as 0.14572662226547733 and parameters 
[-2.64591727  2.30884203 -2.13972006 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.64125778  1.14432445
  1.00926363 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Apr  1 23:38:29 2024]  Iteration number: 0 with current cost as 0.1570153050659922 and parameters 
[-2.69607348  2.29957248 -2.13656604 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.63460477  1.14432445
  1.06798203 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2201.58726477623 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Apr  1 23:46:13 2024]  Iteration number: 0 with current cost as 0.21942668048206151 and parameters 
[-2.6902276   2.29486594 -2.1407811  -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.6346364   1.14432445
  1.06662923 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Apr  1 23:52:52 2024]  Iteration number: 0 with current cost as 0.20707944059971262 and parameters 
[-2.72237926  2.28813301 -2.139446   -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.63044636  1.14432445
  1.10399249 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Apr  1 23:59:42 2024]  Iteration number: 0 with current cost as 0.19978001779527402 and parameters 
[-2.66474662  2.29833623 -2.14040938 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.63688176  1.14432446
  1.03641236 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Tue Apr  2 00:06:53 2024]  Iteration number: 0 with current cost as 0.14572662226547733 and parameters 
[-2.64591727  2.30884203 -2.13972006 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.64125778  1.14432445
  1.00926363 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Tue Apr  2 00:15:06 2024]  Iteration number: 0 with current cost as 0.1570153050659922 and parameters 
[-2.69607348  2.29957248 -2.13656604 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.63460477  1.14432445
  1.06798203 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2193.876016139984 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Tue Apr  2 00:22:46 2024]  Iteration number: 0 with current cost as 0.21942668048206151 and parameters 
[-2.6902276   2.29486594 -2.1407811  -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.6346364   1.14432445
  1.06662923 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Tue Apr  2 00:29:29 2024]  Iteration number: 0 with current cost as 0.20707944059971262 and parameters 
[-2.72237926  2.28813301 -2.139446   -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.63044636  1.14432445
  1.10399249 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Tue Apr  2 00:36:21 2024]  Iteration number: 0 with current cost as 0.19978001779527402 and parameters 
[-2.66474662  2.29833623 -2.14040938 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.63688176  1.14432446
  1.03641236 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Tue Apr  2 00:43:32 2024]  Iteration number: 0 with current cost as 0.14572662226547733 and parameters 
[-2.64591727  2.30884203 -2.13972006 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.64125778  1.14432445
  1.00926363 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Tue Apr  2 00:51:44 2024]  Iteration number: 0 with current cost as 0.1570153050659922 and parameters 
[-2.69607348  2.29957248 -2.13656604 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.63460477  1.14432445
  1.06798203 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2201.0468657016754 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Tue Apr  2 00:59:26 2024]  Iteration number: 0 with current cost as 0.21942668048206151 and parameters 
[-2.6902276   2.29486594 -2.1407811  -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.6346364   1.14432445
  1.06662923 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Tue Apr  2 01:06:10 2024]  Iteration number: 0 with current cost as 0.20707944059971262 and parameters 
[-2.72237926  2.28813301 -2.139446   -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.63044636  1.14432445
  1.10399249 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Tue Apr  2 01:13:02 2024]  Iteration number: 0 with current cost as 0.19978001779527402 and parameters 
[-2.66474662  2.29833623 -2.14040938 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.63688176  1.14432446
  1.03641236 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Tue Apr  2 01:20:12 2024]  Iteration number: 0 with current cost as 0.14572662226547733 and parameters 
[-2.64591727  2.30884203 -2.13972006 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.64125778  1.14432445
  1.00926363 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Tue Apr  2 01:28:20 2024]  Iteration number: 0 with current cost as 0.1570153050659922 and parameters 
[-2.69607348  2.29957248 -2.13656604 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.63460477  1.14432445
  1.06798203 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2194.2622950077057 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Tue Apr  2 01:36:00 2024]  Iteration number: 0 with current cost as 0.21942668048206151 and parameters 
[-2.6902276   2.29486594 -2.1407811  -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.6346364   1.14432445
  1.06662923 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Tue Apr  2 01:42:44 2024]  Iteration number: 0 with current cost as 0.20707944059971262 and parameters 
[-2.72237926  2.28813301 -2.139446   -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.63044636  1.14432445
  1.10399249 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Tue Apr  2 01:49:35 2024]  Iteration number: 0 with current cost as 0.19978001779527402 and parameters 
[-2.66474662  2.29833623 -2.14040938 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.63688176  1.14432446
  1.03641236 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Tue Apr  2 01:56:52 2024]  Iteration number: 0 with current cost as 0.14572662226547733 and parameters 
[-2.64591727  2.30884203 -2.13972006 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.64125778  1.14432445
  1.00926363 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Tue Apr  2 02:05:05 2024]  Iteration number: 0 with current cost as 0.1570153050659922 and parameters 
[-2.69607348  2.29957248 -2.13656604 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.63460477  1.14432445
  1.06798203 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2226.480385541916 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Tue Apr  2 02:13:07 2024]  Iteration number: 0 with current cost as 0.21942668048206151 and parameters 
[-2.6902276   2.29486594 -2.1407811  -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.6346364   1.14432445
  1.06662923 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Tue Apr  2 02:19:56 2024]  Iteration number: 0 with current cost as 0.20707944059971262 and parameters 
[-2.72237926  2.28813301 -2.139446   -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.63044636  1.14432445
  1.10399249 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Tue Apr  2 02:26:52 2024]  Iteration number: 0 with current cost as 0.19978001779527402 and parameters 
[-2.66474662  2.29833623 -2.14040938 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.63688176  1.14432446
  1.03641236 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Tue Apr  2 02:34:18 2024]  Iteration number: 0 with current cost as 0.14572662226547733 and parameters 
[-2.64591727  2.30884203 -2.13972006 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.64125778  1.14432445
  1.00926363 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Tue Apr  2 02:42:31 2024]  Iteration number: 0 with current cost as 0.1570153050659922 and parameters 
[-2.69607348  2.29957248 -2.13656604 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.63460477  1.14432445
  1.06798203 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2225.4065759181976 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Tue Apr  2 02:50:13 2024]  Iteration number: 0 with current cost as 0.21942668048206151 and parameters 
[-2.6902276   2.29486594 -2.1407811  -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.6346364   1.14432445
  1.06662923 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Tue Apr  2 02:56:55 2024]  Iteration number: 0 with current cost as 0.20707944059971262 and parameters 
[-2.72237926  2.28813301 -2.139446   -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.63044636  1.14432445
  1.10399249 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Tue Apr  2 03:03:49 2024]  Iteration number: 0 with current cost as 0.19978001779527402 and parameters 
[-2.66474662  2.29833623 -2.14040938 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.63688176  1.14432446
  1.03641236 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Tue Apr  2 03:11:03 2024]  Iteration number: 0 with current cost as 0.14572662226547733 and parameters 
[-2.64591727  2.30884203 -2.13972006 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.64125778  1.14432445
  1.00926363 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Tue Apr  2 03:19:16 2024]  Iteration number: 0 with current cost as 0.1570153050659922 and parameters 
[-2.69607348  2.29957248 -2.13656604 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.63460477  1.14432445
  1.06798203 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2205.5309715270996 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Tue Apr  2 03:26:59 2024]  Iteration number: 0 with current cost as 0.21942668048206151 and parameters 
[-2.6902276   2.29486594 -2.1407811  -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.6346364   1.14432445
  1.06662923 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Tue Apr  2 03:34:05 2024]  Iteration number: 0 with current cost as 0.20707944059971262 and parameters 
[-2.72237926  2.28813301 -2.139446   -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.63044636  1.14432445
  1.10399249 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Tue Apr  2 03:41:17 2024]  Iteration number: 0 with current cost as 0.19978001779527402 and parameters 
[-2.66474662  2.29833623 -2.14040938 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.63688176  1.14432446
  1.03641236 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Tue Apr  2 03:48:45 2024]  Iteration number: 0 with current cost as 0.14572662226547733 and parameters 
[-2.64591727  2.30884203 -2.13972006 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.64125778  1.14432445
  1.00926363 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Tue Apr  2 03:56:59 2024]  Iteration number: 0 with current cost as 0.1570153050659922 and parameters 
[-2.69607348  2.29957248 -2.13656604 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.63460477  1.14432445
  1.06798203 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2263.918486595154 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Tue Apr  2 04:04:42 2024]  Iteration number: 0 with current cost as 0.21942668048206151 and parameters 
[-2.6902276   2.29486594 -2.1407811  -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.6346364   1.14432445
  1.06662923 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Tue Apr  2 04:11:28 2024]  Iteration number: 0 with current cost as 0.20707944059971262 and parameters 
[-2.72237926  2.28813301 -2.139446   -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.63044636  1.14432445
  1.10399249 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Tue Apr  2 04:18:21 2024]  Iteration number: 0 with current cost as 0.19978001779527402 and parameters 
[-2.66474662  2.29833623 -2.14040938 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.63688176  1.14432446
  1.03641236 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Tue Apr  2 04:25:42 2024]  Iteration number: 0 with current cost as 0.14572662226547733 and parameters 
[-2.64591727  2.30884203 -2.13972006 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.64125778  1.14432445
  1.00926363 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Tue Apr  2 04:33:55 2024]  Iteration number: 0 with current cost as 0.1570153050659922 and parameters 
[-2.69607348  2.29957248 -2.13656604 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.63460477  1.14432445
  1.06798203 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2215.902268886566 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Tue Apr  2 04:41:39 2024]  Iteration number: 0 with current cost as 0.21942668048206151 and parameters 
[-2.6902276   2.29486594 -2.1407811  -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.6346364   1.14432445
  1.06662923 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Tue Apr  2 04:48:53 2024]  Iteration number: 0 with current cost as 0.20707944059971262 and parameters 
[-2.72237926  2.28813301 -2.139446   -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.63044636  1.14432445
  1.10399249 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Tue Apr  2 04:55:59 2024]  Iteration number: 0 with current cost as 0.19978001779527402 and parameters 
[-2.66474662  2.29833623 -2.14040938 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.63688176  1.14432446
  1.03641236 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Tue Apr  2 05:03:11 2024]  Iteration number: 0 with current cost as 0.14572662226547733 and parameters 
[-2.64591727  2.30884203 -2.13972006 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.64125778  1.14432445
  1.00926363 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Tue Apr  2 05:11:22 2024]  Iteration number: 0 with current cost as 0.1570153050659922 and parameters 
[-2.69607348  2.29957248 -2.13656604 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.63460477  1.14432445
  1.06798203 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2245.764996290207 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Tue Apr  2 05:19:05 2024]  Iteration number: 0 with current cost as 0.21942668048206151 and parameters 
[-2.6902276   2.29486594 -2.1407811  -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.6346364   1.14432445
  1.06662923 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Tue Apr  2 05:25:49 2024]  Iteration number: 0 with current cost as 0.20707944059971262 and parameters 
[-2.72237926  2.28813301 -2.139446   -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.63044636  1.14432445
  1.10399249 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Tue Apr  2 05:32:43 2024]  Iteration number: 0 with current cost as 0.19978001779527402 and parameters 
[-2.66474662  2.29833623 -2.14040938 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.63688176  1.14432446
  1.03641236 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Tue Apr  2 05:39:57 2024]  Iteration number: 0 with current cost as 0.14572662226547733 and parameters 
[-2.64591727  2.30884203 -2.13972006 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.64125778  1.14432445
  1.00926363 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Tue Apr  2 05:48:08 2024]  Iteration number: 0 with current cost as 0.1570153050659922 and parameters 
[-2.69607348  2.29957248 -2.13656604 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.63460477  1.14432445
  1.06798203 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2204.784541606903 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Tue Apr  2 05:55:48 2024]  Iteration number: 0 with current cost as 0.21942668048206151 and parameters 
[-2.6902276   2.29486594 -2.1407811  -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.6346364   1.14432445
  1.06662923 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Tue Apr  2 06:02:33 2024]  Iteration number: 0 with current cost as 0.20707944059971262 and parameters 
[-2.72237926  2.28813301 -2.139446   -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.63044636  1.14432445
  1.10399249 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Tue Apr  2 06:09:28 2024]  Iteration number: 0 with current cost as 0.19978001779527402 and parameters 
[-2.66474662  2.29833623 -2.14040938 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.63688176  1.14432446
  1.03641236 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Tue Apr  2 06:16:41 2024]  Iteration number: 0 with current cost as 0.14572662226547733 and parameters 
[-2.64591727  2.30884203 -2.13972006 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.64125778  1.14432445
  1.00926363 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Tue Apr  2 06:24:53 2024]  Iteration number: 0 with current cost as 0.1570153050659922 and parameters 
[-2.69607348  2.29957248 -2.13656604 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.63460477  1.14432445
  1.06798203 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2206.7432000637054 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Tue Apr  2 06:32:36 2024]  Iteration number: 0 with current cost as 0.21942668048206151 and parameters 
[-2.6902276   2.29486594 -2.1407811  -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.6346364   1.14432445
  1.06662923 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Tue Apr  2 06:39:18 2024]  Iteration number: 0 with current cost as 0.20707944059971262 and parameters 
[-2.72237926  2.28813301 -2.139446   -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.63044636  1.14432445
  1.10399249 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Tue Apr  2 06:46:15 2024]  Iteration number: 0 with current cost as 0.19978001779527402 and parameters 
[-2.66474662  2.29833623 -2.14040938 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.63688176  1.14432446
  1.03641236 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Tue Apr  2 06:53:41 2024]  Iteration number: 0 with current cost as 0.14572662226547733 and parameters 
[-2.64591727  2.30884203 -2.13972006 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.64125778  1.14432445
  1.00926363 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Tue Apr  2 07:01:58 2024]  Iteration number: 0 with current cost as 0.1570153050659922 and parameters 
[-2.69607348  2.29957248 -2.13656604 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.63460477  1.14432445
  1.06798203 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2226.438990831375 seconds. 
Discarding model... 

Training complete taking 55514.1578066349 total seconds. 
Now scoring model... 
Scoring complete taking 0.9612951278686523 seconds. 
Saved predicted values as A2-A2-CNOT_Full-Pauli-CRZ_predicted_values.csv
Model scores: {'MSE_train': (3.021791272468062,), 'R2_train': 0.5850853108830226, 'MAE_train': 1.1973087836666134, 'MSE_test': 2.8025205255889256, 'R2_test': 0.7206148840211244, 'MAE_test': 1.3618575649291074}. 
Saved model results as A2-A2-CNOT_Full-Pauli-CRZ_results.json. 
