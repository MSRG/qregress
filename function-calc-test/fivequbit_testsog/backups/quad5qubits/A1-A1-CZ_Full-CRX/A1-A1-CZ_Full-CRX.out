/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /scratch/j/jacobsen/gjones/quad5qubits/quadratic_train.bin... 
Successfully loaded /scratch/j/jacobsen/gjones/quad5qubits/quadratic_train.bin into X and y data. 
Loading dataset from /scratch/j/jacobsen/gjones/quad5qubits/quadratic_test.bin... 
Successfully loaded /scratch/j/jacobsen/gjones/quad5qubits/quadratic_test.bin into X and y data. 
Training model with dataset /scratch/j/jacobsen/gjones/quad5qubits/quadratic_train.bin 
 at time Mon Apr  1 16:18:57 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Apr  1 16:22:01 2024]  Iteration number: 0 with current cost as 0.15958078336700227 and parameters 
[-2.49771543  2.23743467 -2.1242796  -0.11653101  0.55388708 -2.77010895
  3.06858502  2.18960147  1.18552002 -1.06648308  0.60271514  1.14432445
  1.310299   -1.87354676  0.72965082  2.88578421 -0.54534335 -0.47522481
 -2.0265424   0.72897371  1.60512664  2.83077107 -1.2645671  -0.25136101
 -2.39279216 -2.27309771  3.13337155  2.54856958 -0.67550785 -2.69002202]. 
Working on 0.4 fold... 
[Mon Apr  1 16:27:34 2024]  Iteration number: 0 with current cost as 0.16883560063022118 and parameters 
[-2.45188492  2.23743462 -2.12427962 -0.11653105  0.55388706 -2.77010899
  3.06858498  2.18960143  1.18551998 -1.06648312  0.60271512  1.14432445
  1.31029897 -1.87354678  0.72965078  2.88578417 -0.54534337 -0.47522487
 -2.02654242  0.72897368  1.60512662  2.83077105 -1.26456712 -0.25136105
 -2.39279218 -2.27309774  3.13337153  2.54856958 -0.67550789 -2.69002204]. 
Working on 0.6 fold... 
[Mon Apr  1 16:33:06 2024]  Iteration number: 0 with current cost as 0.1642196386099384 and parameters 
[-2.51311811  2.23743464 -2.12427962 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960147  1.18552    -1.06648312  0.60271514  1.14432445
  1.31029899 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Apr  1 16:38:36 2024]  Iteration number: 0 with current cost as 0.12811785346541532 and parameters 
[-2.52172059  2.23743464 -2.1242796  -0.11653103  0.55388708 -2.77010897
  3.068585    2.18960147  1.18552    -1.0664831   0.60271512  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002204]. 
Working on 1.0 fold... 
[Mon Apr  1 16:44:08 2024]  Iteration number: 0 with current cost as 0.14843409407386665 and parameters 
[-2.45687503  2.23743461 -2.12427962 -0.11653105  0.55388706 -2.77010899
  3.068585    2.18960143  1.18552001 -1.06648313  0.60271512  1.14432445
  1.31029896 -1.8735468   0.72965078  2.88578417 -0.54534337 -0.47522483
 -2.02654243  0.72897367  1.60512662  2.83077105 -1.2645671  -0.25136103
 -2.39279218 -2.27309776  3.13337153  2.5485696  -0.67550787 -2.69002204]. 
Training complete taking 1659.8135011196136 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Apr  1 16:49:42 2024]  Iteration number: 0 with current cost as 0.15958078336700227 and parameters 
[-2.49771543  2.23743467 -2.1242796  -0.11653101  0.55388708 -2.77010895
  3.06858502  2.18960147  1.18552002 -1.06648308  0.60271514  1.14432445
  1.310299   -1.87354676  0.72965082  2.88578421 -0.54534335 -0.47522481
 -2.0265424   0.72897371  1.60512664  2.83077107 -1.2645671  -0.25136101
 -2.39279216 -2.27309771  3.13337155  2.54856958 -0.67550785 -2.69002202]. 
Working on 0.4 fold... 
[Mon Apr  1 16:55:14 2024]  Iteration number: 0 with current cost as 0.16883560063022118 and parameters 
[-2.45188492  2.23743462 -2.12427962 -0.11653105  0.55388706 -2.77010899
  3.06858498  2.18960143  1.18551998 -1.06648312  0.60271512  1.14432445
  1.31029897 -1.87354678  0.72965078  2.88578417 -0.54534337 -0.47522487
 -2.02654242  0.72897368  1.60512662  2.83077105 -1.26456712 -0.25136105
 -2.39279218 -2.27309774  3.13337153  2.54856958 -0.67550789 -2.69002204]. 
Working on 0.6 fold... 
[Mon Apr  1 17:00:51 2024]  Iteration number: 0 with current cost as 0.1642196386099384 and parameters 
[-2.51311811  2.23743464 -2.12427962 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960147  1.18552    -1.06648312  0.60271514  1.14432445
  1.31029899 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Apr  1 17:06:31 2024]  Iteration number: 0 with current cost as 0.12811785346541532 and parameters 
[-2.52172059  2.23743464 -2.1242796  -0.11653103  0.55388708 -2.77010897
  3.068585    2.18960147  1.18552    -1.0664831   0.60271512  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002204]. 
Working on 1.0 fold... 
[Mon Apr  1 17:12:07 2024]  Iteration number: 0 with current cost as 0.14843409407386665 and parameters 
[-2.45687503  2.23743461 -2.12427962 -0.11653105  0.55388706 -2.77010899
  3.068585    2.18960143  1.18552001 -1.06648313  0.60271512  1.14432445
  1.31029896 -1.8735468   0.72965078  2.88578417 -0.54534337 -0.47522483
 -2.02654243  0.72897367  1.60512662  2.83077105 -1.2645671  -0.25136103
 -2.39279218 -2.27309776  3.13337153  2.5485696  -0.67550787 -2.69002204]. 
Training complete taking 1681.5301311016083 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Apr  1 17:17:46 2024]  Iteration number: 0 with current cost as 0.15958078336700227 and parameters 
[-2.49771543  2.23743467 -2.1242796  -0.11653101  0.55388708 -2.77010895
  3.06858502  2.18960147  1.18552002 -1.06648308  0.60271514  1.14432445
  1.310299   -1.87354676  0.72965082  2.88578421 -0.54534335 -0.47522481
 -2.0265424   0.72897371  1.60512664  2.83077107 -1.2645671  -0.25136101
 -2.39279216 -2.27309771  3.13337155  2.54856958 -0.67550785 -2.69002202]. 
Working on 0.4 fold... 
[Mon Apr  1 17:23:22 2024]  Iteration number: 0 with current cost as 0.16883560063022118 and parameters 
[-2.45188492  2.23743462 -2.12427962 -0.11653105  0.55388706 -2.77010899
  3.06858498  2.18960143  1.18551998 -1.06648312  0.60271512  1.14432445
  1.31029897 -1.87354678  0.72965078  2.88578417 -0.54534337 -0.47522487
 -2.02654242  0.72897368  1.60512662  2.83077105 -1.26456712 -0.25136105
 -2.39279218 -2.27309774  3.13337153  2.54856958 -0.67550789 -2.69002204]. 
Working on 0.6 fold... 
[Mon Apr  1 17:28:57 2024]  Iteration number: 0 with current cost as 0.1642196386099384 and parameters 
[-2.51311811  2.23743464 -2.12427962 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960147  1.18552    -1.06648312  0.60271514  1.14432445
  1.31029899 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Apr  1 17:34:30 2024]  Iteration number: 0 with current cost as 0.12811785346541532 and parameters 
[-2.52172059  2.23743464 -2.1242796  -0.11653103  0.55388708 -2.77010897
  3.068585    2.18960147  1.18552    -1.0664831   0.60271512  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002204]. 
Working on 1.0 fold... 
[Mon Apr  1 17:40:06 2024]  Iteration number: 0 with current cost as 0.14843409407386665 and parameters 
[-2.45687503  2.23743461 -2.12427962 -0.11653105  0.55388706 -2.77010899
  3.068585    2.18960143  1.18552001 -1.06648313  0.60271512  1.14432445
  1.31029896 -1.8735468   0.72965078  2.88578417 -0.54534337 -0.47522483
 -2.02654243  0.72897367  1.60512662  2.83077105 -1.2645671  -0.25136103
 -2.39279218 -2.27309776  3.13337153  2.5485696  -0.67550787 -2.69002204]. 
Training complete taking 1678.9388496875763 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Mon Apr  1 17:45:42 2024]  Iteration number: 0 with current cost as 0.15958078336700227 and parameters 
[-2.49771543  2.23743467 -2.1242796  -0.11653101  0.55388708 -2.77010895
  3.06858502  2.18960147  1.18552002 -1.06648308  0.60271514  1.14432445
  1.310299   -1.87354676  0.72965082  2.88578421 -0.54534335 -0.47522481
 -2.0265424   0.72897371  1.60512664  2.83077107 -1.2645671  -0.25136101
 -2.39279216 -2.27309771  3.13337155  2.54856958 -0.67550785 -2.69002202]. 
Working on 0.4 fold... 
[Mon Apr  1 17:51:16 2024]  Iteration number: 0 with current cost as 0.16883560063022118 and parameters 
[-2.45188492  2.23743462 -2.12427962 -0.11653105  0.55388706 -2.77010899
  3.06858498  2.18960143  1.18551998 -1.06648312  0.60271512  1.14432445
  1.31029897 -1.87354678  0.72965078  2.88578417 -0.54534337 -0.47522487
 -2.02654242  0.72897368  1.60512662  2.83077105 -1.26456712 -0.25136105
 -2.39279218 -2.27309774  3.13337153  2.54856958 -0.67550789 -2.69002204]. 
Working on 0.6 fold... 
[Mon Apr  1 17:56:51 2024]  Iteration number: 0 with current cost as 0.1642196386099384 and parameters 
[-2.51311811  2.23743464 -2.12427962 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960147  1.18552    -1.06648312  0.60271514  1.14432445
  1.31029899 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Apr  1 18:02:28 2024]  Iteration number: 0 with current cost as 0.12811785346541532 and parameters 
[-2.52172059  2.23743464 -2.1242796  -0.11653103  0.55388708 -2.77010897
  3.068585    2.18960147  1.18552    -1.0664831   0.60271512  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002204]. 
Working on 1.0 fold... 
[Mon Apr  1 18:08:03 2024]  Iteration number: 0 with current cost as 0.14843409407386665 and parameters 
[-2.45687503  2.23743461 -2.12427962 -0.11653105  0.55388706 -2.77010899
  3.068585    2.18960143  1.18552001 -1.06648313  0.60271512  1.14432445
  1.31029896 -1.8735468   0.72965078  2.88578417 -0.54534337 -0.47522483
 -2.02654243  0.72897367  1.60512662  2.83077105 -1.2645671  -0.25136103
 -2.39279218 -2.27309776  3.13337153  2.5485696  -0.67550787 -2.69002204]. 
Training complete taking 1675.828729391098 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Mon Apr  1 18:13:42 2024]  Iteration number: 0 with current cost as 0.15958078336700227 and parameters 
[-2.49771543  2.23743467 -2.1242796  -0.11653101  0.55388708 -2.77010895
  3.06858502  2.18960147  1.18552002 -1.06648308  0.60271514  1.14432445
  1.310299   -1.87354676  0.72965082  2.88578421 -0.54534335 -0.47522481
 -2.0265424   0.72897371  1.60512664  2.83077107 -1.2645671  -0.25136101
 -2.39279216 -2.27309771  3.13337155  2.54856958 -0.67550785 -2.69002202]. 
Working on 0.4 fold... 
[Mon Apr  1 18:19:20 2024]  Iteration number: 0 with current cost as 0.16883560063022118 and parameters 
[-2.45188492  2.23743462 -2.12427962 -0.11653105  0.55388706 -2.77010899
  3.06858498  2.18960143  1.18551998 -1.06648312  0.60271512  1.14432445
  1.31029897 -1.87354678  0.72965078  2.88578417 -0.54534337 -0.47522487
 -2.02654242  0.72897368  1.60512662  2.83077105 -1.26456712 -0.25136105
 -2.39279218 -2.27309774  3.13337153  2.54856958 -0.67550789 -2.69002204]. 
Working on 0.6 fold... 
[Mon Apr  1 18:24:51 2024]  Iteration number: 0 with current cost as 0.1642196386099384 and parameters 
[-2.51311811  2.23743464 -2.12427962 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960147  1.18552    -1.06648312  0.60271514  1.14432445
  1.31029899 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Apr  1 18:30:25 2024]  Iteration number: 0 with current cost as 0.12811785346541532 and parameters 
[-2.52172059  2.23743464 -2.1242796  -0.11653103  0.55388708 -2.77010897
  3.068585    2.18960147  1.18552    -1.0664831   0.60271512  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002204]. 
Working on 1.0 fold... 
[Mon Apr  1 18:36:01 2024]  Iteration number: 0 with current cost as 0.14843409407386665 and parameters 
[-2.45687503  2.23743461 -2.12427962 -0.11653105  0.55388706 -2.77010899
  3.068585    2.18960143  1.18552001 -1.06648313  0.60271512  1.14432445
  1.31029896 -1.8735468   0.72965078  2.88578417 -0.54534337 -0.47522483
 -2.02654243  0.72897367  1.60512662  2.83077105 -1.2645671  -0.25136103
 -2.39279218 -2.27309776  3.13337153  2.5485696  -0.67550787 -2.69002204]. 
Training complete taking 1675.085646390915 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Apr  1 18:41:37 2024]  Iteration number: 0 with current cost as 0.15958078336700227 and parameters 
[-2.49771543  2.23743467 -2.1242796  -0.11653101  0.55388708 -2.77010895
  3.06858502  2.18960147  1.18552002 -1.06648308  0.60271514  1.14432445
  1.310299   -1.87354676  0.72965082  2.88578421 -0.54534335 -0.47522481
 -2.0265424   0.72897371  1.60512664  2.83077107 -1.2645671  -0.25136101
 -2.39279216 -2.27309771  3.13337155  2.54856958 -0.67550785 -2.69002202]. 
Working on 0.4 fold... 
[Mon Apr  1 18:47:12 2024]  Iteration number: 0 with current cost as 0.16883560063022118 and parameters 
[-2.45188492  2.23743462 -2.12427962 -0.11653105  0.55388706 -2.77010899
  3.06858498  2.18960143  1.18551998 -1.06648312  0.60271512  1.14432445
  1.31029897 -1.87354678  0.72965078  2.88578417 -0.54534337 -0.47522487
 -2.02654242  0.72897368  1.60512662  2.83077105 -1.26456712 -0.25136105
 -2.39279218 -2.27309774  3.13337153  2.54856958 -0.67550789 -2.69002204]. 
Working on 0.6 fold... 
[Mon Apr  1 18:52:43 2024]  Iteration number: 0 with current cost as 0.1642196386099384 and parameters 
[-2.51311811  2.23743464 -2.12427962 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960147  1.18552    -1.06648312  0.60271514  1.14432445
  1.31029899 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Apr  1 18:58:19 2024]  Iteration number: 0 with current cost as 0.12811785346541532 and parameters 
[-2.52172059  2.23743464 -2.1242796  -0.11653103  0.55388708 -2.77010897
  3.068585    2.18960147  1.18552    -1.0664831   0.60271512  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002204]. 
Working on 1.0 fold... 
[Mon Apr  1 19:03:55 2024]  Iteration number: 0 with current cost as 0.14843409407386665 and parameters 
[-2.45687503  2.23743461 -2.12427962 -0.11653105  0.55388706 -2.77010899
  3.068585    2.18960143  1.18552001 -1.06648313  0.60271512  1.14432445
  1.31029896 -1.8735468   0.72965078  2.88578417 -0.54534337 -0.47522483
 -2.02654243  0.72897367  1.60512662  2.83077105 -1.2645671  -0.25136103
 -2.39279218 -2.27309776  3.13337153  2.5485696  -0.67550787 -2.69002204]. 
Training complete taking 1673.7972671985626 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Apr  1 19:09:28 2024]  Iteration number: 0 with current cost as 0.15958078336700227 and parameters 
[-2.49771543  2.23743467 -2.1242796  -0.11653101  0.55388708 -2.77010895
  3.06858502  2.18960147  1.18552002 -1.06648308  0.60271514  1.14432445
  1.310299   -1.87354676  0.72965082  2.88578421 -0.54534335 -0.47522481
 -2.0265424   0.72897371  1.60512664  2.83077107 -1.2645671  -0.25136101
 -2.39279216 -2.27309771  3.13337155  2.54856958 -0.67550785 -2.69002202]. 
Working on 0.4 fold... 
[Mon Apr  1 19:15:03 2024]  Iteration number: 0 with current cost as 0.16883560063022118 and parameters 
[-2.45188492  2.23743462 -2.12427962 -0.11653105  0.55388706 -2.77010899
  3.06858498  2.18960143  1.18551998 -1.06648312  0.60271512  1.14432445
  1.31029897 -1.87354678  0.72965078  2.88578417 -0.54534337 -0.47522487
 -2.02654242  0.72897368  1.60512662  2.83077105 -1.26456712 -0.25136105
 -2.39279218 -2.27309774  3.13337153  2.54856958 -0.67550789 -2.69002204]. 
Working on 0.6 fold... 
[Mon Apr  1 19:20:34 2024]  Iteration number: 0 with current cost as 0.1642196386099384 and parameters 
[-2.51311811  2.23743464 -2.12427962 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960147  1.18552    -1.06648312  0.60271514  1.14432445
  1.31029899 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Apr  1 19:26:07 2024]  Iteration number: 0 with current cost as 0.12811785346541532 and parameters 
[-2.52172059  2.23743464 -2.1242796  -0.11653103  0.55388708 -2.77010897
  3.068585    2.18960147  1.18552    -1.0664831   0.60271512  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002204]. 
Working on 1.0 fold... 
[Mon Apr  1 19:31:43 2024]  Iteration number: 0 with current cost as 0.14843409407386665 and parameters 
[-2.45687503  2.23743461 -2.12427962 -0.11653105  0.55388706 -2.77010899
  3.068585    2.18960143  1.18552001 -1.06648313  0.60271512  1.14432445
  1.31029896 -1.8735468   0.72965078  2.88578417 -0.54534337 -0.47522483
 -2.02654243  0.72897367  1.60512662  2.83077105 -1.2645671  -0.25136103
 -2.39279218 -2.27309776  3.13337153  2.5485696  -0.67550787 -2.69002204]. 
Training complete taking 1668.2600536346436 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Apr  1 19:37:17 2024]  Iteration number: 0 with current cost as 0.15958078336700227 and parameters 
[-2.49771543  2.23743467 -2.1242796  -0.11653101  0.55388708 -2.77010895
  3.06858502  2.18960147  1.18552002 -1.06648308  0.60271514  1.14432445
  1.310299   -1.87354676  0.72965082  2.88578421 -0.54534335 -0.47522481
 -2.0265424   0.72897371  1.60512664  2.83077107 -1.2645671  -0.25136101
 -2.39279216 -2.27309771  3.13337155  2.54856958 -0.67550785 -2.69002202]. 
Working on 0.4 fold... 
[Mon Apr  1 19:42:49 2024]  Iteration number: 0 with current cost as 0.16883560063022118 and parameters 
[-2.45188492  2.23743462 -2.12427962 -0.11653105  0.55388706 -2.77010899
  3.06858498  2.18960143  1.18551998 -1.06648312  0.60271512  1.14432445
  1.31029897 -1.87354678  0.72965078  2.88578417 -0.54534337 -0.47522487
 -2.02654242  0.72897368  1.60512662  2.83077105 -1.26456712 -0.25136105
 -2.39279218 -2.27309774  3.13337153  2.54856958 -0.67550789 -2.69002204]. 
Working on 0.6 fold... 
[Mon Apr  1 19:48:19 2024]  Iteration number: 0 with current cost as 0.1642196386099384 and parameters 
[-2.51311811  2.23743464 -2.12427962 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960147  1.18552    -1.06648312  0.60271514  1.14432445
  1.31029899 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Apr  1 19:53:52 2024]  Iteration number: 0 with current cost as 0.12811785346541532 and parameters 
[-2.52172059  2.23743464 -2.1242796  -0.11653103  0.55388708 -2.77010897
  3.068585    2.18960147  1.18552    -1.0664831   0.60271512  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002204]. 
Working on 1.0 fold... 
[Mon Apr  1 19:59:26 2024]  Iteration number: 0 with current cost as 0.14843409407386665 and parameters 
[-2.45687503  2.23743461 -2.12427962 -0.11653105  0.55388706 -2.77010899
  3.068585    2.18960143  1.18552001 -1.06648313  0.60271512  1.14432445
  1.31029896 -1.8735468   0.72965078  2.88578417 -0.54534337 -0.47522483
 -2.02654243  0.72897367  1.60512662  2.83077105 -1.2645671  -0.25136103
 -2.39279218 -2.27309776  3.13337153  2.5485696  -0.67550787 -2.69002204]. 
Training complete taking 1664.6704995632172 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Mon Apr  1 20:04:57 2024]  Iteration number: 0 with current cost as 0.15958078336700227 and parameters 
[-2.49771543  2.23743467 -2.1242796  -0.11653101  0.55388708 -2.77010895
  3.06858502  2.18960147  1.18552002 -1.06648308  0.60271514  1.14432445
  1.310299   -1.87354676  0.72965082  2.88578421 -0.54534335 -0.47522481
 -2.0265424   0.72897371  1.60512664  2.83077107 -1.2645671  -0.25136101
 -2.39279216 -2.27309771  3.13337155  2.54856958 -0.67550785 -2.69002202]. 
Working on 0.4 fold... 
[Mon Apr  1 20:10:31 2024]  Iteration number: 0 with current cost as 0.16883560063022118 and parameters 
[-2.45188492  2.23743462 -2.12427962 -0.11653105  0.55388706 -2.77010899
  3.06858498  2.18960143  1.18551998 -1.06648312  0.60271512  1.14432445
  1.31029897 -1.87354678  0.72965078  2.88578417 -0.54534337 -0.47522487
 -2.02654242  0.72897368  1.60512662  2.83077105 -1.26456712 -0.25136105
 -2.39279218 -2.27309774  3.13337153  2.54856958 -0.67550789 -2.69002204]. 
Working on 0.6 fold... 
[Mon Apr  1 20:16:09 2024]  Iteration number: 0 with current cost as 0.1642196386099384 and parameters 
[-2.51311811  2.23743464 -2.12427962 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960147  1.18552    -1.06648312  0.60271514  1.14432445
  1.31029899 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Apr  1 20:21:43 2024]  Iteration number: 0 with current cost as 0.12811785346541532 and parameters 
[-2.52172059  2.23743464 -2.1242796  -0.11653103  0.55388708 -2.77010897
  3.068585    2.18960147  1.18552    -1.0664831   0.60271512  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002204]. 
Working on 1.0 fold... 
[Mon Apr  1 20:27:11 2024]  Iteration number: 0 with current cost as 0.14843409407386665 and parameters 
[-2.45687503  2.23743461 -2.12427962 -0.11653105  0.55388706 -2.77010899
  3.068585    2.18960143  1.18552001 -1.06648313  0.60271512  1.14432445
  1.31029896 -1.8735468   0.72965078  2.88578417 -0.54534337 -0.47522483
 -2.02654243  0.72897367  1.60512662  2.83077105 -1.2645671  -0.25136103
 -2.39279218 -2.27309776  3.13337153  2.5485696  -0.67550787 -2.69002204]. 
Training complete taking 1665.151916027069 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Mon Apr  1 20:32:45 2024]  Iteration number: 0 with current cost as 0.15958078336700227 and parameters 
[-2.49771543  2.23743467 -2.1242796  -0.11653101  0.55388708 -2.77010895
  3.06858502  2.18960147  1.18552002 -1.06648308  0.60271514  1.14432445
  1.310299   -1.87354676  0.72965082  2.88578421 -0.54534335 -0.47522481
 -2.0265424   0.72897371  1.60512664  2.83077107 -1.2645671  -0.25136101
 -2.39279216 -2.27309771  3.13337155  2.54856958 -0.67550785 -2.69002202]. 
Working on 0.4 fold... 
[Mon Apr  1 20:38:15 2024]  Iteration number: 0 with current cost as 0.16883560063022118 and parameters 
[-2.45188492  2.23743462 -2.12427962 -0.11653105  0.55388706 -2.77010899
  3.06858498  2.18960143  1.18551998 -1.06648312  0.60271512  1.14432445
  1.31029897 -1.87354678  0.72965078  2.88578417 -0.54534337 -0.47522487
 -2.02654242  0.72897368  1.60512662  2.83077105 -1.26456712 -0.25136105
 -2.39279218 -2.27309774  3.13337153  2.54856958 -0.67550789 -2.69002204]. 
Working on 0.6 fold... 
[Mon Apr  1 20:43:45 2024]  Iteration number: 0 with current cost as 0.1642196386099384 and parameters 
[-2.51311811  2.23743464 -2.12427962 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960147  1.18552    -1.06648312  0.60271514  1.14432445
  1.31029899 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Apr  1 20:49:17 2024]  Iteration number: 0 with current cost as 0.12811785346541532 and parameters 
[-2.52172059  2.23743464 -2.1242796  -0.11653103  0.55388708 -2.77010897
  3.068585    2.18960147  1.18552    -1.0664831   0.60271512  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002204]. 
Working on 1.0 fold... 
[Mon Apr  1 20:54:46 2024]  Iteration number: 0 with current cost as 0.14843409407386665 and parameters 
[-2.45687503  2.23743461 -2.12427962 -0.11653105  0.55388706 -2.77010899
  3.068585    2.18960143  1.18552001 -1.06648313  0.60271512  1.14432445
  1.31029896 -1.8735468   0.72965078  2.88578417 -0.54534337 -0.47522483
 -2.02654243  0.72897367  1.60512662  2.83077105 -1.2645671  -0.25136103
 -2.39279218 -2.27309776  3.13337153  2.5485696  -0.67550787 -2.69002204]. 
Training complete taking 1651.9627966880798 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Apr  1 21:00:17 2024]  Iteration number: 0 with current cost as 0.15958078336700227 and parameters 
[-2.49771543  2.23743467 -2.1242796  -0.11653101  0.55388708 -2.77010895
  3.06858502  2.18960147  1.18552002 -1.06648308  0.60271514  1.14432445
  1.310299   -1.87354676  0.72965082  2.88578421 -0.54534335 -0.47522481
 -2.0265424   0.72897371  1.60512664  2.83077107 -1.2645671  -0.25136101
 -2.39279216 -2.27309771  3.13337155  2.54856958 -0.67550785 -2.69002202]. 
Working on 0.4 fold... 
[Mon Apr  1 21:05:54 2024]  Iteration number: 0 with current cost as 0.16883560063022118 and parameters 
[-2.45188492  2.23743462 -2.12427962 -0.11653105  0.55388706 -2.77010899
  3.06858498  2.18960143  1.18551998 -1.06648312  0.60271512  1.14432445
  1.31029897 -1.87354678  0.72965078  2.88578417 -0.54534337 -0.47522487
 -2.02654242  0.72897368  1.60512662  2.83077105 -1.26456712 -0.25136105
 -2.39279218 -2.27309774  3.13337153  2.54856958 -0.67550789 -2.69002204]. 
Working on 0.6 fold... 
[Mon Apr  1 21:11:29 2024]  Iteration number: 0 with current cost as 0.1642196386099384 and parameters 
[-2.51311811  2.23743464 -2.12427962 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960147  1.18552    -1.06648312  0.60271514  1.14432445
  1.31029899 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Apr  1 21:17:03 2024]  Iteration number: 0 with current cost as 0.12811785346541532 and parameters 
[-2.52172059  2.23743464 -2.1242796  -0.11653103  0.55388708 -2.77010897
  3.068585    2.18960147  1.18552    -1.0664831   0.60271512  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002204]. 
Working on 1.0 fold... 
[Mon Apr  1 21:22:34 2024]  Iteration number: 0 with current cost as 0.14843409407386665 and parameters 
[-2.45687503  2.23743461 -2.12427962 -0.11653105  0.55388706 -2.77010899
  3.068585    2.18960143  1.18552001 -1.06648313  0.60271512  1.14432445
  1.31029896 -1.8735468   0.72965078  2.88578417 -0.54534337 -0.47522483
 -2.02654243  0.72897367  1.60512662  2.83077105 -1.2645671  -0.25136103
 -2.39279218 -2.27309776  3.13337153  2.5485696  -0.67550787 -2.69002204]. 
Training complete taking 1671.414957523346 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Apr  1 21:28:07 2024]  Iteration number: 0 with current cost as 0.15958078336700227 and parameters 
[-2.49771543  2.23743467 -2.1242796  -0.11653101  0.55388708 -2.77010895
  3.06858502  2.18960147  1.18552002 -1.06648308  0.60271514  1.14432445
  1.310299   -1.87354676  0.72965082  2.88578421 -0.54534335 -0.47522481
 -2.0265424   0.72897371  1.60512664  2.83077107 -1.2645671  -0.25136101
 -2.39279216 -2.27309771  3.13337155  2.54856958 -0.67550785 -2.69002202]. 
Working on 0.4 fold... 
[Mon Apr  1 21:33:39 2024]  Iteration number: 0 with current cost as 0.16883560063022118 and parameters 
[-2.45188492  2.23743462 -2.12427962 -0.11653105  0.55388706 -2.77010899
  3.06858498  2.18960143  1.18551998 -1.06648312  0.60271512  1.14432445
  1.31029897 -1.87354678  0.72965078  2.88578417 -0.54534337 -0.47522487
 -2.02654242  0.72897368  1.60512662  2.83077105 -1.26456712 -0.25136105
 -2.39279218 -2.27309774  3.13337153  2.54856958 -0.67550789 -2.69002204]. 
Working on 0.6 fold... 
[Mon Apr  1 21:39:13 2024]  Iteration number: 0 with current cost as 0.1642196386099384 and parameters 
[-2.51311811  2.23743464 -2.12427962 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960147  1.18552    -1.06648312  0.60271514  1.14432445
  1.31029899 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Apr  1 21:44:49 2024]  Iteration number: 0 with current cost as 0.12811785346541532 and parameters 
[-2.52172059  2.23743464 -2.1242796  -0.11653103  0.55388708 -2.77010897
  3.068585    2.18960147  1.18552    -1.0664831   0.60271512  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002204]. 
Working on 1.0 fold... 
[Mon Apr  1 21:50:23 2024]  Iteration number: 0 with current cost as 0.14843409407386665 and parameters 
[-2.45687503  2.23743461 -2.12427962 -0.11653105  0.55388706 -2.77010899
  3.068585    2.18960143  1.18552001 -1.06648313  0.60271512  1.14432445
  1.31029896 -1.8735468   0.72965078  2.88578417 -0.54534337 -0.47522483
 -2.02654243  0.72897367  1.60512662  2.83077105 -1.2645671  -0.25136103
 -2.39279218 -2.27309776  3.13337153  2.5485696  -0.67550787 -2.69002204]. 
Training complete taking 1667.6959166526794 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Apr  1 21:55:54 2024]  Iteration number: 0 with current cost as 0.15958078336700227 and parameters 
[-2.49771543  2.23743467 -2.1242796  -0.11653101  0.55388708 -2.77010895
  3.06858502  2.18960147  1.18552002 -1.06648308  0.60271514  1.14432445
  1.310299   -1.87354676  0.72965082  2.88578421 -0.54534335 -0.47522481
 -2.0265424   0.72897371  1.60512664  2.83077107 -1.2645671  -0.25136101
 -2.39279216 -2.27309771  3.13337155  2.54856958 -0.67550785 -2.69002202]. 
Working on 0.4 fold... 
[Mon Apr  1 22:01:24 2024]  Iteration number: 0 with current cost as 0.16883560063022118 and parameters 
[-2.45188492  2.23743462 -2.12427962 -0.11653105  0.55388706 -2.77010899
  3.06858498  2.18960143  1.18551998 -1.06648312  0.60271512  1.14432445
  1.31029897 -1.87354678  0.72965078  2.88578417 -0.54534337 -0.47522487
 -2.02654242  0.72897368  1.60512662  2.83077105 -1.26456712 -0.25136105
 -2.39279218 -2.27309774  3.13337153  2.54856958 -0.67550789 -2.69002204]. 
Working on 0.6 fold... 
[Mon Apr  1 22:06:59 2024]  Iteration number: 0 with current cost as 0.1642196386099384 and parameters 
[-2.51311811  2.23743464 -2.12427962 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960147  1.18552    -1.06648312  0.60271514  1.14432445
  1.31029899 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Apr  1 22:12:25 2024]  Iteration number: 0 with current cost as 0.12811785346541532 and parameters 
[-2.52172059  2.23743464 -2.1242796  -0.11653103  0.55388708 -2.77010897
  3.068585    2.18960147  1.18552    -1.0664831   0.60271512  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002204]. 
Working on 1.0 fold... 
[Mon Apr  1 22:17:55 2024]  Iteration number: 0 with current cost as 0.14843409407386665 and parameters 
[-2.45687503  2.23743461 -2.12427962 -0.11653105  0.55388706 -2.77010899
  3.068585    2.18960143  1.18552001 -1.06648313  0.60271512  1.14432445
  1.31029896 -1.8735468   0.72965078  2.88578417 -0.54534337 -0.47522483
 -2.02654243  0.72897367  1.60512662  2.83077105 -1.2645671  -0.25136103
 -2.39279218 -2.27309776  3.13337153  2.5485696  -0.67550787 -2.69002204]. 
Training complete taking 1651.319741010666 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Apr  1 22:23:26 2024]  Iteration number: 0 with current cost as 0.15958078336700227 and parameters 
[-2.49771543  2.23743467 -2.1242796  -0.11653101  0.55388708 -2.77010895
  3.06858502  2.18960147  1.18552002 -1.06648308  0.60271514  1.14432445
  1.310299   -1.87354676  0.72965082  2.88578421 -0.54534335 -0.47522481
 -2.0265424   0.72897371  1.60512664  2.83077107 -1.2645671  -0.25136101
 -2.39279216 -2.27309771  3.13337155  2.54856958 -0.67550785 -2.69002202]. 
Working on 0.4 fold... 
[Mon Apr  1 22:28:59 2024]  Iteration number: 0 with current cost as 0.16883560063022118 and parameters 
[-2.45188492  2.23743462 -2.12427962 -0.11653105  0.55388706 -2.77010899
  3.06858498  2.18960143  1.18551998 -1.06648312  0.60271512  1.14432445
  1.31029897 -1.87354678  0.72965078  2.88578417 -0.54534337 -0.47522487
 -2.02654242  0.72897368  1.60512662  2.83077105 -1.26456712 -0.25136105
 -2.39279218 -2.27309774  3.13337153  2.54856958 -0.67550789 -2.69002204]. 
Working on 0.6 fold... 
[Mon Apr  1 22:34:34 2024]  Iteration number: 0 with current cost as 0.1642196386099384 and parameters 
[-2.51311811  2.23743464 -2.12427962 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960147  1.18552    -1.06648312  0.60271514  1.14432445
  1.31029899 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Apr  1 22:40:08 2024]  Iteration number: 0 with current cost as 0.12811785346541532 and parameters 
[-2.52172059  2.23743464 -2.1242796  -0.11653103  0.55388708 -2.77010897
  3.068585    2.18960147  1.18552    -1.0664831   0.60271512  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002204]. 
Working on 1.0 fold... 
[Mon Apr  1 22:45:37 2024]  Iteration number: 0 with current cost as 0.14843409407386665 and parameters 
[-2.45687503  2.23743461 -2.12427962 -0.11653105  0.55388706 -2.77010899
  3.068585    2.18960143  1.18552001 -1.06648313  0.60271512  1.14432445
  1.31029896 -1.8735468   0.72965078  2.88578417 -0.54534337 -0.47522483
 -2.02654243  0.72897367  1.60512662  2.83077105 -1.2645671  -0.25136103
 -2.39279218 -2.27309776  3.13337153  2.5485696  -0.67550787 -2.69002204]. 
Training complete taking 1663.3834054470062 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Apr  1 22:51:11 2024]  Iteration number: 0 with current cost as 0.15958078336700227 and parameters 
[-2.49771543  2.23743467 -2.1242796  -0.11653101  0.55388708 -2.77010895
  3.06858502  2.18960147  1.18552002 -1.06648308  0.60271514  1.14432445
  1.310299   -1.87354676  0.72965082  2.88578421 -0.54534335 -0.47522481
 -2.0265424   0.72897371  1.60512664  2.83077107 -1.2645671  -0.25136101
 -2.39279216 -2.27309771  3.13337155  2.54856958 -0.67550785 -2.69002202]. 
Working on 0.4 fold... 
[Mon Apr  1 22:56:44 2024]  Iteration number: 0 with current cost as 0.16883560063022118 and parameters 
[-2.45188492  2.23743462 -2.12427962 -0.11653105  0.55388706 -2.77010899
  3.06858498  2.18960143  1.18551998 -1.06648312  0.60271512  1.14432445
  1.31029897 -1.87354678  0.72965078  2.88578417 -0.54534337 -0.47522487
 -2.02654242  0.72897368  1.60512662  2.83077105 -1.26456712 -0.25136105
 -2.39279218 -2.27309774  3.13337153  2.54856958 -0.67550789 -2.69002204]. 
Working on 0.6 fold... 
[Mon Apr  1 23:02:19 2024]  Iteration number: 0 with current cost as 0.1642196386099384 and parameters 
[-2.51311811  2.23743464 -2.12427962 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960147  1.18552    -1.06648312  0.60271514  1.14432445
  1.31029899 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Apr  1 23:07:53 2024]  Iteration number: 0 with current cost as 0.12811785346541532 and parameters 
[-2.52172059  2.23743464 -2.1242796  -0.11653103  0.55388708 -2.77010897
  3.068585    2.18960147  1.18552    -1.0664831   0.60271512  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002204]. 
Working on 1.0 fold... 
[Mon Apr  1 23:13:27 2024]  Iteration number: 0 with current cost as 0.14843409407386665 and parameters 
[-2.45687503  2.23743461 -2.12427962 -0.11653105  0.55388706 -2.77010899
  3.068585    2.18960143  1.18552001 -1.06648313  0.60271512  1.14432445
  1.31029896 -1.8735468   0.72965078  2.88578417 -0.54534337 -0.47522483
 -2.02654243  0.72897367  1.60512662  2.83077105 -1.2645671  -0.25136103
 -2.39279218 -2.27309776  3.13337153  2.5485696  -0.67550787 -2.69002204]. 
Training complete taking 1668.8840935230255 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Apr  1 23:19:00 2024]  Iteration number: 0 with current cost as 0.15958078336700227 and parameters 
[-2.49771543  2.23743467 -2.1242796  -0.11653101  0.55388708 -2.77010895
  3.06858502  2.18960147  1.18552002 -1.06648308  0.60271514  1.14432445
  1.310299   -1.87354676  0.72965082  2.88578421 -0.54534335 -0.47522481
 -2.0265424   0.72897371  1.60512664  2.83077107 -1.2645671  -0.25136101
 -2.39279216 -2.27309771  3.13337155  2.54856958 -0.67550785 -2.69002202]. 
Working on 0.4 fold... 
[Mon Apr  1 23:24:36 2024]  Iteration number: 0 with current cost as 0.16883560063022118 and parameters 
[-2.45188492  2.23743462 -2.12427962 -0.11653105  0.55388706 -2.77010899
  3.06858498  2.18960143  1.18551998 -1.06648312  0.60271512  1.14432445
  1.31029897 -1.87354678  0.72965078  2.88578417 -0.54534337 -0.47522487
 -2.02654242  0.72897368  1.60512662  2.83077105 -1.26456712 -0.25136105
 -2.39279218 -2.27309774  3.13337153  2.54856958 -0.67550789 -2.69002204]. 
Working on 0.6 fold... 
[Mon Apr  1 23:30:06 2024]  Iteration number: 0 with current cost as 0.1642196386099384 and parameters 
[-2.51311811  2.23743464 -2.12427962 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960147  1.18552    -1.06648312  0.60271514  1.14432445
  1.31029899 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Apr  1 23:35:38 2024]  Iteration number: 0 with current cost as 0.12811785346541532 and parameters 
[-2.52172059  2.23743464 -2.1242796  -0.11653103  0.55388708 -2.77010897
  3.068585    2.18960147  1.18552    -1.0664831   0.60271512  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002204]. 
Working on 1.0 fold... 
[Mon Apr  1 23:41:16 2024]  Iteration number: 0 with current cost as 0.14843409407386665 and parameters 
[-2.45687503  2.23743461 -2.12427962 -0.11653105  0.55388706 -2.77010899
  3.068585    2.18960143  1.18552001 -1.06648313  0.60271512  1.14432445
  1.31029896 -1.8735468   0.72965078  2.88578417 -0.54534337 -0.47522483
 -2.02654243  0.72897367  1.60512662  2.83077105 -1.2645671  -0.25136103
 -2.39279218 -2.27309776  3.13337153  2.5485696  -0.67550787 -2.69002204]. 
Training complete taking 1670.9157218933105 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Apr  1 23:46:51 2024]  Iteration number: 0 with current cost as 0.15958078336700227 and parameters 
[-2.49771543  2.23743467 -2.1242796  -0.11653101  0.55388708 -2.77010895
  3.06858502  2.18960147  1.18552002 -1.06648308  0.60271514  1.14432445
  1.310299   -1.87354676  0.72965082  2.88578421 -0.54534335 -0.47522481
 -2.0265424   0.72897371  1.60512664  2.83077107 -1.2645671  -0.25136101
 -2.39279216 -2.27309771  3.13337155  2.54856958 -0.67550785 -2.69002202]. 
Working on 0.4 fold... 
[Mon Apr  1 23:52:26 2024]  Iteration number: 0 with current cost as 0.16883560063022118 and parameters 
[-2.45188492  2.23743462 -2.12427962 -0.11653105  0.55388706 -2.77010899
  3.06858498  2.18960143  1.18551998 -1.06648312  0.60271512  1.14432445
  1.31029897 -1.87354678  0.72965078  2.88578417 -0.54534337 -0.47522487
 -2.02654242  0.72897368  1.60512662  2.83077105 -1.26456712 -0.25136105
 -2.39279218 -2.27309774  3.13337153  2.54856958 -0.67550789 -2.69002204]. 
Working on 0.6 fold... 
[Mon Apr  1 23:58:01 2024]  Iteration number: 0 with current cost as 0.1642196386099384 and parameters 
[-2.51311811  2.23743464 -2.12427962 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960147  1.18552    -1.06648312  0.60271514  1.14432445
  1.31029899 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Tue Apr  2 00:03:35 2024]  Iteration number: 0 with current cost as 0.12811785346541532 and parameters 
[-2.52172059  2.23743464 -2.1242796  -0.11653103  0.55388708 -2.77010897
  3.068585    2.18960147  1.18552    -1.0664831   0.60271512  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002204]. 
Working on 1.0 fold... 
[Tue Apr  2 00:09:06 2024]  Iteration number: 0 with current cost as 0.14843409407386665 and parameters 
[-2.45687503  2.23743461 -2.12427962 -0.11653105  0.55388706 -2.77010899
  3.068585    2.18960143  1.18552001 -1.06648313  0.60271512  1.14432445
  1.31029896 -1.8735468   0.72965078  2.88578417 -0.54534337 -0.47522483
 -2.02654243  0.72897367  1.60512662  2.83077105 -1.2645671  -0.25136103
 -2.39279218 -2.27309776  3.13337153  2.5485696  -0.67550787 -2.69002204]. 
Training complete taking 1666.506336927414 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Tue Apr  2 00:14:37 2024]  Iteration number: 0 with current cost as 0.15958078336700227 and parameters 
[-2.49771543  2.23743467 -2.1242796  -0.11653101  0.55388708 -2.77010895
  3.06858502  2.18960147  1.18552002 -1.06648308  0.60271514  1.14432445
  1.310299   -1.87354676  0.72965082  2.88578421 -0.54534335 -0.47522481
 -2.0265424   0.72897371  1.60512664  2.83077107 -1.2645671  -0.25136101
 -2.39279216 -2.27309771  3.13337155  2.54856958 -0.67550785 -2.69002202]. 
Working on 0.4 fold... 
[Tue Apr  2 00:20:09 2024]  Iteration number: 0 with current cost as 0.16883560063022118 and parameters 
[-2.45188492  2.23743462 -2.12427962 -0.11653105  0.55388706 -2.77010899
  3.06858498  2.18960143  1.18551998 -1.06648312  0.60271512  1.14432445
  1.31029897 -1.87354678  0.72965078  2.88578417 -0.54534337 -0.47522487
 -2.02654242  0.72897368  1.60512662  2.83077105 -1.26456712 -0.25136105
 -2.39279218 -2.27309774  3.13337153  2.54856958 -0.67550789 -2.69002204]. 
Working on 0.6 fold... 
[Tue Apr  2 00:25:38 2024]  Iteration number: 0 with current cost as 0.1642196386099384 and parameters 
[-2.51311811  2.23743464 -2.12427962 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960147  1.18552    -1.06648312  0.60271514  1.14432445
  1.31029899 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Tue Apr  2 00:31:14 2024]  Iteration number: 0 with current cost as 0.12811785346541532 and parameters 
[-2.52172059  2.23743464 -2.1242796  -0.11653103  0.55388708 -2.77010897
  3.068585    2.18960147  1.18552    -1.0664831   0.60271512  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002204]. 
Working on 1.0 fold... 
[Tue Apr  2 00:36:49 2024]  Iteration number: 0 with current cost as 0.14843409407386665 and parameters 
[-2.45687503  2.23743461 -2.12427962 -0.11653105  0.55388706 -2.77010899
  3.068585    2.18960143  1.18552001 -1.06648313  0.60271512  1.14432445
  1.31029896 -1.8735468   0.72965078  2.88578417 -0.54534337 -0.47522483
 -2.02654243  0.72897367  1.60512662  2.83077105 -1.2645671  -0.25136103
 -2.39279218 -2.27309776  3.13337153  2.5485696  -0.67550787 -2.69002204]. 
Training complete taking 1664.561092376709 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Tue Apr  2 00:42:24 2024]  Iteration number: 0 with current cost as 0.15958078336700227 and parameters 
[-2.49771543  2.23743467 -2.1242796  -0.11653101  0.55388708 -2.77010895
  3.06858502  2.18960147  1.18552002 -1.06648308  0.60271514  1.14432445
  1.310299   -1.87354676  0.72965082  2.88578421 -0.54534335 -0.47522481
 -2.0265424   0.72897371  1.60512664  2.83077107 -1.2645671  -0.25136101
 -2.39279216 -2.27309771  3.13337155  2.54856958 -0.67550785 -2.69002202]. 
Working on 0.4 fold... 
[Tue Apr  2 00:47:55 2024]  Iteration number: 0 with current cost as 0.16883560063022118 and parameters 
[-2.45188492  2.23743462 -2.12427962 -0.11653105  0.55388706 -2.77010899
  3.06858498  2.18960143  1.18551998 -1.06648312  0.60271512  1.14432445
  1.31029897 -1.87354678  0.72965078  2.88578417 -0.54534337 -0.47522487
 -2.02654242  0.72897368  1.60512662  2.83077105 -1.26456712 -0.25136105
 -2.39279218 -2.27309774  3.13337153  2.54856958 -0.67550789 -2.69002204]. 
Working on 0.6 fold... 
[Tue Apr  2 00:53:32 2024]  Iteration number: 0 with current cost as 0.1642196386099384 and parameters 
[-2.51311811  2.23743464 -2.12427962 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960147  1.18552    -1.06648312  0.60271514  1.14432445
  1.31029899 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Tue Apr  2 00:59:03 2024]  Iteration number: 0 with current cost as 0.12811785346541532 and parameters 
[-2.52172059  2.23743464 -2.1242796  -0.11653103  0.55388708 -2.77010897
  3.068585    2.18960147  1.18552    -1.0664831   0.60271512  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002204]. 
Working on 1.0 fold... 
[Tue Apr  2 01:04:33 2024]  Iteration number: 0 with current cost as 0.14843409407386665 and parameters 
[-2.45687503  2.23743461 -2.12427962 -0.11653105  0.55388706 -2.77010899
  3.068585    2.18960143  1.18552001 -1.06648313  0.60271512  1.14432445
  1.31029896 -1.8735468   0.72965078  2.88578417 -0.54534337 -0.47522483
 -2.02654243  0.72897367  1.60512662  2.83077105 -1.2645671  -0.25136103
 -2.39279218 -2.27309776  3.13337153  2.5485696  -0.67550787 -2.69002204]. 
Training complete taking 1663.2810139656067 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Tue Apr  2 01:10:03 2024]  Iteration number: 0 with current cost as 0.15958078336700227 and parameters 
[-2.49771543  2.23743467 -2.1242796  -0.11653101  0.55388708 -2.77010895
  3.06858502  2.18960147  1.18552002 -1.06648308  0.60271514  1.14432445
  1.310299   -1.87354676  0.72965082  2.88578421 -0.54534335 -0.47522481
 -2.0265424   0.72897371  1.60512664  2.83077107 -1.2645671  -0.25136101
 -2.39279216 -2.27309771  3.13337155  2.54856958 -0.67550785 -2.69002202]. 
Working on 0.4 fold... 
[Tue Apr  2 01:15:39 2024]  Iteration number: 0 with current cost as 0.16883560063022118 and parameters 
[-2.45188492  2.23743462 -2.12427962 -0.11653105  0.55388706 -2.77010899
  3.06858498  2.18960143  1.18551998 -1.06648312  0.60271512  1.14432445
  1.31029897 -1.87354678  0.72965078  2.88578417 -0.54534337 -0.47522487
 -2.02654242  0.72897368  1.60512662  2.83077105 -1.26456712 -0.25136105
 -2.39279218 -2.27309774  3.13337153  2.54856958 -0.67550789 -2.69002204]. 
Working on 0.6 fold... 
[Tue Apr  2 01:21:09 2024]  Iteration number: 0 with current cost as 0.1642196386099384 and parameters 
[-2.51311811  2.23743464 -2.12427962 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960147  1.18552    -1.06648312  0.60271514  1.14432445
  1.31029899 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Tue Apr  2 01:26:44 2024]  Iteration number: 0 with current cost as 0.12811785346541532 and parameters 
[-2.52172059  2.23743464 -2.1242796  -0.11653103  0.55388708 -2.77010897
  3.068585    2.18960147  1.18552    -1.0664831   0.60271512  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002204]. 
Working on 1.0 fold... 
[Tue Apr  2 01:32:14 2024]  Iteration number: 0 with current cost as 0.14843409407386665 and parameters 
[-2.45687503  2.23743461 -2.12427962 -0.11653105  0.55388706 -2.77010899
  3.068585    2.18960143  1.18552001 -1.06648313  0.60271512  1.14432445
  1.31029896 -1.8735468   0.72965078  2.88578417 -0.54534337 -0.47522483
 -2.02654243  0.72897367  1.60512662  2.83077105 -1.2645671  -0.25136103
 -2.39279218 -2.27309776  3.13337153  2.5485696  -0.67550787 -2.69002204]. 
Training complete taking 1661.3947882652283 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Tue Apr  2 01:37:45 2024]  Iteration number: 0 with current cost as 0.15958078336700227 and parameters 
[-2.49771543  2.23743467 -2.1242796  -0.11653101  0.55388708 -2.77010895
  3.06858502  2.18960147  1.18552002 -1.06648308  0.60271514  1.14432445
  1.310299   -1.87354676  0.72965082  2.88578421 -0.54534335 -0.47522481
 -2.0265424   0.72897371  1.60512664  2.83077107 -1.2645671  -0.25136101
 -2.39279216 -2.27309771  3.13337155  2.54856958 -0.67550785 -2.69002202]. 
Working on 0.4 fold... 
[Tue Apr  2 01:43:14 2024]  Iteration number: 0 with current cost as 0.16883560063022118 and parameters 
[-2.45188492  2.23743462 -2.12427962 -0.11653105  0.55388706 -2.77010899
  3.06858498  2.18960143  1.18551998 -1.06648312  0.60271512  1.14432445
  1.31029897 -1.87354678  0.72965078  2.88578417 -0.54534337 -0.47522487
 -2.02654242  0.72897368  1.60512662  2.83077105 -1.26456712 -0.25136105
 -2.39279218 -2.27309774  3.13337153  2.54856958 -0.67550789 -2.69002204]. 
Working on 0.6 fold... 
[Tue Apr  2 01:48:47 2024]  Iteration number: 0 with current cost as 0.1642196386099384 and parameters 
[-2.51311811  2.23743464 -2.12427962 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960147  1.18552    -1.06648312  0.60271514  1.14432445
  1.31029899 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Tue Apr  2 01:54:21 2024]  Iteration number: 0 with current cost as 0.12811785346541532 and parameters 
[-2.52172059  2.23743464 -2.1242796  -0.11653103  0.55388708 -2.77010897
  3.068585    2.18960147  1.18552    -1.0664831   0.60271512  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002204]. 
Working on 1.0 fold... 
[Tue Apr  2 01:59:54 2024]  Iteration number: 0 with current cost as 0.14843409407386665 and parameters 
[-2.45687503  2.23743461 -2.12427962 -0.11653105  0.55388706 -2.77010899
  3.068585    2.18960143  1.18552001 -1.06648313  0.60271512  1.14432445
  1.31029896 -1.8735468   0.72965078  2.88578417 -0.54534337 -0.47522483
 -2.02654243  0.72897367  1.60512662  2.83077105 -1.2645671  -0.25136103
 -2.39279218 -2.27309776  3.13337153  2.5485696  -0.67550787 -2.69002204]. 
Training complete taking 1660.9549388885498 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Tue Apr  2 02:05:28 2024]  Iteration number: 0 with current cost as 0.15958078336700227 and parameters 
[-2.49771543  2.23743467 -2.1242796  -0.11653101  0.55388708 -2.77010895
  3.06858502  2.18960147  1.18552002 -1.06648308  0.60271514  1.14432445
  1.310299   -1.87354676  0.72965082  2.88578421 -0.54534335 -0.47522481
 -2.0265424   0.72897371  1.60512664  2.83077107 -1.2645671  -0.25136101
 -2.39279216 -2.27309771  3.13337155  2.54856958 -0.67550785 -2.69002202]. 
Working on 0.4 fold... 
[Tue Apr  2 02:11:01 2024]  Iteration number: 0 with current cost as 0.16883560063022118 and parameters 
[-2.45188492  2.23743462 -2.12427962 -0.11653105  0.55388706 -2.77010899
  3.06858498  2.18960143  1.18551998 -1.06648312  0.60271512  1.14432445
  1.31029897 -1.87354678  0.72965078  2.88578417 -0.54534337 -0.47522487
 -2.02654242  0.72897368  1.60512662  2.83077105 -1.26456712 -0.25136105
 -2.39279218 -2.27309774  3.13337153  2.54856958 -0.67550789 -2.69002204]. 
Working on 0.6 fold... 
[Tue Apr  2 02:16:30 2024]  Iteration number: 0 with current cost as 0.1642196386099384 and parameters 
[-2.51311811  2.23743464 -2.12427962 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960147  1.18552    -1.06648312  0.60271514  1.14432445
  1.31029899 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Tue Apr  2 02:22:08 2024]  Iteration number: 0 with current cost as 0.12811785346541532 and parameters 
[-2.52172059  2.23743464 -2.1242796  -0.11653103  0.55388708 -2.77010897
  3.068585    2.18960147  1.18552    -1.0664831   0.60271512  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002204]. 
Working on 1.0 fold... 
[Tue Apr  2 02:27:45 2024]  Iteration number: 0 with current cost as 0.14843409407386665 and parameters 
[-2.45687503  2.23743461 -2.12427962 -0.11653105  0.55388706 -2.77010899
  3.068585    2.18960143  1.18552001 -1.06648313  0.60271512  1.14432445
  1.31029896 -1.8735468   0.72965078  2.88578417 -0.54534337 -0.47522483
 -2.02654243  0.72897367  1.60512662  2.83077105 -1.2645671  -0.25136103
 -2.39279218 -2.27309776  3.13337153  2.5485696  -0.67550787 -2.69002204]. 
Training complete taking 1672.293424129486 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Tue Apr  2 02:33:17 2024]  Iteration number: 0 with current cost as 0.15958078336700227 and parameters 
[-2.49771543  2.23743467 -2.1242796  -0.11653101  0.55388708 -2.77010895
  3.06858502  2.18960147  1.18552002 -1.06648308  0.60271514  1.14432445
  1.310299   -1.87354676  0.72965082  2.88578421 -0.54534335 -0.47522481
 -2.0265424   0.72897371  1.60512664  2.83077107 -1.2645671  -0.25136101
 -2.39279216 -2.27309771  3.13337155  2.54856958 -0.67550785 -2.69002202]. 
Working on 0.4 fold... 
[Tue Apr  2 02:38:43 2024]  Iteration number: 0 with current cost as 0.16883560063022118 and parameters 
[-2.45188492  2.23743462 -2.12427962 -0.11653105  0.55388706 -2.77010899
  3.06858498  2.18960143  1.18551998 -1.06648312  0.60271512  1.14432445
  1.31029897 -1.87354678  0.72965078  2.88578417 -0.54534337 -0.47522487
 -2.02654242  0.72897368  1.60512662  2.83077105 -1.26456712 -0.25136105
 -2.39279218 -2.27309774  3.13337153  2.54856958 -0.67550789 -2.69002204]. 
Working on 0.6 fold... 
[Tue Apr  2 02:44:14 2024]  Iteration number: 0 with current cost as 0.1642196386099384 and parameters 
[-2.51311811  2.23743464 -2.12427962 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960147  1.18552    -1.06648312  0.60271514  1.14432445
  1.31029899 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Tue Apr  2 02:49:48 2024]  Iteration number: 0 with current cost as 0.12811785346541532 and parameters 
[-2.52172059  2.23743464 -2.1242796  -0.11653103  0.55388708 -2.77010897
  3.068585    2.18960147  1.18552    -1.0664831   0.60271512  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002204]. 
Working on 1.0 fold... 
[Tue Apr  2 02:55:26 2024]  Iteration number: 0 with current cost as 0.14843409407386665 and parameters 
[-2.45687503  2.23743461 -2.12427962 -0.11653105  0.55388706 -2.77010899
  3.068585    2.18960143  1.18552001 -1.06648313  0.60271512  1.14432445
  1.31029896 -1.8735468   0.72965078  2.88578417 -0.54534337 -0.47522483
 -2.02654243  0.72897367  1.60512662  2.83077105 -1.2645671  -0.25136103
 -2.39279218 -2.27309776  3.13337153  2.5485696  -0.67550787 -2.69002204]. 
Training complete taking 1662.5387992858887 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Tue Apr  2 03:01:09 2024]  Iteration number: 0 with current cost as 0.15958078336700227 and parameters 
[-2.49771543  2.23743467 -2.1242796  -0.11653101  0.55388708 -2.77010895
  3.06858502  2.18960147  1.18552002 -1.06648308  0.60271514  1.14432445
  1.310299   -1.87354676  0.72965082  2.88578421 -0.54534335 -0.47522481
 -2.0265424   0.72897371  1.60512664  2.83077107 -1.2645671  -0.25136101
 -2.39279216 -2.27309771  3.13337155  2.54856958 -0.67550785 -2.69002202]. 
Working on 0.4 fold... 
[Tue Apr  2 03:06:41 2024]  Iteration number: 0 with current cost as 0.16883560063022118 and parameters 
[-2.45188492  2.23743462 -2.12427962 -0.11653105  0.55388706 -2.77010899
  3.06858498  2.18960143  1.18551998 -1.06648312  0.60271512  1.14432445
  1.31029897 -1.87354678  0.72965078  2.88578417 -0.54534337 -0.47522487
 -2.02654242  0.72897368  1.60512662  2.83077105 -1.26456712 -0.25136105
 -2.39279218 -2.27309774  3.13337153  2.54856958 -0.67550789 -2.69002204]. 
Working on 0.6 fold... 
[Tue Apr  2 03:12:16 2024]  Iteration number: 0 with current cost as 0.1642196386099384 and parameters 
[-2.51311811  2.23743464 -2.12427962 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960147  1.18552    -1.06648312  0.60271514  1.14432445
  1.31029899 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Tue Apr  2 03:17:49 2024]  Iteration number: 0 with current cost as 0.12811785346541532 and parameters 
[-2.52172059  2.23743464 -2.1242796  -0.11653103  0.55388708 -2.77010897
  3.068585    2.18960147  1.18552    -1.0664831   0.60271512  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002204]. 
Working on 1.0 fold... 
[Tue Apr  2 03:23:22 2024]  Iteration number: 0 with current cost as 0.14843409407386665 and parameters 
[-2.45687503  2.23743461 -2.12427962 -0.11653105  0.55388706 -2.77010899
  3.068585    2.18960143  1.18552001 -1.06648313  0.60271512  1.14432445
  1.31029896 -1.8735468   0.72965078  2.88578417 -0.54534337 -0.47522483
 -2.02654243  0.72897367  1.60512662  2.83077105 -1.2645671  -0.25136103
 -2.39279218 -2.27309776  3.13337153  2.5485696  -0.67550787 -2.69002204]. 
Training complete taking 1670.645779132843 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Tue Apr  2 03:28:51 2024]  Iteration number: 0 with current cost as 0.15958078336700227 and parameters 
[-2.49771543  2.23743467 -2.1242796  -0.11653101  0.55388708 -2.77010895
  3.06858502  2.18960147  1.18552002 -1.06648308  0.60271514  1.14432445
  1.310299   -1.87354676  0.72965082  2.88578421 -0.54534335 -0.47522481
 -2.0265424   0.72897371  1.60512664  2.83077107 -1.2645671  -0.25136101
 -2.39279216 -2.27309771  3.13337155  2.54856958 -0.67550785 -2.69002202]. 
Working on 0.4 fold... 
[Tue Apr  2 03:34:27 2024]  Iteration number: 0 with current cost as 0.16883560063022118 and parameters 
[-2.45188492  2.23743462 -2.12427962 -0.11653105  0.55388706 -2.77010899
  3.06858498  2.18960143  1.18551998 -1.06648312  0.60271512  1.14432445
  1.31029897 -1.87354678  0.72965078  2.88578417 -0.54534337 -0.47522487
 -2.02654242  0.72897368  1.60512662  2.83077105 -1.26456712 -0.25136105
 -2.39279218 -2.27309774  3.13337153  2.54856958 -0.67550789 -2.69002204]. 
Working on 0.6 fold... 
[Tue Apr  2 03:40:00 2024]  Iteration number: 0 with current cost as 0.1642196386099384 and parameters 
[-2.51311811  2.23743464 -2.12427962 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960147  1.18552    -1.06648312  0.60271514  1.14432445
  1.31029899 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Tue Apr  2 03:45:34 2024]  Iteration number: 0 with current cost as 0.12811785346541532 and parameters 
[-2.52172059  2.23743464 -2.1242796  -0.11653103  0.55388708 -2.77010897
  3.068585    2.18960147  1.18552    -1.0664831   0.60271512  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002204]. 
Working on 1.0 fold... 
[Tue Apr  2 03:51:05 2024]  Iteration number: 0 with current cost as 0.14843409407386665 and parameters 
[-2.45687503  2.23743461 -2.12427962 -0.11653105  0.55388706 -2.77010899
  3.068585    2.18960143  1.18552001 -1.06648313  0.60271512  1.14432445
  1.31029896 -1.8735468   0.72965078  2.88578417 -0.54534337 -0.47522483
 -2.02654243  0.72897367  1.60512662  2.83077105 -1.2645671  -0.25136103
 -2.39279218 -2.27309776  3.13337153  2.5485696  -0.67550787 -2.69002204]. 
Training complete taking 1666.1121838092804 seconds. 
Discarding model... 

Training complete taking 41676.9421684742 total seconds. 
Now scoring model... 
Scoring complete taking 1.9775400161743164 seconds. 
Saved predicted values as A1-A1-CZ_Full-CRX_predicted_values.csv
Model scores: {'MSE_train': (3.379615249588902,), 'R2_train': 0.535953385200931, 'MAE_train': 1.2918253801174622, 'MSE_test': 6.433428334911213, 'R2_test': 0.3586472944339115, 'MAE_test': 1.9329044157626725}. 
Saved model results as A1-A1-CZ_Full-CRX_results.json. 
