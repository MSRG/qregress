/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/circuits/Encoders.py:12: RuntimeWarning: invalid value encountered in arcsin
  if np.isnan(np.arcsin(features[feature_idx])) or np.isnan(np.arccos(features[feature_idx])):
Loading dataset from /home/gjones/scratch/quad5qubits/quadratic_train.bin... 
Successfully loaded /home/gjones/scratch/quad5qubits/quadratic_train.bin into X and y data. 
Loading dataset from /home/gjones/scratch/quad5qubits/quadratic_test.bin... 
Successfully loaded /home/gjones/scratch/quad5qubits/quadratic_test.bin into X and y data. 
Training model with dataset /home/gjones/scratch/quad5qubits/quadratic_train.bin 
 at time Wed Mar 27 19:13:10 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Wed Mar 27 19:15:16 2024]  Iteration number: 0 with current cost as 0.16734539203586007 and parameters 
[-3.83548526  2.23743464 -2.12427946 -0.11653103  0.55388717 -2.77010897
  3.06858489  2.18960145  1.18552007 -1.06648308  0.60271519  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Wed Mar 27 19:18:15 2024]  Iteration number: 0 with current cost as 0.18150727317535326 and parameters 
[-3.85734406  2.23743464 -2.12427935 -0.11653132  0.55388722 -2.77010926
  3.06858484  2.18960145  1.18552013 -1.06648308  0.60271525  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Wed Mar 27 19:20:55 2024]  Iteration number: 0 with current cost as 0.16903398754593013 and parameters 
[-3.8029426   2.23743464 -2.12427949 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960152  1.18552013 -1.06648316  0.6027151   1.14432445
  1.31029906 -1.8735468 ]. 
Working on 0.8 fold... 
[Wed Mar 27 19:23:54 2024]  Iteration number: 0 with current cost as 0.18023726351365515 and parameters 
[-2.64172248  2.23743408 -2.12427908 -0.1165313   0.55388708 -2.77010953
  3.06858443  2.18960145  1.18551998 -1.06648364  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Wed Mar 27 19:26:18 2024]  Iteration number: 0 with current cost as 0.19989290852057248 and parameters 
[-2.59250355  2.23743456 -2.12427949 -0.11653103  0.55388715 -2.77010912
  3.06858491  2.18960152  1.1855202  -1.06648316  0.60271517  1.14432452
  1.31029906 -1.87354673]. 
Training complete taking 876.919163942337 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Wed Mar 27 19:29:51 2024]  Iteration number: 0 with current cost as 0.16734539203586007 and parameters 
[-3.83548526  2.23743464 -2.12427946 -0.11653103  0.55388717 -2.77010897
  3.06858489  2.18960145  1.18552007 -1.06648308  0.60271519  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Wed Mar 27 19:32:50 2024]  Iteration number: 0 with current cost as 0.18150727317535326 and parameters 
[-3.85734406  2.23743464 -2.12427935 -0.11653132  0.55388722 -2.77010926
  3.06858484  2.18960145  1.18552013 -1.06648308  0.60271525  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Wed Mar 27 19:35:30 2024]  Iteration number: 0 with current cost as 0.16903398754593013 and parameters 
[-3.8029426   2.23743464 -2.12427949 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960152  1.18552013 -1.06648316  0.6027151   1.14432445
  1.31029906 -1.8735468 ]. 
Working on 0.8 fold... 
[Wed Mar 27 19:38:27 2024]  Iteration number: 0 with current cost as 0.18023726351365515 and parameters 
[-2.64172248  2.23743408 -2.12427908 -0.1165313   0.55388708 -2.77010953
  3.06858443  2.18960145  1.18551998 -1.06648364  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Wed Mar 27 19:40:49 2024]  Iteration number: 0 with current cost as 0.19989290852057248 and parameters 
[-2.59250355  2.23743456 -2.12427949 -0.11653103  0.55388715 -2.77010912
  3.06858491  2.18960152  1.1855202  -1.06648316  0.60271517  1.14432452
  1.31029906 -1.87354673]. 
Training complete taking 871.3759503364563 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Wed Mar 27 19:44:22 2024]  Iteration number: 0 with current cost as 0.16734539203586007 and parameters 
[-3.83548526  2.23743464 -2.12427946 -0.11653103  0.55388717 -2.77010897
  3.06858489  2.18960145  1.18552007 -1.06648308  0.60271519  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Wed Mar 27 19:47:20 2024]  Iteration number: 0 with current cost as 0.18150727317535326 and parameters 
[-3.85734406  2.23743464 -2.12427935 -0.11653132  0.55388722 -2.77010926
  3.06858484  2.18960145  1.18552013 -1.06648308  0.60271525  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Wed Mar 27 19:49:59 2024]  Iteration number: 0 with current cost as 0.16903398754593013 and parameters 
[-3.8029426   2.23743464 -2.12427949 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960152  1.18552013 -1.06648316  0.6027151   1.14432445
  1.31029906 -1.8735468 ]. 
Working on 0.8 fold... 
[Wed Mar 27 19:52:56 2024]  Iteration number: 0 with current cost as 0.18023726351365515 and parameters 
[-2.64172248  2.23743408 -2.12427908 -0.1165313   0.55388708 -2.77010953
  3.06858443  2.18960145  1.18551998 -1.06648364  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Wed Mar 27 19:55:17 2024]  Iteration number: 0 with current cost as 0.19989290852057248 and parameters 
[-2.59250355  2.23743456 -2.12427949 -0.11653103  0.55388715 -2.77010912
  3.06858491  2.18960152  1.1855202  -1.06648316  0.60271517  1.14432452
  1.31029906 -1.87354673]. 
Training complete taking 867.6625487804413 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Wed Mar 27 19:58:49 2024]  Iteration number: 0 with current cost as 0.16734539203586007 and parameters 
[-3.83548526  2.23743464 -2.12427946 -0.11653103  0.55388717 -2.77010897
  3.06858489  2.18960145  1.18552007 -1.06648308  0.60271519  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Wed Mar 27 20:01:46 2024]  Iteration number: 0 with current cost as 0.18150727317535326 and parameters 
[-3.85734406  2.23743464 -2.12427935 -0.11653132  0.55388722 -2.77010926
  3.06858484  2.18960145  1.18552013 -1.06648308  0.60271525  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Wed Mar 27 20:04:25 2024]  Iteration number: 0 with current cost as 0.16903398754593013 and parameters 
[-3.8029426   2.23743464 -2.12427949 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960152  1.18552013 -1.06648316  0.6027151   1.14432445
  1.31029906 -1.8735468 ]. 
Working on 0.8 fold... 
[Wed Mar 27 20:07:22 2024]  Iteration number: 0 with current cost as 0.18023726351365515 and parameters 
[-2.64172248  2.23743408 -2.12427908 -0.1165313   0.55388708 -2.77010953
  3.06858443  2.18960145  1.18551998 -1.06648364  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Wed Mar 27 20:09:43 2024]  Iteration number: 0 with current cost as 0.19989290852057248 and parameters 
[-2.59250355  2.23743456 -2.12427949 -0.11653103  0.55388715 -2.77010912
  3.06858491  2.18960152  1.1855202  -1.06648316  0.60271517  1.14432452
  1.31029906 -1.87354673]. 
Training complete taking 865.5202176570892 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Wed Mar 27 20:13:15 2024]  Iteration number: 0 with current cost as 0.16734539203586007 and parameters 
[-3.83548526  2.23743464 -2.12427946 -0.11653103  0.55388717 -2.77010897
  3.06858489  2.18960145  1.18552007 -1.06648308  0.60271519  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Wed Mar 27 20:16:12 2024]  Iteration number: 0 with current cost as 0.18150727317535326 and parameters 
[-3.85734406  2.23743464 -2.12427935 -0.11653132  0.55388722 -2.77010926
  3.06858484  2.18960145  1.18552013 -1.06648308  0.60271525  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Wed Mar 27 20:18:54 2024]  Iteration number: 0 with current cost as 0.16903398754593013 and parameters 
[-3.8029426   2.23743464 -2.12427949 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960152  1.18552013 -1.06648316  0.6027151   1.14432445
  1.31029906 -1.8735468 ]. 
Working on 0.8 fold... 
[Wed Mar 27 20:21:50 2024]  Iteration number: 0 with current cost as 0.18023726351365515 and parameters 
[-2.64172248  2.23743408 -2.12427908 -0.1165313   0.55388708 -2.77010953
  3.06858443  2.18960145  1.18551998 -1.06648364  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Wed Mar 27 20:24:12 2024]  Iteration number: 0 with current cost as 0.19989290852057248 and parameters 
[-2.59250355  2.23743456 -2.12427949 -0.11653103  0.55388715 -2.77010912
  3.06858491  2.18960152  1.1855202  -1.06648316  0.60271517  1.14432452
  1.31029906 -1.87354673]. 
Training complete taking 868.8809900283813 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Wed Mar 27 20:27:44 2024]  Iteration number: 0 with current cost as 0.16734539203586007 and parameters 
[-3.83548526  2.23743464 -2.12427946 -0.11653103  0.55388717 -2.77010897
  3.06858489  2.18960145  1.18552007 -1.06648308  0.60271519  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Wed Mar 27 20:30:40 2024]  Iteration number: 0 with current cost as 0.18150727317535326 and parameters 
[-3.85734406  2.23743464 -2.12427935 -0.11653132  0.55388722 -2.77010926
  3.06858484  2.18960145  1.18552013 -1.06648308  0.60271525  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Wed Mar 27 20:33:19 2024]  Iteration number: 0 with current cost as 0.16903398754593013 and parameters 
[-3.8029426   2.23743464 -2.12427949 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960152  1.18552013 -1.06648316  0.6027151   1.14432445
  1.31029906 -1.8735468 ]. 
Working on 0.8 fold... 
[Wed Mar 27 20:36:16 2024]  Iteration number: 0 with current cost as 0.18023726351365515 and parameters 
[-2.64172248  2.23743408 -2.12427908 -0.1165313   0.55388708 -2.77010953
  3.06858443  2.18960145  1.18551998 -1.06648364  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Wed Mar 27 20:38:37 2024]  Iteration number: 0 with current cost as 0.19989290852057248 and parameters 
[-2.59250355  2.23743456 -2.12427949 -0.11653103  0.55388715 -2.77010912
  3.06858491  2.18960152  1.1855202  -1.06648316  0.60271517  1.14432452
  1.31029906 -1.87354673]. 
Training complete taking 865.6030654907227 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Wed Mar 27 20:42:09 2024]  Iteration number: 0 with current cost as 0.16734539203586007 and parameters 
[-3.83548526  2.23743464 -2.12427946 -0.11653103  0.55388717 -2.77010897
  3.06858489  2.18960145  1.18552007 -1.06648308  0.60271519  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Wed Mar 27 20:45:06 2024]  Iteration number: 0 with current cost as 0.18150727317535326 and parameters 
[-3.85734406  2.23743464 -2.12427935 -0.11653132  0.55388722 -2.77010926
  3.06858484  2.18960145  1.18552013 -1.06648308  0.60271525  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Wed Mar 27 20:47:44 2024]  Iteration number: 0 with current cost as 0.16903398754593013 and parameters 
[-3.8029426   2.23743464 -2.12427949 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960152  1.18552013 -1.06648316  0.6027151   1.14432445
  1.31029906 -1.8735468 ]. 
Working on 0.8 fold... 
[Wed Mar 27 20:50:41 2024]  Iteration number: 0 with current cost as 0.18023726351365515 and parameters 
[-2.64172248  2.23743408 -2.12427908 -0.1165313   0.55388708 -2.77010953
  3.06858443  2.18960145  1.18551998 -1.06648364  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Wed Mar 27 20:53:02 2024]  Iteration number: 0 with current cost as 0.19989290852057248 and parameters 
[-2.59250355  2.23743456 -2.12427949 -0.11653103  0.55388715 -2.77010912
  3.06858491  2.18960152  1.1855202  -1.06648316  0.60271517  1.14432452
  1.31029906 -1.87354673]. 
Training complete taking 864.1295416355133 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Wed Mar 27 20:56:33 2024]  Iteration number: 0 with current cost as 0.16734539203586007 and parameters 
[-3.83548526  2.23743464 -2.12427946 -0.11653103  0.55388717 -2.77010897
  3.06858489  2.18960145  1.18552007 -1.06648308  0.60271519  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Wed Mar 27 20:59:30 2024]  Iteration number: 0 with current cost as 0.18150727317535326 and parameters 
[-3.85734406  2.23743464 -2.12427935 -0.11653132  0.55388722 -2.77010926
  3.06858484  2.18960145  1.18552013 -1.06648308  0.60271525  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Wed Mar 27 21:02:09 2024]  Iteration number: 0 with current cost as 0.16903398754593013 and parameters 
[-3.8029426   2.23743464 -2.12427949 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960152  1.18552013 -1.06648316  0.6027151   1.14432445
  1.31029906 -1.8735468 ]. 
Working on 0.8 fold... 
[Wed Mar 27 21:05:05 2024]  Iteration number: 0 with current cost as 0.18023726351365515 and parameters 
[-2.64172248  2.23743408 -2.12427908 -0.1165313   0.55388708 -2.77010953
  3.06858443  2.18960145  1.18551998 -1.06648364  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Wed Mar 27 21:07:27 2024]  Iteration number: 0 with current cost as 0.19989290852057248 and parameters 
[-2.59250355  2.23743456 -2.12427949 -0.11653103  0.55388715 -2.77010912
  3.06858491  2.18960152  1.1855202  -1.06648316  0.60271517  1.14432452
  1.31029906 -1.87354673]. 
Training complete taking 865.8754103183746 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Wed Mar 27 21:10:59 2024]  Iteration number: 0 with current cost as 0.16734539203586007 and parameters 
[-3.83548526  2.23743464 -2.12427946 -0.11653103  0.55388717 -2.77010897
  3.06858489  2.18960145  1.18552007 -1.06648308  0.60271519  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Wed Mar 27 21:13:55 2024]  Iteration number: 0 with current cost as 0.18150727317535326 and parameters 
[-3.85734406  2.23743464 -2.12427935 -0.11653132  0.55388722 -2.77010926
  3.06858484  2.18960145  1.18552013 -1.06648308  0.60271525  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Wed Mar 27 21:16:34 2024]  Iteration number: 0 with current cost as 0.16903398754593013 and parameters 
[-3.8029426   2.23743464 -2.12427949 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960152  1.18552013 -1.06648316  0.6027151   1.14432445
  1.31029906 -1.8735468 ]. 
Working on 0.8 fold... 
[Wed Mar 27 21:19:31 2024]  Iteration number: 0 with current cost as 0.18023726351365515 and parameters 
[-2.64172248  2.23743408 -2.12427908 -0.1165313   0.55388708 -2.77010953
  3.06858443  2.18960145  1.18551998 -1.06648364  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Wed Mar 27 21:21:53 2024]  Iteration number: 0 with current cost as 0.19989290852057248 and parameters 
[-2.59250355  2.23743456 -2.12427949 -0.11653103  0.55388715 -2.77010912
  3.06858491  2.18960152  1.1855202  -1.06648316  0.60271517  1.14432452
  1.31029906 -1.87354673]. 
Training complete taking 864.9683618545532 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Wed Mar 27 21:25:24 2024]  Iteration number: 0 with current cost as 0.16734539203586007 and parameters 
[-3.83548526  2.23743464 -2.12427946 -0.11653103  0.55388717 -2.77010897
  3.06858489  2.18960145  1.18552007 -1.06648308  0.60271519  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Wed Mar 27 21:28:21 2024]  Iteration number: 0 with current cost as 0.18150727317535326 and parameters 
[-3.85734406  2.23743464 -2.12427935 -0.11653132  0.55388722 -2.77010926
  3.06858484  2.18960145  1.18552013 -1.06648308  0.60271525  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Wed Mar 27 21:31:00 2024]  Iteration number: 0 with current cost as 0.16903398754593013 and parameters 
[-3.8029426   2.23743464 -2.12427949 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960152  1.18552013 -1.06648316  0.6027151   1.14432445
  1.31029906 -1.8735468 ]. 
Working on 0.8 fold... 
[Wed Mar 27 21:33:57 2024]  Iteration number: 0 with current cost as 0.18023726351365515 and parameters 
[-2.64172248  2.23743408 -2.12427908 -0.1165313   0.55388708 -2.77010953
  3.06858443  2.18960145  1.18551998 -1.06648364  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Wed Mar 27 21:36:18 2024]  Iteration number: 0 with current cost as 0.19989290852057248 and parameters 
[-2.59250355  2.23743456 -2.12427949 -0.11653103  0.55388715 -2.77010912
  3.06858491  2.18960152  1.1855202  -1.06648316  0.60271517  1.14432452
  1.31029906 -1.87354673]. 
Training complete taking 865.5583419799805 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Wed Mar 27 21:39:50 2024]  Iteration number: 0 with current cost as 0.16734539203586007 and parameters 
[-3.83548526  2.23743464 -2.12427946 -0.11653103  0.55388717 -2.77010897
  3.06858489  2.18960145  1.18552007 -1.06648308  0.60271519  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Wed Mar 27 21:42:46 2024]  Iteration number: 0 with current cost as 0.18150727317535326 and parameters 
[-3.85734406  2.23743464 -2.12427935 -0.11653132  0.55388722 -2.77010926
  3.06858484  2.18960145  1.18552013 -1.06648308  0.60271525  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Wed Mar 27 21:45:25 2024]  Iteration number: 0 with current cost as 0.16903398754593013 and parameters 
[-3.8029426   2.23743464 -2.12427949 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960152  1.18552013 -1.06648316  0.6027151   1.14432445
  1.31029906 -1.8735468 ]. 
Working on 0.8 fold... 
[Wed Mar 27 21:48:24 2024]  Iteration number: 0 with current cost as 0.18023726351365515 and parameters 
[-2.64172248  2.23743408 -2.12427908 -0.1165313   0.55388708 -2.77010953
  3.06858443  2.18960145  1.18551998 -1.06648364  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Wed Mar 27 21:50:45 2024]  Iteration number: 0 with current cost as 0.19989290852057248 and parameters 
[-2.59250355  2.23743456 -2.12427949 -0.11653103  0.55388715 -2.77010912
  3.06858491  2.18960152  1.1855202  -1.06648316  0.60271517  1.14432452
  1.31029906 -1.87354673]. 
Training complete taking 867.4495029449463 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Wed Mar 27 21:54:17 2024]  Iteration number: 0 with current cost as 0.16734539203586007 and parameters 
[-3.83548526  2.23743464 -2.12427946 -0.11653103  0.55388717 -2.77010897
  3.06858489  2.18960145  1.18552007 -1.06648308  0.60271519  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Wed Mar 27 21:57:14 2024]  Iteration number: 0 with current cost as 0.18150727317535326 and parameters 
[-3.85734406  2.23743464 -2.12427935 -0.11653132  0.55388722 -2.77010926
  3.06858484  2.18960145  1.18552013 -1.06648308  0.60271525  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Wed Mar 27 22:00:06 2024]  Iteration number: 0 with current cost as 0.16903398754593013 and parameters 
[-3.8029426   2.23743464 -2.12427949 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960152  1.18552013 -1.06648316  0.6027151   1.14432445
  1.31029906 -1.8735468 ]. 
Working on 0.8 fold... 
[Wed Mar 27 22:03:03 2024]  Iteration number: 0 with current cost as 0.18023726351365515 and parameters 
[-2.64172248  2.23743408 -2.12427908 -0.1165313   0.55388708 -2.77010953
  3.06858443  2.18960145  1.18551998 -1.06648364  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Wed Mar 27 22:05:24 2024]  Iteration number: 0 with current cost as 0.19989290852057248 and parameters 
[-2.59250355  2.23743456 -2.12427949 -0.11653103  0.55388715 -2.77010912
  3.06858491  2.18960152  1.1855202  -1.06648316  0.60271517  1.14432452
  1.31029906 -1.87354673]. 
Training complete taking 878.2084486484528 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Wed Mar 27 22:08:56 2024]  Iteration number: 0 with current cost as 0.16734539203586007 and parameters 
[-3.83548526  2.23743464 -2.12427946 -0.11653103  0.55388717 -2.77010897
  3.06858489  2.18960145  1.18552007 -1.06648308  0.60271519  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Wed Mar 27 22:11:52 2024]  Iteration number: 0 with current cost as 0.18150727317535326 and parameters 
[-3.85734406  2.23743464 -2.12427935 -0.11653132  0.55388722 -2.77010926
  3.06858484  2.18960145  1.18552013 -1.06648308  0.60271525  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Wed Mar 27 22:14:31 2024]  Iteration number: 0 with current cost as 0.16903398754593013 and parameters 
[-3.8029426   2.23743464 -2.12427949 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960152  1.18552013 -1.06648316  0.6027151   1.14432445
  1.31029906 -1.8735468 ]. 
Working on 0.8 fold... 
[Wed Mar 27 22:17:28 2024]  Iteration number: 0 with current cost as 0.18023726351365515 and parameters 
[-2.64172248  2.23743408 -2.12427908 -0.1165313   0.55388708 -2.77010953
  3.06858443  2.18960145  1.18551998 -1.06648364  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Wed Mar 27 22:19:52 2024]  Iteration number: 0 with current cost as 0.19989290852057248 and parameters 
[-2.59250355  2.23743456 -2.12427949 -0.11653103  0.55388715 -2.77010912
  3.06858491  2.18960152  1.1855202  -1.06648316  0.60271517  1.14432452
  1.31029906 -1.87354673]. 
Training complete taking 868.2924702167511 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Wed Mar 27 22:23:23 2024]  Iteration number: 0 with current cost as 0.16734539203586007 and parameters 
[-3.83548526  2.23743464 -2.12427946 -0.11653103  0.55388717 -2.77010897
  3.06858489  2.18960145  1.18552007 -1.06648308  0.60271519  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Wed Mar 27 22:26:20 2024]  Iteration number: 0 with current cost as 0.18150727317535326 and parameters 
[-3.85734406  2.23743464 -2.12427935 -0.11653132  0.55388722 -2.77010926
  3.06858484  2.18960145  1.18552013 -1.06648308  0.60271525  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Wed Mar 27 22:28:59 2024]  Iteration number: 0 with current cost as 0.16903398754593013 and parameters 
[-3.8029426   2.23743464 -2.12427949 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960152  1.18552013 -1.06648316  0.6027151   1.14432445
  1.31029906 -1.8735468 ]. 
Working on 0.8 fold... 
[Wed Mar 27 22:31:56 2024]  Iteration number: 0 with current cost as 0.18023726351365515 and parameters 
[-2.64172248  2.23743408 -2.12427908 -0.1165313   0.55388708 -2.77010953
  3.06858443  2.18960145  1.18551998 -1.06648364  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Wed Mar 27 22:34:17 2024]  Iteration number: 0 with current cost as 0.19989290852057248 and parameters 
[-2.59250355  2.23743456 -2.12427949 -0.11653103  0.55388715 -2.77010912
  3.06858491  2.18960152  1.1855202  -1.06648316  0.60271517  1.14432452
  1.31029906 -1.87354673]. 
Training complete taking 864.8756384849548 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Wed Mar 27 22:37:49 2024]  Iteration number: 0 with current cost as 0.16734539203586007 and parameters 
[-3.83548526  2.23743464 -2.12427946 -0.11653103  0.55388717 -2.77010897
  3.06858489  2.18960145  1.18552007 -1.06648308  0.60271519  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Wed Mar 27 22:40:45 2024]  Iteration number: 0 with current cost as 0.18150727317535326 and parameters 
[-3.85734406  2.23743464 -2.12427935 -0.11653132  0.55388722 -2.77010926
  3.06858484  2.18960145  1.18552013 -1.06648308  0.60271525  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Wed Mar 27 22:43:24 2024]  Iteration number: 0 with current cost as 0.16903398754593013 and parameters 
[-3.8029426   2.23743464 -2.12427949 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960152  1.18552013 -1.06648316  0.6027151   1.14432445
  1.31029906 -1.8735468 ]. 
Working on 0.8 fold... 
[Wed Mar 27 22:46:21 2024]  Iteration number: 0 with current cost as 0.18023726351365515 and parameters 
[-2.64172248  2.23743408 -2.12427908 -0.1165313   0.55388708 -2.77010953
  3.06858443  2.18960145  1.18551998 -1.06648364  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Wed Mar 27 22:48:42 2024]  Iteration number: 0 with current cost as 0.19989290852057248 and parameters 
[-2.59250355  2.23743456 -2.12427949 -0.11653103  0.55388715 -2.77010912
  3.06858491  2.18960152  1.1855202  -1.06648316  0.60271517  1.14432452
  1.31029906 -1.87354673]. 
Training complete taking 865.3312604427338 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Wed Mar 27 22:52:13 2024]  Iteration number: 0 with current cost as 0.16734539203586007 and parameters 
[-3.83548526  2.23743464 -2.12427946 -0.11653103  0.55388717 -2.77010897
  3.06858489  2.18960145  1.18552007 -1.06648308  0.60271519  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Wed Mar 27 22:55:10 2024]  Iteration number: 0 with current cost as 0.18150727317535326 and parameters 
[-3.85734406  2.23743464 -2.12427935 -0.11653132  0.55388722 -2.77010926
  3.06858484  2.18960145  1.18552013 -1.06648308  0.60271525  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Wed Mar 27 22:57:49 2024]  Iteration number: 0 with current cost as 0.16903398754593013 and parameters 
[-3.8029426   2.23743464 -2.12427949 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960152  1.18552013 -1.06648316  0.6027151   1.14432445
  1.31029906 -1.8735468 ]. 
Working on 0.8 fold... 
[Wed Mar 27 23:00:46 2024]  Iteration number: 0 with current cost as 0.18023726351365515 and parameters 
[-2.64172248  2.23743408 -2.12427908 -0.1165313   0.55388708 -2.77010953
  3.06858443  2.18960145  1.18551998 -1.06648364  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Wed Mar 27 23:03:07 2024]  Iteration number: 0 with current cost as 0.19989290852057248 and parameters 
[-2.59250355  2.23743456 -2.12427949 -0.11653103  0.55388715 -2.77010912
  3.06858491  2.18960152  1.1855202  -1.06648316  0.60271517  1.14432452
  1.31029906 -1.87354673]. 
Training complete taking 865.2425148487091 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Wed Mar 27 23:06:39 2024]  Iteration number: 0 with current cost as 0.16734539203586007 and parameters 
[-3.83548526  2.23743464 -2.12427946 -0.11653103  0.55388717 -2.77010897
  3.06858489  2.18960145  1.18552007 -1.06648308  0.60271519  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Wed Mar 27 23:09:36 2024]  Iteration number: 0 with current cost as 0.18150727317535326 and parameters 
[-3.85734406  2.23743464 -2.12427935 -0.11653132  0.55388722 -2.77010926
  3.06858484  2.18960145  1.18552013 -1.06648308  0.60271525  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Wed Mar 27 23:12:15 2024]  Iteration number: 0 with current cost as 0.16903398754593013 and parameters 
[-3.8029426   2.23743464 -2.12427949 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960152  1.18552013 -1.06648316  0.6027151   1.14432445
  1.31029906 -1.8735468 ]. 
Working on 0.8 fold... 
[Wed Mar 27 23:15:11 2024]  Iteration number: 0 with current cost as 0.18023726351365515 and parameters 
[-2.64172248  2.23743408 -2.12427908 -0.1165313   0.55388708 -2.77010953
  3.06858443  2.18960145  1.18551998 -1.06648364  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Wed Mar 27 23:17:33 2024]  Iteration number: 0 with current cost as 0.19989290852057248 and parameters 
[-2.59250355  2.23743456 -2.12427949 -0.11653103  0.55388715 -2.77010912
  3.06858491  2.18960152  1.1855202  -1.06648316  0.60271517  1.14432452
  1.31029906 -1.87354673]. 
Training complete taking 865.6504969596863 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Wed Mar 27 23:21:05 2024]  Iteration number: 0 with current cost as 0.16734539203586007 and parameters 
[-3.83548526  2.23743464 -2.12427946 -0.11653103  0.55388717 -2.77010897
  3.06858489  2.18960145  1.18552007 -1.06648308  0.60271519  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Wed Mar 27 23:24:01 2024]  Iteration number: 0 with current cost as 0.18150727317535326 and parameters 
[-3.85734406  2.23743464 -2.12427935 -0.11653132  0.55388722 -2.77010926
  3.06858484  2.18960145  1.18552013 -1.06648308  0.60271525  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Wed Mar 27 23:26:40 2024]  Iteration number: 0 with current cost as 0.16903398754593013 and parameters 
[-3.8029426   2.23743464 -2.12427949 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960152  1.18552013 -1.06648316  0.6027151   1.14432445
  1.31029906 -1.8735468 ]. 
Working on 0.8 fold... 
[Wed Mar 27 23:29:37 2024]  Iteration number: 0 with current cost as 0.18023726351365515 and parameters 
[-2.64172248  2.23743408 -2.12427908 -0.1165313   0.55388708 -2.77010953
  3.06858443  2.18960145  1.18551998 -1.06648364  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Wed Mar 27 23:32:01 2024]  Iteration number: 0 with current cost as 0.19989290852057248 and parameters 
[-2.59250355  2.23743456 -2.12427949 -0.11653103  0.55388715 -2.77010912
  3.06858491  2.18960152  1.1855202  -1.06648316  0.60271517  1.14432452
  1.31029906 -1.87354673]. 
Training complete taking 867.6464538574219 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Wed Mar 27 23:35:32 2024]  Iteration number: 0 with current cost as 0.16734539203586007 and parameters 
[-3.83548526  2.23743464 -2.12427946 -0.11653103  0.55388717 -2.77010897
  3.06858489  2.18960145  1.18552007 -1.06648308  0.60271519  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Wed Mar 27 23:38:29 2024]  Iteration number: 0 with current cost as 0.18150727317535326 and parameters 
[-3.85734406  2.23743464 -2.12427935 -0.11653132  0.55388722 -2.77010926
  3.06858484  2.18960145  1.18552013 -1.06648308  0.60271525  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Wed Mar 27 23:41:07 2024]  Iteration number: 0 with current cost as 0.16903398754593013 and parameters 
[-3.8029426   2.23743464 -2.12427949 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960152  1.18552013 -1.06648316  0.6027151   1.14432445
  1.31029906 -1.8735468 ]. 
Working on 0.8 fold... 
[Wed Mar 27 23:44:04 2024]  Iteration number: 0 with current cost as 0.18023726351365515 and parameters 
[-2.64172248  2.23743408 -2.12427908 -0.1165313   0.55388708 -2.77010953
  3.06858443  2.18960145  1.18551998 -1.06648364  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Wed Mar 27 23:46:25 2024]  Iteration number: 0 with current cost as 0.19989290852057248 and parameters 
[-2.59250355  2.23743456 -2.12427949 -0.11653103  0.55388715 -2.77010912
  3.06858491  2.18960152  1.1855202  -1.06648316  0.60271517  1.14432452
  1.31029906 -1.87354673]. 
Training complete taking 864.522881269455 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Wed Mar 27 23:49:57 2024]  Iteration number: 0 with current cost as 0.16734539203586007 and parameters 
[-3.83548526  2.23743464 -2.12427946 -0.11653103  0.55388717 -2.77010897
  3.06858489  2.18960145  1.18552007 -1.06648308  0.60271519  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Wed Mar 27 23:52:53 2024]  Iteration number: 0 with current cost as 0.18150727317535326 and parameters 
[-3.85734406  2.23743464 -2.12427935 -0.11653132  0.55388722 -2.77010926
  3.06858484  2.18960145  1.18552013 -1.06648308  0.60271525  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Wed Mar 27 23:55:32 2024]  Iteration number: 0 with current cost as 0.16903398754593013 and parameters 
[-3.8029426   2.23743464 -2.12427949 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960152  1.18552013 -1.06648316  0.6027151   1.14432445
  1.31029906 -1.8735468 ]. 
Working on 0.8 fold... 
[Wed Mar 27 23:58:28 2024]  Iteration number: 0 with current cost as 0.18023726351365515 and parameters 
[-2.64172248  2.23743408 -2.12427908 -0.1165313   0.55388708 -2.77010953
  3.06858443  2.18960145  1.18551998 -1.06648364  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Mar 28 00:00:50 2024]  Iteration number: 0 with current cost as 0.19989290852057248 and parameters 
[-2.59250355  2.23743456 -2.12427949 -0.11653103  0.55388715 -2.77010912
  3.06858491  2.18960152  1.1855202  -1.06648316  0.60271517  1.14432452
  1.31029906 -1.87354673]. 
Training complete taking 864.8329062461853 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Mar 28 00:04:22 2024]  Iteration number: 0 with current cost as 0.16734539203586007 and parameters 
[-3.83548526  2.23743464 -2.12427946 -0.11653103  0.55388717 -2.77010897
  3.06858489  2.18960145  1.18552007 -1.06648308  0.60271519  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Mar 28 00:07:18 2024]  Iteration number: 0 with current cost as 0.18150727317535326 and parameters 
[-3.85734406  2.23743464 -2.12427935 -0.11653132  0.55388722 -2.77010926
  3.06858484  2.18960145  1.18552013 -1.06648308  0.60271525  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Mar 28 00:09:57 2024]  Iteration number: 0 with current cost as 0.16903398754593013 and parameters 
[-3.8029426   2.23743464 -2.12427949 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960152  1.18552013 -1.06648316  0.6027151   1.14432445
  1.31029906 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Mar 28 00:12:53 2024]  Iteration number: 0 with current cost as 0.18023726351365515 and parameters 
[-2.64172248  2.23743408 -2.12427908 -0.1165313   0.55388708 -2.77010953
  3.06858443  2.18960145  1.18551998 -1.06648364  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Mar 28 00:15:15 2024]  Iteration number: 0 with current cost as 0.19989290852057248 and parameters 
[-2.59250355  2.23743456 -2.12427949 -0.11653103  0.55388715 -2.77010912
  3.06858491  2.18960152  1.1855202  -1.06648316  0.60271517  1.14432452
  1.31029906 -1.87354673]. 
Training complete taking 865.1444962024689 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Mar 28 00:18:50 2024]  Iteration number: 0 with current cost as 0.16734539203586007 and parameters 
[-3.83548526  2.23743464 -2.12427946 -0.11653103  0.55388717 -2.77010897
  3.06858489  2.18960145  1.18552007 -1.06648308  0.60271519  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Mar 28 00:21:48 2024]  Iteration number: 0 with current cost as 0.18150727317535326 and parameters 
[-3.85734406  2.23743464 -2.12427935 -0.11653132  0.55388722 -2.77010926
  3.06858484  2.18960145  1.18552013 -1.06648308  0.60271525  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Mar 28 00:24:27 2024]  Iteration number: 0 with current cost as 0.16903398754593013 and parameters 
[-3.8029426   2.23743464 -2.12427949 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960152  1.18552013 -1.06648316  0.6027151   1.14432445
  1.31029906 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Mar 28 00:27:23 2024]  Iteration number: 0 with current cost as 0.18023726351365515 and parameters 
[-2.64172248  2.23743408 -2.12427908 -0.1165313   0.55388708 -2.77010953
  3.06858443  2.18960145  1.18551998 -1.06648364  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Mar 28 00:29:45 2024]  Iteration number: 0 with current cost as 0.19989290852057248 and parameters 
[-2.59250355  2.23743456 -2.12427949 -0.11653103  0.55388715 -2.77010912
  3.06858491  2.18960152  1.1855202  -1.06648316  0.60271517  1.14432452
  1.31029906 -1.87354673]. 
Training complete taking 870.0776734352112 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Mar 28 00:33:17 2024]  Iteration number: 0 with current cost as 0.16734539203586007 and parameters 
[-3.83548526  2.23743464 -2.12427946 -0.11653103  0.55388717 -2.77010897
  3.06858489  2.18960145  1.18552007 -1.06648308  0.60271519  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Mar 28 00:36:14 2024]  Iteration number: 0 with current cost as 0.18150727317535326 and parameters 
[-3.85734406  2.23743464 -2.12427935 -0.11653132  0.55388722 -2.77010926
  3.06858484  2.18960145  1.18552013 -1.06648308  0.60271525  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Mar 28 00:38:52 2024]  Iteration number: 0 with current cost as 0.16903398754593013 and parameters 
[-3.8029426   2.23743464 -2.12427949 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960152  1.18552013 -1.06648316  0.6027151   1.14432445
  1.31029906 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Mar 28 00:41:50 2024]  Iteration number: 0 with current cost as 0.18023726351365515 and parameters 
[-2.64172248  2.23743408 -2.12427908 -0.1165313   0.55388708 -2.77010953
  3.06858443  2.18960145  1.18551998 -1.06648364  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Mar 28 00:44:11 2024]  Iteration number: 0 with current cost as 0.19989290852057248 and parameters 
[-2.59250355  2.23743456 -2.12427949 -0.11653103  0.55388715 -2.77010912
  3.06858491  2.18960152  1.1855202  -1.06648316  0.60271517  1.14432452
  1.31029906 -1.87354673]. 
Training complete taking 866.0832576751709 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Thu Mar 28 00:47:44 2024]  Iteration number: 0 with current cost as 0.16734539203586007 and parameters 
[-3.83548526  2.23743464 -2.12427946 -0.11653103  0.55388717 -2.77010897
  3.06858489  2.18960145  1.18552007 -1.06648308  0.60271519  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Mar 28 00:50:40 2024]  Iteration number: 0 with current cost as 0.18150727317535326 and parameters 
[-3.85734406  2.23743464 -2.12427935 -0.11653132  0.55388722 -2.77010926
  3.06858484  2.18960145  1.18552013 -1.06648308  0.60271525  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Mar 28 00:53:19 2024]  Iteration number: 0 with current cost as 0.16903398754593013 and parameters 
[-3.8029426   2.23743464 -2.12427949 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960152  1.18552013 -1.06648316  0.6027151   1.14432445
  1.31029906 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Mar 28 00:56:16 2024]  Iteration number: 0 with current cost as 0.18023726351365515 and parameters 
[-2.64172248  2.23743408 -2.12427908 -0.1165313   0.55388708 -2.77010953
  3.06858443  2.18960145  1.18551998 -1.06648364  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Mar 28 00:58:38 2024]  Iteration number: 0 with current cost as 0.19989290852057248 and parameters 
[-2.59250355  2.23743456 -2.12427949 -0.11653103  0.55388715 -2.77010912
  3.06858491  2.18960152  1.1855202  -1.06648316  0.60271517  1.14432452
  1.31029906 -1.87354673]. 
Training complete taking 866.8167984485626 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Thu Mar 28 01:02:11 2024]  Iteration number: 0 with current cost as 0.16734539203586007 and parameters 
[-3.83548526  2.23743464 -2.12427946 -0.11653103  0.55388717 -2.77010897
  3.06858489  2.18960145  1.18552007 -1.06648308  0.60271519  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Mar 28 01:05:08 2024]  Iteration number: 0 with current cost as 0.18150727317535326 and parameters 
[-3.85734406  2.23743464 -2.12427935 -0.11653132  0.55388722 -2.77010926
  3.06858484  2.18960145  1.18552013 -1.06648308  0.60271525  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Mar 28 01:07:47 2024]  Iteration number: 0 with current cost as 0.16903398754593013 and parameters 
[-3.8029426   2.23743464 -2.12427949 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960152  1.18552013 -1.06648316  0.6027151   1.14432445
  1.31029906 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Mar 28 01:10:43 2024]  Iteration number: 0 with current cost as 0.18023726351365515 and parameters 
[-2.64172248  2.23743408 -2.12427908 -0.1165313   0.55388708 -2.77010953
  3.06858443  2.18960145  1.18551998 -1.06648364  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Mar 28 01:13:05 2024]  Iteration number: 0 with current cost as 0.19989290852057248 and parameters 
[-2.59250355  2.23743456 -2.12427949 -0.11653103  0.55388715 -2.77010912
  3.06858491  2.18960152  1.1855202  -1.06648316  0.60271517  1.14432452
  1.31029906 -1.87354673]. 
Training complete taking 866.4441812038422 seconds. 
Discarding model... 

Training complete taking 21683.113152503967 total seconds. 
Now scoring model... 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Scoring complete taking 2.096848964691162 seconds. 
Saved predicted values as M-A2-CZ_Efficient-CRZ_predicted_values.csv
Model scores: {'MSE_train': (4.648558149994856,), 'R2_train': 0.3617179726407814, 'MAE_train': 1.5491272362780013, 'MSE_test': 12.610030330949007, 'R2_test': -0.2571022243517691, 'MAE_test': 2.5033097922136407}. 
Saved model results as M-A2-CZ_Efficient-CRZ_results.json. 
