/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /scratch/j/jacobsen/gjones/quad5qubits/quadratic_train.bin... 
Successfully loaded /scratch/j/jacobsen/gjones/quad5qubits/quadratic_train.bin into X and y data. 
Loading dataset from /scratch/j/jacobsen/gjones/quad5qubits/quadratic_test.bin... 
Successfully loaded /scratch/j/jacobsen/gjones/quad5qubits/quadratic_test.bin into X and y data. 
Training model with dataset /scratch/j/jacobsen/gjones/quad5qubits/quadratic_train.bin 
 at time Mon Apr  1 15:46:32 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Apr  1 15:48:06 2024]  Iteration number: 0 with current cost as 0.10169638677896314 and parameters 
[-4.08085322  2.23743464 -2.12427961 -0.11653103  0.55388708 -2.77010902
  3.06858496  2.1896015   1.18552001 -1.06648314  0.60271515  1.14432445
  1.31029901 -1.87354673]. 
Working on 0.4 fold... 
[Mon Apr  1 15:50:13 2024]  Iteration number: 0 with current cost as 0.09915977532618409 and parameters 
[-4.08193641  2.23743468 -2.12427959 -0.11653103  0.55388712 -2.77010897
  3.06858498  2.1896015   1.18552003 -1.06648308  0.60271517  1.14432447
  1.31029903 -1.87354671]. 
Working on 0.6 fold... 
[Mon Apr  1 15:53:00 2024]  Iteration number: 0 with current cost as 0.10581478334733793 and parameters 
[-4.07412584  2.23743464 -2.12427959 -0.11653103  0.55388708 -2.77010897
  3.06858496  2.18960145  1.18552001 -1.06648313  0.60271515  1.14432443
  1.31029901 -1.87354675]. 
Working on 0.8 fold... 
[Mon Apr  1 15:55:25 2024]  Iteration number: 0 with current cost as 0.08752176144889784 and parameters 
[-4.05783283  2.23743464 -2.12427961 -0.11653103  0.55388711 -2.77010897
  3.06858492  2.18960148  1.18552001 -1.06648308  0.60271513  1.14432445
  1.31029901 -1.87354674]. 
Working on 1.0 fold... 
[Mon Apr  1 15:57:38 2024]  Iteration number: 0 with current cost as 0.08855552949919476 and parameters 
[-4.09013748  2.23743466 -2.12427961 -0.11653103  0.5538871  -2.77010897
  3.06858496  2.1896015   1.18552001 -1.06648311  0.60271515  1.14432445
  1.31029901 -1.87354672]. 
Training complete taking 722.4076292514801 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Apr  1 16:00:08 2024]  Iteration number: 0 with current cost as 0.10169638677896314 and parameters 
[-4.08085322  2.23743464 -2.12427961 -0.11653103  0.55388708 -2.77010902
  3.06858496  2.1896015   1.18552001 -1.06648314  0.60271515  1.14432445
  1.31029901 -1.87354673]. 
Working on 0.4 fold... 
[Mon Apr  1 16:02:22 2024]  Iteration number: 0 with current cost as 0.09915977532618409 and parameters 
[-4.08193641  2.23743468 -2.12427959 -0.11653103  0.55388712 -2.77010897
  3.06858498  2.1896015   1.18552003 -1.06648308  0.60271517  1.14432447
  1.31029903 -1.87354671]. 
Working on 0.6 fold... 
[Mon Apr  1 16:05:14 2024]  Iteration number: 0 with current cost as 0.10581478334733793 and parameters 
[-4.07412584  2.23743464 -2.12427959 -0.11653103  0.55388708 -2.77010897
  3.06858496  2.18960145  1.18552001 -1.06648313  0.60271515  1.14432443
  1.31029901 -1.87354675]. 
Working on 0.8 fold... 
[Mon Apr  1 16:07:47 2024]  Iteration number: 0 with current cost as 0.08752176144889784 and parameters 
[-4.05783283  2.23743464 -2.12427961 -0.11653103  0.55388711 -2.77010897
  3.06858492  2.18960148  1.18552001 -1.06648308  0.60271513  1.14432445
  1.31029901 -1.87354674]. 
Working on 1.0 fold... 
[Mon Apr  1 16:09:58 2024]  Iteration number: 0 with current cost as 0.08855552949919476 and parameters 
[-4.09013748  2.23743466 -2.12427961 -0.11653103  0.5538871  -2.77010897
  3.06858496  2.1896015   1.18552001 -1.06648311  0.60271515  1.14432445
  1.31029901 -1.87354672]. 
Training complete taking 743.669674873352 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Apr  1 16:12:34 2024]  Iteration number: 0 with current cost as 0.10169638677896314 and parameters 
[-4.08085322  2.23743464 -2.12427961 -0.11653103  0.55388708 -2.77010902
  3.06858496  2.1896015   1.18552001 -1.06648314  0.60271515  1.14432445
  1.31029901 -1.87354673]. 
Working on 0.4 fold... 
[Mon Apr  1 16:14:45 2024]  Iteration number: 0 with current cost as 0.09915977532618409 and parameters 
[-4.08193641  2.23743468 -2.12427959 -0.11653103  0.55388712 -2.77010897
  3.06858498  2.1896015   1.18552003 -1.06648308  0.60271517  1.14432447
  1.31029903 -1.87354671]. 
Working on 0.6 fold... 
[Mon Apr  1 16:17:30 2024]  Iteration number: 0 with current cost as 0.10581478334733793 and parameters 
[-4.07412584  2.23743464 -2.12427959 -0.11653103  0.55388708 -2.77010897
  3.06858496  2.18960145  1.18552001 -1.06648313  0.60271515  1.14432443
  1.31029901 -1.87354675]. 
Working on 0.8 fold... 
[Mon Apr  1 16:20:15 2024]  Iteration number: 0 with current cost as 0.08752176144889784 and parameters 
[-4.05783283  2.23743464 -2.12427961 -0.11653103  0.55388711 -2.77010897
  3.06858492  2.18960148  1.18552001 -1.06648308  0.60271513  1.14432445
  1.31029901 -1.87354674]. 
Working on 1.0 fold... 
[Mon Apr  1 16:22:35 2024]  Iteration number: 0 with current cost as 0.08855552949919476 and parameters 
[-4.09013748  2.23743466 -2.12427961 -0.11653103  0.5538871  -2.77010897
  3.06858496  2.1896015   1.18552001 -1.06648311  0.60271515  1.14432445
  1.31029901 -1.87354672]. 
Training complete taking 754.833683013916 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Mon Apr  1 16:25:10 2024]  Iteration number: 0 with current cost as 0.10169638677896314 and parameters 
[-4.08085322  2.23743464 -2.12427961 -0.11653103  0.55388708 -2.77010902
  3.06858496  2.1896015   1.18552001 -1.06648314  0.60271515  1.14432445
  1.31029901 -1.87354673]. 
Working on 0.4 fold... 
[Mon Apr  1 16:27:27 2024]  Iteration number: 0 with current cost as 0.09915977532618409 and parameters 
[-4.08193641  2.23743468 -2.12427959 -0.11653103  0.55388712 -2.77010897
  3.06858498  2.1896015   1.18552003 -1.06648308  0.60271517  1.14432447
  1.31029903 -1.87354671]. 
Working on 0.6 fold... 
[Mon Apr  1 16:30:32 2024]  Iteration number: 0 with current cost as 0.10581478334733793 and parameters 
[-4.07412584  2.23743464 -2.12427959 -0.11653103  0.55388708 -2.77010897
  3.06858496  2.18960145  1.18552001 -1.06648313  0.60271515  1.14432443
  1.31029901 -1.87354675]. 
Working on 0.8 fold... 
[Mon Apr  1 16:33:18 2024]  Iteration number: 0 with current cost as 0.08752176144889784 and parameters 
[-4.05783283  2.23743464 -2.12427961 -0.11653103  0.55388711 -2.77010897
  3.06858492  2.18960148  1.18552001 -1.06648308  0.60271513  1.14432445
  1.31029901 -1.87354674]. 
Working on 1.0 fold... 
[Mon Apr  1 16:35:42 2024]  Iteration number: 0 with current cost as 0.08855552949919476 and parameters 
[-4.09013748  2.23743466 -2.12427961 -0.11653103  0.5538871  -2.77010897
  3.06858496  2.1896015   1.18552001 -1.06648311  0.60271515  1.14432445
  1.31029901 -1.87354672]. 
Training complete taking 793.8586330413818 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Mon Apr  1 16:38:24 2024]  Iteration number: 0 with current cost as 0.10169638677896314 and parameters 
[-4.08085322  2.23743464 -2.12427961 -0.11653103  0.55388708 -2.77010902
  3.06858496  2.1896015   1.18552001 -1.06648314  0.60271515  1.14432445
  1.31029901 -1.87354673]. 
Working on 0.4 fold... 
[Mon Apr  1 16:40:41 2024]  Iteration number: 0 with current cost as 0.09915977532618409 and parameters 
[-4.08193641  2.23743468 -2.12427959 -0.11653103  0.55388712 -2.77010897
  3.06858498  2.1896015   1.18552003 -1.06648308  0.60271517  1.14432447
  1.31029903 -1.87354671]. 
Working on 0.6 fold... 
[Mon Apr  1 16:43:30 2024]  Iteration number: 0 with current cost as 0.10581478334733793 and parameters 
[-4.07412584  2.23743464 -2.12427959 -0.11653103  0.55388708 -2.77010897
  3.06858496  2.18960145  1.18552001 -1.06648313  0.60271515  1.14432443
  1.31029901 -1.87354675]. 
Working on 0.8 fold... 
[Mon Apr  1 16:46:07 2024]  Iteration number: 0 with current cost as 0.08752176144889784 and parameters 
[-4.05783283  2.23743464 -2.12427961 -0.11653103  0.55388711 -2.77010897
  3.06858492  2.18960148  1.18552001 -1.06648308  0.60271513  1.14432445
  1.31029901 -1.87354674]. 
Working on 1.0 fold... 
[Mon Apr  1 16:48:16 2024]  Iteration number: 0 with current cost as 0.08855552949919476 and parameters 
[-4.09013748  2.23743466 -2.12427961 -0.11653103  0.5538871  -2.77010897
  3.06858496  2.1896015   1.18552001 -1.06648311  0.60271515  1.14432445
  1.31029901 -1.87354672]. 
Training complete taking 751.1088080406189 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Apr  1 16:50:58 2024]  Iteration number: 0 with current cost as 0.10169638677896314 and parameters 
[-4.08085322  2.23743464 -2.12427961 -0.11653103  0.55388708 -2.77010902
  3.06858496  2.1896015   1.18552001 -1.06648314  0.60271515  1.14432445
  1.31029901 -1.87354673]. 
Working on 0.4 fold... 
[Mon Apr  1 16:53:11 2024]  Iteration number: 0 with current cost as 0.09915977532618409 and parameters 
[-4.08193641  2.23743468 -2.12427959 -0.11653103  0.55388712 -2.77010897
  3.06858498  2.1896015   1.18552003 -1.06648308  0.60271517  1.14432447
  1.31029903 -1.87354671]. 
Working on 0.6 fold... 
[Mon Apr  1 16:56:05 2024]  Iteration number: 0 with current cost as 0.10581478334733793 and parameters 
[-4.07412584  2.23743464 -2.12427959 -0.11653103  0.55388708 -2.77010897
  3.06858496  2.18960145  1.18552001 -1.06648313  0.60271515  1.14432443
  1.31029901 -1.87354675]. 
Working on 0.8 fold... 
[Mon Apr  1 16:59:04 2024]  Iteration number: 0 with current cost as 0.08752176144889784 and parameters 
[-4.05783283  2.23743464 -2.12427961 -0.11653103  0.55388711 -2.77010897
  3.06858492  2.18960148  1.18552001 -1.06648308  0.60271513  1.14432445
  1.31029901 -1.87354674]. 
Working on 1.0 fold... 
[Mon Apr  1 17:01:32 2024]  Iteration number: 0 with current cost as 0.08855552949919476 and parameters 
[-4.09013748  2.23743466 -2.12427961 -0.11653103  0.5538871  -2.77010897
  3.06858496  2.1896015   1.18552001 -1.06648311  0.60271515  1.14432445
  1.31029901 -1.87354672]. 
Training complete taking 791.5378654003143 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Apr  1 17:04:06 2024]  Iteration number: 0 with current cost as 0.10169638677896314 and parameters 
[-4.08085322  2.23743464 -2.12427961 -0.11653103  0.55388708 -2.77010902
  3.06858496  2.1896015   1.18552001 -1.06648314  0.60271515  1.14432445
  1.31029901 -1.87354673]. 
Working on 0.4 fold... 
[Mon Apr  1 17:06:29 2024]  Iteration number: 0 with current cost as 0.09915977532618409 and parameters 
[-4.08193641  2.23743468 -2.12427959 -0.11653103  0.55388712 -2.77010897
  3.06858498  2.1896015   1.18552003 -1.06648308  0.60271517  1.14432447
  1.31029903 -1.87354671]. 
Working on 0.6 fold... 
[Mon Apr  1 17:09:25 2024]  Iteration number: 0 with current cost as 0.10581478334733793 and parameters 
[-4.07412584  2.23743464 -2.12427959 -0.11653103  0.55388708 -2.77010897
  3.06858496  2.18960145  1.18552001 -1.06648313  0.60271515  1.14432443
  1.31029901 -1.87354675]. 
Working on 0.8 fold... 
[Mon Apr  1 17:12:16 2024]  Iteration number: 0 with current cost as 0.08752176144889784 and parameters 
[-4.05783283  2.23743464 -2.12427961 -0.11653103  0.55388711 -2.77010897
  3.06858492  2.18960148  1.18552001 -1.06648308  0.60271513  1.14432445
  1.31029901 -1.87354674]. 
Working on 1.0 fold... 
[Mon Apr  1 17:14:26 2024]  Iteration number: 0 with current cost as 0.08855552949919476 and parameters 
[-4.09013748  2.23743466 -2.12427961 -0.11653103  0.5538871  -2.77010897
  3.06858496  2.1896015   1.18552001 -1.06648311  0.60271515  1.14432445
  1.31029901 -1.87354672]. 
Training complete taking 775.5340874195099 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Apr  1 17:17:02 2024]  Iteration number: 0 with current cost as 0.10169638677896314 and parameters 
[-4.08085322  2.23743464 -2.12427961 -0.11653103  0.55388708 -2.77010902
  3.06858496  2.1896015   1.18552001 -1.06648314  0.60271515  1.14432445
  1.31029901 -1.87354673]. 
Working on 0.4 fold... 
[Mon Apr  1 17:19:21 2024]  Iteration number: 0 with current cost as 0.09915977532618409 and parameters 
[-4.08193641  2.23743468 -2.12427959 -0.11653103  0.55388712 -2.77010897
  3.06858498  2.1896015   1.18552003 -1.06648308  0.60271517  1.14432447
  1.31029903 -1.87354671]. 
Working on 0.6 fold... 
[Mon Apr  1 17:22:15 2024]  Iteration number: 0 with current cost as 0.10581478334733793 and parameters 
[-4.07412584  2.23743464 -2.12427959 -0.11653103  0.55388708 -2.77010897
  3.06858496  2.18960145  1.18552001 -1.06648313  0.60271515  1.14432443
  1.31029901 -1.87354675]. 
Working on 0.8 fold... 
[Mon Apr  1 17:24:50 2024]  Iteration number: 0 with current cost as 0.08752176144889784 and parameters 
[-4.05783283  2.23743464 -2.12427961 -0.11653103  0.55388711 -2.77010897
  3.06858492  2.18960148  1.18552001 -1.06648308  0.60271513  1.14432445
  1.31029901 -1.87354674]. 
Working on 1.0 fold... 
[Mon Apr  1 17:26:58 2024]  Iteration number: 0 with current cost as 0.08855552949919476 and parameters 
[-4.09013748  2.23743466 -2.12427961 -0.11653103  0.5538871  -2.77010897
  3.06858496  2.1896015   1.18552001 -1.06648311  0.60271515  1.14432445
  1.31029901 -1.87354672]. 
Training complete taking 750.5830857753754 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Mon Apr  1 17:29:24 2024]  Iteration number: 0 with current cost as 0.10169638677896314 and parameters 
[-4.08085322  2.23743464 -2.12427961 -0.11653103  0.55388708 -2.77010902
  3.06858496  2.1896015   1.18552001 -1.06648314  0.60271515  1.14432445
  1.31029901 -1.87354673]. 
Working on 0.4 fold... 
[Mon Apr  1 17:31:47 2024]  Iteration number: 0 with current cost as 0.09915977532618409 and parameters 
[-4.08193641  2.23743468 -2.12427959 -0.11653103  0.55388712 -2.77010897
  3.06858498  2.1896015   1.18552003 -1.06648308  0.60271517  1.14432447
  1.31029903 -1.87354671]. 
Working on 0.6 fold... 
[Mon Apr  1 17:34:37 2024]  Iteration number: 0 with current cost as 0.10581478334733793 and parameters 
[-4.07412584  2.23743464 -2.12427959 -0.11653103  0.55388708 -2.77010897
  3.06858496  2.18960145  1.18552001 -1.06648313  0.60271515  1.14432443
  1.31029901 -1.87354675]. 
Working on 0.8 fold... 
[Mon Apr  1 17:37:07 2024]  Iteration number: 0 with current cost as 0.08752176144889784 and parameters 
[-4.05783283  2.23743464 -2.12427961 -0.11653103  0.55388711 -2.77010897
  3.06858492  2.18960148  1.18552001 -1.06648308  0.60271513  1.14432445
  1.31029901 -1.87354674]. 
Working on 1.0 fold... 
[Mon Apr  1 17:39:13 2024]  Iteration number: 0 with current cost as 0.08855552949919476 and parameters 
[-4.09013748  2.23743466 -2.12427961 -0.11653103  0.5538871  -2.77010897
  3.06858496  2.1896015   1.18552001 -1.06648311  0.60271515  1.14432445
  1.31029901 -1.87354672]. 
Training complete taking 733.2793354988098 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Mon Apr  1 17:41:45 2024]  Iteration number: 0 with current cost as 0.10169638677896314 and parameters 
[-4.08085322  2.23743464 -2.12427961 -0.11653103  0.55388708 -2.77010902
  3.06858496  2.1896015   1.18552001 -1.06648314  0.60271515  1.14432445
  1.31029901 -1.87354673]. 
Working on 0.4 fold... 
[Mon Apr  1 17:43:56 2024]  Iteration number: 0 with current cost as 0.09915977532618409 and parameters 
[-4.08193641  2.23743468 -2.12427959 -0.11653103  0.55388712 -2.77010897
  3.06858498  2.1896015   1.18552003 -1.06648308  0.60271517  1.14432447
  1.31029903 -1.87354671]. 
Working on 0.6 fold... 
[Mon Apr  1 17:46:39 2024]  Iteration number: 0 with current cost as 0.10581478334733793 and parameters 
[-4.07412584  2.23743464 -2.12427959 -0.11653103  0.55388708 -2.77010897
  3.06858496  2.18960145  1.18552001 -1.06648313  0.60271515  1.14432443
  1.31029901 -1.87354675]. 
Working on 0.8 fold... 
[Mon Apr  1 17:49:12 2024]  Iteration number: 0 with current cost as 0.08752176144889784 and parameters 
[-4.05783283  2.23743464 -2.12427961 -0.11653103  0.55388711 -2.77010897
  3.06858492  2.18960148  1.18552001 -1.06648308  0.60271513  1.14432445
  1.31029901 -1.87354674]. 
Working on 1.0 fold... 
[Mon Apr  1 17:51:21 2024]  Iteration number: 0 with current cost as 0.08855552949919476 and parameters 
[-4.09013748  2.23743466 -2.12427961 -0.11653103  0.5538871  -2.77010897
  3.06858496  2.1896015   1.18552001 -1.06648311  0.60271515  1.14432445
  1.31029901 -1.87354672]. 
Training complete taking 727.164528131485 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Apr  1 17:53:45 2024]  Iteration number: 0 with current cost as 0.10169638677896314 and parameters 
[-4.08085322  2.23743464 -2.12427961 -0.11653103  0.55388708 -2.77010902
  3.06858496  2.1896015   1.18552001 -1.06648314  0.60271515  1.14432445
  1.31029901 -1.87354673]. 
Working on 0.4 fold... 
[Mon Apr  1 17:55:58 2024]  Iteration number: 0 with current cost as 0.09915977532618409 and parameters 
[-4.08193641  2.23743468 -2.12427959 -0.11653103  0.55388712 -2.77010897
  3.06858498  2.1896015   1.18552003 -1.06648308  0.60271517  1.14432447
  1.31029903 -1.87354671]. 
Working on 0.6 fold... 
[Mon Apr  1 17:58:34 2024]  Iteration number: 0 with current cost as 0.10581478334733793 and parameters 
[-4.07412584  2.23743464 -2.12427959 -0.11653103  0.55388708 -2.77010897
  3.06858496  2.18960145  1.18552001 -1.06648313  0.60271515  1.14432443
  1.31029901 -1.87354675]. 
Working on 0.8 fold... 
[Mon Apr  1 18:00:55 2024]  Iteration number: 0 with current cost as 0.08752176144889784 and parameters 
[-4.05783283  2.23743464 -2.12427961 -0.11653103  0.55388711 -2.77010897
  3.06858492  2.18960148  1.18552001 -1.06648308  0.60271513  1.14432445
  1.31029901 -1.87354674]. 
Working on 1.0 fold... 
[Mon Apr  1 18:03:05 2024]  Iteration number: 0 with current cost as 0.08855552949919476 and parameters 
[-4.09013748  2.23743466 -2.12427961 -0.11653103  0.5538871  -2.77010897
  3.06858496  2.1896015   1.18552001 -1.06648311  0.60271515  1.14432445
  1.31029901 -1.87354672]. 
Training complete taking 705.3681311607361 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Apr  1 18:05:36 2024]  Iteration number: 0 with current cost as 0.10169638677896314 and parameters 
[-4.08085322  2.23743464 -2.12427961 -0.11653103  0.55388708 -2.77010902
  3.06858496  2.1896015   1.18552001 -1.06648314  0.60271515  1.14432445
  1.31029901 -1.87354673]. 
Working on 0.4 fold... 
[Mon Apr  1 18:07:46 2024]  Iteration number: 0 with current cost as 0.09915977532618409 and parameters 
[-4.08193641  2.23743468 -2.12427959 -0.11653103  0.55388712 -2.77010897
  3.06858498  2.1896015   1.18552003 -1.06648308  0.60271517  1.14432447
  1.31029903 -1.87354671]. 
Working on 0.6 fold... 
[Mon Apr  1 18:10:34 2024]  Iteration number: 0 with current cost as 0.10581478334733793 and parameters 
[-4.07412584  2.23743464 -2.12427959 -0.11653103  0.55388708 -2.77010897
  3.06858496  2.18960145  1.18552001 -1.06648313  0.60271515  1.14432443
  1.31029901 -1.87354675]. 
Working on 0.8 fold... 
[Mon Apr  1 18:13:06 2024]  Iteration number: 0 with current cost as 0.08752176144889784 and parameters 
[-4.05783283  2.23743464 -2.12427961 -0.11653103  0.55388711 -2.77010897
  3.06858492  2.18960148  1.18552001 -1.06648308  0.60271513  1.14432445
  1.31029901 -1.87354674]. 
Working on 1.0 fold... 
[Mon Apr  1 18:15:23 2024]  Iteration number: 0 with current cost as 0.08855552949919476 and parameters 
[-4.09013748  2.23743466 -2.12427961 -0.11653103  0.5538871  -2.77010897
  3.06858496  2.1896015   1.18552001 -1.06648311  0.60271515  1.14432445
  1.31029901 -1.87354672]. 
Training complete taking 737.7588562965393 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Apr  1 18:17:55 2024]  Iteration number: 0 with current cost as 0.10169638677896314 and parameters 
[-4.08085322  2.23743464 -2.12427961 -0.11653103  0.55388708 -2.77010902
  3.06858496  2.1896015   1.18552001 -1.06648314  0.60271515  1.14432445
  1.31029901 -1.87354673]. 
Working on 0.4 fold... 
[Mon Apr  1 18:20:02 2024]  Iteration number: 0 with current cost as 0.09915977532618409 and parameters 
[-4.08193641  2.23743468 -2.12427959 -0.11653103  0.55388712 -2.77010897
  3.06858498  2.1896015   1.18552003 -1.06648308  0.60271517  1.14432447
  1.31029903 -1.87354671]. 
Working on 0.6 fold... 
[Mon Apr  1 18:22:49 2024]  Iteration number: 0 with current cost as 0.10581478334733793 and parameters 
[-4.07412584  2.23743464 -2.12427959 -0.11653103  0.55388708 -2.77010897
  3.06858496  2.18960145  1.18552001 -1.06648313  0.60271515  1.14432443
  1.31029901 -1.87354675]. 
Working on 0.8 fold... 
[Mon Apr  1 18:25:13 2024]  Iteration number: 0 with current cost as 0.08752176144889784 and parameters 
[-4.05783283  2.23743464 -2.12427961 -0.11653103  0.55388711 -2.77010897
  3.06858492  2.18960148  1.18552001 -1.06648308  0.60271513  1.14432445
  1.31029901 -1.87354674]. 
Working on 1.0 fold... 
[Mon Apr  1 18:27:25 2024]  Iteration number: 0 with current cost as 0.08855552949919476 and parameters 
[-4.09013748  2.23743466 -2.12427961 -0.11653103  0.5538871  -2.77010897
  3.06858496  2.1896015   1.18552001 -1.06648311  0.60271515  1.14432445
  1.31029901 -1.87354672]. 
Training complete taking 720.6355645656586 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Apr  1 18:29:52 2024]  Iteration number: 0 with current cost as 0.10169638677896314 and parameters 
[-4.08085322  2.23743464 -2.12427961 -0.11653103  0.55388708 -2.77010902
  3.06858496  2.1896015   1.18552001 -1.06648314  0.60271515  1.14432445
  1.31029901 -1.87354673]. 
Working on 0.4 fold... 
[Mon Apr  1 18:32:04 2024]  Iteration number: 0 with current cost as 0.09915977532618409 and parameters 
[-4.08193641  2.23743468 -2.12427959 -0.11653103  0.55388712 -2.77010897
  3.06858498  2.1896015   1.18552003 -1.06648308  0.60271517  1.14432447
  1.31029903 -1.87354671]. 
Working on 0.6 fold... 
[Mon Apr  1 18:34:51 2024]  Iteration number: 0 with current cost as 0.10581478334733793 and parameters 
[-4.07412584  2.23743464 -2.12427959 -0.11653103  0.55388708 -2.77010897
  3.06858496  2.18960145  1.18552001 -1.06648313  0.60271515  1.14432443
  1.31029901 -1.87354675]. 
Working on 0.8 fold... 
[Mon Apr  1 18:37:27 2024]  Iteration number: 0 with current cost as 0.08752176144889784 and parameters 
[-4.05783283  2.23743464 -2.12427961 -0.11653103  0.55388711 -2.77010897
  3.06858492  2.18960148  1.18552001 -1.06648308  0.60271513  1.14432445
  1.31029901 -1.87354674]. 
Working on 1.0 fold... 
[Mon Apr  1 18:39:35 2024]  Iteration number: 0 with current cost as 0.08855552949919476 and parameters 
[-4.09013748  2.23743466 -2.12427961 -0.11653103  0.5538871  -2.77010897
  3.06858496  2.1896015   1.18552001 -1.06648311  0.60271515  1.14432445
  1.31029901 -1.87354672]. 
Training complete taking 731.4415349960327 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Apr  1 18:42:01 2024]  Iteration number: 0 with current cost as 0.10169638677896314 and parameters 
[-4.08085322  2.23743464 -2.12427961 -0.11653103  0.55388708 -2.77010902
  3.06858496  2.1896015   1.18552001 -1.06648314  0.60271515  1.14432445
  1.31029901 -1.87354673]. 
Working on 0.4 fold... 
[Mon Apr  1 18:44:09 2024]  Iteration number: 0 with current cost as 0.09915977532618409 and parameters 
[-4.08193641  2.23743468 -2.12427959 -0.11653103  0.55388712 -2.77010897
  3.06858498  2.1896015   1.18552003 -1.06648308  0.60271517  1.14432447
  1.31029903 -1.87354671]. 
Working on 0.6 fold... 
[Mon Apr  1 18:46:52 2024]  Iteration number: 0 with current cost as 0.10581478334733793 and parameters 
[-4.07412584  2.23743464 -2.12427959 -0.11653103  0.55388708 -2.77010897
  3.06858496  2.18960145  1.18552001 -1.06648313  0.60271515  1.14432443
  1.31029901 -1.87354675]. 
Working on 0.8 fold... 
[Mon Apr  1 18:49:12 2024]  Iteration number: 0 with current cost as 0.08752176144889784 and parameters 
[-4.05783283  2.23743464 -2.12427961 -0.11653103  0.55388711 -2.77010897
  3.06858492  2.18960148  1.18552001 -1.06648308  0.60271513  1.14432445
  1.31029901 -1.87354674]. 
Working on 1.0 fold... 
[Mon Apr  1 18:51:18 2024]  Iteration number: 0 with current cost as 0.08855552949919476 and parameters 
[-4.09013748  2.23743466 -2.12427961 -0.11653103  0.5538871  -2.77010897
  3.06858496  2.1896015   1.18552001 -1.06648311  0.60271515  1.14432445
  1.31029901 -1.87354672]. 
Training complete taking 699.9545800685883 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Apr  1 18:53:43 2024]  Iteration number: 0 with current cost as 0.10169638677896314 and parameters 
[-4.08085322  2.23743464 -2.12427961 -0.11653103  0.55388708 -2.77010902
  3.06858496  2.1896015   1.18552001 -1.06648314  0.60271515  1.14432445
  1.31029901 -1.87354673]. 
Working on 0.4 fold... 
[Mon Apr  1 18:55:54 2024]  Iteration number: 0 with current cost as 0.09915977532618409 and parameters 
[-4.08193641  2.23743468 -2.12427959 -0.11653103  0.55388712 -2.77010897
  3.06858498  2.1896015   1.18552003 -1.06648308  0.60271517  1.14432447
  1.31029903 -1.87354671]. 
Working on 0.6 fold... 
[Mon Apr  1 18:58:42 2024]  Iteration number: 0 with current cost as 0.10581478334733793 and parameters 
[-4.07412584  2.23743464 -2.12427959 -0.11653103  0.55388708 -2.77010897
  3.06858496  2.18960145  1.18552001 -1.06648313  0.60271515  1.14432443
  1.31029901 -1.87354675]. 
Working on 0.8 fold... 
[Mon Apr  1 19:01:07 2024]  Iteration number: 0 with current cost as 0.08752176144889784 and parameters 
[-4.05783283  2.23743464 -2.12427961 -0.11653103  0.55388711 -2.77010897
  3.06858492  2.18960148  1.18552001 -1.06648308  0.60271513  1.14432445
  1.31029901 -1.87354674]. 
Working on 1.0 fold... 
[Mon Apr  1 19:03:20 2024]  Iteration number: 0 with current cost as 0.08855552949919476 and parameters 
[-4.09013748  2.23743466 -2.12427961 -0.11653103  0.5538871  -2.77010897
  3.06858496  2.1896015   1.18552001 -1.06648311  0.60271515  1.14432445
  1.31029901 -1.87354672]. 
Training complete taking 723.0083091259003 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Apr  1 19:05:46 2024]  Iteration number: 0 with current cost as 0.10169638677896314 and parameters 
[-4.08085322  2.23743464 -2.12427961 -0.11653103  0.55388708 -2.77010902
  3.06858496  2.1896015   1.18552001 -1.06648314  0.60271515  1.14432445
  1.31029901 -1.87354673]. 
Working on 0.4 fold... 
[Mon Apr  1 19:07:53 2024]  Iteration number: 0 with current cost as 0.09915977532618409 and parameters 
[-4.08193641  2.23743468 -2.12427959 -0.11653103  0.55388712 -2.77010897
  3.06858498  2.1896015   1.18552003 -1.06648308  0.60271517  1.14432447
  1.31029903 -1.87354671]. 
Working on 0.6 fold... 
[Mon Apr  1 19:10:35 2024]  Iteration number: 0 with current cost as 0.10581478334733793 and parameters 
[-4.07412584  2.23743464 -2.12427959 -0.11653103  0.55388708 -2.77010897
  3.06858496  2.18960145  1.18552001 -1.06648313  0.60271515  1.14432443
  1.31029901 -1.87354675]. 
Working on 0.8 fold... 
[Mon Apr  1 19:13:04 2024]  Iteration number: 0 with current cost as 0.08752176144889784 and parameters 
[-4.05783283  2.23743464 -2.12427961 -0.11653103  0.55388711 -2.77010897
  3.06858492  2.18960148  1.18552001 -1.06648308  0.60271513  1.14432445
  1.31029901 -1.87354674]. 
Working on 1.0 fold... 
[Mon Apr  1 19:15:13 2024]  Iteration number: 0 with current cost as 0.08855552949919476 and parameters 
[-4.09013748  2.23743466 -2.12427961 -0.11653103  0.5538871  -2.77010897
  3.06858496  2.1896015   1.18552001 -1.06648311  0.60271515  1.14432445
  1.31029901 -1.87354672]. 
Training complete taking 718.8687913417816 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Apr  1 19:17:42 2024]  Iteration number: 0 with current cost as 0.10169638677896314 and parameters 
[-4.08085322  2.23743464 -2.12427961 -0.11653103  0.55388708 -2.77010902
  3.06858496  2.1896015   1.18552001 -1.06648314  0.60271515  1.14432445
  1.31029901 -1.87354673]. 
Working on 0.4 fold... 
[Mon Apr  1 19:19:58 2024]  Iteration number: 0 with current cost as 0.09915977532618409 and parameters 
[-4.08193641  2.23743468 -2.12427959 -0.11653103  0.55388712 -2.77010897
  3.06858498  2.1896015   1.18552003 -1.06648308  0.60271517  1.14432447
  1.31029903 -1.87354671]. 
Working on 0.6 fold... 
[Mon Apr  1 19:22:56 2024]  Iteration number: 0 with current cost as 0.10581478334733793 and parameters 
[-4.07412584  2.23743464 -2.12427959 -0.11653103  0.55388708 -2.77010897
  3.06858496  2.18960145  1.18552001 -1.06648313  0.60271515  1.14432443
  1.31029901 -1.87354675]. 
Working on 0.8 fold... 
[Mon Apr  1 19:25:24 2024]  Iteration number: 0 with current cost as 0.08752176144889784 and parameters 
[-4.05783283  2.23743464 -2.12427961 -0.11653103  0.55388711 -2.77010897
  3.06858492  2.18960148  1.18552001 -1.06648308  0.60271513  1.14432445
  1.31029901 -1.87354674]. 
Working on 1.0 fold... 
[Mon Apr  1 19:27:29 2024]  Iteration number: 0 with current cost as 0.08855552949919476 and parameters 
[-4.09013748  2.23743466 -2.12427961 -0.11653103  0.5538871  -2.77010897
  3.06858496  2.1896015   1.18552001 -1.06648311  0.60271515  1.14432445
  1.31029901 -1.87354672]. 
Training complete taking 732.9470806121826 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Apr  1 19:29:57 2024]  Iteration number: 0 with current cost as 0.10169638677896314 and parameters 
[-4.08085322  2.23743464 -2.12427961 -0.11653103  0.55388708 -2.77010902
  3.06858496  2.1896015   1.18552001 -1.06648314  0.60271515  1.14432445
  1.31029901 -1.87354673]. 
Working on 0.4 fold... 
[Mon Apr  1 19:32:03 2024]  Iteration number: 0 with current cost as 0.09915977532618409 and parameters 
[-4.08193641  2.23743468 -2.12427959 -0.11653103  0.55388712 -2.77010897
  3.06858498  2.1896015   1.18552003 -1.06648308  0.60271517  1.14432447
  1.31029903 -1.87354671]. 
Working on 0.6 fold... 
[Mon Apr  1 19:34:58 2024]  Iteration number: 0 with current cost as 0.10581478334733793 and parameters 
[-4.07412584  2.23743464 -2.12427959 -0.11653103  0.55388708 -2.77010897
  3.06858496  2.18960145  1.18552001 -1.06648313  0.60271515  1.14432443
  1.31029901 -1.87354675]. 
Working on 0.8 fold... 
[Mon Apr  1 19:37:30 2024]  Iteration number: 0 with current cost as 0.08752176144889784 and parameters 
[-4.05783283  2.23743464 -2.12427961 -0.11653103  0.55388711 -2.77010897
  3.06858492  2.18960148  1.18552001 -1.06648308  0.60271513  1.14432445
  1.31029901 -1.87354674]. 
Working on 1.0 fold... 
[Mon Apr  1 19:39:38 2024]  Iteration number: 0 with current cost as 0.08855552949919476 and parameters 
[-4.09013748  2.23743466 -2.12427961 -0.11653103  0.5538871  -2.77010897
  3.06858496  2.1896015   1.18552001 -1.06648311  0.60271515  1.14432445
  1.31029901 -1.87354672]. 
Training complete taking 733.2129502296448 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Apr  1 19:42:18 2024]  Iteration number: 0 with current cost as 0.10169638677896314 and parameters 
[-4.08085322  2.23743464 -2.12427961 -0.11653103  0.55388708 -2.77010902
  3.06858496  2.1896015   1.18552001 -1.06648314  0.60271515  1.14432445
  1.31029901 -1.87354673]. 
Working on 0.4 fold... 
[Mon Apr  1 19:44:21 2024]  Iteration number: 0 with current cost as 0.09915977532618409 and parameters 
[-4.08193641  2.23743468 -2.12427959 -0.11653103  0.55388712 -2.77010897
  3.06858498  2.1896015   1.18552003 -1.06648308  0.60271517  1.14432447
  1.31029903 -1.87354671]. 
Working on 0.6 fold... 
[Mon Apr  1 19:47:04 2024]  Iteration number: 0 with current cost as 0.10581478334733793 and parameters 
[-4.07412584  2.23743464 -2.12427959 -0.11653103  0.55388708 -2.77010897
  3.06858496  2.18960145  1.18552001 -1.06648313  0.60271515  1.14432443
  1.31029901 -1.87354675]. 
Working on 0.8 fold... 
[Mon Apr  1 19:49:30 2024]  Iteration number: 0 with current cost as 0.08752176144889784 and parameters 
[-4.05783283  2.23743464 -2.12427961 -0.11653103  0.55388711 -2.77010897
  3.06858492  2.18960148  1.18552001 -1.06648308  0.60271513  1.14432445
  1.31029901 -1.87354674]. 
Working on 1.0 fold... 
[Mon Apr  1 19:51:39 2024]  Iteration number: 0 with current cost as 0.08855552949919476 and parameters 
[-4.09013748  2.23743466 -2.12427961 -0.11653103  0.5538871  -2.77010897
  3.06858496  2.1896015   1.18552001 -1.06648311  0.60271515  1.14432445
  1.31029901 -1.87354672]. 
Training complete taking 718.74094414711 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Apr  1 19:54:07 2024]  Iteration number: 0 with current cost as 0.10169638677896314 and parameters 
[-4.08085322  2.23743464 -2.12427961 -0.11653103  0.55388708 -2.77010902
  3.06858496  2.1896015   1.18552001 -1.06648314  0.60271515  1.14432445
  1.31029901 -1.87354673]. 
Working on 0.4 fold... 
[Mon Apr  1 19:56:17 2024]  Iteration number: 0 with current cost as 0.09915977532618409 and parameters 
[-4.08193641  2.23743468 -2.12427959 -0.11653103  0.55388712 -2.77010897
  3.06858498  2.1896015   1.18552003 -1.06648308  0.60271517  1.14432447
  1.31029903 -1.87354671]. 
Working on 0.6 fold... 
[Mon Apr  1 19:58:59 2024]  Iteration number: 0 with current cost as 0.10581478334733793 and parameters 
[-4.07412584  2.23743464 -2.12427959 -0.11653103  0.55388708 -2.77010897
  3.06858496  2.18960145  1.18552001 -1.06648313  0.60271515  1.14432443
  1.31029901 -1.87354675]. 
Working on 0.8 fold... 
[Mon Apr  1 20:01:29 2024]  Iteration number: 0 with current cost as 0.08752176144889784 and parameters 
[-4.05783283  2.23743464 -2.12427961 -0.11653103  0.55388711 -2.77010897
  3.06858492  2.18960148  1.18552001 -1.06648308  0.60271513  1.14432445
  1.31029901 -1.87354674]. 
Working on 1.0 fold... 
[Mon Apr  1 20:03:40 2024]  Iteration number: 0 with current cost as 0.08855552949919476 and parameters 
[-4.09013748  2.23743466 -2.12427961 -0.11653103  0.5538871  -2.77010897
  3.06858496  2.1896015   1.18552001 -1.06648311  0.60271515  1.14432445
  1.31029901 -1.87354672]. 
Training complete taking 720.3415853977203 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Apr  1 20:06:09 2024]  Iteration number: 0 with current cost as 0.10169638677896314 and parameters 
[-4.08085322  2.23743464 -2.12427961 -0.11653103  0.55388708 -2.77010902
  3.06858496  2.1896015   1.18552001 -1.06648314  0.60271515  1.14432445
  1.31029901 -1.87354673]. 
Working on 0.4 fold... 
[Mon Apr  1 20:08:21 2024]  Iteration number: 0 with current cost as 0.09915977532618409 and parameters 
[-4.08193641  2.23743468 -2.12427959 -0.11653103  0.55388712 -2.77010897
  3.06858498  2.1896015   1.18552003 -1.06648308  0.60271517  1.14432447
  1.31029903 -1.87354671]. 
Working on 0.6 fold... 
[Mon Apr  1 20:11:03 2024]  Iteration number: 0 with current cost as 0.10581478334733793 and parameters 
[-4.07412584  2.23743464 -2.12427959 -0.11653103  0.55388708 -2.77010897
  3.06858496  2.18960145  1.18552001 -1.06648313  0.60271515  1.14432443
  1.31029901 -1.87354675]. 
Working on 0.8 fold... 
[Mon Apr  1 20:13:23 2024]  Iteration number: 0 with current cost as 0.08752176144889784 and parameters 
[-4.05783283  2.23743464 -2.12427961 -0.11653103  0.55388711 -2.77010897
  3.06858492  2.18960148  1.18552001 -1.06648308  0.60271513  1.14432445
  1.31029901 -1.87354674]. 
Working on 1.0 fold... 
[Mon Apr  1 20:15:25 2024]  Iteration number: 0 with current cost as 0.08855552949919476 and parameters 
[-4.09013748  2.23743466 -2.12427961 -0.11653103  0.5538871  -2.77010897
  3.06858496  2.1896015   1.18552001 -1.06648311  0.60271515  1.14432445
  1.31029901 -1.87354672]. 
Training complete taking 703.130273103714 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Apr  1 20:17:55 2024]  Iteration number: 0 with current cost as 0.10169638677896314 and parameters 
[-4.08085322  2.23743464 -2.12427961 -0.11653103  0.55388708 -2.77010902
  3.06858496  2.1896015   1.18552001 -1.06648314  0.60271515  1.14432445
  1.31029901 -1.87354673]. 
Working on 0.4 fold... 
[Mon Apr  1 20:20:07 2024]  Iteration number: 0 with current cost as 0.09915977532618409 and parameters 
[-4.08193641  2.23743468 -2.12427959 -0.11653103  0.55388712 -2.77010897
  3.06858498  2.1896015   1.18552003 -1.06648308  0.60271517  1.14432447
  1.31029903 -1.87354671]. 
Working on 0.6 fold... 
[Mon Apr  1 20:22:46 2024]  Iteration number: 0 with current cost as 0.10581478334733793 and parameters 
[-4.07412584  2.23743464 -2.12427959 -0.11653103  0.55388708 -2.77010897
  3.06858496  2.18960145  1.18552001 -1.06648313  0.60271515  1.14432443
  1.31029901 -1.87354675]. 
Working on 0.8 fold... 
[Mon Apr  1 20:25:20 2024]  Iteration number: 0 with current cost as 0.08752176144889784 and parameters 
[-4.05783283  2.23743464 -2.12427961 -0.11653103  0.55388711 -2.77010897
  3.06858492  2.18960148  1.18552001 -1.06648308  0.60271513  1.14432445
  1.31029901 -1.87354674]. 
Working on 1.0 fold... 
[Mon Apr  1 20:27:28 2024]  Iteration number: 0 with current cost as 0.08855552949919476 and parameters 
[-4.09013748  2.23743466 -2.12427961 -0.11653103  0.5538871  -2.77010897
  3.06858496  2.1896015   1.18552001 -1.06648311  0.60271515  1.14432445
  1.31029901 -1.87354672]. 
Training complete taking 720.1200935840607 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Mon Apr  1 20:29:52 2024]  Iteration number: 0 with current cost as 0.10169638677896314 and parameters 
[-4.08085322  2.23743464 -2.12427961 -0.11653103  0.55388708 -2.77010902
  3.06858496  2.1896015   1.18552001 -1.06648314  0.60271515  1.14432445
  1.31029901 -1.87354673]. 
Working on 0.4 fold... 
[Mon Apr  1 20:32:10 2024]  Iteration number: 0 with current cost as 0.09915977532618409 and parameters 
[-4.08193641  2.23743468 -2.12427959 -0.11653103  0.55388712 -2.77010897
  3.06858498  2.1896015   1.18552003 -1.06648308  0.60271517  1.14432447
  1.31029903 -1.87354671]. 
Working on 0.6 fold... 
[Mon Apr  1 20:34:56 2024]  Iteration number: 0 with current cost as 0.10581478334733793 and parameters 
[-4.07412584  2.23743464 -2.12427959 -0.11653103  0.55388708 -2.77010897
  3.06858496  2.18960145  1.18552001 -1.06648313  0.60271515  1.14432443
  1.31029901 -1.87354675]. 
Working on 0.8 fold... 
[Mon Apr  1 20:37:17 2024]  Iteration number: 0 with current cost as 0.08752176144889784 and parameters 
[-4.05783283  2.23743464 -2.12427961 -0.11653103  0.55388711 -2.77010897
  3.06858492  2.18960148  1.18552001 -1.06648308  0.60271513  1.14432445
  1.31029901 -1.87354674]. 
Working on 1.0 fold... 
[Mon Apr  1 20:39:25 2024]  Iteration number: 0 with current cost as 0.08855552949919476 and parameters 
[-4.09013748  2.23743466 -2.12427961 -0.11653103  0.5538871  -2.77010897
  3.06858496  2.1896015   1.18552001 -1.06648311  0.60271515  1.14432445
  1.31029901 -1.87354672]. 
Training complete taking 716.4997470378876 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Mon Apr  1 20:41:51 2024]  Iteration number: 0 with current cost as 0.10169638677896314 and parameters 
[-4.08085322  2.23743464 -2.12427961 -0.11653103  0.55388708 -2.77010902
  3.06858496  2.1896015   1.18552001 -1.06648314  0.60271515  1.14432445
  1.31029901 -1.87354673]. 
Working on 0.4 fold... 
[Mon Apr  1 20:44:05 2024]  Iteration number: 0 with current cost as 0.09915977532618409 and parameters 
[-4.08193641  2.23743468 -2.12427959 -0.11653103  0.55388712 -2.77010897
  3.06858498  2.1896015   1.18552003 -1.06648308  0.60271517  1.14432447
  1.31029903 -1.87354671]. 
Working on 0.6 fold... 
[Mon Apr  1 20:46:58 2024]  Iteration number: 0 with current cost as 0.10581478334733793 and parameters 
[-4.07412584  2.23743464 -2.12427959 -0.11653103  0.55388708 -2.77010897
  3.06858496  2.18960145  1.18552001 -1.06648313  0.60271515  1.14432443
  1.31029901 -1.87354675]. 
Working on 0.8 fold... 
[Mon Apr  1 20:49:18 2024]  Iteration number: 0 with current cost as 0.08752176144889784 and parameters 
[-4.05783283  2.23743464 -2.12427961 -0.11653103  0.55388711 -2.77010897
  3.06858492  2.18960148  1.18552001 -1.06648308  0.60271513  1.14432445
  1.31029901 -1.87354674]. 
Working on 1.0 fold... 
[Mon Apr  1 20:51:24 2024]  Iteration number: 0 with current cost as 0.08855552949919476 and parameters 
[-4.09013748  2.23743466 -2.12427961 -0.11653103  0.5538871  -2.77010897
  3.06858496  2.1896015   1.18552001 -1.06648311  0.60271515  1.14432445
  1.31029901 -1.87354672]. 
Training complete taking 717.0770318508148 seconds. 
Discarding model... 

Training complete taking 18343.083703517914 total seconds. 
Now scoring model... 
Scoring complete taking 2.2453410625457764 seconds. 
Saved predicted values as A2_Efficient-CRX_predicted_values.csv
Model scores: {'MSE_train': (2.122479239292234,), 'R2_train': 0.7085676228692981, 'MAE_train': 1.2181502326860383, 'MSE_test': 3.0395138423682457, 'R2_test': 0.6969888642685322, 'MAE_test': 1.5096786804942124}. 
Saved model results as A2_Efficient-CRX_results.json. 
