/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/circuits/Encoders.py:12: RuntimeWarning: invalid value encountered in arcsin
  if np.isnan(np.arcsin(features[feature_idx])) or np.isnan(np.arccos(features[feature_idx])):
Loading dataset from /scratch/j/jacobsen/gjones/quad5qubits/quadratic_train.bin... 
Successfully loaded /scratch/j/jacobsen/gjones/quad5qubits/quadratic_train.bin into X and y data. 
Loading dataset from /scratch/j/jacobsen/gjones/quad5qubits/quadratic_test.bin... 
Successfully loaded /scratch/j/jacobsen/gjones/quad5qubits/quadratic_test.bin into X and y data. 
Training model with dataset /scratch/j/jacobsen/gjones/quad5qubits/quadratic_train.bin 
 at time Mon Apr  1 15:52:09 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Apr  1 15:54:08 2024]  Iteration number: 0 with current cost as 0.2827689606154648 and parameters 
[-2.30530751  2.23743467 -2.12427953 -0.11653099  0.55388711 -2.77010894
  3.06858498  2.18960149  1.18552005 -1.06648312  0.60271514  1.14432445
  1.31029902 -1.87354673]. 
Working on 0.4 fold... 
[Mon Apr  1 15:57:56 2024]  Iteration number: 0 with current cost as 0.2639135090175912 and parameters 
[-2.28729065  2.23743464 -2.12427956 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552006 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.87354673]. 
Working on 0.6 fold... 
[Mon Apr  1 16:01:51 2024]  Iteration number: 0 with current cost as 0.2531625415119122 and parameters 
[-2.37353658  2.2374347  -2.12427953 -0.11653099  0.55388708 -2.77010894
  3.06858502  2.18960149  1.18552005 -1.06648312  0.60271514  1.14432445
  1.31029902 -1.8735467 ]. 
Working on 0.8 fold... 
[Mon Apr  1 16:06:08 2024]  Iteration number: 0 with current cost as 0.19339387390915408 and parameters 
[-2.44440213  2.23743466 -2.12427952 -0.11653105  0.55388708 -2.77010897
  3.06858495  2.18960148  1.18552001 -1.06648311  0.60271519  1.14432445
  1.31029904 -1.87354672]. 
Working on 1.0 fold... 
[Mon Apr  1 16:09:25 2024]  Iteration number: 0 with current cost as 0.20577283546119968 and parameters 
[-2.36037789  2.23743466 -2.12427955 -0.11653103  0.55388711 -2.770109
  3.06858504  2.18960142  1.18552007 -1.06648314  0.60271513  1.14432445
  1.31029901 -1.87354674]. 
Training complete taking 1088.299946308136 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Apr  1 16:12:11 2024]  Iteration number: 0 with current cost as 0.2827689606154648 and parameters 
[-2.30530751  2.23743467 -2.12427953 -0.11653099  0.55388711 -2.77010894
  3.06858498  2.18960149  1.18552005 -1.06648312  0.60271514  1.14432445
  1.31029902 -1.87354673]. 
Working on 0.4 fold... 
[Mon Apr  1 16:15:59 2024]  Iteration number: 0 with current cost as 0.2639135090175912 and parameters 
[-2.28729065  2.23743464 -2.12427956 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552006 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.87354673]. 
Working on 0.6 fold... 
[Mon Apr  1 16:19:49 2024]  Iteration number: 0 with current cost as 0.2531625415119122 and parameters 
[-2.37353658  2.2374347  -2.12427953 -0.11653099  0.55388708 -2.77010894
  3.06858502  2.18960149  1.18552005 -1.06648312  0.60271514  1.14432445
  1.31029902 -1.8735467 ]. 
Working on 0.8 fold... 
[Mon Apr  1 16:24:02 2024]  Iteration number: 0 with current cost as 0.19339387390915408 and parameters 
[-2.44440213  2.23743466 -2.12427952 -0.11653105  0.55388708 -2.77010897
  3.06858495  2.18960148  1.18552001 -1.06648311  0.60271519  1.14432445
  1.31029904 -1.87354672]. 
Working on 1.0 fold... 
[Mon Apr  1 16:27:14 2024]  Iteration number: 0 with current cost as 0.20577283546119968 and parameters 
[-2.36037789  2.23743466 -2.12427955 -0.11653103  0.55388711 -2.770109
  3.06858504  2.18960142  1.18552007 -1.06648314  0.60271513  1.14432445
  1.31029901 -1.87354674]. 
Training complete taking 1060.7753818035126 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Apr  1 16:29:52 2024]  Iteration number: 0 with current cost as 0.2827689606154648 and parameters 
[-2.30530751  2.23743467 -2.12427953 -0.11653099  0.55388711 -2.77010894
  3.06858498  2.18960149  1.18552005 -1.06648312  0.60271514  1.14432445
  1.31029902 -1.87354673]. 
Working on 0.4 fold... 
[Mon Apr  1 16:33:34 2024]  Iteration number: 0 with current cost as 0.2639135090175912 and parameters 
[-2.28729065  2.23743464 -2.12427956 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552006 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.87354673]. 
Working on 0.6 fold... 
[Mon Apr  1 16:37:16 2024]  Iteration number: 0 with current cost as 0.2531625415119122 and parameters 
[-2.37353658  2.2374347  -2.12427953 -0.11653099  0.55388708 -2.77010894
  3.06858502  2.18960149  1.18552005 -1.06648312  0.60271514  1.14432445
  1.31029902 -1.8735467 ]. 
Working on 0.8 fold... 
[Mon Apr  1 16:41:30 2024]  Iteration number: 0 with current cost as 0.19339387390915408 and parameters 
[-2.44440213  2.23743466 -2.12427952 -0.11653105  0.55388708 -2.77010897
  3.06858495  2.18960148  1.18552001 -1.06648311  0.60271519  1.14432445
  1.31029904 -1.87354672]. 
Working on 1.0 fold... 
[Mon Apr  1 16:44:24 2024]  Iteration number: 0 with current cost as 0.20577283546119968 and parameters 
[-2.36037789  2.23743466 -2.12427955 -0.11653103  0.55388711 -2.770109
  3.06858504  2.18960142  1.18552007 -1.06648314  0.60271513  1.14432445
  1.31029901 -1.87354674]. 
Training complete taking 1030.2401070594788 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Mon Apr  1 16:46:59 2024]  Iteration number: 0 with current cost as 0.2827689606154648 and parameters 
[-2.30530751  2.23743467 -2.12427953 -0.11653099  0.55388711 -2.77010894
  3.06858498  2.18960149  1.18552005 -1.06648312  0.60271514  1.14432445
  1.31029902 -1.87354673]. 
Working on 0.4 fold... 
[Mon Apr  1 16:50:40 2024]  Iteration number: 0 with current cost as 0.2639135090175912 and parameters 
[-2.28729065  2.23743464 -2.12427956 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552006 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.87354673]. 
Working on 0.6 fold... 
[Mon Apr  1 16:54:11 2024]  Iteration number: 0 with current cost as 0.2531625415119122 and parameters 
[-2.37353658  2.2374347  -2.12427953 -0.11653099  0.55388708 -2.77010894
  3.06858502  2.18960149  1.18552005 -1.06648312  0.60271514  1.14432445
  1.31029902 -1.8735467 ]. 
Working on 0.8 fold... 
[Mon Apr  1 16:58:11 2024]  Iteration number: 0 with current cost as 0.19339387390915408 and parameters 
[-2.44440213  2.23743466 -2.12427952 -0.11653105  0.55388708 -2.77010897
  3.06858495  2.18960148  1.18552001 -1.06648311  0.60271519  1.14432445
  1.31029904 -1.87354672]. 
Working on 1.0 fold... 
[Mon Apr  1 17:01:05 2024]  Iteration number: 0 with current cost as 0.20577283546119968 and parameters 
[-2.36037789  2.23743466 -2.12427955 -0.11653103  0.55388711 -2.770109
  3.06858504  2.18960142  1.18552007 -1.06648314  0.60271513  1.14432445
  1.31029901 -1.87354674]. 
Training complete taking 1000.9071414470673 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Mon Apr  1 17:03:35 2024]  Iteration number: 0 with current cost as 0.2827689606154648 and parameters 
[-2.30530751  2.23743467 -2.12427953 -0.11653099  0.55388711 -2.77010894
  3.06858498  2.18960149  1.18552005 -1.06648312  0.60271514  1.14432445
  1.31029902 -1.87354673]. 
Working on 0.4 fold... 
[Mon Apr  1 17:07:13 2024]  Iteration number: 0 with current cost as 0.2639135090175912 and parameters 
[-2.28729065  2.23743464 -2.12427956 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552006 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.87354673]. 
Working on 0.6 fold... 
[Mon Apr  1 17:10:54 2024]  Iteration number: 0 with current cost as 0.2531625415119122 and parameters 
[-2.37353658  2.2374347  -2.12427953 -0.11653099  0.55388708 -2.77010894
  3.06858502  2.18960149  1.18552005 -1.06648312  0.60271514  1.14432445
  1.31029902 -1.8735467 ]. 
Working on 0.8 fold... 
[Mon Apr  1 17:14:52 2024]  Iteration number: 0 with current cost as 0.19339387390915408 and parameters 
[-2.44440213  2.23743466 -2.12427952 -0.11653105  0.55388708 -2.77010897
  3.06858495  2.18960148  1.18552001 -1.06648311  0.60271519  1.14432445
  1.31029904 -1.87354672]. 
Working on 1.0 fold... 
[Mon Apr  1 17:17:45 2024]  Iteration number: 0 with current cost as 0.20577283546119968 and parameters 
[-2.36037789  2.23743466 -2.12427955 -0.11653103  0.55388711 -2.770109
  3.06858504  2.18960142  1.18552007 -1.06648314  0.60271513  1.14432445
  1.31029901 -1.87354674]. 
Training complete taking 999.0066738128662 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Apr  1 17:20:17 2024]  Iteration number: 0 with current cost as 0.2827689606154648 and parameters 
[-2.30530751  2.23743467 -2.12427953 -0.11653099  0.55388711 -2.77010894
  3.06858498  2.18960149  1.18552005 -1.06648312  0.60271514  1.14432445
  1.31029902 -1.87354673]. 
Working on 0.4 fold... 
[Mon Apr  1 17:23:56 2024]  Iteration number: 0 with current cost as 0.2639135090175912 and parameters 
[-2.28729065  2.23743464 -2.12427956 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552006 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.87354673]. 
Working on 0.6 fold... 
[Mon Apr  1 17:27:36 2024]  Iteration number: 0 with current cost as 0.2531625415119122 and parameters 
[-2.37353658  2.2374347  -2.12427953 -0.11653099  0.55388708 -2.77010894
  3.06858502  2.18960149  1.18552005 -1.06648312  0.60271514  1.14432445
  1.31029902 -1.8735467 ]. 
Working on 0.8 fold... 
[Mon Apr  1 17:31:38 2024]  Iteration number: 0 with current cost as 0.19339387390915408 and parameters 
[-2.44440213  2.23743466 -2.12427952 -0.11653105  0.55388708 -2.77010897
  3.06858495  2.18960148  1.18552001 -1.06648311  0.60271519  1.14432445
  1.31029904 -1.87354672]. 
Working on 1.0 fold... 
[Mon Apr  1 17:34:33 2024]  Iteration number: 0 with current cost as 0.20577283546119968 and parameters 
[-2.36037789  2.23743466 -2.12427955 -0.11653103  0.55388711 -2.770109
  3.06858504  2.18960142  1.18552007 -1.06648314  0.60271513  1.14432445
  1.31029901 -1.87354674]. 
Training complete taking 1005.8018724918365 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Apr  1 17:37:03 2024]  Iteration number: 0 with current cost as 0.2827689606154648 and parameters 
[-2.30530751  2.23743467 -2.12427953 -0.11653099  0.55388711 -2.77010894
  3.06858498  2.18960149  1.18552005 -1.06648312  0.60271514  1.14432445
  1.31029902 -1.87354673]. 
Working on 0.4 fold... 
[Mon Apr  1 17:40:42 2024]  Iteration number: 0 with current cost as 0.2639135090175912 and parameters 
[-2.28729065  2.23743464 -2.12427956 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552006 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.87354673]. 
Working on 0.6 fold... 
[Mon Apr  1 17:44:21 2024]  Iteration number: 0 with current cost as 0.2531625415119122 and parameters 
[-2.37353658  2.2374347  -2.12427953 -0.11653099  0.55388708 -2.77010894
  3.06858502  2.18960149  1.18552005 -1.06648312  0.60271514  1.14432445
  1.31029902 -1.8735467 ]. 
Working on 0.8 fold... 
[Mon Apr  1 17:48:21 2024]  Iteration number: 0 with current cost as 0.19339387390915408 and parameters 
[-2.44440213  2.23743466 -2.12427952 -0.11653105  0.55388708 -2.77010897
  3.06858495  2.18960148  1.18552001 -1.06648311  0.60271519  1.14432445
  1.31029904 -1.87354672]. 
Working on 1.0 fold... 
[Mon Apr  1 17:51:10 2024]  Iteration number: 0 with current cost as 0.20577283546119968 and parameters 
[-2.36037789  2.23743466 -2.12427955 -0.11653103  0.55388711 -2.770109
  3.06858504  2.18960142  1.18552007 -1.06648314  0.60271513  1.14432445
  1.31029901 -1.87354674]. 
Training complete taking 998.7847354412079 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Apr  1 17:53:40 2024]  Iteration number: 0 with current cost as 0.2827689606154648 and parameters 
[-2.30530751  2.23743467 -2.12427953 -0.11653099  0.55388711 -2.77010894
  3.06858498  2.18960149  1.18552005 -1.06648312  0.60271514  1.14432445
  1.31029902 -1.87354673]. 
Working on 0.4 fold... 
[Mon Apr  1 17:57:18 2024]  Iteration number: 0 with current cost as 0.2639135090175912 and parameters 
[-2.28729065  2.23743464 -2.12427956 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552006 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.87354673]. 
Working on 0.6 fold... 
[Mon Apr  1 18:00:57 2024]  Iteration number: 0 with current cost as 0.2531625415119122 and parameters 
[-2.37353658  2.2374347  -2.12427953 -0.11653099  0.55388708 -2.77010894
  3.06858502  2.18960149  1.18552005 -1.06648312  0.60271514  1.14432445
  1.31029902 -1.8735467 ]. 
Working on 0.8 fold... 
[Mon Apr  1 18:04:58 2024]  Iteration number: 0 with current cost as 0.19339387390915408 and parameters 
[-2.44440213  2.23743466 -2.12427952 -0.11653105  0.55388708 -2.77010897
  3.06858495  2.18960148  1.18552001 -1.06648311  0.60271519  1.14432445
  1.31029904 -1.87354672]. 
Working on 1.0 fold... 
[Mon Apr  1 18:07:54 2024]  Iteration number: 0 with current cost as 0.20577283546119968 and parameters 
[-2.36037789  2.23743466 -2.12427955 -0.11653103  0.55388711 -2.770109
  3.06858504  2.18960142  1.18552007 -1.06648314  0.60271513  1.14432445
  1.31029901 -1.87354674]. 
Training complete taking 1004.4578847885132 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Mon Apr  1 18:10:32 2024]  Iteration number: 0 with current cost as 0.2827689606154648 and parameters 
[-2.30530751  2.23743467 -2.12427953 -0.11653099  0.55388711 -2.77010894
  3.06858498  2.18960149  1.18552005 -1.06648312  0.60271514  1.14432445
  1.31029902 -1.87354673]. 
Working on 0.4 fold... 
[Mon Apr  1 18:14:21 2024]  Iteration number: 0 with current cost as 0.2639135090175912 and parameters 
[-2.28729065  2.23743464 -2.12427956 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552006 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.87354673]. 
Working on 0.6 fold... 
[Mon Apr  1 18:18:01 2024]  Iteration number: 0 with current cost as 0.2531625415119122 and parameters 
[-2.37353658  2.2374347  -2.12427953 -0.11653099  0.55388708 -2.77010894
  3.06858502  2.18960149  1.18552005 -1.06648312  0.60271514  1.14432445
  1.31029902 -1.8735467 ]. 
Working on 0.8 fold... 
[Mon Apr  1 18:22:02 2024]  Iteration number: 0 with current cost as 0.19339387390915408 and parameters 
[-2.44440213  2.23743466 -2.12427952 -0.11653105  0.55388708 -2.77010897
  3.06858495  2.18960148  1.18552001 -1.06648311  0.60271519  1.14432445
  1.31029904 -1.87354672]. 
Working on 1.0 fold... 
[Mon Apr  1 18:24:55 2024]  Iteration number: 0 with current cost as 0.20577283546119968 and parameters 
[-2.36037789  2.23743466 -2.12427955 -0.11653103  0.55388711 -2.770109
  3.06858504  2.18960142  1.18552007 -1.06648314  0.60271513  1.14432445
  1.31029901 -1.87354674]. 
Training complete taking 1019.6911780834198 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Mon Apr  1 18:27:27 2024]  Iteration number: 0 with current cost as 0.2827689606154648 and parameters 
[-2.30530751  2.23743467 -2.12427953 -0.11653099  0.55388711 -2.77010894
  3.06858498  2.18960149  1.18552005 -1.06648312  0.60271514  1.14432445
  1.31029902 -1.87354673]. 
Working on 0.4 fold... 
[Mon Apr  1 18:31:08 2024]  Iteration number: 0 with current cost as 0.2639135090175912 and parameters 
[-2.28729065  2.23743464 -2.12427956 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552006 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.87354673]. 
Working on 0.6 fold... 
[Mon Apr  1 18:34:45 2024]  Iteration number: 0 with current cost as 0.2531625415119122 and parameters 
[-2.37353658  2.2374347  -2.12427953 -0.11653099  0.55388708 -2.77010894
  3.06858502  2.18960149  1.18552005 -1.06648312  0.60271514  1.14432445
  1.31029902 -1.8735467 ]. 
Working on 0.8 fold... 
[Mon Apr  1 18:38:48 2024]  Iteration number: 0 with current cost as 0.19339387390915408 and parameters 
[-2.44440213  2.23743466 -2.12427952 -0.11653105  0.55388708 -2.77010897
  3.06858495  2.18960148  1.18552001 -1.06648311  0.60271519  1.14432445
  1.31029904 -1.87354672]. 
Working on 1.0 fold... 
[Mon Apr  1 18:41:45 2024]  Iteration number: 0 with current cost as 0.20577283546119968 and parameters 
[-2.36037789  2.23743466 -2.12427955 -0.11653103  0.55388711 -2.770109
  3.06858504  2.18960142  1.18552007 -1.06648314  0.60271513  1.14432445
  1.31029901 -1.87354674]. 
Training complete taking 1010.5405232906342 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Apr  1 18:44:14 2024]  Iteration number: 0 with current cost as 0.2827689606154648 and parameters 
[-2.30530751  2.23743467 -2.12427953 -0.11653099  0.55388711 -2.77010894
  3.06858498  2.18960149  1.18552005 -1.06648312  0.60271514  1.14432445
  1.31029902 -1.87354673]. 
Working on 0.4 fold... 
[Mon Apr  1 18:47:55 2024]  Iteration number: 0 with current cost as 0.2639135090175912 and parameters 
[-2.28729065  2.23743464 -2.12427956 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552006 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.87354673]. 
Working on 0.6 fold... 
[Mon Apr  1 18:51:32 2024]  Iteration number: 0 with current cost as 0.2531625415119122 and parameters 
[-2.37353658  2.2374347  -2.12427953 -0.11653099  0.55388708 -2.77010894
  3.06858502  2.18960149  1.18552005 -1.06648312  0.60271514  1.14432445
  1.31029902 -1.8735467 ]. 
Working on 0.8 fold... 
[Mon Apr  1 18:55:32 2024]  Iteration number: 0 with current cost as 0.19339387390915408 and parameters 
[-2.44440213  2.23743466 -2.12427952 -0.11653105  0.55388708 -2.77010897
  3.06858495  2.18960148  1.18552001 -1.06648311  0.60271519  1.14432445
  1.31029904 -1.87354672]. 
Working on 1.0 fold... 
[Mon Apr  1 18:58:24 2024]  Iteration number: 0 with current cost as 0.20577283546119968 and parameters 
[-2.36037789  2.23743466 -2.12427955 -0.11653103  0.55388711 -2.770109
  3.06858504  2.18960142  1.18552007 -1.06648314  0.60271513  1.14432445
  1.31029901 -1.87354674]. 
Training complete taking 999.4102566242218 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Apr  1 19:00:59 2024]  Iteration number: 0 with current cost as 0.2827689606154648 and parameters 
[-2.30530751  2.23743467 -2.12427953 -0.11653099  0.55388711 -2.77010894
  3.06858498  2.18960149  1.18552005 -1.06648312  0.60271514  1.14432445
  1.31029902 -1.87354673]. 
Working on 0.4 fold... 
[Mon Apr  1 19:04:41 2024]  Iteration number: 0 with current cost as 0.2639135090175912 and parameters 
[-2.28729065  2.23743464 -2.12427956 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552006 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.87354673]. 
Working on 0.6 fold... 
[Mon Apr  1 19:08:25 2024]  Iteration number: 0 with current cost as 0.2531625415119122 and parameters 
[-2.37353658  2.2374347  -2.12427953 -0.11653099  0.55388708 -2.77010894
  3.06858502  2.18960149  1.18552005 -1.06648312  0.60271514  1.14432445
  1.31029902 -1.8735467 ]. 
Working on 0.8 fold... 
[Mon Apr  1 19:12:34 2024]  Iteration number: 0 with current cost as 0.19339387390915408 and parameters 
[-2.44440213  2.23743466 -2.12427952 -0.11653105  0.55388708 -2.77010897
  3.06858495  2.18960148  1.18552001 -1.06648311  0.60271519  1.14432445
  1.31029904 -1.87354672]. 
Working on 1.0 fold... 
[Mon Apr  1 19:15:41 2024]  Iteration number: 0 with current cost as 0.20577283546119968 and parameters 
[-2.36037789  2.23743466 -2.12427955 -0.11653103  0.55388711 -2.770109
  3.06858504  2.18960142  1.18552007 -1.06648314  0.60271513  1.14432445
  1.31029901 -1.87354674]. 
Training complete taking 1040.149819135666 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Apr  1 19:18:17 2024]  Iteration number: 0 with current cost as 0.2827689606154648 and parameters 
[-2.30530751  2.23743467 -2.12427953 -0.11653099  0.55388711 -2.77010894
  3.06858498  2.18960149  1.18552005 -1.06648312  0.60271514  1.14432445
  1.31029902 -1.87354673]. 
Working on 0.4 fold... 
[Mon Apr  1 19:22:08 2024]  Iteration number: 0 with current cost as 0.2639135090175912 and parameters 
[-2.28729065  2.23743464 -2.12427956 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552006 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.87354673]. 
Working on 0.6 fold... 
[Mon Apr  1 19:26:01 2024]  Iteration number: 0 with current cost as 0.2531625415119122 and parameters 
[-2.37353658  2.2374347  -2.12427953 -0.11653099  0.55388708 -2.77010894
  3.06858502  2.18960149  1.18552005 -1.06648312  0.60271514  1.14432445
  1.31029902 -1.8735467 ]. 
Working on 0.8 fold... 
[Mon Apr  1 19:30:18 2024]  Iteration number: 0 with current cost as 0.19339387390915408 and parameters 
[-2.44440213  2.23743466 -2.12427952 -0.11653105  0.55388708 -2.77010897
  3.06858495  2.18960148  1.18552001 -1.06648311  0.60271519  1.14432445
  1.31029904 -1.87354672]. 
Working on 1.0 fold... 
[Mon Apr  1 19:33:17 2024]  Iteration number: 0 with current cost as 0.20577283546119968 and parameters 
[-2.36037789  2.23743466 -2.12427955 -0.11653103  0.55388711 -2.770109
  3.06858504  2.18960142  1.18552007 -1.06648314  0.60271513  1.14432445
  1.31029901 -1.87354674]. 
Training complete taking 1056.54607462883 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Apr  1 19:36:01 2024]  Iteration number: 0 with current cost as 0.2827689606154648 and parameters 
[-2.30530751  2.23743467 -2.12427953 -0.11653099  0.55388711 -2.77010894
  3.06858498  2.18960149  1.18552005 -1.06648312  0.60271514  1.14432445
  1.31029902 -1.87354673]. 
Working on 0.4 fold... 
[Mon Apr  1 19:39:59 2024]  Iteration number: 0 with current cost as 0.2639135090175912 and parameters 
[-2.28729065  2.23743464 -2.12427956 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552006 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.87354673]. 
Working on 0.6 fold... 
[Mon Apr  1 19:43:49 2024]  Iteration number: 0 with current cost as 0.2531625415119122 and parameters 
[-2.37353658  2.2374347  -2.12427953 -0.11653099  0.55388708 -2.77010894
  3.06858502  2.18960149  1.18552005 -1.06648312  0.60271514  1.14432445
  1.31029902 -1.8735467 ]. 
Working on 0.8 fold... 
[Mon Apr  1 19:48:05 2024]  Iteration number: 0 with current cost as 0.19339387390915408 and parameters 
[-2.44440213  2.23743466 -2.12427952 -0.11653105  0.55388708 -2.77010897
  3.06858495  2.18960148  1.18552001 -1.06648311  0.60271519  1.14432445
  1.31029904 -1.87354672]. 
Working on 1.0 fold... 
[Mon Apr  1 19:51:08 2024]  Iteration number: 0 with current cost as 0.20577283546119968 and parameters 
[-2.36037789  2.23743466 -2.12427955 -0.11653103  0.55388711 -2.770109
  3.06858504  2.18960142  1.18552007 -1.06648314  0.60271513  1.14432445
  1.31029901 -1.87354674]. 
Training complete taking 1068.6447157859802 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Apr  1 19:53:49 2024]  Iteration number: 0 with current cost as 0.2827689606154648 and parameters 
[-2.30530751  2.23743467 -2.12427953 -0.11653099  0.55388711 -2.77010894
  3.06858498  2.18960149  1.18552005 -1.06648312  0.60271514  1.14432445
  1.31029902 -1.87354673]. 
Working on 0.4 fold... 
[Mon Apr  1 19:57:37 2024]  Iteration number: 0 with current cost as 0.2639135090175912 and parameters 
[-2.28729065  2.23743464 -2.12427956 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552006 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.87354673]. 
Working on 0.6 fold... 
[Mon Apr  1 20:01:31 2024]  Iteration number: 0 with current cost as 0.2531625415119122 and parameters 
[-2.37353658  2.2374347  -2.12427953 -0.11653099  0.55388708 -2.77010894
  3.06858502  2.18960149  1.18552005 -1.06648312  0.60271514  1.14432445
  1.31029902 -1.8735467 ]. 
Working on 0.8 fold... 
[Mon Apr  1 20:05:45 2024]  Iteration number: 0 with current cost as 0.19339387390915408 and parameters 
[-2.44440213  2.23743466 -2.12427952 -0.11653105  0.55388708 -2.77010897
  3.06858495  2.18960148  1.18552001 -1.06648311  0.60271519  1.14432445
  1.31029904 -1.87354672]. 
Working on 1.0 fold... 
[Mon Apr  1 20:08:45 2024]  Iteration number: 0 with current cost as 0.20577283546119968 and parameters 
[-2.36037789  2.23743466 -2.12427955 -0.11653103  0.55388711 -2.770109
  3.06858504  2.18960142  1.18552007 -1.06648314  0.60271513  1.14432445
  1.31029901 -1.87354674]. 
Training complete taking 1058.5177216529846 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Apr  1 20:11:21 2024]  Iteration number: 0 with current cost as 0.2827689606154648 and parameters 
[-2.30530751  2.23743467 -2.12427953 -0.11653099  0.55388711 -2.77010894
  3.06858498  2.18960149  1.18552005 -1.06648312  0.60271514  1.14432445
  1.31029902 -1.87354673]. 
Working on 0.4 fold... 
[Mon Apr  1 20:15:11 2024]  Iteration number: 0 with current cost as 0.2639135090175912 and parameters 
[-2.28729065  2.23743464 -2.12427956 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552006 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.87354673]. 
Working on 0.6 fold... 
[Mon Apr  1 20:19:04 2024]  Iteration number: 0 with current cost as 0.2531625415119122 and parameters 
[-2.37353658  2.2374347  -2.12427953 -0.11653099  0.55388708 -2.77010894
  3.06858502  2.18960149  1.18552005 -1.06648312  0.60271514  1.14432445
  1.31029902 -1.8735467 ]. 
Working on 0.8 fold... 
[Mon Apr  1 20:23:09 2024]  Iteration number: 0 with current cost as 0.19339387390915408 and parameters 
[-2.44440213  2.23743466 -2.12427952 -0.11653105  0.55388708 -2.77010897
  3.06858495  2.18960148  1.18552001 -1.06648311  0.60271519  1.14432445
  1.31029904 -1.87354672]. 
Working on 1.0 fold... 
[Mon Apr  1 20:26:09 2024]  Iteration number: 0 with current cost as 0.20577283546119968 and parameters 
[-2.36037789  2.23743466 -2.12427955 -0.11653103  0.55388711 -2.770109
  3.06858504  2.18960142  1.18552007 -1.06648314  0.60271513  1.14432445
  1.31029901 -1.87354674]. 
Training complete taking 1043.8817818164825 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Apr  1 20:28:54 2024]  Iteration number: 0 with current cost as 0.2827689606154648 and parameters 
[-2.30530751  2.23743467 -2.12427953 -0.11653099  0.55388711 -2.77010894
  3.06858498  2.18960149  1.18552005 -1.06648312  0.60271514  1.14432445
  1.31029902 -1.87354673]. 
Working on 0.4 fold... 
[Mon Apr  1 20:32:47 2024]  Iteration number: 0 with current cost as 0.2639135090175912 and parameters 
[-2.28729065  2.23743464 -2.12427956 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552006 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.87354673]. 
Working on 0.6 fold... 
[Mon Apr  1 20:36:33 2024]  Iteration number: 0 with current cost as 0.2531625415119122 and parameters 
[-2.37353658  2.2374347  -2.12427953 -0.11653099  0.55388708 -2.77010894
  3.06858502  2.18960149  1.18552005 -1.06648312  0.60271514  1.14432445
  1.31029902 -1.8735467 ]. 
Working on 0.8 fold... 
[Mon Apr  1 20:40:48 2024]  Iteration number: 0 with current cost as 0.19339387390915408 and parameters 
[-2.44440213  2.23743466 -2.12427952 -0.11653105  0.55388708 -2.77010897
  3.06858495  2.18960148  1.18552001 -1.06648311  0.60271519  1.14432445
  1.31029904 -1.87354672]. 
Working on 1.0 fold... 
[Mon Apr  1 20:43:48 2024]  Iteration number: 0 with current cost as 0.20577283546119968 and parameters 
[-2.36037789  2.23743466 -2.12427955 -0.11653103  0.55388711 -2.770109
  3.06858504  2.18960142  1.18552007 -1.06648314  0.60271513  1.14432445
  1.31029901 -1.87354674]. 
Training complete taking 1057.2826254367828 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Apr  1 20:46:24 2024]  Iteration number: 0 with current cost as 0.2827689606154648 and parameters 
[-2.30530751  2.23743467 -2.12427953 -0.11653099  0.55388711 -2.77010894
  3.06858498  2.18960149  1.18552005 -1.06648312  0.60271514  1.14432445
  1.31029902 -1.87354673]. 
Working on 0.4 fold... 
[Mon Apr  1 20:50:06 2024]  Iteration number: 0 with current cost as 0.2639135090175912 and parameters 
[-2.28729065  2.23743464 -2.12427956 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552006 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.87354673]. 
Working on 0.6 fold... 
[Mon Apr  1 20:53:57 2024]  Iteration number: 0 with current cost as 0.2531625415119122 and parameters 
[-2.37353658  2.2374347  -2.12427953 -0.11653099  0.55388708 -2.77010894
  3.06858502  2.18960149  1.18552005 -1.06648312  0.60271514  1.14432445
  1.31029902 -1.8735467 ]. 
Working on 0.8 fold... 
[Mon Apr  1 20:58:06 2024]  Iteration number: 0 with current cost as 0.19339387390915408 and parameters 
[-2.44440213  2.23743466 -2.12427952 -0.11653105  0.55388708 -2.77010897
  3.06858495  2.18960148  1.18552001 -1.06648311  0.60271519  1.14432445
  1.31029904 -1.87354672]. 
Working on 1.0 fold... 
[Mon Apr  1 21:01:06 2024]  Iteration number: 0 with current cost as 0.20577283546119968 and parameters 
[-2.36037789  2.23743466 -2.12427955 -0.11653103  0.55388711 -2.770109
  3.06858504  2.18960142  1.18552007 -1.06648314  0.60271513  1.14432445
  1.31029901 -1.87354674]. 
Training complete taking 1041.533117055893 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Apr  1 21:03:43 2024]  Iteration number: 0 with current cost as 0.2827689606154648 and parameters 
[-2.30530751  2.23743467 -2.12427953 -0.11653099  0.55388711 -2.77010894
  3.06858498  2.18960149  1.18552005 -1.06648312  0.60271514  1.14432445
  1.31029902 -1.87354673]. 
Working on 0.4 fold... 
[Mon Apr  1 21:07:25 2024]  Iteration number: 0 with current cost as 0.2639135090175912 and parameters 
[-2.28729065  2.23743464 -2.12427956 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552006 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.87354673]. 
Working on 0.6 fold... 
[Mon Apr  1 21:11:03 2024]  Iteration number: 0 with current cost as 0.2531625415119122 and parameters 
[-2.37353658  2.2374347  -2.12427953 -0.11653099  0.55388708 -2.77010894
  3.06858502  2.18960149  1.18552005 -1.06648312  0.60271514  1.14432445
  1.31029902 -1.8735467 ]. 
Working on 0.8 fold... 
[Mon Apr  1 21:15:04 2024]  Iteration number: 0 with current cost as 0.19339387390915408 and parameters 
[-2.44440213  2.23743466 -2.12427952 -0.11653105  0.55388708 -2.77010897
  3.06858495  2.18960148  1.18552001 -1.06648311  0.60271519  1.14432445
  1.31029904 -1.87354672]. 
Working on 1.0 fold... 
[Mon Apr  1 21:18:00 2024]  Iteration number: 0 with current cost as 0.20577283546119968 and parameters 
[-2.36037789  2.23743466 -2.12427955 -0.11653103  0.55388711 -2.770109
  3.06858504  2.18960142  1.18552007 -1.06648314  0.60271513  1.14432445
  1.31029901 -1.87354674]. 
Training complete taking 1009.7580554485321 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Apr  1 21:20:34 2024]  Iteration number: 0 with current cost as 0.2827689606154648 and parameters 
[-2.30530751  2.23743467 -2.12427953 -0.11653099  0.55388711 -2.77010894
  3.06858498  2.18960149  1.18552005 -1.06648312  0.60271514  1.14432445
  1.31029902 -1.87354673]. 
Working on 0.4 fold... 
[Mon Apr  1 21:24:12 2024]  Iteration number: 0 with current cost as 0.2639135090175912 and parameters 
[-2.28729065  2.23743464 -2.12427956 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552006 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.87354673]. 
Working on 0.6 fold... 
[Mon Apr  1 21:27:50 2024]  Iteration number: 0 with current cost as 0.2531625415119122 and parameters 
[-2.37353658  2.2374347  -2.12427953 -0.11653099  0.55388708 -2.77010894
  3.06858502  2.18960149  1.18552005 -1.06648312  0.60271514  1.14432445
  1.31029902 -1.8735467 ]. 
Working on 0.8 fold... 
[Mon Apr  1 21:31:55 2024]  Iteration number: 0 with current cost as 0.19339387390915408 and parameters 
[-2.44440213  2.23743466 -2.12427952 -0.11653105  0.55388708 -2.77010897
  3.06858495  2.18960148  1.18552001 -1.06648311  0.60271519  1.14432445
  1.31029904 -1.87354672]. 
Working on 1.0 fold... 
[Mon Apr  1 21:34:58 2024]  Iteration number: 0 with current cost as 0.20577283546119968 and parameters 
[-2.36037789  2.23743466 -2.12427955 -0.11653103  0.55388711 -2.770109
  3.06858504  2.18960142  1.18552007 -1.06648314  0.60271513  1.14432445
  1.31029901 -1.87354674]. 
Training complete taking 1018.9321036338806 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Apr  1 21:37:34 2024]  Iteration number: 0 with current cost as 0.2827689606154648 and parameters 
[-2.30530751  2.23743467 -2.12427953 -0.11653099  0.55388711 -2.77010894
  3.06858498  2.18960149  1.18552005 -1.06648312  0.60271514  1.14432445
  1.31029902 -1.87354673]. 
Working on 0.4 fold... 
[Mon Apr  1 21:41:20 2024]  Iteration number: 0 with current cost as 0.2639135090175912 and parameters 
[-2.28729065  2.23743464 -2.12427956 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552006 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.87354673]. 
Working on 0.6 fold... 
[Mon Apr  1 21:45:11 2024]  Iteration number: 0 with current cost as 0.2531625415119122 and parameters 
[-2.37353658  2.2374347  -2.12427953 -0.11653099  0.55388708 -2.77010894
  3.06858502  2.18960149  1.18552005 -1.06648312  0.60271514  1.14432445
  1.31029902 -1.8735467 ]. 
Working on 0.8 fold... 
[Mon Apr  1 21:49:21 2024]  Iteration number: 0 with current cost as 0.19339387390915408 and parameters 
[-2.44440213  2.23743466 -2.12427952 -0.11653105  0.55388708 -2.77010897
  3.06858495  2.18960148  1.18552001 -1.06648311  0.60271519  1.14432445
  1.31029904 -1.87354672]. 
Working on 1.0 fold... 
[Mon Apr  1 21:52:21 2024]  Iteration number: 0 with current cost as 0.20577283546119968 and parameters 
[-2.36037789  2.23743466 -2.12427955 -0.11653103  0.55388711 -2.770109
  3.06858504  2.18960142  1.18552007 -1.06648314  0.60271513  1.14432445
  1.31029901 -1.87354674]. 
Training complete taking 1043.9546508789062 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Apr  1 21:55:01 2024]  Iteration number: 0 with current cost as 0.2827689606154648 and parameters 
[-2.30530751  2.23743467 -2.12427953 -0.11653099  0.55388711 -2.77010894
  3.06858498  2.18960149  1.18552005 -1.06648312  0.60271514  1.14432445
  1.31029902 -1.87354673]. 
Working on 0.4 fold... 
[Mon Apr  1 21:58:55 2024]  Iteration number: 0 with current cost as 0.2639135090175912 and parameters 
[-2.28729065  2.23743464 -2.12427956 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552006 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.87354673]. 
Working on 0.6 fold... 
[Mon Apr  1 22:02:41 2024]  Iteration number: 0 with current cost as 0.2531625415119122 and parameters 
[-2.37353658  2.2374347  -2.12427953 -0.11653099  0.55388708 -2.77010894
  3.06858502  2.18960149  1.18552005 -1.06648312  0.60271514  1.14432445
  1.31029902 -1.8735467 ]. 
Working on 0.8 fold... 
[Mon Apr  1 22:06:53 2024]  Iteration number: 0 with current cost as 0.19339387390915408 and parameters 
[-2.44440213  2.23743466 -2.12427952 -0.11653105  0.55388708 -2.77010897
  3.06858495  2.18960148  1.18552001 -1.06648311  0.60271519  1.14432445
  1.31029904 -1.87354672]. 
Working on 1.0 fold... 
[Mon Apr  1 22:10:01 2024]  Iteration number: 0 with current cost as 0.20577283546119968 and parameters 
[-2.36037789  2.23743466 -2.12427955 -0.11653103  0.55388711 -2.770109
  3.06858504  2.18960142  1.18552007 -1.06648314  0.60271513  1.14432445
  1.31029901 -1.87354674]. 
Training complete taking 1060.1006581783295 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Apr  1 22:12:38 2024]  Iteration number: 0 with current cost as 0.2827689606154648 and parameters 
[-2.30530751  2.23743467 -2.12427953 -0.11653099  0.55388711 -2.77010894
  3.06858498  2.18960149  1.18552005 -1.06648312  0.60271514  1.14432445
  1.31029902 -1.87354673]. 
Working on 0.4 fold... 
[Mon Apr  1 22:16:28 2024]  Iteration number: 0 with current cost as 0.2639135090175912 and parameters 
[-2.28729065  2.23743464 -2.12427956 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552006 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.87354673]. 
Working on 0.6 fold... 
[Mon Apr  1 22:20:11 2024]  Iteration number: 0 with current cost as 0.2531625415119122 and parameters 
[-2.37353658  2.2374347  -2.12427953 -0.11653099  0.55388708 -2.77010894
  3.06858502  2.18960149  1.18552005 -1.06648312  0.60271514  1.14432445
  1.31029902 -1.8735467 ]. 
Working on 0.8 fold... 
[Mon Apr  1 22:24:18 2024]  Iteration number: 0 with current cost as 0.19339387390915408 and parameters 
[-2.44440213  2.23743466 -2.12427952 -0.11653105  0.55388708 -2.77010897
  3.06858495  2.18960148  1.18552001 -1.06648311  0.60271519  1.14432445
  1.31029904 -1.87354672]. 
Working on 1.0 fold... 
[Mon Apr  1 22:27:24 2024]  Iteration number: 0 with current cost as 0.20577283546119968 and parameters 
[-2.36037789  2.23743466 -2.12427955 -0.11653103  0.55388711 -2.770109
  3.06858504  2.18960142  1.18552007 -1.06648314  0.60271513  1.14432445
  1.31029901 -1.87354674]. 
Training complete taking 1044.6583323478699 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Mon Apr  1 22:30:07 2024]  Iteration number: 0 with current cost as 0.2827689606154648 and parameters 
[-2.30530751  2.23743467 -2.12427953 -0.11653099  0.55388711 -2.77010894
  3.06858498  2.18960149  1.18552005 -1.06648312  0.60271514  1.14432445
  1.31029902 -1.87354673]. 
Working on 0.4 fold... 
[Mon Apr  1 22:33:59 2024]  Iteration number: 0 with current cost as 0.2639135090175912 and parameters 
[-2.28729065  2.23743464 -2.12427956 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552006 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.87354673]. 
Working on 0.6 fold... 
[Mon Apr  1 22:37:47 2024]  Iteration number: 0 with current cost as 0.2531625415119122 and parameters 
[-2.37353658  2.2374347  -2.12427953 -0.11653099  0.55388708 -2.77010894
  3.06858502  2.18960149  1.18552005 -1.06648312  0.60271514  1.14432445
  1.31029902 -1.8735467 ]. 
Working on 0.8 fold... 
[Mon Apr  1 22:41:55 2024]  Iteration number: 0 with current cost as 0.19339387390915408 and parameters 
[-2.44440213  2.23743466 -2.12427952 -0.11653105  0.55388708 -2.77010897
  3.06858495  2.18960148  1.18552001 -1.06648311  0.60271519  1.14432445
  1.31029904 -1.87354672]. 
Working on 1.0 fold... 
[Mon Apr  1 22:45:00 2024]  Iteration number: 0 with current cost as 0.20577283546119968 and parameters 
[-2.36037789  2.23743466 -2.12427955 -0.11653103  0.55388711 -2.770109
  3.06858504  2.18960142  1.18552007 -1.06648314  0.60271513  1.14432445
  1.31029901 -1.87354674]. 
Training complete taking 1054.6901178359985 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Mon Apr  1 22:47:46 2024]  Iteration number: 0 with current cost as 0.2827689606154648 and parameters 
[-2.30530751  2.23743467 -2.12427953 -0.11653099  0.55388711 -2.77010894
  3.06858498  2.18960149  1.18552005 -1.06648312  0.60271514  1.14432445
  1.31029902 -1.87354673]. 
Working on 0.4 fold... 
[Mon Apr  1 22:51:40 2024]  Iteration number: 0 with current cost as 0.2639135090175912 and parameters 
[-2.28729065  2.23743464 -2.12427956 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552006 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.87354673]. 
Working on 0.6 fold... 
[Mon Apr  1 22:55:33 2024]  Iteration number: 0 with current cost as 0.2531625415119122 and parameters 
[-2.37353658  2.2374347  -2.12427953 -0.11653099  0.55388708 -2.77010894
  3.06858502  2.18960149  1.18552005 -1.06648312  0.60271514  1.14432445
  1.31029902 -1.8735467 ]. 
Working on 0.8 fold... 
[Mon Apr  1 22:59:44 2024]  Iteration number: 0 with current cost as 0.19339387390915408 and parameters 
[-2.44440213  2.23743466 -2.12427952 -0.11653105  0.55388708 -2.77010897
  3.06858495  2.18960148  1.18552001 -1.06648311  0.60271519  1.14432445
  1.31029904 -1.87354672]. 
Working on 1.0 fold... 
[Mon Apr  1 23:02:45 2024]  Iteration number: 0 with current cost as 0.20577283546119968 and parameters 
[-2.36037789  2.23743466 -2.12427955 -0.11653103  0.55388711 -2.770109
  3.06858504  2.18960142  1.18552007 -1.06648314  0.60271513  1.14432445
  1.31029901 -1.87354674]. 
Training complete taking 1070.0208604335785 seconds. 
Discarding model... 

Training complete taking 25886.58745789528 total seconds. 
Now scoring model... 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Scoring complete taking 3.433406114578247 seconds. 
Saved predicted values as M-M-CNOT_Efficient-CRX_predicted_values.csv
Model scores: {'MSE_train': (5.300948236868843,), 'R2_train': 0.272139902658061, 'MAE_train': 1.554626072655997, 'MSE_test': 14.149148472227477, 'R2_test': -0.4105379249933958, 'MAE_test': 2.590218646918206}. 
Saved model results as M-M-CNOT_Efficient-CRX_results.json. 
