/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/circuits/Encoders.py:12: RuntimeWarning: invalid value encountered in arcsin
  if np.isnan(np.arcsin(features[feature_idx])) or np.isnan(np.arccos(features[feature_idx])):
Loading dataset from /home/gjones/scratch/quad5qubits/quadratic_train.bin... 
Successfully loaded /home/gjones/scratch/quad5qubits/quadratic_train.bin into X and y data. 
Loading dataset from /home/gjones/scratch/quad5qubits/quadratic_test.bin... 
Successfully loaded /home/gjones/scratch/quad5qubits/quadratic_test.bin into X and y data. 
Training model with dataset /home/gjones/scratch/quad5qubits/quadratic_train.bin 
 at time Wed Mar 27 19:25:21 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Wed Mar 27 19:25:54 2024]  Iteration number: 0 with current cost as 0.11698850091636781 and parameters 
[-3.05376927  2.46927284 -2.04927282 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.92383606  1.14432445
  1.31029899 -1.87354678  0.7296508   2.88578418 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Wed Mar 27 19:30:01 2024]  Iteration number: 0 with current cost as 0.1197197862522623 and parameters 
[-3.02702905  2.47202593 -2.05506616 -0.11653103  0.55388708 -2.77010897
  3.06858496  2.18960145  1.18551998 -1.06648309  0.95519411  1.14432446
  1.31029899 -1.87354678  0.72965079  2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Wed Mar 27 19:35:10 2024]  Iteration number: 0 with current cost as 0.11494981341934012 and parameters 
[-3.05599894  2.48155719 -2.05685987 -0.11653103  0.55388708 -2.77010896
  3.06858498  2.18960146  1.18552    -1.06648308  0.9484506   1.14432447
  1.310299   -1.87354678  0.7296508   2.8857842  -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Wed Mar 27 19:38:54 2024]  Iteration number: 0 with current cost as 0.10077028894774623 and parameters 
[-3.01848564  2.40314933 -2.08842121 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.89495669  1.14432446
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Wed Mar 27 19:43:52 2024]  Iteration number: 0 with current cost as 0.10625381182772078 and parameters 
[-2.99517064  2.40484136 -2.07926075 -0.11653103  0.55388708 -2.77010896
  3.06858497  2.18960144  1.18551999 -1.06648308  0.97705377  1.14432446
  1.31029899 -1.87354678  0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1389.1448338031769 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Wed Mar 27 19:49:02 2024]  Iteration number: 0 with current cost as 0.11698850091636781 and parameters 
[-3.05376927  2.46927284 -2.04927282 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.92383606  1.14432445
  1.31029899 -1.87354678  0.7296508   2.88578418 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Wed Mar 27 19:53:06 2024]  Iteration number: 0 with current cost as 0.1197197862522623 and parameters 
[-3.02702905  2.47202593 -2.05506616 -0.11653103  0.55388708 -2.77010897
  3.06858496  2.18960145  1.18551998 -1.06648309  0.95519411  1.14432446
  1.31029899 -1.87354678  0.72965079  2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Wed Mar 27 19:58:16 2024]  Iteration number: 0 with current cost as 0.11494981341934012 and parameters 
[-3.05599894  2.48155719 -2.05685987 -0.11653103  0.55388708 -2.77010896
  3.06858498  2.18960146  1.18552    -1.06648308  0.9484506   1.14432447
  1.310299   -1.87354678  0.7296508   2.8857842  -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Wed Mar 27 20:01:57 2024]  Iteration number: 0 with current cost as 0.10077028894774623 and parameters 
[-3.01848564  2.40314933 -2.08842121 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.89495669  1.14432446
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Wed Mar 27 20:06:52 2024]  Iteration number: 0 with current cost as 0.10625381182772078 and parameters 
[-2.99517064  2.40484136 -2.07926075 -0.11653103  0.55388708 -2.77010896
  3.06858497  2.18960144  1.18551999 -1.06648308  0.97705377  1.14432446
  1.31029899 -1.87354678  0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1373.251591205597 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Wed Mar 27 20:11:55 2024]  Iteration number: 0 with current cost as 0.11698850091636781 and parameters 
[-3.05376927  2.46927284 -2.04927282 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.92383606  1.14432445
  1.31029899 -1.87354678  0.7296508   2.88578418 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Wed Mar 27 20:15:57 2024]  Iteration number: 0 with current cost as 0.1197197862522623 and parameters 
[-3.02702905  2.47202593 -2.05506616 -0.11653103  0.55388708 -2.77010897
  3.06858496  2.18960145  1.18551998 -1.06648309  0.95519411  1.14432446
  1.31029899 -1.87354678  0.72965079  2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Wed Mar 27 20:21:04 2024]  Iteration number: 0 with current cost as 0.11494981341934012 and parameters 
[-3.05599894  2.48155719 -2.05685987 -0.11653103  0.55388708 -2.77010896
  3.06858498  2.18960146  1.18552    -1.06648308  0.9484506   1.14432447
  1.310299   -1.87354678  0.7296508   2.8857842  -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Wed Mar 27 20:24:45 2024]  Iteration number: 0 with current cost as 0.10077028894774623 and parameters 
[-3.01848564  2.40314933 -2.08842121 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.89495669  1.14432446
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Wed Mar 27 20:29:38 2024]  Iteration number: 0 with current cost as 0.10625381182772078 and parameters 
[-2.99517064  2.40484136 -2.07926075 -0.11653103  0.55388708 -2.77010896
  3.06858497  2.18960144  1.18551999 -1.06648308  0.97705377  1.14432446
  1.31029899 -1.87354678  0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1367.3712747097015 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Wed Mar 27 20:34:43 2024]  Iteration number: 0 with current cost as 0.11698850091636781 and parameters 
[-3.05376927  2.46927284 -2.04927282 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.92383606  1.14432445
  1.31029899 -1.87354678  0.7296508   2.88578418 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Wed Mar 27 20:38:44 2024]  Iteration number: 0 with current cost as 0.1197197862522623 and parameters 
[-3.02702905  2.47202593 -2.05506616 -0.11653103  0.55388708 -2.77010897
  3.06858496  2.18960145  1.18551998 -1.06648309  0.95519411  1.14432446
  1.31029899 -1.87354678  0.72965079  2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Wed Mar 27 20:43:50 2024]  Iteration number: 0 with current cost as 0.11494981341934012 and parameters 
[-3.05599894  2.48155719 -2.05685987 -0.11653103  0.55388708 -2.77010896
  3.06858498  2.18960146  1.18552    -1.06648308  0.9484506   1.14432447
  1.310299   -1.87354678  0.7296508   2.8857842  -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Wed Mar 27 20:47:31 2024]  Iteration number: 0 with current cost as 0.10077028894774623 and parameters 
[-3.01848564  2.40314933 -2.08842121 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.89495669  1.14432446
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Wed Mar 27 20:52:28 2024]  Iteration number: 0 with current cost as 0.10625381182772078 and parameters 
[-2.99517064  2.40484136 -2.07926075 -0.11653103  0.55388708 -2.77010896
  3.06858497  2.18960144  1.18551999 -1.06648308  0.97705377  1.14432446
  1.31029899 -1.87354678  0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1370.774745941162 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Wed Mar 27 20:57:33 2024]  Iteration number: 0 with current cost as 0.11698850091636781 and parameters 
[-3.05376927  2.46927284 -2.04927282 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.92383606  1.14432445
  1.31029899 -1.87354678  0.7296508   2.88578418 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Wed Mar 27 21:01:36 2024]  Iteration number: 0 with current cost as 0.1197197862522623 and parameters 
[-3.02702905  2.47202593 -2.05506616 -0.11653103  0.55388708 -2.77010897
  3.06858496  2.18960145  1.18551998 -1.06648309  0.95519411  1.14432446
  1.31029899 -1.87354678  0.72965079  2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Wed Mar 27 21:06:41 2024]  Iteration number: 0 with current cost as 0.11494981341934012 and parameters 
[-3.05599894  2.48155719 -2.05685987 -0.11653103  0.55388708 -2.77010896
  3.06858498  2.18960146  1.18552    -1.06648308  0.9484506   1.14432447
  1.310299   -1.87354678  0.7296508   2.8857842  -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Wed Mar 27 21:10:23 2024]  Iteration number: 0 with current cost as 0.10077028894774623 and parameters 
[-3.01848564  2.40314933 -2.08842121 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.89495669  1.14432446
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Wed Mar 27 21:15:20 2024]  Iteration number: 0 with current cost as 0.10625381182772078 and parameters 
[-2.99517064  2.40484136 -2.07926075 -0.11653103  0.55388708 -2.77010896
  3.06858497  2.18960144  1.18551999 -1.06648308  0.97705377  1.14432446
  1.31029899 -1.87354678  0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1375.9056735038757 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Wed Mar 27 21:20:30 2024]  Iteration number: 0 with current cost as 0.11698850091636781 and parameters 
[-3.05376927  2.46927284 -2.04927282 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.92383606  1.14432445
  1.31029899 -1.87354678  0.7296508   2.88578418 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Wed Mar 27 21:24:33 2024]  Iteration number: 0 with current cost as 0.1197197862522623 and parameters 
[-3.02702905  2.47202593 -2.05506616 -0.11653103  0.55388708 -2.77010897
  3.06858496  2.18960145  1.18551998 -1.06648309  0.95519411  1.14432446
  1.31029899 -1.87354678  0.72965079  2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Wed Mar 27 21:29:39 2024]  Iteration number: 0 with current cost as 0.11494981341934012 and parameters 
[-3.05599894  2.48155719 -2.05685987 -0.11653103  0.55388708 -2.77010896
  3.06858498  2.18960146  1.18552    -1.06648308  0.9484506   1.14432447
  1.310299   -1.87354678  0.7296508   2.8857842  -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Wed Mar 27 21:33:20 2024]  Iteration number: 0 with current cost as 0.10077028894774623 and parameters 
[-3.01848564  2.40314933 -2.08842121 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.89495669  1.14432446
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Wed Mar 27 21:38:18 2024]  Iteration number: 0 with current cost as 0.10625381182772078 and parameters 
[-2.99517064  2.40484136 -2.07926075 -0.11653103  0.55388708 -2.77010896
  3.06858497  2.18960144  1.18551999 -1.06648308  0.97705377  1.14432446
  1.31029899 -1.87354678  0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1377.3008441925049 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Wed Mar 27 21:43:27 2024]  Iteration number: 0 with current cost as 0.11698850091636781 and parameters 
[-3.05376927  2.46927284 -2.04927282 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.92383606  1.14432445
  1.31029899 -1.87354678  0.7296508   2.88578418 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Wed Mar 27 21:47:31 2024]  Iteration number: 0 with current cost as 0.1197197862522623 and parameters 
[-3.02702905  2.47202593 -2.05506616 -0.11653103  0.55388708 -2.77010897
  3.06858496  2.18960145  1.18551998 -1.06648309  0.95519411  1.14432446
  1.31029899 -1.87354678  0.72965079  2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Wed Mar 27 21:52:38 2024]  Iteration number: 0 with current cost as 0.11494981341934012 and parameters 
[-3.05599894  2.48155719 -2.05685987 -0.11653103  0.55388708 -2.77010896
  3.06858498  2.18960146  1.18552    -1.06648308  0.9484506   1.14432447
  1.310299   -1.87354678  0.7296508   2.8857842  -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Wed Mar 27 21:56:27 2024]  Iteration number: 0 with current cost as 0.10077028894774623 and parameters 
[-3.01848564  2.40314933 -2.08842121 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.89495669  1.14432446
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Wed Mar 27 22:01:22 2024]  Iteration number: 0 with current cost as 0.10625381182772078 and parameters 
[-2.99517064  2.40484136 -2.07926075 -0.11653103  0.55388708 -2.77010896
  3.06858497  2.18960144  1.18551999 -1.06648308  0.97705377  1.14432446
  1.31029899 -1.87354678  0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1383.2651052474976 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Wed Mar 27 22:06:30 2024]  Iteration number: 0 with current cost as 0.11698850091636781 and parameters 
[-3.05376927  2.46927284 -2.04927282 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.92383606  1.14432445
  1.31029899 -1.87354678  0.7296508   2.88578418 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Wed Mar 27 22:10:36 2024]  Iteration number: 0 with current cost as 0.1197197862522623 and parameters 
[-3.02702905  2.47202593 -2.05506616 -0.11653103  0.55388708 -2.77010897
  3.06858496  2.18960145  1.18551998 -1.06648309  0.95519411  1.14432446
  1.31029899 -1.87354678  0.72965079  2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Wed Mar 27 22:15:44 2024]  Iteration number: 0 with current cost as 0.11494981341934012 and parameters 
[-3.05599894  2.48155719 -2.05685987 -0.11653103  0.55388708 -2.77010896
  3.06858498  2.18960146  1.18552    -1.06648308  0.9484506   1.14432447
  1.310299   -1.87354678  0.7296508   2.8857842  -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Wed Mar 27 22:19:26 2024]  Iteration number: 0 with current cost as 0.10077028894774623 and parameters 
[-3.01848564  2.40314933 -2.08842121 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.89495669  1.14432446
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Wed Mar 27 22:24:23 2024]  Iteration number: 0 with current cost as 0.10625381182772078 and parameters 
[-2.99517064  2.40484136 -2.07926075 -0.11653103  0.55388708 -2.77010896
  3.06858497  2.18960144  1.18551999 -1.06648308  0.97705377  1.14432446
  1.31029899 -1.87354678  0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1377.8623442649841 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Wed Mar 27 22:29:28 2024]  Iteration number: 0 with current cost as 0.11698850091636781 and parameters 
[-3.05376927  2.46927284 -2.04927282 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.92383606  1.14432445
  1.31029899 -1.87354678  0.7296508   2.88578418 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Wed Mar 27 22:33:33 2024]  Iteration number: 0 with current cost as 0.1197197862522623 and parameters 
[-3.02702905  2.47202593 -2.05506616 -0.11653103  0.55388708 -2.77010897
  3.06858496  2.18960145  1.18551998 -1.06648309  0.95519411  1.14432446
  1.31029899 -1.87354678  0.72965079  2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Wed Mar 27 22:38:42 2024]  Iteration number: 0 with current cost as 0.11494981341934012 and parameters 
[-3.05599894  2.48155719 -2.05685987 -0.11653103  0.55388708 -2.77010896
  3.06858498  2.18960146  1.18552    -1.06648308  0.9484506   1.14432447
  1.310299   -1.87354678  0.7296508   2.8857842  -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Wed Mar 27 22:42:25 2024]  Iteration number: 0 with current cost as 0.10077028894774623 and parameters 
[-3.01848564  2.40314933 -2.08842121 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.89495669  1.14432446
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Wed Mar 27 22:47:23 2024]  Iteration number: 0 with current cost as 0.10625381182772078 and parameters 
[-2.99517064  2.40484136 -2.07926075 -0.11653103  0.55388708 -2.77010896
  3.06858497  2.18960144  1.18551999 -1.06648308  0.97705377  1.14432446
  1.31029899 -1.87354678  0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1383.3952479362488 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Wed Mar 27 22:52:32 2024]  Iteration number: 0 with current cost as 0.11698850091636781 and parameters 
[-3.05376927  2.46927284 -2.04927282 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.92383606  1.14432445
  1.31029899 -1.87354678  0.7296508   2.88578418 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Wed Mar 27 22:56:35 2024]  Iteration number: 0 with current cost as 0.1197197862522623 and parameters 
[-3.02702905  2.47202593 -2.05506616 -0.11653103  0.55388708 -2.77010897
  3.06858496  2.18960145  1.18551998 -1.06648309  0.95519411  1.14432446
  1.31029899 -1.87354678  0.72965079  2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Wed Mar 27 23:01:42 2024]  Iteration number: 0 with current cost as 0.11494981341934012 and parameters 
[-3.05599894  2.48155719 -2.05685987 -0.11653103  0.55388708 -2.77010896
  3.06858498  2.18960146  1.18552    -1.06648308  0.9484506   1.14432447
  1.310299   -1.87354678  0.7296508   2.8857842  -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Wed Mar 27 23:05:25 2024]  Iteration number: 0 with current cost as 0.10077028894774623 and parameters 
[-3.01848564  2.40314933 -2.08842121 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.89495669  1.14432446
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Wed Mar 27 23:10:23 2024]  Iteration number: 0 with current cost as 0.10625381182772078 and parameters 
[-2.99517064  2.40484136 -2.07926075 -0.11653103  0.55388708 -2.77010896
  3.06858497  2.18960144  1.18551999 -1.06648308  0.97705377  1.14432446
  1.31029899 -1.87354678  0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1379.655011177063 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Wed Mar 27 23:15:31 2024]  Iteration number: 0 with current cost as 0.11698850091636781 and parameters 
[-3.05376927  2.46927284 -2.04927282 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.92383606  1.14432445
  1.31029899 -1.87354678  0.7296508   2.88578418 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Wed Mar 27 23:19:36 2024]  Iteration number: 0 with current cost as 0.1197197862522623 and parameters 
[-3.02702905  2.47202593 -2.05506616 -0.11653103  0.55388708 -2.77010897
  3.06858496  2.18960145  1.18551998 -1.06648309  0.95519411  1.14432446
  1.31029899 -1.87354678  0.72965079  2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Wed Mar 27 23:24:41 2024]  Iteration number: 0 with current cost as 0.11494981341934012 and parameters 
[-3.05599894  2.48155719 -2.05685987 -0.11653103  0.55388708 -2.77010896
  3.06858498  2.18960146  1.18552    -1.06648308  0.9484506   1.14432447
  1.310299   -1.87354678  0.7296508   2.8857842  -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Wed Mar 27 23:28:25 2024]  Iteration number: 0 with current cost as 0.10077028894774623 and parameters 
[-3.01848564  2.40314933 -2.08842121 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.89495669  1.14432446
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Wed Mar 27 23:33:23 2024]  Iteration number: 0 with current cost as 0.10625381182772078 and parameters 
[-2.99517064  2.40484136 -2.07926075 -0.11653103  0.55388708 -2.77010896
  3.06858497  2.18960144  1.18551999 -1.06648308  0.97705377  1.14432446
  1.31029899 -1.87354678  0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1380.8325202465057 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Wed Mar 27 23:38:32 2024]  Iteration number: 0 with current cost as 0.11698850091636781 and parameters 
[-3.05376927  2.46927284 -2.04927282 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.92383606  1.14432445
  1.31029899 -1.87354678  0.7296508   2.88578418 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Wed Mar 27 23:42:37 2024]  Iteration number: 0 with current cost as 0.1197197862522623 and parameters 
[-3.02702905  2.47202593 -2.05506616 -0.11653103  0.55388708 -2.77010897
  3.06858496  2.18960145  1.18551998 -1.06648309  0.95519411  1.14432446
  1.31029899 -1.87354678  0.72965079  2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Wed Mar 27 23:47:46 2024]  Iteration number: 0 with current cost as 0.11494981341934012 and parameters 
[-3.05599894  2.48155719 -2.05685987 -0.11653103  0.55388708 -2.77010896
  3.06858498  2.18960146  1.18552    -1.06648308  0.9484506   1.14432447
  1.310299   -1.87354678  0.7296508   2.8857842  -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Wed Mar 27 23:51:29 2024]  Iteration number: 0 with current cost as 0.10077028894774623 and parameters 
[-3.01848564  2.40314933 -2.08842121 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.89495669  1.14432446
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Wed Mar 27 23:56:32 2024]  Iteration number: 0 with current cost as 0.10625381182772078 and parameters 
[-2.99517064  2.40484136 -2.07926075 -0.11653103  0.55388708 -2.77010896
  3.06858497  2.18960144  1.18551999 -1.06648308  0.97705377  1.14432446
  1.31029899 -1.87354678  0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1390.5146720409393 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Mar 28 00:01:43 2024]  Iteration number: 0 with current cost as 0.11698850091636781 and parameters 
[-3.05376927  2.46927284 -2.04927282 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.92383606  1.14432445
  1.31029899 -1.87354678  0.7296508   2.88578418 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Mar 28 00:05:48 2024]  Iteration number: 0 with current cost as 0.1197197862522623 and parameters 
[-3.02702905  2.47202593 -2.05506616 -0.11653103  0.55388708 -2.77010897
  3.06858496  2.18960145  1.18551998 -1.06648309  0.95519411  1.14432446
  1.31029899 -1.87354678  0.72965079  2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Mar 28 00:10:57 2024]  Iteration number: 0 with current cost as 0.11494981341934012 and parameters 
[-3.05599894  2.48155719 -2.05685987 -0.11653103  0.55388708 -2.77010896
  3.06858498  2.18960146  1.18552    -1.06648308  0.9484506   1.14432447
  1.310299   -1.87354678  0.7296508   2.8857842  -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Mar 28 00:14:39 2024]  Iteration number: 0 with current cost as 0.10077028894774623 and parameters 
[-3.01848564  2.40314933 -2.08842121 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.89495669  1.14432446
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Mar 28 00:19:36 2024]  Iteration number: 0 with current cost as 0.10625381182772078 and parameters 
[-2.99517064  2.40484136 -2.07926075 -0.11653103  0.55388708 -2.77010896
  3.06858497  2.18960144  1.18551999 -1.06648308  0.97705377  1.14432446
  1.31029899 -1.87354678  0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1380.9258394241333 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Mar 28 00:24:44 2024]  Iteration number: 0 with current cost as 0.11698850091636781 and parameters 
[-3.05376927  2.46927284 -2.04927282 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.92383606  1.14432445
  1.31029899 -1.87354678  0.7296508   2.88578418 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Mar 28 00:28:49 2024]  Iteration number: 0 with current cost as 0.1197197862522623 and parameters 
[-3.02702905  2.47202593 -2.05506616 -0.11653103  0.55388708 -2.77010897
  3.06858496  2.18960145  1.18551998 -1.06648309  0.95519411  1.14432446
  1.31029899 -1.87354678  0.72965079  2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Mar 28 00:33:59 2024]  Iteration number: 0 with current cost as 0.11494981341934012 and parameters 
[-3.05599894  2.48155719 -2.05685987 -0.11653103  0.55388708 -2.77010896
  3.06858498  2.18960146  1.18552    -1.06648308  0.9484506   1.14432447
  1.310299   -1.87354678  0.7296508   2.8857842  -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Mar 28 00:37:44 2024]  Iteration number: 0 with current cost as 0.10077028894774623 and parameters 
[-3.01848564  2.40314933 -2.08842121 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.89495669  1.14432446
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Mar 28 00:42:40 2024]  Iteration number: 0 with current cost as 0.10625381182772078 and parameters 
[-2.99517064  2.40484136 -2.07926075 -0.11653103  0.55388708 -2.77010896
  3.06858497  2.18960144  1.18551999 -1.06648308  0.97705377  1.14432446
  1.31029899 -1.87354678  0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1384.8071703910828 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Mar 28 00:47:49 2024]  Iteration number: 0 with current cost as 0.11698850091636781 and parameters 
[-3.05376927  2.46927284 -2.04927282 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.92383606  1.14432445
  1.31029899 -1.87354678  0.7296508   2.88578418 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Mar 28 00:51:54 2024]  Iteration number: 0 with current cost as 0.1197197862522623 and parameters 
[-3.02702905  2.47202593 -2.05506616 -0.11653103  0.55388708 -2.77010897
  3.06858496  2.18960145  1.18551998 -1.06648309  0.95519411  1.14432446
  1.31029899 -1.87354678  0.72965079  2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Mar 28 00:57:04 2024]  Iteration number: 0 with current cost as 0.11494981341934012 and parameters 
[-3.05599894  2.48155719 -2.05685987 -0.11653103  0.55388708 -2.77010896
  3.06858498  2.18960146  1.18552    -1.06648308  0.9484506   1.14432447
  1.310299   -1.87354678  0.7296508   2.8857842  -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Mar 28 01:00:48 2024]  Iteration number: 0 with current cost as 0.10077028894774623 and parameters 
[-3.01848564  2.40314933 -2.08842121 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.89495669  1.14432446
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Mar 28 01:05:48 2024]  Iteration number: 0 with current cost as 0.10625381182772078 and parameters 
[-2.99517064  2.40484136 -2.07926075 -0.11653103  0.55388708 -2.77010896
  3.06858497  2.18960144  1.18551999 -1.06648308  0.97705377  1.14432446
  1.31029899 -1.87354678  0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1387.8794021606445 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Mar 28 01:10:56 2024]  Iteration number: 0 with current cost as 0.11698850091636781 and parameters 
[-3.05376927  2.46927284 -2.04927282 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.92383606  1.14432445
  1.31029899 -1.87354678  0.7296508   2.88578418 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Mar 28 01:15:03 2024]  Iteration number: 0 with current cost as 0.1197197862522623 and parameters 
[-3.02702905  2.47202593 -2.05506616 -0.11653103  0.55388708 -2.77010897
  3.06858496  2.18960145  1.18551998 -1.06648309  0.95519411  1.14432446
  1.31029899 -1.87354678  0.72965079  2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Mar 28 01:20:11 2024]  Iteration number: 0 with current cost as 0.11494981341934012 and parameters 
[-3.05599894  2.48155719 -2.05685987 -0.11653103  0.55388708 -2.77010896
  3.06858498  2.18960146  1.18552    -1.06648308  0.9484506   1.14432447
  1.310299   -1.87354678  0.7296508   2.8857842  -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Mar 28 01:23:56 2024]  Iteration number: 0 with current cost as 0.10077028894774623 and parameters 
[-3.01848564  2.40314933 -2.08842121 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.89495669  1.14432446
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Mar 28 01:28:53 2024]  Iteration number: 0 with current cost as 0.10625381182772078 and parameters 
[-2.99517064  2.40484136 -2.07926075 -0.11653103  0.55388708 -2.77010896
  3.06858497  2.18960144  1.18551999 -1.06648308  0.97705377  1.14432446
  1.31029899 -1.87354678  0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1385.4382421970367 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Mar 28 01:34:01 2024]  Iteration number: 0 with current cost as 0.11698850091636781 and parameters 
[-3.05376927  2.46927284 -2.04927282 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.92383606  1.14432445
  1.31029899 -1.87354678  0.7296508   2.88578418 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Mar 28 01:38:06 2024]  Iteration number: 0 with current cost as 0.1197197862522623 and parameters 
[-3.02702905  2.47202593 -2.05506616 -0.11653103  0.55388708 -2.77010897
  3.06858496  2.18960145  1.18551998 -1.06648309  0.95519411  1.14432446
  1.31029899 -1.87354678  0.72965079  2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Mar 28 01:43:13 2024]  Iteration number: 0 with current cost as 0.11494981341934012 and parameters 
[-3.05599894  2.48155719 -2.05685987 -0.11653103  0.55388708 -2.77010896
  3.06858498  2.18960146  1.18552    -1.06648308  0.9484506   1.14432447
  1.310299   -1.87354678  0.7296508   2.8857842  -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Mar 28 01:46:56 2024]  Iteration number: 0 with current cost as 0.10077028894774623 and parameters 
[-3.01848564  2.40314933 -2.08842121 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.89495669  1.14432446
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Mar 28 01:51:56 2024]  Iteration number: 0 with current cost as 0.10625381182772078 and parameters 
[-2.99517064  2.40484136 -2.07926075 -0.11653103  0.55388708 -2.77010896
  3.06858497  2.18960144  1.18551999 -1.06648308  0.97705377  1.14432446
  1.31029899 -1.87354678  0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1387.508980512619 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Mar 28 01:57:09 2024]  Iteration number: 0 with current cost as 0.11698850091636781 and parameters 
[-3.05376927  2.46927284 -2.04927282 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.92383606  1.14432445
  1.31029899 -1.87354678  0.7296508   2.88578418 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Mar 28 02:01:14 2024]  Iteration number: 0 with current cost as 0.1197197862522623 and parameters 
[-3.02702905  2.47202593 -2.05506616 -0.11653103  0.55388708 -2.77010897
  3.06858496  2.18960145  1.18551998 -1.06648309  0.95519411  1.14432446
  1.31029899 -1.87354678  0.72965079  2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Mar 28 02:06:22 2024]  Iteration number: 0 with current cost as 0.11494981341934012 and parameters 
[-3.05599894  2.48155719 -2.05685987 -0.11653103  0.55388708 -2.77010896
  3.06858498  2.18960146  1.18552    -1.06648308  0.9484506   1.14432447
  1.310299   -1.87354678  0.7296508   2.8857842  -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Mar 28 02:10:05 2024]  Iteration number: 0 with current cost as 0.10077028894774623 and parameters 
[-3.01848564  2.40314933 -2.08842121 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.89495669  1.14432446
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Mar 28 02:15:03 2024]  Iteration number: 0 with current cost as 0.10625381182772078 and parameters 
[-2.99517064  2.40484136 -2.07926075 -0.11653103  0.55388708 -2.77010896
  3.06858497  2.18960144  1.18551999 -1.06648308  0.97705377  1.14432446
  1.31029899 -1.87354678  0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1383.0868141651154 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Mar 28 02:20:12 2024]  Iteration number: 0 with current cost as 0.11698850091636781 and parameters 
[-3.05376927  2.46927284 -2.04927282 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.92383606  1.14432445
  1.31029899 -1.87354678  0.7296508   2.88578418 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Mar 28 02:24:17 2024]  Iteration number: 0 with current cost as 0.1197197862522623 and parameters 
[-3.02702905  2.47202593 -2.05506616 -0.11653103  0.55388708 -2.77010897
  3.06858496  2.18960145  1.18551998 -1.06648309  0.95519411  1.14432446
  1.31029899 -1.87354678  0.72965079  2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Mar 28 02:29:26 2024]  Iteration number: 0 with current cost as 0.11494981341934012 and parameters 
[-3.05599894  2.48155719 -2.05685987 -0.11653103  0.55388708 -2.77010896
  3.06858498  2.18960146  1.18552    -1.06648308  0.9484506   1.14432447
  1.310299   -1.87354678  0.7296508   2.8857842  -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Mar 28 02:33:08 2024]  Iteration number: 0 with current cost as 0.10077028894774623 and parameters 
[-3.01848564  2.40314933 -2.08842121 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.89495669  1.14432446
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Mar 28 02:38:03 2024]  Iteration number: 0 with current cost as 0.10625381182772078 and parameters 
[-2.99517064  2.40484136 -2.07926075 -0.11653103  0.55388708 -2.77010896
  3.06858497  2.18960144  1.18551999 -1.06648308  0.97705377  1.14432446
  1.31029899 -1.87354678  0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1379.2196443080902 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Mar 28 02:43:12 2024]  Iteration number: 0 with current cost as 0.11698850091636781 and parameters 
[-3.05376927  2.46927284 -2.04927282 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.92383606  1.14432445
  1.31029899 -1.87354678  0.7296508   2.88578418 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Mar 28 02:47:19 2024]  Iteration number: 0 with current cost as 0.1197197862522623 and parameters 
[-3.02702905  2.47202593 -2.05506616 -0.11653103  0.55388708 -2.77010897
  3.06858496  2.18960145  1.18551998 -1.06648309  0.95519411  1.14432446
  1.31029899 -1.87354678  0.72965079  2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Mar 28 02:52:28 2024]  Iteration number: 0 with current cost as 0.11494981341934012 and parameters 
[-3.05599894  2.48155719 -2.05685987 -0.11653103  0.55388708 -2.77010896
  3.06858498  2.18960146  1.18552    -1.06648308  0.9484506   1.14432447
  1.310299   -1.87354678  0.7296508   2.8857842  -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Mar 28 02:56:11 2024]  Iteration number: 0 with current cost as 0.10077028894774623 and parameters 
[-3.01848564  2.40314933 -2.08842121 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.89495669  1.14432446
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Mar 28 03:01:10 2024]  Iteration number: 0 with current cost as 0.10625381182772078 and parameters 
[-2.99517064  2.40484136 -2.07926075 -0.11653103  0.55388708 -2.77010896
  3.06858497  2.18960144  1.18551999 -1.06648308  0.97705377  1.14432446
  1.31029899 -1.87354678  0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1384.5041279792786 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Mar 28 03:06:16 2024]  Iteration number: 0 with current cost as 0.11698850091636781 and parameters 
[-3.05376927  2.46927284 -2.04927282 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.92383606  1.14432445
  1.31029899 -1.87354678  0.7296508   2.88578418 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Mar 28 03:10:23 2024]  Iteration number: 0 with current cost as 0.1197197862522623 and parameters 
[-3.02702905  2.47202593 -2.05506616 -0.11653103  0.55388708 -2.77010897
  3.06858496  2.18960145  1.18551998 -1.06648309  0.95519411  1.14432446
  1.31029899 -1.87354678  0.72965079  2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Mar 28 03:15:33 2024]  Iteration number: 0 with current cost as 0.11494981341934012 and parameters 
[-3.05599894  2.48155719 -2.05685987 -0.11653103  0.55388708 -2.77010896
  3.06858498  2.18960146  1.18552    -1.06648308  0.9484506   1.14432447
  1.310299   -1.87354678  0.7296508   2.8857842  -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Mar 28 03:19:17 2024]  Iteration number: 0 with current cost as 0.10077028894774623 and parameters 
[-3.01848564  2.40314933 -2.08842121 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.89495669  1.14432446
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Mar 28 03:24:15 2024]  Iteration number: 0 with current cost as 0.10625381182772078 and parameters 
[-2.99517064  2.40484136 -2.07926075 -0.11653103  0.55388708 -2.77010896
  3.06858497  2.18960144  1.18551999 -1.06648308  0.97705377  1.14432446
  1.31029899 -1.87354678  0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1386.486525774002 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Mar 28 03:29:22 2024]  Iteration number: 0 with current cost as 0.11698850091636781 and parameters 
[-3.05376927  2.46927284 -2.04927282 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.92383606  1.14432445
  1.31029899 -1.87354678  0.7296508   2.88578418 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Mar 28 03:33:25 2024]  Iteration number: 0 with current cost as 0.1197197862522623 and parameters 
[-3.02702905  2.47202593 -2.05506616 -0.11653103  0.55388708 -2.77010897
  3.06858496  2.18960145  1.18551998 -1.06648309  0.95519411  1.14432446
  1.31029899 -1.87354678  0.72965079  2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Mar 28 03:38:33 2024]  Iteration number: 0 with current cost as 0.11494981341934012 and parameters 
[-3.05599894  2.48155719 -2.05685987 -0.11653103  0.55388708 -2.77010896
  3.06858498  2.18960146  1.18552    -1.06648308  0.9484506   1.14432447
  1.310299   -1.87354678  0.7296508   2.8857842  -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Mar 28 03:42:17 2024]  Iteration number: 0 with current cost as 0.10077028894774623 and parameters 
[-3.01848564  2.40314933 -2.08842121 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.89495669  1.14432446
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Mar 28 03:47:16 2024]  Iteration number: 0 with current cost as 0.10625381182772078 and parameters 
[-2.99517064  2.40484136 -2.07926075 -0.11653103  0.55388708 -2.77010896
  3.06858497  2.18960144  1.18551999 -1.06648308  0.97705377  1.14432446
  1.31029899 -1.87354678  0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1382.3183431625366 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Mar 28 03:52:25 2024]  Iteration number: 0 with current cost as 0.11698850091636781 and parameters 
[-3.05376927  2.46927284 -2.04927282 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.92383606  1.14432445
  1.31029899 -1.87354678  0.7296508   2.88578418 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Mar 28 03:56:33 2024]  Iteration number: 0 with current cost as 0.1197197862522623 and parameters 
[-3.02702905  2.47202593 -2.05506616 -0.11653103  0.55388708 -2.77010897
  3.06858496  2.18960145  1.18551998 -1.06648309  0.95519411  1.14432446
  1.31029899 -1.87354678  0.72965079  2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Mar 28 04:01:39 2024]  Iteration number: 0 with current cost as 0.11494981341934012 and parameters 
[-3.05599894  2.48155719 -2.05685987 -0.11653103  0.55388708 -2.77010896
  3.06858498  2.18960146  1.18552    -1.06648308  0.9484506   1.14432447
  1.310299   -1.87354678  0.7296508   2.8857842  -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Mar 28 04:05:24 2024]  Iteration number: 0 with current cost as 0.10077028894774623 and parameters 
[-3.01848564  2.40314933 -2.08842121 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.89495669  1.14432446
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Mar 28 04:10:22 2024]  Iteration number: 0 with current cost as 0.10625381182772078 and parameters 
[-2.99517064  2.40484136 -2.07926075 -0.11653103  0.55388708 -2.77010896
  3.06858497  2.18960144  1.18551999 -1.06648308  0.97705377  1.14432446
  1.31029899 -1.87354678  0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1385.9895157814026 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Thu Mar 28 04:15:31 2024]  Iteration number: 0 with current cost as 0.11698850091636781 and parameters 
[-3.05376927  2.46927284 -2.04927282 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.92383606  1.14432445
  1.31029899 -1.87354678  0.7296508   2.88578418 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Mar 28 04:19:36 2024]  Iteration number: 0 with current cost as 0.1197197862522623 and parameters 
[-3.02702905  2.47202593 -2.05506616 -0.11653103  0.55388708 -2.77010897
  3.06858496  2.18960145  1.18551998 -1.06648309  0.95519411  1.14432446
  1.31029899 -1.87354678  0.72965079  2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Mar 28 04:24:43 2024]  Iteration number: 0 with current cost as 0.11494981341934012 and parameters 
[-3.05599894  2.48155719 -2.05685987 -0.11653103  0.55388708 -2.77010896
  3.06858498  2.18960146  1.18552    -1.06648308  0.9484506   1.14432447
  1.310299   -1.87354678  0.7296508   2.8857842  -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Mar 28 04:28:25 2024]  Iteration number: 0 with current cost as 0.10077028894774623 and parameters 
[-3.01848564  2.40314933 -2.08842121 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.89495669  1.14432446
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Mar 28 04:33:24 2024]  Iteration number: 0 with current cost as 0.10625381182772078 and parameters 
[-2.99517064  2.40484136 -2.07926075 -0.11653103  0.55388708 -2.77010896
  3.06858497  2.18960144  1.18551999 -1.06648308  0.97705377  1.14432446
  1.31029899 -1.87354678  0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1384.3425693511963 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Thu Mar 28 04:38:35 2024]  Iteration number: 0 with current cost as 0.11698850091636781 and parameters 
[-3.05376927  2.46927284 -2.04927282 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.92383606  1.14432445
  1.31029899 -1.87354678  0.7296508   2.88578418 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Mar 28 04:42:42 2024]  Iteration number: 0 with current cost as 0.1197197862522623 and parameters 
[-3.02702905  2.47202593 -2.05506616 -0.11653103  0.55388708 -2.77010897
  3.06858496  2.18960145  1.18551998 -1.06648309  0.95519411  1.14432446
  1.31029899 -1.87354678  0.72965079  2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Mar 28 04:47:52 2024]  Iteration number: 0 with current cost as 0.11494981341934012 and parameters 
[-3.05599894  2.48155719 -2.05685987 -0.11653103  0.55388708 -2.77010896
  3.06858498  2.18960146  1.18552    -1.06648308  0.9484506   1.14432447
  1.310299   -1.87354678  0.7296508   2.8857842  -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Mar 28 04:51:36 2024]  Iteration number: 0 with current cost as 0.10077028894774623 and parameters 
[-3.01848564  2.40314933 -2.08842121 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.89495669  1.14432446
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Mar 28 04:56:32 2024]  Iteration number: 0 with current cost as 0.10625381182772078 and parameters 
[-2.99517064  2.40484136 -2.07926075 -0.11653103  0.55388708 -2.77010896
  3.06858497  2.18960144  1.18551999 -1.06648308  0.97705377  1.14432446
  1.31029899 -1.87354678  0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1385.423796415329 seconds. 
Discarding model... 

Training complete taking 34547.20571613312 total seconds. 
Now scoring model... 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Scoring complete taking 1.1754999160766602 seconds. 
Saved predicted values as M-M-CZ_Modified-Pauli-CRX_predicted_values.csv
Model scores: {'MSE_train': (1.065321969634424,), 'R2_train': 0.8537232740501851, 'MAE_train': 0.8499189343194699, 'MSE_test': 13.089138093491993, 'R2_test': -0.3048647925764294, 'MAE_test': 2.1992302012475076}. 
Saved model results as M-M-CZ_Modified-Pauli-CRX_results.json. 
