/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/circuits/Encoders.py:12: RuntimeWarning: invalid value encountered in arcsin
  if np.isnan(np.arcsin(features[feature_idx])) or np.isnan(np.arccos(features[feature_idx])):
Loading dataset from /scratch/j/jacobsen/gjones/quad5qubits/quadratic_train.bin... 
Successfully loaded /scratch/j/jacobsen/gjones/quad5qubits/quadratic_train.bin into X and y data. 
Loading dataset from /scratch/j/jacobsen/gjones/quad5qubits/quadratic_test.bin... 
Successfully loaded /scratch/j/jacobsen/gjones/quad5qubits/quadratic_test.bin into X and y data. 
Training model with dataset /scratch/j/jacobsen/gjones/quad5qubits/quadratic_train.bin 
 at time Mon Apr  1 16:13:23 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Apr  1 16:16:12 2024]  Iteration number: 0 with current cost as 0.1803940773690416 and parameters 
[-3.77749035  2.23743464 -2.12427958 -0.11653108  0.55388702 -2.77010903
  3.06858493  2.18960134  1.18552004 -1.06648314  0.60271516  1.14432445
  1.31029899 -1.87354674]. 
Working on 0.4 fold... 
[Mon Apr  1 16:19:44 2024]  Iteration number: 0 with current cost as 0.20244007916364481 and parameters 
[-3.7957913   2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010939
  3.06858478  2.18960114  1.18552009 -1.06648339  0.6027151   1.14432445
  1.31029899 -1.8735467 ]. 
Working on 0.6 fold... 
[Mon Apr  1 16:23:59 2024]  Iteration number: 0 with current cost as 0.18506972219119625 and parameters 
[-3.74431282  2.2374347  -2.1242795  -0.11653103  0.55388708 -2.77010911
  3.06858498  2.18960145  1.18552012 -1.06648308  0.60271517  1.14432452
  1.31029912 -1.8735466 ]. 
Working on 0.8 fold... 
[Mon Apr  1 16:26:42 2024]  Iteration number: 0 with current cost as 0.17922244063925127 and parameters 
[-2.87751899  2.23743464 -2.12427956 -0.11653103  0.55388708 -2.77010905
  3.06858498  2.18960145  1.18552006 -1.06648308  0.6027151   1.14432445
  1.31029902 -1.87354668]. 
Working on 1.0 fold... 
[Mon Apr  1 16:29:23 2024]  Iteration number: 0 with current cost as 0.1975416204408943 and parameters 
[-2.81429673  2.23743464 -2.12427962 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960144  1.18551999 -1.06648309  0.60271511  1.14432446
  1.31029899 -1.87354677]. 
Training complete taking 1087.0988585948944 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Apr  1 16:34:13 2024]  Iteration number: 0 with current cost as 0.1803940773690416 and parameters 
[-3.77749035  2.23743464 -2.12427958 -0.11653108  0.55388702 -2.77010903
  3.06858493  2.18960134  1.18552004 -1.06648314  0.60271516  1.14432445
  1.31029899 -1.87354674]. 
Working on 0.4 fold... 
[Mon Apr  1 16:37:50 2024]  Iteration number: 0 with current cost as 0.20244007916364481 and parameters 
[-3.7957913   2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010939
  3.06858478  2.18960114  1.18552009 -1.06648339  0.6027151   1.14432445
  1.31029899 -1.8735467 ]. 
Working on 0.6 fold... 
[Mon Apr  1 16:42:32 2024]  Iteration number: 0 with current cost as 0.18506972219119625 and parameters 
[-3.74431282  2.2374347  -2.1242795  -0.11653103  0.55388708 -2.77010911
  3.06858498  2.18960145  1.18552012 -1.06648308  0.60271517  1.14432452
  1.31029912 -1.8735466 ]. 
Working on 0.8 fold... 
[Mon Apr  1 16:45:21 2024]  Iteration number: 0 with current cost as 0.17922244063925127 and parameters 
[-2.87751899  2.23743464 -2.12427956 -0.11653103  0.55388708 -2.77010905
  3.06858498  2.18960145  1.18552006 -1.06648308  0.6027151   1.14432445
  1.31029902 -1.87354668]. 
Working on 1.0 fold... 
[Mon Apr  1 16:48:05 2024]  Iteration number: 0 with current cost as 0.1975416204408943 and parameters 
[-2.81429673  2.23743464 -2.12427962 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960144  1.18551999 -1.06648309  0.60271511  1.14432446
  1.31029899 -1.87354677]. 
Training complete taking 1110.7976195812225 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Apr  1 16:52:43 2024]  Iteration number: 0 with current cost as 0.1803940773690416 and parameters 
[-3.77749035  2.23743464 -2.12427958 -0.11653108  0.55388702 -2.77010903
  3.06858493  2.18960134  1.18552004 -1.06648314  0.60271516  1.14432445
  1.31029899 -1.87354674]. 
Working on 0.4 fold... 
[Mon Apr  1 16:56:12 2024]  Iteration number: 0 with current cost as 0.20244007916364481 and parameters 
[-3.7957913   2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010939
  3.06858478  2.18960114  1.18552009 -1.06648339  0.6027151   1.14432445
  1.31029899 -1.8735467 ]. 
Working on 0.6 fold... 
[Mon Apr  1 17:00:31 2024]  Iteration number: 0 with current cost as 0.18506972219119625 and parameters 
[-3.74431282  2.2374347  -2.1242795  -0.11653103  0.55388708 -2.77010911
  3.06858498  2.18960145  1.18552012 -1.06648308  0.60271517  1.14432452
  1.31029912 -1.8735466 ]. 
Working on 0.8 fold... 
[Mon Apr  1 17:03:31 2024]  Iteration number: 0 with current cost as 0.17922244063925127 and parameters 
[-2.87751899  2.23743464 -2.12427956 -0.11653103  0.55388708 -2.77010905
  3.06858498  2.18960145  1.18552006 -1.06648308  0.6027151   1.14432445
  1.31029902 -1.87354668]. 
Working on 1.0 fold... 
[Mon Apr  1 17:06:13 2024]  Iteration number: 0 with current cost as 0.1975416204408943 and parameters 
[-2.81429673  2.23743464 -2.12427962 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960144  1.18551999 -1.06648309  0.60271511  1.14432446
  1.31029899 -1.87354677]. 
Training complete taking 1089.030107498169 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Mon Apr  1 17:10:58 2024]  Iteration number: 0 with current cost as 0.1803940773690416 and parameters 
[-3.77749035  2.23743464 -2.12427958 -0.11653108  0.55388702 -2.77010903
  3.06858493  2.18960134  1.18552004 -1.06648314  0.60271516  1.14432445
  1.31029899 -1.87354674]. 
Working on 0.4 fold... 
[Mon Apr  1 17:14:25 2024]  Iteration number: 0 with current cost as 0.20244007916364481 and parameters 
[-3.7957913   2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010939
  3.06858478  2.18960114  1.18552009 -1.06648339  0.6027151   1.14432445
  1.31029899 -1.8735467 ]. 
Working on 0.6 fold... 
[Mon Apr  1 17:18:52 2024]  Iteration number: 0 with current cost as 0.18506972219119625 and parameters 
[-3.74431282  2.2374347  -2.1242795  -0.11653103  0.55388708 -2.77010911
  3.06858498  2.18960145  1.18552012 -1.06648308  0.60271517  1.14432452
  1.31029912 -1.8735466 ]. 
Working on 0.8 fold... 
[Mon Apr  1 17:21:47 2024]  Iteration number: 0 with current cost as 0.17922244063925127 and parameters 
[-2.87751899  2.23743464 -2.12427956 -0.11653103  0.55388708 -2.77010905
  3.06858498  2.18960145  1.18552006 -1.06648308  0.6027151   1.14432445
  1.31029902 -1.87354668]. 
Working on 1.0 fold... 
[Mon Apr  1 17:24:51 2024]  Iteration number: 0 with current cost as 0.1975416204408943 and parameters 
[-2.81429673  2.23743464 -2.12427962 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960144  1.18551999 -1.06648309  0.60271511  1.14432446
  1.31029899 -1.87354677]. 
Training complete taking 1134.7803580760956 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Mon Apr  1 17:30:15 2024]  Iteration number: 0 with current cost as 0.1803940773690416 and parameters 
[-3.77749035  2.23743464 -2.12427958 -0.11653108  0.55388702 -2.77010903
  3.06858493  2.18960134  1.18552004 -1.06648314  0.60271516  1.14432445
  1.31029899 -1.87354674]. 
Working on 0.4 fold... 
[Mon Apr  1 17:34:14 2024]  Iteration number: 0 with current cost as 0.20244007916364481 and parameters 
[-3.7957913   2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010939
  3.06858478  2.18960114  1.18552009 -1.06648339  0.6027151   1.14432445
  1.31029899 -1.8735467 ]. 
Working on 0.6 fold... 
[Mon Apr  1 17:38:58 2024]  Iteration number: 0 with current cost as 0.18506972219119625 and parameters 
[-3.74431282  2.2374347  -2.1242795  -0.11653103  0.55388708 -2.77010911
  3.06858498  2.18960145  1.18552012 -1.06648308  0.60271517  1.14432452
  1.31029912 -1.8735466 ]. 
Working on 0.8 fold... 
[Mon Apr  1 17:41:53 2024]  Iteration number: 0 with current cost as 0.17922244063925127 and parameters 
[-2.87751899  2.23743464 -2.12427956 -0.11653103  0.55388708 -2.77010905
  3.06858498  2.18960145  1.18552006 -1.06648308  0.6027151   1.14432445
  1.31029902 -1.87354668]. 
Working on 1.0 fold... 
[Mon Apr  1 17:44:40 2024]  Iteration number: 0 with current cost as 0.1975416204408943 and parameters 
[-2.81429673  2.23743464 -2.12427962 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960144  1.18551999 -1.06648309  0.60271511  1.14432446
  1.31029899 -1.87354677]. 
Training complete taking 1188.5811347961426 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Apr  1 17:49:51 2024]  Iteration number: 0 with current cost as 0.1803940773690416 and parameters 
[-3.77749035  2.23743464 -2.12427958 -0.11653108  0.55388702 -2.77010903
  3.06858493  2.18960134  1.18552004 -1.06648314  0.60271516  1.14432445
  1.31029899 -1.87354674]. 
Working on 0.4 fold... 
[Mon Apr  1 17:53:40 2024]  Iteration number: 0 with current cost as 0.20244007916364481 and parameters 
[-3.7957913   2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010939
  3.06858478  2.18960114  1.18552009 -1.06648339  0.6027151   1.14432445
  1.31029899 -1.8735467 ]. 
Working on 0.6 fold... 
[Mon Apr  1 17:58:36 2024]  Iteration number: 0 with current cost as 0.18506972219119625 and parameters 
[-3.74431282  2.2374347  -2.1242795  -0.11653103  0.55388708 -2.77010911
  3.06858498  2.18960145  1.18552012 -1.06648308  0.60271517  1.14432452
  1.31029912 -1.8735466 ]. 
Working on 0.8 fold... 
[Mon Apr  1 18:01:37 2024]  Iteration number: 0 with current cost as 0.17922244063925127 and parameters 
[-2.87751899  2.23743464 -2.12427956 -0.11653103  0.55388708 -2.77010905
  3.06858498  2.18960145  1.18552006 -1.06648308  0.6027151   1.14432445
  1.31029902 -1.87354668]. 
Working on 1.0 fold... 
[Mon Apr  1 18:04:40 2024]  Iteration number: 0 with current cost as 0.1975416204408943 and parameters 
[-2.81429673  2.23743464 -2.12427962 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960144  1.18551999 -1.06648309  0.60271511  1.14432446
  1.31029899 -1.87354677]. 
Training complete taking 1197.8471479415894 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Apr  1 18:09:49 2024]  Iteration number: 0 with current cost as 0.1803940773690416 and parameters 
[-3.77749035  2.23743464 -2.12427958 -0.11653108  0.55388702 -2.77010903
  3.06858493  2.18960134  1.18552004 -1.06648314  0.60271516  1.14432445
  1.31029899 -1.87354674]. 
Working on 0.4 fold... 
[Mon Apr  1 18:13:29 2024]  Iteration number: 0 with current cost as 0.20244007916364481 and parameters 
[-3.7957913   2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010939
  3.06858478  2.18960114  1.18552009 -1.06648339  0.6027151   1.14432445
  1.31029899 -1.8735467 ]. 
Working on 0.6 fold... 
[Mon Apr  1 18:18:05 2024]  Iteration number: 0 with current cost as 0.18506972219119625 and parameters 
[-3.74431282  2.2374347  -2.1242795  -0.11653103  0.55388708 -2.77010911
  3.06858498  2.18960145  1.18552012 -1.06648308  0.60271517  1.14432452
  1.31029912 -1.8735466 ]. 
Working on 0.8 fold... 
[Mon Apr  1 18:21:09 2024]  Iteration number: 0 with current cost as 0.17922244063925127 and parameters 
[-2.87751899  2.23743464 -2.12427956 -0.11653103  0.55388708 -2.77010905
  3.06858498  2.18960145  1.18552006 -1.06648308  0.6027151   1.14432445
  1.31029902 -1.87354668]. 
Working on 1.0 fold... 
[Mon Apr  1 18:23:47 2024]  Iteration number: 0 with current cost as 0.1975416204408943 and parameters 
[-2.81429673  2.23743464 -2.12427962 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960144  1.18551999 -1.06648309  0.60271511  1.14432446
  1.31029899 -1.87354677]. 
Training complete taking 1129.993689775467 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Apr  1 18:28:34 2024]  Iteration number: 0 with current cost as 0.1803940773690416 and parameters 
[-3.77749035  2.23743464 -2.12427958 -0.11653108  0.55388702 -2.77010903
  3.06858493  2.18960134  1.18552004 -1.06648314  0.60271516  1.14432445
  1.31029899 -1.87354674]. 
Working on 0.4 fold... 
[Mon Apr  1 18:31:59 2024]  Iteration number: 0 with current cost as 0.20244007916364481 and parameters 
[-3.7957913   2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010939
  3.06858478  2.18960114  1.18552009 -1.06648339  0.6027151   1.14432445
  1.31029899 -1.8735467 ]. 
Working on 0.6 fold... 
[Mon Apr  1 18:36:11 2024]  Iteration number: 0 with current cost as 0.18506972219119625 and parameters 
[-3.74431282  2.2374347  -2.1242795  -0.11653103  0.55388708 -2.77010911
  3.06858498  2.18960145  1.18552012 -1.06648308  0.60271517  1.14432452
  1.31029912 -1.8735466 ]. 
Working on 0.8 fold... 
[Mon Apr  1 18:38:51 2024]  Iteration number: 0 with current cost as 0.17922244063925127 and parameters 
[-2.87751899  2.23743464 -2.12427956 -0.11653103  0.55388708 -2.77010905
  3.06858498  2.18960145  1.18552006 -1.06648308  0.6027151   1.14432445
  1.31029902 -1.87354668]. 
Working on 1.0 fold... 
[Mon Apr  1 18:41:34 2024]  Iteration number: 0 with current cost as 0.1975416204408943 and parameters 
[-2.81429673  2.23743464 -2.12427962 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960144  1.18551999 -1.06648309  0.60271511  1.14432446
  1.31029899 -1.87354677]. 
Training complete taking 1070.5744602680206 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Mon Apr  1 18:46:14 2024]  Iteration number: 0 with current cost as 0.1803940773690416 and parameters 
[-3.77749035  2.23743464 -2.12427958 -0.11653108  0.55388702 -2.77010903
  3.06858493  2.18960134  1.18552004 -1.06648314  0.60271516  1.14432445
  1.31029899 -1.87354674]. 
Working on 0.4 fold... 
[Mon Apr  1 18:49:40 2024]  Iteration number: 0 with current cost as 0.20244007916364481 and parameters 
[-3.7957913   2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010939
  3.06858478  2.18960114  1.18552009 -1.06648339  0.6027151   1.14432445
  1.31029899 -1.8735467 ]. 
Working on 0.6 fold... 
[Mon Apr  1 18:53:53 2024]  Iteration number: 0 with current cost as 0.18506972219119625 and parameters 
[-3.74431282  2.2374347  -2.1242795  -0.11653103  0.55388708 -2.77010911
  3.06858498  2.18960145  1.18552012 -1.06648308  0.60271517  1.14432452
  1.31029912 -1.8735466 ]. 
Working on 0.8 fold... 
[Mon Apr  1 18:56:33 2024]  Iteration number: 0 with current cost as 0.17922244063925127 and parameters 
[-2.87751899  2.23743464 -2.12427956 -0.11653103  0.55388708 -2.77010905
  3.06858498  2.18960145  1.18552006 -1.06648308  0.6027151   1.14432445
  1.31029902 -1.87354668]. 
Working on 1.0 fold... 
[Mon Apr  1 18:59:15 2024]  Iteration number: 0 with current cost as 0.1975416204408943 and parameters 
[-2.81429673  2.23743464 -2.12427962 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960144  1.18551999 -1.06648309  0.60271511  1.14432446
  1.31029899 -1.87354677]. 
Training complete taking 1058.2505736351013 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Mon Apr  1 19:03:50 2024]  Iteration number: 0 with current cost as 0.1803940773690416 and parameters 
[-3.77749035  2.23743464 -2.12427958 -0.11653108  0.55388702 -2.77010903
  3.06858493  2.18960134  1.18552004 -1.06648314  0.60271516  1.14432445
  1.31029899 -1.87354674]. 
Working on 0.4 fold... 
[Mon Apr  1 19:07:19 2024]  Iteration number: 0 with current cost as 0.20244007916364481 and parameters 
[-3.7957913   2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010939
  3.06858478  2.18960114  1.18552009 -1.06648339  0.6027151   1.14432445
  1.31029899 -1.8735467 ]. 
Working on 0.6 fold... 
[Mon Apr  1 19:11:28 2024]  Iteration number: 0 with current cost as 0.18506972219119625 and parameters 
[-3.74431282  2.2374347  -2.1242795  -0.11653103  0.55388708 -2.77010911
  3.06858498  2.18960145  1.18552012 -1.06648308  0.60271517  1.14432452
  1.31029912 -1.8735466 ]. 
Working on 0.8 fold... 
[Mon Apr  1 19:14:07 2024]  Iteration number: 0 with current cost as 0.17922244063925127 and parameters 
[-2.87751899  2.23743464 -2.12427956 -0.11653103  0.55388708 -2.77010905
  3.06858498  2.18960145  1.18552006 -1.06648308  0.6027151   1.14432445
  1.31029902 -1.87354668]. 
Working on 1.0 fold... 
[Mon Apr  1 19:16:49 2024]  Iteration number: 0 with current cost as 0.1975416204408943 and parameters 
[-2.81429673  2.23743464 -2.12427962 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960144  1.18551999 -1.06648309  0.60271511  1.14432446
  1.31029899 -1.87354677]. 
Training complete taking 1057.0140264034271 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Apr  1 19:21:23 2024]  Iteration number: 0 with current cost as 0.1803940773690416 and parameters 
[-3.77749035  2.23743464 -2.12427958 -0.11653108  0.55388702 -2.77010903
  3.06858493  2.18960134  1.18552004 -1.06648314  0.60271516  1.14432445
  1.31029899 -1.87354674]. 
Working on 0.4 fold... 
[Mon Apr  1 19:24:46 2024]  Iteration number: 0 with current cost as 0.20244007916364481 and parameters 
[-3.7957913   2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010939
  3.06858478  2.18960114  1.18552009 -1.06648339  0.6027151   1.14432445
  1.31029899 -1.8735467 ]. 
Working on 0.6 fold... 
[Mon Apr  1 19:28:59 2024]  Iteration number: 0 with current cost as 0.18506972219119625 and parameters 
[-3.74431282  2.2374347  -2.1242795  -0.11653103  0.55388708 -2.77010911
  3.06858498  2.18960145  1.18552012 -1.06648308  0.60271517  1.14432452
  1.31029912 -1.8735466 ]. 
Working on 0.8 fold... 
[Mon Apr  1 19:31:39 2024]  Iteration number: 0 with current cost as 0.17922244063925127 and parameters 
[-2.87751899  2.23743464 -2.12427956 -0.11653103  0.55388708 -2.77010905
  3.06858498  2.18960145  1.18552006 -1.06648308  0.6027151   1.14432445
  1.31029902 -1.87354668]. 
Working on 1.0 fold... 
[Mon Apr  1 19:34:19 2024]  Iteration number: 0 with current cost as 0.1975416204408943 and parameters 
[-2.81429673  2.23743464 -2.12427962 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960144  1.18551999 -1.06648309  0.60271511  1.14432446
  1.31029899 -1.87354677]. 
Training complete taking 1050.5139212608337 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Apr  1 19:39:04 2024]  Iteration number: 0 with current cost as 0.1803940773690416 and parameters 
[-3.77749035  2.23743464 -2.12427958 -0.11653108  0.55388702 -2.77010903
  3.06858493  2.18960134  1.18552004 -1.06648314  0.60271516  1.14432445
  1.31029899 -1.87354674]. 
Working on 0.4 fold... 
[Mon Apr  1 19:42:30 2024]  Iteration number: 0 with current cost as 0.20244007916364481 and parameters 
[-3.7957913   2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010939
  3.06858478  2.18960114  1.18552009 -1.06648339  0.6027151   1.14432445
  1.31029899 -1.8735467 ]. 
Working on 0.6 fold... 
[Mon Apr  1 19:46:41 2024]  Iteration number: 0 with current cost as 0.18506972219119625 and parameters 
[-3.74431282  2.2374347  -2.1242795  -0.11653103  0.55388708 -2.77010911
  3.06858498  2.18960145  1.18552012 -1.06648308  0.60271517  1.14432452
  1.31029912 -1.8735466 ]. 
Working on 0.8 fold... 
[Mon Apr  1 19:49:24 2024]  Iteration number: 0 with current cost as 0.17922244063925127 and parameters 
[-2.87751899  2.23743464 -2.12427956 -0.11653103  0.55388708 -2.77010905
  3.06858498  2.18960145  1.18552006 -1.06648308  0.6027151   1.14432445
  1.31029902 -1.87354668]. 
Working on 1.0 fold... 
[Mon Apr  1 19:52:11 2024]  Iteration number: 0 with current cost as 0.1975416204408943 and parameters 
[-2.81429673  2.23743464 -2.12427962 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960144  1.18551999 -1.06648309  0.60271511  1.14432446
  1.31029899 -1.87354677]. 
Training complete taking 1069.3928921222687 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Apr  1 19:56:59 2024]  Iteration number: 0 with current cost as 0.1803940773690416 and parameters 
[-3.77749035  2.23743464 -2.12427958 -0.11653108  0.55388702 -2.77010903
  3.06858493  2.18960134  1.18552004 -1.06648314  0.60271516  1.14432445
  1.31029899 -1.87354674]. 
Working on 0.4 fold... 
[Mon Apr  1 20:00:21 2024]  Iteration number: 0 with current cost as 0.20244007916364481 and parameters 
[-3.7957913   2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010939
  3.06858478  2.18960114  1.18552009 -1.06648339  0.6027151   1.14432445
  1.31029899 -1.8735467 ]. 
Working on 0.6 fold... 
[Mon Apr  1 20:04:39 2024]  Iteration number: 0 with current cost as 0.18506972219119625 and parameters 
[-3.74431282  2.2374347  -2.1242795  -0.11653103  0.55388708 -2.77010911
  3.06858498  2.18960145  1.18552012 -1.06648308  0.60271517  1.14432452
  1.31029912 -1.8735466 ]. 
Working on 0.8 fold... 
[Mon Apr  1 20:07:21 2024]  Iteration number: 0 with current cost as 0.17922244063925127 and parameters 
[-2.87751899  2.23743464 -2.12427956 -0.11653103  0.55388708 -2.77010905
  3.06858498  2.18960145  1.18552006 -1.06648308  0.6027151   1.14432445
  1.31029902 -1.87354668]. 
Working on 1.0 fold... 
[Mon Apr  1 20:10:02 2024]  Iteration number: 0 with current cost as 0.1975416204408943 and parameters 
[-2.81429673  2.23743464 -2.12427962 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960144  1.18551999 -1.06648309  0.60271511  1.14432446
  1.31029899 -1.87354677]. 
Training complete taking 1076.6529052257538 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Apr  1 20:14:50 2024]  Iteration number: 0 with current cost as 0.1803940773690416 and parameters 
[-3.77749035  2.23743464 -2.12427958 -0.11653108  0.55388702 -2.77010903
  3.06858493  2.18960134  1.18552004 -1.06648314  0.60271516  1.14432445
  1.31029899 -1.87354674]. 
Working on 0.4 fold... 
[Mon Apr  1 20:18:19 2024]  Iteration number: 0 with current cost as 0.20244007916364481 and parameters 
[-3.7957913   2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010939
  3.06858478  2.18960114  1.18552009 -1.06648339  0.6027151   1.14432445
  1.31029899 -1.8735467 ]. 
Working on 0.6 fold... 
[Mon Apr  1 20:22:35 2024]  Iteration number: 0 with current cost as 0.18506972219119625 and parameters 
[-3.74431282  2.2374347  -2.1242795  -0.11653103  0.55388708 -2.77010911
  3.06858498  2.18960145  1.18552012 -1.06648308  0.60271517  1.14432452
  1.31029912 -1.8735466 ]. 
Working on 0.8 fold... 
[Mon Apr  1 20:25:20 2024]  Iteration number: 0 with current cost as 0.17922244063925127 and parameters 
[-2.87751899  2.23743464 -2.12427956 -0.11653103  0.55388708 -2.77010905
  3.06858498  2.18960145  1.18552006 -1.06648308  0.6027151   1.14432445
  1.31029902 -1.87354668]. 
Working on 1.0 fold... 
[Mon Apr  1 20:28:04 2024]  Iteration number: 0 with current cost as 0.1975416204408943 and parameters 
[-2.81429673  2.23743464 -2.12427962 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960144  1.18551999 -1.06648309  0.60271511  1.14432446
  1.31029899 -1.87354677]. 
Training complete taking 1073.9231214523315 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Apr  1 20:32:43 2024]  Iteration number: 0 with current cost as 0.1803940773690416 and parameters 
[-3.77749035  2.23743464 -2.12427958 -0.11653108  0.55388702 -2.77010903
  3.06858493  2.18960134  1.18552004 -1.06648314  0.60271516  1.14432445
  1.31029899 -1.87354674]. 
Working on 0.4 fold... 
[Mon Apr  1 20:36:13 2024]  Iteration number: 0 with current cost as 0.20244007916364481 and parameters 
[-3.7957913   2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010939
  3.06858478  2.18960114  1.18552009 -1.06648339  0.6027151   1.14432445
  1.31029899 -1.8735467 ]. 
Working on 0.6 fold... 
[Mon Apr  1 20:40:26 2024]  Iteration number: 0 with current cost as 0.18506972219119625 and parameters 
[-3.74431282  2.2374347  -2.1242795  -0.11653103  0.55388708 -2.77010911
  3.06858498  2.18960145  1.18552012 -1.06648308  0.60271517  1.14432452
  1.31029912 -1.8735466 ]. 
Working on 0.8 fold... 
[Mon Apr  1 20:43:12 2024]  Iteration number: 0 with current cost as 0.17922244063925127 and parameters 
[-2.87751899  2.23743464 -2.12427956 -0.11653103  0.55388708 -2.77010905
  3.06858498  2.18960145  1.18552006 -1.06648308  0.6027151   1.14432445
  1.31029902 -1.87354668]. 
Working on 1.0 fold... 
[Mon Apr  1 20:45:51 2024]  Iteration number: 0 with current cost as 0.1975416204408943 and parameters 
[-2.81429673  2.23743464 -2.12427962 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960144  1.18551999 -1.06648309  0.60271511  1.14432446
  1.31029899 -1.87354677]. 
Training complete taking 1069.4349555969238 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Apr  1 20:50:38 2024]  Iteration number: 0 with current cost as 0.1803940773690416 and parameters 
[-3.77749035  2.23743464 -2.12427958 -0.11653108  0.55388702 -2.77010903
  3.06858493  2.18960134  1.18552004 -1.06648314  0.60271516  1.14432445
  1.31029899 -1.87354674]. 
Working on 0.4 fold... 
[Mon Apr  1 20:54:10 2024]  Iteration number: 0 with current cost as 0.20244007916364481 and parameters 
[-3.7957913   2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010939
  3.06858478  2.18960114  1.18552009 -1.06648339  0.6027151   1.14432445
  1.31029899 -1.8735467 ]. 
Working on 0.6 fold... 
[Mon Apr  1 20:58:20 2024]  Iteration number: 0 with current cost as 0.18506972219119625 and parameters 
[-3.74431282  2.2374347  -2.1242795  -0.11653103  0.55388708 -2.77010911
  3.06858498  2.18960145  1.18552012 -1.06648308  0.60271517  1.14432452
  1.31029912 -1.8735466 ]. 
Working on 0.8 fold... 
[Mon Apr  1 21:01:03 2024]  Iteration number: 0 with current cost as 0.17922244063925127 and parameters 
[-2.87751899  2.23743464 -2.12427956 -0.11653103  0.55388708 -2.77010905
  3.06858498  2.18960145  1.18552006 -1.06648308  0.6027151   1.14432445
  1.31029902 -1.87354668]. 
Working on 1.0 fold... 
[Mon Apr  1 21:03:43 2024]  Iteration number: 0 with current cost as 0.1975416204408943 and parameters 
[-2.81429673  2.23743464 -2.12427962 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960144  1.18551999 -1.06648309  0.60271511  1.14432446
  1.31029899 -1.87354677]. 
Training complete taking 1069.8447811603546 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Apr  1 21:08:14 2024]  Iteration number: 0 with current cost as 0.1803940773690416 and parameters 
[-3.77749035  2.23743464 -2.12427958 -0.11653108  0.55388702 -2.77010903
  3.06858493  2.18960134  1.18552004 -1.06648314  0.60271516  1.14432445
  1.31029899 -1.87354674]. 
Working on 0.4 fold... 
[Mon Apr  1 21:11:38 2024]  Iteration number: 0 with current cost as 0.20244007916364481 and parameters 
[-3.7957913   2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010939
  3.06858478  2.18960114  1.18552009 -1.06648339  0.6027151   1.14432445
  1.31029899 -1.8735467 ]. 
Working on 0.6 fold... 
[Mon Apr  1 21:16:01 2024]  Iteration number: 0 with current cost as 0.18506972219119625 and parameters 
[-3.74431282  2.2374347  -2.1242795  -0.11653103  0.55388708 -2.77010911
  3.06858498  2.18960145  1.18552012 -1.06648308  0.60271517  1.14432452
  1.31029912 -1.8735466 ]. 
Working on 0.8 fold... 
[Mon Apr  1 21:18:41 2024]  Iteration number: 0 with current cost as 0.17922244063925127 and parameters 
[-2.87751899  2.23743464 -2.12427956 -0.11653103  0.55388708 -2.77010905
  3.06858498  2.18960145  1.18552006 -1.06648308  0.6027151   1.14432445
  1.31029902 -1.87354668]. 
Working on 1.0 fold... 
[Mon Apr  1 21:21:27 2024]  Iteration number: 0 with current cost as 0.1975416204408943 and parameters 
[-2.81429673  2.23743464 -2.12427962 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960144  1.18551999 -1.06648309  0.60271511  1.14432446
  1.31029899 -1.87354677]. 
Training complete taking 1065.6732511520386 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Apr  1 21:26:15 2024]  Iteration number: 0 with current cost as 0.1803940773690416 and parameters 
[-3.77749035  2.23743464 -2.12427958 -0.11653108  0.55388702 -2.77010903
  3.06858493  2.18960134  1.18552004 -1.06648314  0.60271516  1.14432445
  1.31029899 -1.87354674]. 
Working on 0.4 fold... 
[Mon Apr  1 21:29:43 2024]  Iteration number: 0 with current cost as 0.20244007916364481 and parameters 
[-3.7957913   2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010939
  3.06858478  2.18960114  1.18552009 -1.06648339  0.6027151   1.14432445
  1.31029899 -1.8735467 ]. 
Working on 0.6 fold... 
[Mon Apr  1 21:33:56 2024]  Iteration number: 0 with current cost as 0.18506972219119625 and parameters 
[-3.74431282  2.2374347  -2.1242795  -0.11653103  0.55388708 -2.77010911
  3.06858498  2.18960145  1.18552012 -1.06648308  0.60271517  1.14432452
  1.31029912 -1.8735466 ]. 
Working on 0.8 fold... 
[Mon Apr  1 21:36:34 2024]  Iteration number: 0 with current cost as 0.17922244063925127 and parameters 
[-2.87751899  2.23743464 -2.12427956 -0.11653103  0.55388708 -2.77010905
  3.06858498  2.18960145  1.18552006 -1.06648308  0.6027151   1.14432445
  1.31029902 -1.87354668]. 
Working on 1.0 fold... 
[Mon Apr  1 21:39:26 2024]  Iteration number: 0 with current cost as 0.1975416204408943 and parameters 
[-2.81429673  2.23743464 -2.12427962 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960144  1.18551999 -1.06648309  0.60271511  1.14432446
  1.31029899 -1.87354677]. 
Training complete taking 1078.975389957428 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Apr  1 21:44:00 2024]  Iteration number: 0 with current cost as 0.1803940773690416 and parameters 
[-3.77749035  2.23743464 -2.12427958 -0.11653108  0.55388702 -2.77010903
  3.06858493  2.18960134  1.18552004 -1.06648314  0.60271516  1.14432445
  1.31029899 -1.87354674]. 
Working on 0.4 fold... 
[Mon Apr  1 21:47:24 2024]  Iteration number: 0 with current cost as 0.20244007916364481 and parameters 
[-3.7957913   2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010939
  3.06858478  2.18960114  1.18552009 -1.06648339  0.6027151   1.14432445
  1.31029899 -1.8735467 ]. 
Working on 0.6 fold... 
[Mon Apr  1 21:51:45 2024]  Iteration number: 0 with current cost as 0.18506972219119625 and parameters 
[-3.74431282  2.2374347  -2.1242795  -0.11653103  0.55388708 -2.77010911
  3.06858498  2.18960145  1.18552012 -1.06648308  0.60271517  1.14432452
  1.31029912 -1.8735466 ]. 
Working on 0.8 fold... 
[Mon Apr  1 21:54:32 2024]  Iteration number: 0 with current cost as 0.17922244063925127 and parameters 
[-2.87751899  2.23743464 -2.12427956 -0.11653103  0.55388708 -2.77010905
  3.06858498  2.18960145  1.18552006 -1.06648308  0.6027151   1.14432445
  1.31029902 -1.87354668]. 
Working on 1.0 fold... 
[Mon Apr  1 21:57:13 2024]  Iteration number: 0 with current cost as 0.1975416204408943 and parameters 
[-2.81429673  2.23743464 -2.12427962 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960144  1.18551999 -1.06648309  0.60271511  1.14432446
  1.31029899 -1.87354677]. 
Training complete taking 1068.786591053009 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Apr  1 22:01:51 2024]  Iteration number: 0 with current cost as 0.1803940773690416 and parameters 
[-3.77749035  2.23743464 -2.12427958 -0.11653108  0.55388702 -2.77010903
  3.06858493  2.18960134  1.18552004 -1.06648314  0.60271516  1.14432445
  1.31029899 -1.87354674]. 
Working on 0.4 fold... 
[Mon Apr  1 22:05:17 2024]  Iteration number: 0 with current cost as 0.20244007916364481 and parameters 
[-3.7957913   2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010939
  3.06858478  2.18960114  1.18552009 -1.06648339  0.6027151   1.14432445
  1.31029899 -1.8735467 ]. 
Working on 0.6 fold... 
[Mon Apr  1 22:09:28 2024]  Iteration number: 0 with current cost as 0.18506972219119625 and parameters 
[-3.74431282  2.2374347  -2.1242795  -0.11653103  0.55388708 -2.77010911
  3.06858498  2.18960145  1.18552012 -1.06648308  0.60271517  1.14432452
  1.31029912 -1.8735466 ]. 
Working on 0.8 fold... 
[Mon Apr  1 22:12:05 2024]  Iteration number: 0 with current cost as 0.17922244063925127 and parameters 
[-2.87751899  2.23743464 -2.12427956 -0.11653103  0.55388708 -2.77010905
  3.06858498  2.18960145  1.18552006 -1.06648308  0.6027151   1.14432445
  1.31029902 -1.87354668]. 
Working on 1.0 fold... 
[Mon Apr  1 22:14:49 2024]  Iteration number: 0 with current cost as 0.1975416204408943 and parameters 
[-2.81429673  2.23743464 -2.12427962 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960144  1.18551999 -1.06648309  0.60271511  1.14432446
  1.31029899 -1.87354677]. 
Training complete taking 1051.8325181007385 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Apr  1 22:19:19 2024]  Iteration number: 0 with current cost as 0.1803940773690416 and parameters 
[-3.77749035  2.23743464 -2.12427958 -0.11653108  0.55388702 -2.77010903
  3.06858493  2.18960134  1.18552004 -1.06648314  0.60271516  1.14432445
  1.31029899 -1.87354674]. 
Working on 0.4 fold... 
[Mon Apr  1 22:22:42 2024]  Iteration number: 0 with current cost as 0.20244007916364481 and parameters 
[-3.7957913   2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010939
  3.06858478  2.18960114  1.18552009 -1.06648339  0.6027151   1.14432445
  1.31029899 -1.8735467 ]. 
Working on 0.6 fold... 
[Mon Apr  1 22:26:53 2024]  Iteration number: 0 with current cost as 0.18506972219119625 and parameters 
[-3.74431282  2.2374347  -2.1242795  -0.11653103  0.55388708 -2.77010911
  3.06858498  2.18960145  1.18552012 -1.06648308  0.60271517  1.14432452
  1.31029912 -1.8735466 ]. 
Working on 0.8 fold... 
[Mon Apr  1 22:29:37 2024]  Iteration number: 0 with current cost as 0.17922244063925127 and parameters 
[-2.87751899  2.23743464 -2.12427956 -0.11653103  0.55388708 -2.77010905
  3.06858498  2.18960145  1.18552006 -1.06648308  0.6027151   1.14432445
  1.31029902 -1.87354668]. 
Working on 1.0 fold... 
[Mon Apr  1 22:32:16 2024]  Iteration number: 0 with current cost as 0.1975416204408943 and parameters 
[-2.81429673  2.23743464 -2.12427962 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960144  1.18551999 -1.06648309  0.60271511  1.14432446
  1.31029899 -1.87354677]. 
Training complete taking 1048.21124625206 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Apr  1 22:36:48 2024]  Iteration number: 0 with current cost as 0.1803940773690416 and parameters 
[-3.77749035  2.23743464 -2.12427958 -0.11653108  0.55388702 -2.77010903
  3.06858493  2.18960134  1.18552004 -1.06648314  0.60271516  1.14432445
  1.31029899 -1.87354674]. 
Working on 0.4 fold... 
[Mon Apr  1 22:40:12 2024]  Iteration number: 0 with current cost as 0.20244007916364481 and parameters 
[-3.7957913   2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010939
  3.06858478  2.18960114  1.18552009 -1.06648339  0.6027151   1.14432445
  1.31029899 -1.8735467 ]. 
Working on 0.6 fold... 
[Mon Apr  1 22:44:23 2024]  Iteration number: 0 with current cost as 0.18506972219119625 and parameters 
[-3.74431282  2.2374347  -2.1242795  -0.11653103  0.55388708 -2.77010911
  3.06858498  2.18960145  1.18552012 -1.06648308  0.60271517  1.14432452
  1.31029912 -1.8735466 ]. 
Working on 0.8 fold... 
[Mon Apr  1 22:47:08 2024]  Iteration number: 0 with current cost as 0.17922244063925127 and parameters 
[-2.87751899  2.23743464 -2.12427956 -0.11653103  0.55388708 -2.77010905
  3.06858498  2.18960145  1.18552006 -1.06648308  0.6027151   1.14432445
  1.31029902 -1.87354668]. 
Working on 1.0 fold... 
[Mon Apr  1 22:49:56 2024]  Iteration number: 0 with current cost as 0.1975416204408943 and parameters 
[-2.81429673  2.23743464 -2.12427962 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960144  1.18551999 -1.06648309  0.60271511  1.14432446
  1.31029899 -1.87354677]. 
Training complete taking 1064.364026069641 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Apr  1 22:54:43 2024]  Iteration number: 0 with current cost as 0.1803940773690416 and parameters 
[-3.77749035  2.23743464 -2.12427958 -0.11653108  0.55388702 -2.77010903
  3.06858493  2.18960134  1.18552004 -1.06648314  0.60271516  1.14432445
  1.31029899 -1.87354674]. 
Working on 0.4 fold... 
[Mon Apr  1 22:58:17 2024]  Iteration number: 0 with current cost as 0.20244007916364481 and parameters 
[-3.7957913   2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010939
  3.06858478  2.18960114  1.18552009 -1.06648339  0.6027151   1.14432445
  1.31029899 -1.8735467 ]. 
Working on 0.6 fold... 
[Mon Apr  1 23:02:32 2024]  Iteration number: 0 with current cost as 0.18506972219119625 and parameters 
[-3.74431282  2.2374347  -2.1242795  -0.11653103  0.55388708 -2.77010911
  3.06858498  2.18960145  1.18552012 -1.06648308  0.60271517  1.14432452
  1.31029912 -1.8735466 ]. 
Working on 0.8 fold... 
[Mon Apr  1 23:05:18 2024]  Iteration number: 0 with current cost as 0.17922244063925127 and parameters 
[-2.87751899  2.23743464 -2.12427956 -0.11653103  0.55388708 -2.77010905
  3.06858498  2.18960145  1.18552006 -1.06648308  0.6027151   1.14432445
  1.31029902 -1.87354668]. 
Working on 1.0 fold... 
[Mon Apr  1 23:07:57 2024]  Iteration number: 0 with current cost as 0.1975416204408943 and parameters 
[-2.81429673  2.23743464 -2.12427962 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960144  1.18551999 -1.06648309  0.60271511  1.14432446
  1.31029899 -1.87354677]. 
Training complete taking 1074.6226296424866 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Mon Apr  1 23:12:31 2024]  Iteration number: 0 with current cost as 0.1803940773690416 and parameters 
[-3.77749035  2.23743464 -2.12427958 -0.11653108  0.55388702 -2.77010903
  3.06858493  2.18960134  1.18552004 -1.06648314  0.60271516  1.14432445
  1.31029899 -1.87354674]. 
Working on 0.4 fold... 
[Mon Apr  1 23:15:55 2024]  Iteration number: 0 with current cost as 0.20244007916364481 and parameters 
[-3.7957913   2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010939
  3.06858478  2.18960114  1.18552009 -1.06648339  0.6027151   1.14432445
  1.31029899 -1.8735467 ]. 
Working on 0.6 fold... 
[Mon Apr  1 23:20:07 2024]  Iteration number: 0 with current cost as 0.18506972219119625 and parameters 
[-3.74431282  2.2374347  -2.1242795  -0.11653103  0.55388708 -2.77010911
  3.06858498  2.18960145  1.18552012 -1.06648308  0.60271517  1.14432452
  1.31029912 -1.8735466 ]. 
Working on 0.8 fold... 
[Mon Apr  1 23:22:49 2024]  Iteration number: 0 with current cost as 0.17922244063925127 and parameters 
[-2.87751899  2.23743464 -2.12427956 -0.11653103  0.55388708 -2.77010905
  3.06858498  2.18960145  1.18552006 -1.06648308  0.6027151   1.14432445
  1.31029902 -1.87354668]. 
Working on 1.0 fold... 
[Mon Apr  1 23:25:26 2024]  Iteration number: 0 with current cost as 0.1975416204408943 and parameters 
[-2.81429673  2.23743464 -2.12427962 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960144  1.18551999 -1.06648309  0.60271511  1.14432446
  1.31029899 -1.87354677]. 
Training complete taking 1048.6279253959656 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Mon Apr  1 23:30:03 2024]  Iteration number: 0 with current cost as 0.1803940773690416 and parameters 
[-3.77749035  2.23743464 -2.12427958 -0.11653108  0.55388702 -2.77010903
  3.06858493  2.18960134  1.18552004 -1.06648314  0.60271516  1.14432445
  1.31029899 -1.87354674]. 
Working on 0.4 fold... 
[Mon Apr  1 23:33:31 2024]  Iteration number: 0 with current cost as 0.20244007916364481 and parameters 
[-3.7957913   2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010939
  3.06858478  2.18960114  1.18552009 -1.06648339  0.6027151   1.14432445
  1.31029899 -1.8735467 ]. 
Working on 0.6 fold... 
[Mon Apr  1 23:37:47 2024]  Iteration number: 0 with current cost as 0.18506972219119625 and parameters 
[-3.74431282  2.2374347  -2.1242795  -0.11653103  0.55388708 -2.77010911
  3.06858498  2.18960145  1.18552012 -1.06648308  0.60271517  1.14432452
  1.31029912 -1.8735466 ]. 
Working on 0.8 fold... 
[Mon Apr  1 23:40:24 2024]  Iteration number: 0 with current cost as 0.17922244063925127 and parameters 
[-2.87751899  2.23743464 -2.12427956 -0.11653103  0.55388708 -2.77010905
  3.06858498  2.18960145  1.18552006 -1.06648308  0.6027151   1.14432445
  1.31029902 -1.87354668]. 
Working on 1.0 fold... 
[Mon Apr  1 23:43:10 2024]  Iteration number: 0 with current cost as 0.1975416204408943 and parameters 
[-2.81429673  2.23743464 -2.12427962 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960144  1.18551999 -1.06648309  0.60271511  1.14432446
  1.31029899 -1.87354677]. 
Training complete taking 1066.9460124969482 seconds. 
Discarding model... 

Training complete taking 27101.770914793015 total seconds. 
Now scoring model... 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Scoring complete taking 2.560732364654541 seconds. 
Saved predicted values as M-A1-CZ_Efficient-CRX_predicted_values.csv
Model scores: {'MSE_train': (4.6235247712062275,), 'R2_train': 0.36515524399445365, 'MAE_train': 1.5177881590455444, 'MSE_test': 11.981639151093828, 'R2_test': -0.19445749398818024, 'MAE_test': 2.4706209626480202}. 
Saved model results as M-A1-CZ_Efficient-CRX_results.json. 
