/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/circuits/Encoders.py:12: RuntimeWarning: invalid value encountered in arcsin
  if np.isnan(np.arcsin(features[feature_idx])) or np.isnan(np.arccos(features[feature_idx])):
Loading dataset from /scratch/j/jacobsen/gjones/quad5qubits/quadratic_train.bin... 
Successfully loaded /scratch/j/jacobsen/gjones/quad5qubits/quadratic_train.bin into X and y data. 
Loading dataset from /scratch/j/jacobsen/gjones/quad5qubits/quadratic_test.bin... 
Successfully loaded /scratch/j/jacobsen/gjones/quad5qubits/quadratic_test.bin into X and y data. 
Training model with dataset /scratch/j/jacobsen/gjones/quad5qubits/quadratic_train.bin 
 at time Mon Apr  1 16:07:21 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Apr  1 16:08:52 2024]  Iteration number: 0 with current cost as 0.16568026380362966 and parameters 
[-2.63993165  2.23743461 -2.12427959 -0.11653103  0.55388708 -2.77010897
  3.06858494  2.18960145  1.18551998 -1.06648311  0.6027151   1.14432445
  1.31029901 -1.87354675]. 
Working on 0.4 fold... 
[Mon Apr  1 16:11:51 2024]  Iteration number: 0 with current cost as 0.1731327017582769 and parameters 
[-2.62073801  2.23743464 -2.1242796  -0.11653101  0.55388708 -2.77010897
  3.06858494  2.18960145  1.18551998 -1.06648308  0.60271512  1.14432445
  1.310299   -1.87354678]. 
Working on 0.6 fold... 
[Mon Apr  1 16:14:47 2024]  Iteration number: 0 with current cost as 0.16753750374922186 and parameters 
[-2.62635377  2.23743464 -2.12427964 -0.11653103  0.55388705 -2.77010903
  3.06858493  2.18960145  1.18551998 -1.06648311  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.8 fold... 
[Mon Apr  1 16:17:44 2024]  Iteration number: 0 with current cost as 0.1315683515530679 and parameters 
[-2.57114679  2.23743464 -2.12427964 -0.11653103  0.55388704 -2.77010899
  3.06858494  2.18960143  1.18551998 -1.06648312  0.6027151   1.14432443
  1.31029899 -1.87354678]. 
Working on 1.0 fold... 
[Mon Apr  1 16:20:23 2024]  Iteration number: 0 with current cost as 0.14829497434213593 and parameters 
[-2.49771559  2.2374346  -2.12427964 -0.11653101  0.55388706 -2.77010899
  3.06858494  2.18960145  1.18551998 -1.06648312  0.60271512  1.14432443
  1.31029899 -1.87354676]. 
Training complete taking 853.8900210857391 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Apr  1 16:23:04 2024]  Iteration number: 0 with current cost as 0.16568026380362966 and parameters 
[-2.63993165  2.23743461 -2.12427959 -0.11653103  0.55388708 -2.77010897
  3.06858494  2.18960145  1.18551998 -1.06648311  0.6027151   1.14432445
  1.31029901 -1.87354675]. 
Working on 0.4 fold... 
[Mon Apr  1 16:26:03 2024]  Iteration number: 0 with current cost as 0.1731327017582769 and parameters 
[-2.62073801  2.23743464 -2.1242796  -0.11653101  0.55388708 -2.77010897
  3.06858494  2.18960145  1.18551998 -1.06648308  0.60271512  1.14432445
  1.310299   -1.87354678]. 
Working on 0.6 fold... 
[Mon Apr  1 16:29:01 2024]  Iteration number: 0 with current cost as 0.16753750374922186 and parameters 
[-2.62635377  2.23743464 -2.12427964 -0.11653103  0.55388705 -2.77010903
  3.06858493  2.18960145  1.18551998 -1.06648311  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.8 fold... 
[Mon Apr  1 16:32:01 2024]  Iteration number: 0 with current cost as 0.1315683515530679 and parameters 
[-2.57114679  2.23743464 -2.12427964 -0.11653103  0.55388704 -2.77010899
  3.06858494  2.18960143  1.18551998 -1.06648312  0.6027151   1.14432443
  1.31029899 -1.87354678]. 
Working on 1.0 fold... 
[Mon Apr  1 16:34:40 2024]  Iteration number: 0 with current cost as 0.14829497434213593 and parameters 
[-2.49771559  2.2374346  -2.12427964 -0.11653101  0.55388706 -2.77010899
  3.06858494  2.18960145  1.18551998 -1.06648312  0.60271512  1.14432443
  1.31029899 -1.87354676]. 
Training complete taking 856.7971680164337 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Apr  1 16:37:20 2024]  Iteration number: 0 with current cost as 0.16568026380362966 and parameters 
[-2.63993165  2.23743461 -2.12427959 -0.11653103  0.55388708 -2.77010897
  3.06858494  2.18960145  1.18551998 -1.06648311  0.6027151   1.14432445
  1.31029901 -1.87354675]. 
Working on 0.4 fold... 
[Mon Apr  1 16:40:16 2024]  Iteration number: 0 with current cost as 0.1731327017582769 and parameters 
[-2.62073801  2.23743464 -2.1242796  -0.11653101  0.55388708 -2.77010897
  3.06858494  2.18960145  1.18551998 -1.06648308  0.60271512  1.14432445
  1.310299   -1.87354678]. 
Working on 0.6 fold... 
[Mon Apr  1 16:43:17 2024]  Iteration number: 0 with current cost as 0.16753750374922186 and parameters 
[-2.62635377  2.23743464 -2.12427964 -0.11653103  0.55388705 -2.77010903
  3.06858493  2.18960145  1.18551998 -1.06648311  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.8 fold... 
[Mon Apr  1 16:46:16 2024]  Iteration number: 0 with current cost as 0.1315683515530679 and parameters 
[-2.57114679  2.23743464 -2.12427964 -0.11653103  0.55388704 -2.77010899
  3.06858494  2.18960143  1.18551998 -1.06648312  0.6027151   1.14432443
  1.31029899 -1.87354678]. 
Working on 1.0 fold... 
[Mon Apr  1 16:48:54 2024]  Iteration number: 0 with current cost as 0.14829497434213593 and parameters 
[-2.49771559  2.2374346  -2.12427964 -0.11653101  0.55388706 -2.77010899
  3.06858494  2.18960145  1.18551998 -1.06648312  0.60271512  1.14432443
  1.31029899 -1.87354676]. 
Training complete taking 853.5357940196991 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Mon Apr  1 16:51:35 2024]  Iteration number: 0 with current cost as 0.16568026380362966 and parameters 
[-2.63993165  2.23743461 -2.12427959 -0.11653103  0.55388708 -2.77010897
  3.06858494  2.18960145  1.18551998 -1.06648311  0.6027151   1.14432445
  1.31029901 -1.87354675]. 
Working on 0.4 fold... 
[Mon Apr  1 16:54:30 2024]  Iteration number: 0 with current cost as 0.1731327017582769 and parameters 
[-2.62073801  2.23743464 -2.1242796  -0.11653101  0.55388708 -2.77010897
  3.06858494  2.18960145  1.18551998 -1.06648308  0.60271512  1.14432445
  1.310299   -1.87354678]. 
Working on 0.6 fold... 
[Mon Apr  1 16:57:25 2024]  Iteration number: 0 with current cost as 0.16753750374922186 and parameters 
[-2.62635377  2.23743464 -2.12427964 -0.11653103  0.55388705 -2.77010903
  3.06858493  2.18960145  1.18551998 -1.06648311  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.8 fold... 
[Mon Apr  1 17:00:20 2024]  Iteration number: 0 with current cost as 0.1315683515530679 and parameters 
[-2.57114679  2.23743464 -2.12427964 -0.11653103  0.55388704 -2.77010899
  3.06858494  2.18960143  1.18551998 -1.06648312  0.6027151   1.14432443
  1.31029899 -1.87354678]. 
Working on 1.0 fold... 
[Mon Apr  1 17:02:58 2024]  Iteration number: 0 with current cost as 0.14829497434213593 and parameters 
[-2.49771559  2.2374346  -2.12427964 -0.11653101  0.55388706 -2.77010899
  3.06858494  2.18960145  1.18551998 -1.06648312  0.60271512  1.14432443
  1.31029899 -1.87354676]. 
Training complete taking 842.946307182312 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Mon Apr  1 17:05:36 2024]  Iteration number: 0 with current cost as 0.16568026380362966 and parameters 
[-2.63993165  2.23743461 -2.12427959 -0.11653103  0.55388708 -2.77010897
  3.06858494  2.18960145  1.18551998 -1.06648311  0.6027151   1.14432445
  1.31029901 -1.87354675]. 
Working on 0.4 fold... 
[Mon Apr  1 17:08:35 2024]  Iteration number: 0 with current cost as 0.1731327017582769 and parameters 
[-2.62073801  2.23743464 -2.1242796  -0.11653101  0.55388708 -2.77010897
  3.06858494  2.18960145  1.18551998 -1.06648308  0.60271512  1.14432445
  1.310299   -1.87354678]. 
Working on 0.6 fold... 
[Mon Apr  1 17:11:32 2024]  Iteration number: 0 with current cost as 0.16753750374922186 and parameters 
[-2.62635377  2.23743464 -2.12427964 -0.11653103  0.55388705 -2.77010903
  3.06858493  2.18960145  1.18551998 -1.06648311  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.8 fold... 
[Mon Apr  1 17:14:32 2024]  Iteration number: 0 with current cost as 0.1315683515530679 and parameters 
[-2.57114679  2.23743464 -2.12427964 -0.11653103  0.55388704 -2.77010899
  3.06858494  2.18960143  1.18551998 -1.06648312  0.6027151   1.14432443
  1.31029899 -1.87354678]. 
Working on 1.0 fold... 
[Mon Apr  1 17:17:16 2024]  Iteration number: 0 with current cost as 0.14829497434213593 and parameters 
[-2.49771559  2.2374346  -2.12427964 -0.11653101  0.55388706 -2.77010899
  3.06858494  2.18960145  1.18551998 -1.06648312  0.60271512  1.14432443
  1.31029899 -1.87354676]. 
Training complete taking 857.0361475944519 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Apr  1 17:19:54 2024]  Iteration number: 0 with current cost as 0.16568026380362966 and parameters 
[-2.63993165  2.23743461 -2.12427959 -0.11653103  0.55388708 -2.77010897
  3.06858494  2.18960145  1.18551998 -1.06648311  0.6027151   1.14432445
  1.31029901 -1.87354675]. 
Working on 0.4 fold... 
[Mon Apr  1 17:22:52 2024]  Iteration number: 0 with current cost as 0.1731327017582769 and parameters 
[-2.62073801  2.23743464 -2.1242796  -0.11653101  0.55388708 -2.77010897
  3.06858494  2.18960145  1.18551998 -1.06648308  0.60271512  1.14432445
  1.310299   -1.87354678]. 
Working on 0.6 fold... 
[Mon Apr  1 17:25:49 2024]  Iteration number: 0 with current cost as 0.16753750374922186 and parameters 
[-2.62635377  2.23743464 -2.12427964 -0.11653103  0.55388705 -2.77010903
  3.06858493  2.18960145  1.18551998 -1.06648311  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.8 fold... 
[Mon Apr  1 17:28:45 2024]  Iteration number: 0 with current cost as 0.1315683515530679 and parameters 
[-2.57114679  2.23743464 -2.12427964 -0.11653103  0.55388704 -2.77010899
  3.06858494  2.18960143  1.18551998 -1.06648312  0.6027151   1.14432443
  1.31029899 -1.87354678]. 
Working on 1.0 fold... 
[Mon Apr  1 17:31:26 2024]  Iteration number: 0 with current cost as 0.14829497434213593 and parameters 
[-2.49771559  2.2374346  -2.12427964 -0.11653101  0.55388706 -2.77010899
  3.06858494  2.18960145  1.18551998 -1.06648312  0.60271512  1.14432443
  1.31029899 -1.87354676]. 
Training complete taking 852.4565975666046 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Apr  1 17:34:05 2024]  Iteration number: 0 with current cost as 0.16568026380362966 and parameters 
[-2.63993165  2.23743461 -2.12427959 -0.11653103  0.55388708 -2.77010897
  3.06858494  2.18960145  1.18551998 -1.06648311  0.6027151   1.14432445
  1.31029901 -1.87354675]. 
Working on 0.4 fold... 
[Mon Apr  1 17:37:02 2024]  Iteration number: 0 with current cost as 0.1731327017582769 and parameters 
[-2.62073801  2.23743464 -2.1242796  -0.11653101  0.55388708 -2.77010897
  3.06858494  2.18960145  1.18551998 -1.06648308  0.60271512  1.14432445
  1.310299   -1.87354678]. 
Working on 0.6 fold... 
[Mon Apr  1 17:40:01 2024]  Iteration number: 0 with current cost as 0.16753750374922186 and parameters 
[-2.62635377  2.23743464 -2.12427964 -0.11653103  0.55388705 -2.77010903
  3.06858493  2.18960145  1.18551998 -1.06648311  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.8 fold... 
[Mon Apr  1 17:43:00 2024]  Iteration number: 0 with current cost as 0.1315683515530679 and parameters 
[-2.57114679  2.23743464 -2.12427964 -0.11653103  0.55388704 -2.77010899
  3.06858494  2.18960143  1.18551998 -1.06648312  0.6027151   1.14432443
  1.31029899 -1.87354678]. 
Working on 1.0 fold... 
[Mon Apr  1 17:45:42 2024]  Iteration number: 0 with current cost as 0.14829497434213593 and parameters 
[-2.49771559  2.2374346  -2.12427964 -0.11653101  0.55388706 -2.77010899
  3.06858494  2.18960145  1.18551998 -1.06648312  0.60271512  1.14432443
  1.31029899 -1.87354676]. 
Training complete taking 855.6012740135193 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Apr  1 17:48:26 2024]  Iteration number: 0 with current cost as 0.16568026380362966 and parameters 
[-2.63993165  2.23743461 -2.12427959 -0.11653103  0.55388708 -2.77010897
  3.06858494  2.18960145  1.18551998 -1.06648311  0.6027151   1.14432445
  1.31029901 -1.87354675]. 
Working on 0.4 fold... 
[Mon Apr  1 17:51:27 2024]  Iteration number: 0 with current cost as 0.1731327017582769 and parameters 
[-2.62073801  2.23743464 -2.1242796  -0.11653101  0.55388708 -2.77010897
  3.06858494  2.18960145  1.18551998 -1.06648308  0.60271512  1.14432445
  1.310299   -1.87354678]. 
Working on 0.6 fold... 
[Mon Apr  1 17:54:25 2024]  Iteration number: 0 with current cost as 0.16753750374922186 and parameters 
[-2.62635377  2.23743464 -2.12427964 -0.11653103  0.55388705 -2.77010903
  3.06858493  2.18960145  1.18551998 -1.06648311  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.8 fold... 
[Mon Apr  1 17:57:26 2024]  Iteration number: 0 with current cost as 0.1315683515530679 and parameters 
[-2.57114679  2.23743464 -2.12427964 -0.11653103  0.55388704 -2.77010899
  3.06858494  2.18960143  1.18551998 -1.06648312  0.6027151   1.14432443
  1.31029899 -1.87354678]. 
Working on 1.0 fold... 
[Mon Apr  1 18:00:11 2024]  Iteration number: 0 with current cost as 0.14829497434213593 and parameters 
[-2.49771559  2.2374346  -2.12427964 -0.11653101  0.55388706 -2.77010899
  3.06858494  2.18960145  1.18551998 -1.06648312  0.60271512  1.14432443
  1.31029899 -1.87354676]. 
Training complete taking 871.1264536380768 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Mon Apr  1 18:02:54 2024]  Iteration number: 0 with current cost as 0.16568026380362966 and parameters 
[-2.63993165  2.23743461 -2.12427959 -0.11653103  0.55388708 -2.77010897
  3.06858494  2.18960145  1.18551998 -1.06648311  0.6027151   1.14432445
  1.31029901 -1.87354675]. 
Working on 0.4 fold... 
[Mon Apr  1 18:05:58 2024]  Iteration number: 0 with current cost as 0.1731327017582769 and parameters 
[-2.62073801  2.23743464 -2.1242796  -0.11653101  0.55388708 -2.77010897
  3.06858494  2.18960145  1.18551998 -1.06648308  0.60271512  1.14432445
  1.310299   -1.87354678]. 
Working on 0.6 fold... 
[Mon Apr  1 18:08:58 2024]  Iteration number: 0 with current cost as 0.16753750374922186 and parameters 
[-2.62635377  2.23743464 -2.12427964 -0.11653103  0.55388705 -2.77010903
  3.06858493  2.18960145  1.18551998 -1.06648311  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.8 fold... 
[Mon Apr  1 18:12:01 2024]  Iteration number: 0 with current cost as 0.1315683515530679 and parameters 
[-2.57114679  2.23743464 -2.12427964 -0.11653103  0.55388704 -2.77010899
  3.06858494  2.18960143  1.18551998 -1.06648312  0.6027151   1.14432443
  1.31029899 -1.87354678]. 
Working on 1.0 fold... 
[Mon Apr  1 18:14:44 2024]  Iteration number: 0 with current cost as 0.14829497434213593 and parameters 
[-2.49771559  2.2374346  -2.12427964 -0.11653101  0.55388706 -2.77010899
  3.06858494  2.18960145  1.18551998 -1.06648312  0.60271512  1.14432443
  1.31029899 -1.87354676]. 
Training complete taking 868.8159446716309 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Mon Apr  1 18:17:22 2024]  Iteration number: 0 with current cost as 0.16568026380362966 and parameters 
[-2.63993165  2.23743461 -2.12427959 -0.11653103  0.55388708 -2.77010897
  3.06858494  2.18960145  1.18551998 -1.06648311  0.6027151   1.14432445
  1.31029901 -1.87354675]. 
Working on 0.4 fold... 
[Mon Apr  1 18:20:20 2024]  Iteration number: 0 with current cost as 0.1731327017582769 and parameters 
[-2.62073801  2.23743464 -2.1242796  -0.11653101  0.55388708 -2.77010897
  3.06858494  2.18960145  1.18551998 -1.06648308  0.60271512  1.14432445
  1.310299   -1.87354678]. 
Working on 0.6 fold... 
[Mon Apr  1 18:23:16 2024]  Iteration number: 0 with current cost as 0.16753750374922186 and parameters 
[-2.62635377  2.23743464 -2.12427964 -0.11653103  0.55388705 -2.77010903
  3.06858493  2.18960145  1.18551998 -1.06648311  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.8 fold... 
[Mon Apr  1 18:26:13 2024]  Iteration number: 0 with current cost as 0.1315683515530679 and parameters 
[-2.57114679  2.23743464 -2.12427964 -0.11653103  0.55388704 -2.77010899
  3.06858494  2.18960143  1.18551998 -1.06648312  0.6027151   1.14432443
  1.31029899 -1.87354678]. 
Working on 1.0 fold... 
[Mon Apr  1 18:28:53 2024]  Iteration number: 0 with current cost as 0.14829497434213593 and parameters 
[-2.49771559  2.2374346  -2.12427964 -0.11653101  0.55388706 -2.77010899
  3.06858494  2.18960145  1.18551998 -1.06648312  0.60271512  1.14432443
  1.31029899 -1.87354676]. 
Training complete taking 851.6412122249603 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Apr  1 18:31:33 2024]  Iteration number: 0 with current cost as 0.16568026380362966 and parameters 
[-2.63993165  2.23743461 -2.12427959 -0.11653103  0.55388708 -2.77010897
  3.06858494  2.18960145  1.18551998 -1.06648311  0.6027151   1.14432445
  1.31029901 -1.87354675]. 
Working on 0.4 fold... 
[Mon Apr  1 18:34:28 2024]  Iteration number: 0 with current cost as 0.1731327017582769 and parameters 
[-2.62073801  2.23743464 -2.1242796  -0.11653101  0.55388708 -2.77010897
  3.06858494  2.18960145  1.18551998 -1.06648308  0.60271512  1.14432445
  1.310299   -1.87354678]. 
Working on 0.6 fold... 
[Mon Apr  1 18:37:22 2024]  Iteration number: 0 with current cost as 0.16753750374922186 and parameters 
[-2.62635377  2.23743464 -2.12427964 -0.11653103  0.55388705 -2.77010903
  3.06858493  2.18960145  1.18551998 -1.06648311  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.8 fold... 
[Mon Apr  1 18:40:17 2024]  Iteration number: 0 with current cost as 0.1315683515530679 and parameters 
[-2.57114679  2.23743464 -2.12427964 -0.11653103  0.55388704 -2.77010899
  3.06858494  2.18960143  1.18551998 -1.06648312  0.6027151   1.14432443
  1.31029899 -1.87354678]. 
Working on 1.0 fold... 
[Mon Apr  1 18:42:57 2024]  Iteration number: 0 with current cost as 0.14829497434213593 and parameters 
[-2.49771559  2.2374346  -2.12427964 -0.11653101  0.55388706 -2.77010899
  3.06858494  2.18960145  1.18551998 -1.06648312  0.60271512  1.14432443
  1.31029899 -1.87354676]. 
Training complete taking 842.7159380912781 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Apr  1 18:45:36 2024]  Iteration number: 0 with current cost as 0.16568026380362966 and parameters 
[-2.63993165  2.23743461 -2.12427959 -0.11653103  0.55388708 -2.77010897
  3.06858494  2.18960145  1.18551998 -1.06648311  0.6027151   1.14432445
  1.31029901 -1.87354675]. 
Working on 0.4 fold... 
[Mon Apr  1 18:48:36 2024]  Iteration number: 0 with current cost as 0.1731327017582769 and parameters 
[-2.62073801  2.23743464 -2.1242796  -0.11653101  0.55388708 -2.77010897
  3.06858494  2.18960145  1.18551998 -1.06648308  0.60271512  1.14432445
  1.310299   -1.87354678]. 
Working on 0.6 fold... 
[Mon Apr  1 18:51:33 2024]  Iteration number: 0 with current cost as 0.16753750374922186 and parameters 
[-2.62635377  2.23743464 -2.12427964 -0.11653103  0.55388705 -2.77010903
  3.06858493  2.18960145  1.18551998 -1.06648311  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.8 fold... 
[Mon Apr  1 18:54:30 2024]  Iteration number: 0 with current cost as 0.1315683515530679 and parameters 
[-2.57114679  2.23743464 -2.12427964 -0.11653103  0.55388704 -2.77010899
  3.06858494  2.18960143  1.18551998 -1.06648312  0.6027151   1.14432443
  1.31029899 -1.87354678]. 
Working on 1.0 fold... 
[Mon Apr  1 18:57:08 2024]  Iteration number: 0 with current cost as 0.14829497434213593 and parameters 
[-2.49771559  2.2374346  -2.12427964 -0.11653101  0.55388706 -2.77010899
  3.06858494  2.18960145  1.18551998 -1.06648312  0.60271512  1.14432443
  1.31029899 -1.87354676]. 
Training complete taking 851.7503752708435 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Apr  1 18:59:48 2024]  Iteration number: 0 with current cost as 0.16568026380362966 and parameters 
[-2.63993165  2.23743461 -2.12427959 -0.11653103  0.55388708 -2.77010897
  3.06858494  2.18960145  1.18551998 -1.06648311  0.6027151   1.14432445
  1.31029901 -1.87354675]. 
Working on 0.4 fold... 
[Mon Apr  1 19:02:46 2024]  Iteration number: 0 with current cost as 0.1731327017582769 and parameters 
[-2.62073801  2.23743464 -2.1242796  -0.11653101  0.55388708 -2.77010897
  3.06858494  2.18960145  1.18551998 -1.06648308  0.60271512  1.14432445
  1.310299   -1.87354678]. 
Working on 0.6 fold... 
[Mon Apr  1 19:05:45 2024]  Iteration number: 0 with current cost as 0.16753750374922186 and parameters 
[-2.62635377  2.23743464 -2.12427964 -0.11653103  0.55388705 -2.77010903
  3.06858493  2.18960145  1.18551998 -1.06648311  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.8 fold... 
[Mon Apr  1 19:08:40 2024]  Iteration number: 0 with current cost as 0.1315683515530679 and parameters 
[-2.57114679  2.23743464 -2.12427964 -0.11653103  0.55388704 -2.77010899
  3.06858494  2.18960143  1.18551998 -1.06648312  0.6027151   1.14432443
  1.31029899 -1.87354678]. 
Working on 1.0 fold... 
[Mon Apr  1 19:11:21 2024]  Iteration number: 0 with current cost as 0.14829497434213593 and parameters 
[-2.49771559  2.2374346  -2.12427964 -0.11653101  0.55388706 -2.77010899
  3.06858494  2.18960145  1.18551998 -1.06648312  0.60271512  1.14432443
  1.31029899 -1.87354676]. 
Training complete taking 850.7348766326904 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Apr  1 19:13:58 2024]  Iteration number: 0 with current cost as 0.16568026380362966 and parameters 
[-2.63993165  2.23743461 -2.12427959 -0.11653103  0.55388708 -2.77010897
  3.06858494  2.18960145  1.18551998 -1.06648311  0.6027151   1.14432445
  1.31029901 -1.87354675]. 
Working on 0.4 fold... 
[Mon Apr  1 19:16:53 2024]  Iteration number: 0 with current cost as 0.1731327017582769 and parameters 
[-2.62073801  2.23743464 -2.1242796  -0.11653101  0.55388708 -2.77010897
  3.06858494  2.18960145  1.18551998 -1.06648308  0.60271512  1.14432445
  1.310299   -1.87354678]. 
Working on 0.6 fold... 
[Mon Apr  1 19:19:47 2024]  Iteration number: 0 with current cost as 0.16753750374922186 and parameters 
[-2.62635377  2.23743464 -2.12427964 -0.11653103  0.55388705 -2.77010903
  3.06858493  2.18960145  1.18551998 -1.06648311  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.8 fold... 
[Mon Apr  1 19:22:43 2024]  Iteration number: 0 with current cost as 0.1315683515530679 and parameters 
[-2.57114679  2.23743464 -2.12427964 -0.11653103  0.55388704 -2.77010899
  3.06858494  2.18960143  1.18551998 -1.06648312  0.6027151   1.14432443
  1.31029899 -1.87354678]. 
Working on 1.0 fold... 
[Mon Apr  1 19:25:24 2024]  Iteration number: 0 with current cost as 0.14829497434213593 and parameters 
[-2.49771559  2.2374346  -2.12427964 -0.11653101  0.55388706 -2.77010899
  3.06858494  2.18960145  1.18551998 -1.06648312  0.60271512  1.14432443
  1.31029899 -1.87354676]. 
Training complete taking 844.0004279613495 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Apr  1 19:28:04 2024]  Iteration number: 0 with current cost as 0.16568026380362966 and parameters 
[-2.63993165  2.23743461 -2.12427959 -0.11653103  0.55388708 -2.77010897
  3.06858494  2.18960145  1.18551998 -1.06648311  0.6027151   1.14432445
  1.31029901 -1.87354675]. 
Working on 0.4 fold... 
[Mon Apr  1 19:31:03 2024]  Iteration number: 0 with current cost as 0.1731327017582769 and parameters 
[-2.62073801  2.23743464 -2.1242796  -0.11653101  0.55388708 -2.77010897
  3.06858494  2.18960145  1.18551998 -1.06648308  0.60271512  1.14432445
  1.310299   -1.87354678]. 
Working on 0.6 fold... 
[Mon Apr  1 19:34:00 2024]  Iteration number: 0 with current cost as 0.16753750374922186 and parameters 
[-2.62635377  2.23743464 -2.12427964 -0.11653103  0.55388705 -2.77010903
  3.06858493  2.18960145  1.18551998 -1.06648311  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.8 fold... 
[Mon Apr  1 19:36:58 2024]  Iteration number: 0 with current cost as 0.1315683515530679 and parameters 
[-2.57114679  2.23743464 -2.12427964 -0.11653103  0.55388704 -2.77010899
  3.06858494  2.18960143  1.18551998 -1.06648312  0.6027151   1.14432443
  1.31029899 -1.87354678]. 
Working on 1.0 fold... 
[Mon Apr  1 19:39:37 2024]  Iteration number: 0 with current cost as 0.14829497434213593 and parameters 
[-2.49771559  2.2374346  -2.12427964 -0.11653101  0.55388706 -2.77010899
  3.06858494  2.18960145  1.18551998 -1.06648312  0.60271512  1.14432443
  1.31029899 -1.87354676]. 
Training complete taking 853.003390789032 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Apr  1 19:42:15 2024]  Iteration number: 0 with current cost as 0.16568026380362966 and parameters 
[-2.63993165  2.23743461 -2.12427959 -0.11653103  0.55388708 -2.77010897
  3.06858494  2.18960145  1.18551998 -1.06648311  0.6027151   1.14432445
  1.31029901 -1.87354675]. 
Working on 0.4 fold... 
[Mon Apr  1 19:45:13 2024]  Iteration number: 0 with current cost as 0.1731327017582769 and parameters 
[-2.62073801  2.23743464 -2.1242796  -0.11653101  0.55388708 -2.77010897
  3.06858494  2.18960145  1.18551998 -1.06648308  0.60271512  1.14432445
  1.310299   -1.87354678]. 
Working on 0.6 fold... 
[Mon Apr  1 19:48:09 2024]  Iteration number: 0 with current cost as 0.16753750374922186 and parameters 
[-2.62635377  2.23743464 -2.12427964 -0.11653103  0.55388705 -2.77010903
  3.06858493  2.18960145  1.18551998 -1.06648311  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.8 fold... 
[Mon Apr  1 19:51:04 2024]  Iteration number: 0 with current cost as 0.1315683515530679 and parameters 
[-2.57114679  2.23743464 -2.12427964 -0.11653103  0.55388704 -2.77010899
  3.06858494  2.18960143  1.18551998 -1.06648312  0.6027151   1.14432443
  1.31029899 -1.87354678]. 
Working on 1.0 fold... 
[Mon Apr  1 19:53:42 2024]  Iteration number: 0 with current cost as 0.14829497434213593 and parameters 
[-2.49771559  2.2374346  -2.12427964 -0.11653101  0.55388706 -2.77010899
  3.06858494  2.18960145  1.18551998 -1.06648312  0.60271512  1.14432443
  1.31029899 -1.87354676]. 
Training complete taking 845.2538096904755 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Apr  1 19:56:22 2024]  Iteration number: 0 with current cost as 0.16568026380362966 and parameters 
[-2.63993165  2.23743461 -2.12427959 -0.11653103  0.55388708 -2.77010897
  3.06858494  2.18960145  1.18551998 -1.06648311  0.6027151   1.14432445
  1.31029901 -1.87354675]. 
Working on 0.4 fold... 
[Mon Apr  1 19:59:21 2024]  Iteration number: 0 with current cost as 0.1731327017582769 and parameters 
[-2.62073801  2.23743464 -2.1242796  -0.11653101  0.55388708 -2.77010897
  3.06858494  2.18960145  1.18551998 -1.06648308  0.60271512  1.14432445
  1.310299   -1.87354678]. 
Working on 0.6 fold... 
[Mon Apr  1 20:02:20 2024]  Iteration number: 0 with current cost as 0.16753750374922186 and parameters 
[-2.62635377  2.23743464 -2.12427964 -0.11653103  0.55388705 -2.77010903
  3.06858493  2.18960145  1.18551998 -1.06648311  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.8 fold... 
[Mon Apr  1 20:05:17 2024]  Iteration number: 0 with current cost as 0.1315683515530679 and parameters 
[-2.57114679  2.23743464 -2.12427964 -0.11653103  0.55388704 -2.77010899
  3.06858494  2.18960143  1.18551998 -1.06648312  0.6027151   1.14432443
  1.31029899 -1.87354678]. 
Working on 1.0 fold... 
[Mon Apr  1 20:07:56 2024]  Iteration number: 0 with current cost as 0.14829497434213593 and parameters 
[-2.49771559  2.2374346  -2.12427964 -0.11653101  0.55388706 -2.77010899
  3.06858494  2.18960145  1.18551998 -1.06648312  0.60271512  1.14432443
  1.31029899 -1.87354676]. 
Training complete taking 853.0090153217316 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Apr  1 20:10:35 2024]  Iteration number: 0 with current cost as 0.16568026380362966 and parameters 
[-2.63993165  2.23743461 -2.12427959 -0.11653103  0.55388708 -2.77010897
  3.06858494  2.18960145  1.18551998 -1.06648311  0.6027151   1.14432445
  1.31029901 -1.87354675]. 
Working on 0.4 fold... 
[Mon Apr  1 20:13:32 2024]  Iteration number: 0 with current cost as 0.1731327017582769 and parameters 
[-2.62073801  2.23743464 -2.1242796  -0.11653101  0.55388708 -2.77010897
  3.06858494  2.18960145  1.18551998 -1.06648308  0.60271512  1.14432445
  1.310299   -1.87354678]. 
Working on 0.6 fold... 
[Mon Apr  1 20:16:28 2024]  Iteration number: 0 with current cost as 0.16753750374922186 and parameters 
[-2.62635377  2.23743464 -2.12427964 -0.11653103  0.55388705 -2.77010903
  3.06858493  2.18960145  1.18551998 -1.06648311  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.8 fold... 
[Mon Apr  1 20:19:23 2024]  Iteration number: 0 with current cost as 0.1315683515530679 and parameters 
[-2.57114679  2.23743464 -2.12427964 -0.11653103  0.55388704 -2.77010899
  3.06858494  2.18960143  1.18551998 -1.06648312  0.6027151   1.14432443
  1.31029899 -1.87354678]. 
Working on 1.0 fold... 
[Mon Apr  1 20:22:03 2024]  Iteration number: 0 with current cost as 0.14829497434213593 and parameters 
[-2.49771559  2.2374346  -2.12427964 -0.11653101  0.55388706 -2.77010899
  3.06858494  2.18960145  1.18551998 -1.06648312  0.60271512  1.14432443
  1.31029899 -1.87354676]. 
Training complete taking 849.117830991745 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Apr  1 20:24:42 2024]  Iteration number: 0 with current cost as 0.16568026380362966 and parameters 
[-2.63993165  2.23743461 -2.12427959 -0.11653103  0.55388708 -2.77010897
  3.06858494  2.18960145  1.18551998 -1.06648311  0.6027151   1.14432445
  1.31029901 -1.87354675]. 
Working on 0.4 fold... 
[Mon Apr  1 20:27:40 2024]  Iteration number: 0 with current cost as 0.1731327017582769 and parameters 
[-2.62073801  2.23743464 -2.1242796  -0.11653101  0.55388708 -2.77010897
  3.06858494  2.18960145  1.18551998 -1.06648308  0.60271512  1.14432445
  1.310299   -1.87354678]. 
Working on 0.6 fold... 
[Mon Apr  1 20:30:39 2024]  Iteration number: 0 with current cost as 0.16753750374922186 and parameters 
[-2.62635377  2.23743464 -2.12427964 -0.11653103  0.55388705 -2.77010903
  3.06858493  2.18960145  1.18551998 -1.06648311  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.8 fold... 
[Mon Apr  1 20:33:36 2024]  Iteration number: 0 with current cost as 0.1315683515530679 and parameters 
[-2.57114679  2.23743464 -2.12427964 -0.11653103  0.55388704 -2.77010899
  3.06858494  2.18960143  1.18551998 -1.06648312  0.6027151   1.14432443
  1.31029899 -1.87354678]. 
Working on 1.0 fold... 
[Mon Apr  1 20:36:14 2024]  Iteration number: 0 with current cost as 0.14829497434213593 and parameters 
[-2.49771559  2.2374346  -2.12427964 -0.11653101  0.55388706 -2.77010899
  3.06858494  2.18960145  1.18551998 -1.06648312  0.60271512  1.14432443
  1.31029899 -1.87354676]. 
Training complete taking 848.5199415683746 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Apr  1 20:38:51 2024]  Iteration number: 0 with current cost as 0.16568026380362966 and parameters 
[-2.63993165  2.23743461 -2.12427959 -0.11653103  0.55388708 -2.77010897
  3.06858494  2.18960145  1.18551998 -1.06648311  0.6027151   1.14432445
  1.31029901 -1.87354675]. 
Working on 0.4 fold... 
[Mon Apr  1 20:41:47 2024]  Iteration number: 0 with current cost as 0.1731327017582769 and parameters 
[-2.62073801  2.23743464 -2.1242796  -0.11653101  0.55388708 -2.77010897
  3.06858494  2.18960145  1.18551998 -1.06648308  0.60271512  1.14432445
  1.310299   -1.87354678]. 
Working on 0.6 fold... 
[Mon Apr  1 20:44:40 2024]  Iteration number: 0 with current cost as 0.16753750374922186 and parameters 
[-2.62635377  2.23743464 -2.12427964 -0.11653103  0.55388705 -2.77010903
  3.06858493  2.18960145  1.18551998 -1.06648311  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.8 fold... 
[Mon Apr  1 20:47:37 2024]  Iteration number: 0 with current cost as 0.1315683515530679 and parameters 
[-2.57114679  2.23743464 -2.12427964 -0.11653103  0.55388704 -2.77010899
  3.06858494  2.18960143  1.18551998 -1.06648312  0.6027151   1.14432443
  1.31029899 -1.87354678]. 
Working on 1.0 fold... 
[Mon Apr  1 20:50:17 2024]  Iteration number: 0 with current cost as 0.14829497434213593 and parameters 
[-2.49771559  2.2374346  -2.12427964 -0.11653101  0.55388706 -2.77010899
  3.06858494  2.18960145  1.18551998 -1.06648312  0.60271512  1.14432443
  1.31029899 -1.87354676]. 
Training complete taking 843.210976600647 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Apr  1 20:52:54 2024]  Iteration number: 0 with current cost as 0.16568026380362966 and parameters 
[-2.63993165  2.23743461 -2.12427959 -0.11653103  0.55388708 -2.77010897
  3.06858494  2.18960145  1.18551998 -1.06648311  0.6027151   1.14432445
  1.31029901 -1.87354675]. 
Working on 0.4 fold... 
[Mon Apr  1 20:55:48 2024]  Iteration number: 0 with current cost as 0.1731327017582769 and parameters 
[-2.62073801  2.23743464 -2.1242796  -0.11653101  0.55388708 -2.77010897
  3.06858494  2.18960145  1.18551998 -1.06648308  0.60271512  1.14432445
  1.310299   -1.87354678]. 
Working on 0.6 fold... 
[Mon Apr  1 20:58:43 2024]  Iteration number: 0 with current cost as 0.16753750374922186 and parameters 
[-2.62635377  2.23743464 -2.12427964 -0.11653103  0.55388705 -2.77010903
  3.06858493  2.18960145  1.18551998 -1.06648311  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.8 fold... 
[Mon Apr  1 21:01:38 2024]  Iteration number: 0 with current cost as 0.1315683515530679 and parameters 
[-2.57114679  2.23743464 -2.12427964 -0.11653103  0.55388704 -2.77010899
  3.06858494  2.18960143  1.18551998 -1.06648312  0.6027151   1.14432443
  1.31029899 -1.87354678]. 
Working on 1.0 fold... 
[Mon Apr  1 21:04:17 2024]  Iteration number: 0 with current cost as 0.14829497434213593 and parameters 
[-2.49771559  2.2374346  -2.12427964 -0.11653101  0.55388706 -2.77010899
  3.06858494  2.18960145  1.18551998 -1.06648312  0.60271512  1.14432443
  1.31029899 -1.87354676]. 
Training complete taking 842.4822173118591 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Apr  1 21:06:58 2024]  Iteration number: 0 with current cost as 0.16568026380362966 and parameters 
[-2.63993165  2.23743461 -2.12427959 -0.11653103  0.55388708 -2.77010897
  3.06858494  2.18960145  1.18551998 -1.06648311  0.6027151   1.14432445
  1.31029901 -1.87354675]. 
Working on 0.4 fold... 
[Mon Apr  1 21:09:53 2024]  Iteration number: 0 with current cost as 0.1731327017582769 and parameters 
[-2.62073801  2.23743464 -2.1242796  -0.11653101  0.55388708 -2.77010897
  3.06858494  2.18960145  1.18551998 -1.06648308  0.60271512  1.14432445
  1.310299   -1.87354678]. 
Working on 0.6 fold... 
[Mon Apr  1 21:12:50 2024]  Iteration number: 0 with current cost as 0.16753750374922186 and parameters 
[-2.62635377  2.23743464 -2.12427964 -0.11653103  0.55388705 -2.77010903
  3.06858493  2.18960145  1.18551998 -1.06648311  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.8 fold... 
[Mon Apr  1 21:15:45 2024]  Iteration number: 0 with current cost as 0.1315683515530679 and parameters 
[-2.57114679  2.23743464 -2.12427964 -0.11653103  0.55388704 -2.77010899
  3.06858494  2.18960143  1.18551998 -1.06648312  0.6027151   1.14432443
  1.31029899 -1.87354678]. 
Working on 1.0 fold... 
[Mon Apr  1 21:18:22 2024]  Iteration number: 0 with current cost as 0.14829497434213593 and parameters 
[-2.49771559  2.2374346  -2.12427964 -0.11653101  0.55388706 -2.77010899
  3.06858494  2.18960145  1.18551998 -1.06648312  0.60271512  1.14432443
  1.31029899 -1.87354676]. 
Training complete taking 843.7545642852783 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Apr  1 21:21:00 2024]  Iteration number: 0 with current cost as 0.16568026380362966 and parameters 
[-2.63993165  2.23743461 -2.12427959 -0.11653103  0.55388708 -2.77010897
  3.06858494  2.18960145  1.18551998 -1.06648311  0.6027151   1.14432445
  1.31029901 -1.87354675]. 
Working on 0.4 fold... 
[Mon Apr  1 21:23:57 2024]  Iteration number: 0 with current cost as 0.1731327017582769 and parameters 
[-2.62073801  2.23743464 -2.1242796  -0.11653101  0.55388708 -2.77010897
  3.06858494  2.18960145  1.18551998 -1.06648308  0.60271512  1.14432445
  1.310299   -1.87354678]. 
Working on 0.6 fold... 
[Mon Apr  1 21:26:53 2024]  Iteration number: 0 with current cost as 0.16753750374922186 and parameters 
[-2.62635377  2.23743464 -2.12427964 -0.11653103  0.55388705 -2.77010903
  3.06858493  2.18960145  1.18551998 -1.06648311  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.8 fold... 
[Mon Apr  1 21:29:50 2024]  Iteration number: 0 with current cost as 0.1315683515530679 and parameters 
[-2.57114679  2.23743464 -2.12427964 -0.11653103  0.55388704 -2.77010899
  3.06858494  2.18960143  1.18551998 -1.06648312  0.6027151   1.14432443
  1.31029899 -1.87354678]. 
Working on 1.0 fold... 
[Mon Apr  1 21:32:27 2024]  Iteration number: 0 with current cost as 0.14829497434213593 and parameters 
[-2.49771559  2.2374346  -2.12427964 -0.11653101  0.55388706 -2.77010899
  3.06858494  2.18960145  1.18551998 -1.06648312  0.60271512  1.14432443
  1.31029899 -1.87354676]. 
Training complete taking 844.1645600795746 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Mon Apr  1 21:35:06 2024]  Iteration number: 0 with current cost as 0.16568026380362966 and parameters 
[-2.63993165  2.23743461 -2.12427959 -0.11653103  0.55388708 -2.77010897
  3.06858494  2.18960145  1.18551998 -1.06648311  0.6027151   1.14432445
  1.31029901 -1.87354675]. 
Working on 0.4 fold... 
[Mon Apr  1 21:38:06 2024]  Iteration number: 0 with current cost as 0.1731327017582769 and parameters 
[-2.62073801  2.23743464 -2.1242796  -0.11653101  0.55388708 -2.77010897
  3.06858494  2.18960145  1.18551998 -1.06648308  0.60271512  1.14432445
  1.310299   -1.87354678]. 
Working on 0.6 fold... 
[Mon Apr  1 21:41:01 2024]  Iteration number: 0 with current cost as 0.16753750374922186 and parameters 
[-2.62635377  2.23743464 -2.12427964 -0.11653103  0.55388705 -2.77010903
  3.06858493  2.18960145  1.18551998 -1.06648311  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.8 fold... 
[Mon Apr  1 21:43:59 2024]  Iteration number: 0 with current cost as 0.1315683515530679 and parameters 
[-2.57114679  2.23743464 -2.12427964 -0.11653103  0.55388704 -2.77010899
  3.06858494  2.18960143  1.18551998 -1.06648312  0.6027151   1.14432443
  1.31029899 -1.87354678]. 
Working on 1.0 fold... 
[Mon Apr  1 21:46:37 2024]  Iteration number: 0 with current cost as 0.14829497434213593 and parameters 
[-2.49771559  2.2374346  -2.12427964 -0.11653101  0.55388706 -2.77010899
  3.06858494  2.18960145  1.18551998 -1.06648312  0.60271512  1.14432443
  1.31029899 -1.87354676]. 
Training complete taking 848.9707579612732 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Mon Apr  1 21:49:16 2024]  Iteration number: 0 with current cost as 0.16568026380362966 and parameters 
[-2.63993165  2.23743461 -2.12427959 -0.11653103  0.55388708 -2.77010897
  3.06858494  2.18960145  1.18551998 -1.06648311  0.6027151   1.14432445
  1.31029901 -1.87354675]. 
Working on 0.4 fold... 
[Mon Apr  1 21:52:11 2024]  Iteration number: 0 with current cost as 0.1731327017582769 and parameters 
[-2.62073801  2.23743464 -2.1242796  -0.11653101  0.55388708 -2.77010897
  3.06858494  2.18960145  1.18551998 -1.06648308  0.60271512  1.14432445
  1.310299   -1.87354678]. 
Working on 0.6 fold... 
[Mon Apr  1 21:55:06 2024]  Iteration number: 0 with current cost as 0.16753750374922186 and parameters 
[-2.62635377  2.23743464 -2.12427964 -0.11653103  0.55388705 -2.77010903
  3.06858493  2.18960145  1.18551998 -1.06648311  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.8 fold... 
[Mon Apr  1 21:58:01 2024]  Iteration number: 0 with current cost as 0.1315683515530679 and parameters 
[-2.57114679  2.23743464 -2.12427964 -0.11653103  0.55388704 -2.77010899
  3.06858494  2.18960143  1.18551998 -1.06648312  0.6027151   1.14432443
  1.31029899 -1.87354678]. 
Working on 1.0 fold... 
[Mon Apr  1 22:00:39 2024]  Iteration number: 0 with current cost as 0.14829497434213593 and parameters 
[-2.49771559  2.2374346  -2.12427964 -0.11653101  0.55388706 -2.77010899
  3.06858494  2.18960145  1.18551998 -1.06648312  0.60271512  1.14432443
  1.31029899 -1.87354676]. 
Training complete taking 844.5618448257446 seconds. 
Discarding model... 

Training complete taking 21269.09815788269 total seconds. 
Now scoring model... 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Scoring complete taking 2.295520067214966 seconds. 
Saved predicted values as M-M-CZ_Efficient-CRX_predicted_values.csv
Model scores: {'MSE_train': (3.461421915439965,), 'R2_train': 0.5247207141562604, 'MAE_train': 1.4055384747159398, 'MSE_test': 15.279955877492728, 'R2_test': -0.5232688595878581, 'MAE_test': 2.617093485338324}. 
Saved model results as M-M-CZ_Efficient-CRX_results.json. 
