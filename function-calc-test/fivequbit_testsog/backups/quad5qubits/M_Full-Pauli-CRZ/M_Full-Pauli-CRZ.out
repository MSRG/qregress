/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/circuits/Encoders.py:12: RuntimeWarning: invalid value encountered in arcsin
  if np.isnan(np.arcsin(features[feature_idx])) or np.isnan(np.arccos(features[feature_idx])):
Loading dataset from /scratch/j/jacobsen/gjones/quad5qubits/quadratic_train.bin... 
Successfully loaded /scratch/j/jacobsen/gjones/quad5qubits/quadratic_train.bin into X and y data. 
Loading dataset from /scratch/j/jacobsen/gjones/quad5qubits/quadratic_test.bin... 
Successfully loaded /scratch/j/jacobsen/gjones/quad5qubits/quadratic_test.bin into X and y data. 
Training model with dataset /scratch/j/jacobsen/gjones/quad5qubits/quadratic_train.bin 
 at time Mon Apr  1 16:18:57 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Apr  1 16:19:09 2024]  Iteration number: 0 with current cost as 0.2945579755076184 and parameters 
[-2.66255238  2.23342181 -2.12572203 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.64429716  1.14432445
  1.06511119 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Apr  1 16:23:15 2024]  Iteration number: 0 with current cost as 0.36704351422165193 and parameters 
[-2.6991548   2.26637059 -2.12840634 -0.11653101  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18552    -1.06648308  0.67513688  1.14432446
  1.1039725  -1.87354679  0.72965081  2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Apr  1 16:27:55 2024]  Iteration number: 0 with current cost as 0.3310831103680938 and parameters 
[-2.62448912  2.28934466 -2.13155377 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18552    -1.06648308  0.67451669  1.14432446
  1.01056468 -1.87354679  0.72965081  2.8857842  -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Apr  1 16:32:01 2024]  Iteration number: 0 with current cost as 0.32729569619886867 and parameters 
[-2.63851801  2.26884444 -2.13078182 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18552    -1.06648308  0.6710005   1.14432446
  1.03403377 -1.87354679  0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136105]. 
Working on 1.0 fold... 
[Mon Apr  1 16:36:21 2024]  Iteration number: 0 with current cost as 0.40280800028468594 and parameters 
[-2.68432774  2.28019185 -2.12736399 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.69288021  1.14432446
  1.09048497 -1.87354679  0.72965081  2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1284.645174741745 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Apr  1 16:40:34 2024]  Iteration number: 0 with current cost as 0.2945579755076184 and parameters 
[-2.66255238  2.23342181 -2.12572203 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.64429716  1.14432445
  1.06511119 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Apr  1 16:44:34 2024]  Iteration number: 0 with current cost as 0.36704351422165193 and parameters 
[-2.6991548   2.26637059 -2.12840634 -0.11653101  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18552    -1.06648308  0.67513688  1.14432446
  1.1039725  -1.87354679  0.72965081  2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Apr  1 16:49:04 2024]  Iteration number: 0 with current cost as 0.3310831103680938 and parameters 
[-2.62448912  2.28934466 -2.13155377 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18552    -1.06648308  0.67451669  1.14432446
  1.01056468 -1.87354679  0.72965081  2.8857842  -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Apr  1 16:53:10 2024]  Iteration number: 0 with current cost as 0.32729569619886867 and parameters 
[-2.63851801  2.26884444 -2.13078182 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18552    -1.06648308  0.6710005   1.14432446
  1.03403377 -1.87354679  0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136105]. 
Working on 1.0 fold... 
[Mon Apr  1 16:57:31 2024]  Iteration number: 0 with current cost as 0.40280800028468594 and parameters 
[-2.68432774  2.28019185 -2.12736399 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.69288021  1.14432446
  1.09048497 -1.87354679  0.72965081  2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1269.4992232322693 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Apr  1 17:01:43 2024]  Iteration number: 0 with current cost as 0.2945579755076184 and parameters 
[-2.66255238  2.23342181 -2.12572203 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.64429716  1.14432445
  1.06511119 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Apr  1 17:05:43 2024]  Iteration number: 0 with current cost as 0.36704351422165193 and parameters 
[-2.6991548   2.26637059 -2.12840634 -0.11653101  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18552    -1.06648308  0.67513688  1.14432446
  1.1039725  -1.87354679  0.72965081  2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Apr  1 17:10:16 2024]  Iteration number: 0 with current cost as 0.3310831103680938 and parameters 
[-2.62448912  2.28934466 -2.13155377 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18552    -1.06648308  0.67451669  1.14432446
  1.01056468 -1.87354679  0.72965081  2.8857842  -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Apr  1 17:14:28 2024]  Iteration number: 0 with current cost as 0.32729569619886867 and parameters 
[-2.63851801  2.26884444 -2.13078182 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18552    -1.06648308  0.6710005   1.14432446
  1.03403377 -1.87354679  0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136105]. 
Working on 1.0 fold... 
[Mon Apr  1 17:18:46 2024]  Iteration number: 0 with current cost as 0.40280800028468594 and parameters 
[-2.68432774  2.28019185 -2.12736399 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.69288021  1.14432446
  1.09048497 -1.87354679  0.72965081  2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1276.1610181331635 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Mon Apr  1 17:23:00 2024]  Iteration number: 0 with current cost as 0.2945579755076184 and parameters 
[-2.66255238  2.23342181 -2.12572203 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.64429716  1.14432445
  1.06511119 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Apr  1 17:27:01 2024]  Iteration number: 0 with current cost as 0.36704351422165193 and parameters 
[-2.6991548   2.26637059 -2.12840634 -0.11653101  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18552    -1.06648308  0.67513688  1.14432446
  1.1039725  -1.87354679  0.72965081  2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Apr  1 17:31:36 2024]  Iteration number: 0 with current cost as 0.3310831103680938 and parameters 
[-2.62448912  2.28934466 -2.13155377 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18552    -1.06648308  0.67451669  1.14432446
  1.01056468 -1.87354679  0.72965081  2.8857842  -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Apr  1 17:35:43 2024]  Iteration number: 0 with current cost as 0.32729569619886867 and parameters 
[-2.63851801  2.26884444 -2.13078182 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18552    -1.06648308  0.6710005   1.14432446
  1.03403377 -1.87354679  0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136105]. 
Working on 1.0 fold... 
[Mon Apr  1 17:40:03 2024]  Iteration number: 0 with current cost as 0.40280800028468594 and parameters 
[-2.68432774  2.28019185 -2.12736399 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.69288021  1.14432446
  1.09048497 -1.87354679  0.72965081  2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1283.1325526237488 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Mon Apr  1 17:44:23 2024]  Iteration number: 0 with current cost as 0.2945579755076184 and parameters 
[-2.66255238  2.23342181 -2.12572203 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.64429716  1.14432445
  1.06511119 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Apr  1 17:48:24 2024]  Iteration number: 0 with current cost as 0.36704351422165193 and parameters 
[-2.6991548   2.26637059 -2.12840634 -0.11653101  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18552    -1.06648308  0.67513688  1.14432446
  1.1039725  -1.87354679  0.72965081  2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Apr  1 17:53:02 2024]  Iteration number: 0 with current cost as 0.3310831103680938 and parameters 
[-2.62448912  2.28934466 -2.13155377 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18552    -1.06648308  0.67451669  1.14432446
  1.01056468 -1.87354679  0.72965081  2.8857842  -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Apr  1 17:57:14 2024]  Iteration number: 0 with current cost as 0.32729569619886867 and parameters 
[-2.63851801  2.26884444 -2.13078182 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18552    -1.06648308  0.6710005   1.14432446
  1.03403377 -1.87354679  0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136105]. 
Working on 1.0 fold... 
[Mon Apr  1 18:01:36 2024]  Iteration number: 0 with current cost as 0.40280800028468594 and parameters 
[-2.68432774  2.28019185 -2.12736399 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.69288021  1.14432446
  1.09048497 -1.87354679  0.72965081  2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1286.1834456920624 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Apr  1 18:05:49 2024]  Iteration number: 0 with current cost as 0.2945579755076184 and parameters 
[-2.66255238  2.23342181 -2.12572203 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.64429716  1.14432445
  1.06511119 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Apr  1 18:09:48 2024]  Iteration number: 0 with current cost as 0.36704351422165193 and parameters 
[-2.6991548   2.26637059 -2.12840634 -0.11653101  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18552    -1.06648308  0.67513688  1.14432446
  1.1039725  -1.87354679  0.72965081  2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Apr  1 18:14:21 2024]  Iteration number: 0 with current cost as 0.3310831103680938 and parameters 
[-2.62448912  2.28934466 -2.13155377 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18552    -1.06648308  0.67451669  1.14432446
  1.01056468 -1.87354679  0.72965081  2.8857842  -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Apr  1 18:18:28 2024]  Iteration number: 0 with current cost as 0.32729569619886867 and parameters 
[-2.63851801  2.26884444 -2.13078182 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18552    -1.06648308  0.6710005   1.14432446
  1.03403377 -1.87354679  0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136105]. 
Working on 1.0 fold... 
[Mon Apr  1 18:22:46 2024]  Iteration number: 0 with current cost as 0.40280800028468594 and parameters 
[-2.68432774  2.28019185 -2.12736399 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.69288021  1.14432446
  1.09048497 -1.87354679  0.72965081  2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1268.0349655151367 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Apr  1 18:26:58 2024]  Iteration number: 0 with current cost as 0.2945579755076184 and parameters 
[-2.66255238  2.23342181 -2.12572203 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.64429716  1.14432445
  1.06511119 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Apr  1 18:30:56 2024]  Iteration number: 0 with current cost as 0.36704351422165193 and parameters 
[-2.6991548   2.26637059 -2.12840634 -0.11653101  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18552    -1.06648308  0.67513688  1.14432446
  1.1039725  -1.87354679  0.72965081  2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Apr  1 18:35:29 2024]  Iteration number: 0 with current cost as 0.3310831103680938 and parameters 
[-2.62448912  2.28934466 -2.13155377 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18552    -1.06648308  0.67451669  1.14432446
  1.01056468 -1.87354679  0.72965081  2.8857842  -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Apr  1 18:39:36 2024]  Iteration number: 0 with current cost as 0.32729569619886867 and parameters 
[-2.63851801  2.26884444 -2.13078182 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18552    -1.06648308  0.6710005   1.14432446
  1.03403377 -1.87354679  0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136105]. 
Working on 1.0 fold... 
[Mon Apr  1 18:43:56 2024]  Iteration number: 0 with current cost as 0.40280800028468594 and parameters 
[-2.68432774  2.28019185 -2.12736399 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.69288021  1.14432446
  1.09048497 -1.87354679  0.72965081  2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1274.2445905208588 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Apr  1 18:48:11 2024]  Iteration number: 0 with current cost as 0.2945579755076184 and parameters 
[-2.66255238  2.23342181 -2.12572203 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.64429716  1.14432445
  1.06511119 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Apr  1 18:52:11 2024]  Iteration number: 0 with current cost as 0.36704351422165193 and parameters 
[-2.6991548   2.26637059 -2.12840634 -0.11653101  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18552    -1.06648308  0.67513688  1.14432446
  1.1039725  -1.87354679  0.72965081  2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Apr  1 18:56:46 2024]  Iteration number: 0 with current cost as 0.3310831103680938 and parameters 
[-2.62448912  2.28934466 -2.13155377 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18552    -1.06648308  0.67451669  1.14432446
  1.01056468 -1.87354679  0.72965081  2.8857842  -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Apr  1 19:00:53 2024]  Iteration number: 0 with current cost as 0.32729569619886867 and parameters 
[-2.63851801  2.26884444 -2.13078182 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18552    -1.06648308  0.6710005   1.14432446
  1.03403377 -1.87354679  0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136105]. 
Working on 1.0 fold... 
[Mon Apr  1 19:05:12 2024]  Iteration number: 0 with current cost as 0.40280800028468594 and parameters 
[-2.68432774  2.28019185 -2.12736399 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.69288021  1.14432446
  1.09048497 -1.87354679  0.72965081  2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1273.5578286647797 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Mon Apr  1 19:09:24 2024]  Iteration number: 0 with current cost as 0.2945579755076184 and parameters 
[-2.66255238  2.23342181 -2.12572203 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.64429716  1.14432445
  1.06511119 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Apr  1 19:13:22 2024]  Iteration number: 0 with current cost as 0.36704351422165193 and parameters 
[-2.6991548   2.26637059 -2.12840634 -0.11653101  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18552    -1.06648308  0.67513688  1.14432446
  1.1039725  -1.87354679  0.72965081  2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Apr  1 19:17:51 2024]  Iteration number: 0 with current cost as 0.3310831103680938 and parameters 
[-2.62448912  2.28934466 -2.13155377 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18552    -1.06648308  0.67451669  1.14432446
  1.01056468 -1.87354679  0.72965081  2.8857842  -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Apr  1 19:21:57 2024]  Iteration number: 0 with current cost as 0.32729569619886867 and parameters 
[-2.63851801  2.26884444 -2.13078182 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18552    -1.06648308  0.6710005   1.14432446
  1.03403377 -1.87354679  0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136105]. 
Working on 1.0 fold... 
[Mon Apr  1 19:26:18 2024]  Iteration number: 0 with current cost as 0.40280800028468594 and parameters 
[-2.68432774  2.28019185 -2.12736399 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.69288021  1.14432446
  1.09048497 -1.87354679  0.72965081  2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1264.5061600208282 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Mon Apr  1 19:30:30 2024]  Iteration number: 0 with current cost as 0.2945579755076184 and parameters 
[-2.66255238  2.23342181 -2.12572203 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.64429716  1.14432445
  1.06511119 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Apr  1 19:34:28 2024]  Iteration number: 0 with current cost as 0.36704351422165193 and parameters 
[-2.6991548   2.26637059 -2.12840634 -0.11653101  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18552    -1.06648308  0.67513688  1.14432446
  1.1039725  -1.87354679  0.72965081  2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Apr  1 19:38:56 2024]  Iteration number: 0 with current cost as 0.3310831103680938 and parameters 
[-2.62448912  2.28934466 -2.13155377 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18552    -1.06648308  0.67451669  1.14432446
  1.01056468 -1.87354679  0.72965081  2.8857842  -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Apr  1 19:43:02 2024]  Iteration number: 0 with current cost as 0.32729569619886867 and parameters 
[-2.63851801  2.26884444 -2.13078182 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18552    -1.06648308  0.6710005   1.14432446
  1.03403377 -1.87354679  0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136105]. 
Working on 1.0 fold... 
[Mon Apr  1 19:47:19 2024]  Iteration number: 0 with current cost as 0.40280800028468594 and parameters 
[-2.68432774  2.28019185 -2.12736399 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.69288021  1.14432446
  1.09048497 -1.87354679  0.72965081  2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1263.0312185287476 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Apr  1 19:51:33 2024]  Iteration number: 0 with current cost as 0.2945579755076184 and parameters 
[-2.66255238  2.23342181 -2.12572203 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.64429716  1.14432445
  1.06511119 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Apr  1 19:55:33 2024]  Iteration number: 0 with current cost as 0.36704351422165193 and parameters 
[-2.6991548   2.26637059 -2.12840634 -0.11653101  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18552    -1.06648308  0.67513688  1.14432446
  1.1039725  -1.87354679  0.72965081  2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Apr  1 20:00:04 2024]  Iteration number: 0 with current cost as 0.3310831103680938 and parameters 
[-2.62448912  2.28934466 -2.13155377 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18552    -1.06648308  0.67451669  1.14432446
  1.01056468 -1.87354679  0.72965081  2.8857842  -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Apr  1 20:04:11 2024]  Iteration number: 0 with current cost as 0.32729569619886867 and parameters 
[-2.63851801  2.26884444 -2.13078182 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18552    -1.06648308  0.6710005   1.14432446
  1.03403377 -1.87354679  0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136105]. 
Working on 1.0 fold... 
[Mon Apr  1 20:08:29 2024]  Iteration number: 0 with current cost as 0.40280800028468594 and parameters 
[-2.68432774  2.28019185 -2.12736399 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.69288021  1.14432446
  1.09048497 -1.87354679  0.72965081  2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1271.5755050182343 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Apr  1 20:12:44 2024]  Iteration number: 0 with current cost as 0.2945579755076184 and parameters 
[-2.66255238  2.23342181 -2.12572203 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.64429716  1.14432445
  1.06511119 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Apr  1 20:16:43 2024]  Iteration number: 0 with current cost as 0.36704351422165193 and parameters 
[-2.6991548   2.26637059 -2.12840634 -0.11653101  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18552    -1.06648308  0.67513688  1.14432446
  1.1039725  -1.87354679  0.72965081  2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Apr  1 20:21:19 2024]  Iteration number: 0 with current cost as 0.3310831103680938 and parameters 
[-2.62448912  2.28934466 -2.13155377 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18552    -1.06648308  0.67451669  1.14432446
  1.01056468 -1.87354679  0.72965081  2.8857842  -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Apr  1 20:25:27 2024]  Iteration number: 0 with current cost as 0.32729569619886867 and parameters 
[-2.63851801  2.26884444 -2.13078182 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18552    -1.06648308  0.6710005   1.14432446
  1.03403377 -1.87354679  0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136105]. 
Working on 1.0 fold... 
[Mon Apr  1 20:29:45 2024]  Iteration number: 0 with current cost as 0.40280800028468594 and parameters 
[-2.68432774  2.28019185 -2.12736399 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.69288021  1.14432446
  1.09048497 -1.87354679  0.72965081  2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1278.5556962490082 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Apr  1 20:34:03 2024]  Iteration number: 0 with current cost as 0.2945579755076184 and parameters 
[-2.66255238  2.23342181 -2.12572203 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.64429716  1.14432445
  1.06511119 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Apr  1 20:38:02 2024]  Iteration number: 0 with current cost as 0.36704351422165193 and parameters 
[-2.6991548   2.26637059 -2.12840634 -0.11653101  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18552    -1.06648308  0.67513688  1.14432446
  1.1039725  -1.87354679  0.72965081  2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Apr  1 20:42:35 2024]  Iteration number: 0 with current cost as 0.3310831103680938 and parameters 
[-2.62448912  2.28934466 -2.13155377 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18552    -1.06648308  0.67451669  1.14432446
  1.01056468 -1.87354679  0.72965081  2.8857842  -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Apr  1 20:46:40 2024]  Iteration number: 0 with current cost as 0.32729569619886867 and parameters 
[-2.63851801  2.26884444 -2.13078182 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18552    -1.06648308  0.6710005   1.14432446
  1.03403377 -1.87354679  0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136105]. 
Working on 1.0 fold... 
[Mon Apr  1 20:50:56 2024]  Iteration number: 0 with current cost as 0.40280800028468594 and parameters 
[-2.68432774  2.28019185 -2.12736399 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.69288021  1.14432446
  1.09048497 -1.87354679  0.72965081  2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1266.7539229393005 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Apr  1 20:55:10 2024]  Iteration number: 0 with current cost as 0.2945579755076184 and parameters 
[-2.66255238  2.23342181 -2.12572203 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.64429716  1.14432445
  1.06511119 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Apr  1 20:59:09 2024]  Iteration number: 0 with current cost as 0.36704351422165193 and parameters 
[-2.6991548   2.26637059 -2.12840634 -0.11653101  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18552    -1.06648308  0.67513688  1.14432446
  1.1039725  -1.87354679  0.72965081  2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Apr  1 21:03:47 2024]  Iteration number: 0 with current cost as 0.3310831103680938 and parameters 
[-2.62448912  2.28934466 -2.13155377 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18552    -1.06648308  0.67451669  1.14432446
  1.01056468 -1.87354679  0.72965081  2.8857842  -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Apr  1 21:07:53 2024]  Iteration number: 0 with current cost as 0.32729569619886867 and parameters 
[-2.63851801  2.26884444 -2.13078182 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18552    -1.06648308  0.6710005   1.14432446
  1.03403377 -1.87354679  0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136105]. 
Working on 1.0 fold... 
[Mon Apr  1 21:12:15 2024]  Iteration number: 0 with current cost as 0.40280800028468594 and parameters 
[-2.68432774  2.28019185 -2.12736399 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.69288021  1.14432446
  1.09048497 -1.87354679  0.72965081  2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1279.5215184688568 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Apr  1 21:16:28 2024]  Iteration number: 0 with current cost as 0.2945579755076184 and parameters 
[-2.66255238  2.23342181 -2.12572203 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.64429716  1.14432445
  1.06511119 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Apr  1 21:20:27 2024]  Iteration number: 0 with current cost as 0.36704351422165193 and parameters 
[-2.6991548   2.26637059 -2.12840634 -0.11653101  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18552    -1.06648308  0.67513688  1.14432446
  1.1039725  -1.87354679  0.72965081  2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Apr  1 21:25:00 2024]  Iteration number: 0 with current cost as 0.3310831103680938 and parameters 
[-2.62448912  2.28934466 -2.13155377 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18552    -1.06648308  0.67451669  1.14432446
  1.01056468 -1.87354679  0.72965081  2.8857842  -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Apr  1 21:29:05 2024]  Iteration number: 0 with current cost as 0.32729569619886867 and parameters 
[-2.63851801  2.26884444 -2.13078182 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18552    -1.06648308  0.6710005   1.14432446
  1.03403377 -1.87354679  0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136105]. 
Working on 1.0 fold... 
[Mon Apr  1 21:33:25 2024]  Iteration number: 0 with current cost as 0.40280800028468594 and parameters 
[-2.68432774  2.28019185 -2.12736399 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.69288021  1.14432446
  1.09048497 -1.87354679  0.72965081  2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1268.89541554451 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Apr  1 21:37:38 2024]  Iteration number: 0 with current cost as 0.2945579755076184 and parameters 
[-2.66255238  2.23342181 -2.12572203 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.64429716  1.14432445
  1.06511119 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Apr  1 21:41:38 2024]  Iteration number: 0 with current cost as 0.36704351422165193 and parameters 
[-2.6991548   2.26637059 -2.12840634 -0.11653101  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18552    -1.06648308  0.67513688  1.14432446
  1.1039725  -1.87354679  0.72965081  2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Apr  1 21:46:09 2024]  Iteration number: 0 with current cost as 0.3310831103680938 and parameters 
[-2.62448912  2.28934466 -2.13155377 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18552    -1.06648308  0.67451669  1.14432446
  1.01056468 -1.87354679  0.72965081  2.8857842  -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Apr  1 21:50:13 2024]  Iteration number: 0 with current cost as 0.32729569619886867 and parameters 
[-2.63851801  2.26884444 -2.13078182 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18552    -1.06648308  0.6710005   1.14432446
  1.03403377 -1.87354679  0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136105]. 
Working on 1.0 fold... 
[Mon Apr  1 21:54:33 2024]  Iteration number: 0 with current cost as 0.40280800028468594 and parameters 
[-2.68432774  2.28019185 -2.12736399 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.69288021  1.14432446
  1.09048497 -1.87354679  0.72965081  2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1269.7429072856903 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Apr  1 21:58:48 2024]  Iteration number: 0 with current cost as 0.2945579755076184 and parameters 
[-2.66255238  2.23342181 -2.12572203 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.64429716  1.14432445
  1.06511119 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Apr  1 22:02:49 2024]  Iteration number: 0 with current cost as 0.36704351422165193 and parameters 
[-2.6991548   2.26637059 -2.12840634 -0.11653101  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18552    -1.06648308  0.67513688  1.14432446
  1.1039725  -1.87354679  0.72965081  2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Apr  1 22:07:21 2024]  Iteration number: 0 with current cost as 0.3310831103680938 and parameters 
[-2.62448912  2.28934466 -2.13155377 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18552    -1.06648308  0.67451669  1.14432446
  1.01056468 -1.87354679  0.72965081  2.8857842  -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Apr  1 22:11:28 2024]  Iteration number: 0 with current cost as 0.32729569619886867 and parameters 
[-2.63851801  2.26884444 -2.13078182 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18552    -1.06648308  0.6710005   1.14432446
  1.03403377 -1.87354679  0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136105]. 
Working on 1.0 fold... 
[Mon Apr  1 22:15:48 2024]  Iteration number: 0 with current cost as 0.40280800028468594 and parameters 
[-2.68432774  2.28019185 -2.12736399 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.69288021  1.14432446
  1.09048497 -1.87354679  0.72965081  2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1272.7244112491608 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Apr  1 22:20:00 2024]  Iteration number: 0 with current cost as 0.2945579755076184 and parameters 
[-2.66255238  2.23342181 -2.12572203 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.64429716  1.14432445
  1.06511119 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Apr  1 22:24:01 2024]  Iteration number: 0 with current cost as 0.36704351422165193 and parameters 
[-2.6991548   2.26637059 -2.12840634 -0.11653101  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18552    -1.06648308  0.67513688  1.14432446
  1.1039725  -1.87354679  0.72965081  2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Apr  1 22:28:31 2024]  Iteration number: 0 with current cost as 0.3310831103680938 and parameters 
[-2.62448912  2.28934466 -2.13155377 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18552    -1.06648308  0.67451669  1.14432446
  1.01056468 -1.87354679  0.72965081  2.8857842  -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Apr  1 22:32:35 2024]  Iteration number: 0 with current cost as 0.32729569619886867 and parameters 
[-2.63851801  2.26884444 -2.13078182 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18552    -1.06648308  0.6710005   1.14432446
  1.03403377 -1.87354679  0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136105]. 
Working on 1.0 fold... 
[Mon Apr  1 22:36:54 2024]  Iteration number: 0 with current cost as 0.40280800028468594 and parameters 
[-2.68432774  2.28019185 -2.12736399 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.69288021  1.14432446
  1.09048497 -1.87354679  0.72965081  2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1266.5387444496155 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Apr  1 22:41:06 2024]  Iteration number: 0 with current cost as 0.2945579755076184 and parameters 
[-2.66255238  2.23342181 -2.12572203 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.64429716  1.14432445
  1.06511119 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Apr  1 22:45:08 2024]  Iteration number: 0 with current cost as 0.36704351422165193 and parameters 
[-2.6991548   2.26637059 -2.12840634 -0.11653101  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18552    -1.06648308  0.67513688  1.14432446
  1.1039725  -1.87354679  0.72965081  2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Apr  1 22:49:38 2024]  Iteration number: 0 with current cost as 0.3310831103680938 and parameters 
[-2.62448912  2.28934466 -2.13155377 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18552    -1.06648308  0.67451669  1.14432446
  1.01056468 -1.87354679  0.72965081  2.8857842  -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Apr  1 22:53:46 2024]  Iteration number: 0 with current cost as 0.32729569619886867 and parameters 
[-2.63851801  2.26884444 -2.13078182 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18552    -1.06648308  0.6710005   1.14432446
  1.03403377 -1.87354679  0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136105]. 
Working on 1.0 fold... 
[Mon Apr  1 22:58:03 2024]  Iteration number: 0 with current cost as 0.40280800028468594 and parameters 
[-2.68432774  2.28019185 -2.12736399 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.69288021  1.14432446
  1.09048497 -1.87354679  0.72965081  2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1269.2388718128204 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Apr  1 23:02:16 2024]  Iteration number: 0 with current cost as 0.2945579755076184 and parameters 
[-2.66255238  2.23342181 -2.12572203 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.64429716  1.14432445
  1.06511119 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Apr  1 23:06:16 2024]  Iteration number: 0 with current cost as 0.36704351422165193 and parameters 
[-2.6991548   2.26637059 -2.12840634 -0.11653101  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18552    -1.06648308  0.67513688  1.14432446
  1.1039725  -1.87354679  0.72965081  2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Apr  1 23:10:47 2024]  Iteration number: 0 with current cost as 0.3310831103680938 and parameters 
[-2.62448912  2.28934466 -2.13155377 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18552    -1.06648308  0.67451669  1.14432446
  1.01056468 -1.87354679  0.72965081  2.8857842  -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Apr  1 23:14:53 2024]  Iteration number: 0 with current cost as 0.32729569619886867 and parameters 
[-2.63851801  2.26884444 -2.13078182 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18552    -1.06648308  0.6710005   1.14432446
  1.03403377 -1.87354679  0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136105]. 
Working on 1.0 fold... 
[Mon Apr  1 23:19:13 2024]  Iteration number: 0 with current cost as 0.40280800028468594 and parameters 
[-2.68432774  2.28019185 -2.12736399 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.69288021  1.14432446
  1.09048497 -1.87354679  0.72965081  2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1271.233095407486 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Apr  1 23:23:27 2024]  Iteration number: 0 with current cost as 0.2945579755076184 and parameters 
[-2.66255238  2.23342181 -2.12572203 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.64429716  1.14432445
  1.06511119 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Apr  1 23:27:24 2024]  Iteration number: 0 with current cost as 0.36704351422165193 and parameters 
[-2.6991548   2.26637059 -2.12840634 -0.11653101  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18552    -1.06648308  0.67513688  1.14432446
  1.1039725  -1.87354679  0.72965081  2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Apr  1 23:31:54 2024]  Iteration number: 0 with current cost as 0.3310831103680938 and parameters 
[-2.62448912  2.28934466 -2.13155377 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18552    -1.06648308  0.67451669  1.14432446
  1.01056468 -1.87354679  0.72965081  2.8857842  -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Apr  1 23:36:01 2024]  Iteration number: 0 with current cost as 0.32729569619886867 and parameters 
[-2.63851801  2.26884444 -2.13078182 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18552    -1.06648308  0.6710005   1.14432446
  1.03403377 -1.87354679  0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136105]. 
Working on 1.0 fold... 
[Mon Apr  1 23:40:20 2024]  Iteration number: 0 with current cost as 0.40280800028468594 and parameters 
[-2.68432774  2.28019185 -2.12736399 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.69288021  1.14432446
  1.09048497 -1.87354679  0.72965081  2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1265.5142414569855 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Apr  1 23:44:32 2024]  Iteration number: 0 with current cost as 0.2945579755076184 and parameters 
[-2.66255238  2.23342181 -2.12572203 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.64429716  1.14432445
  1.06511119 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Apr  1 23:48:32 2024]  Iteration number: 0 with current cost as 0.36704351422165193 and parameters 
[-2.6991548   2.26637059 -2.12840634 -0.11653101  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18552    -1.06648308  0.67513688  1.14432446
  1.1039725  -1.87354679  0.72965081  2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Apr  1 23:53:03 2024]  Iteration number: 0 with current cost as 0.3310831103680938 and parameters 
[-2.62448912  2.28934466 -2.13155377 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18552    -1.06648308  0.67451669  1.14432446
  1.01056468 -1.87354679  0.72965081  2.8857842  -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Apr  1 23:57:09 2024]  Iteration number: 0 with current cost as 0.32729569619886867 and parameters 
[-2.63851801  2.26884444 -2.13078182 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18552    -1.06648308  0.6710005   1.14432446
  1.03403377 -1.87354679  0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136105]. 
Working on 1.0 fold... 
[Tue Apr  2 00:01:28 2024]  Iteration number: 0 with current cost as 0.40280800028468594 and parameters 
[-2.68432774  2.28019185 -2.12736399 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.69288021  1.14432446
  1.09048497 -1.87354679  0.72965081  2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1268.6435086727142 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Tue Apr  2 00:05:41 2024]  Iteration number: 0 with current cost as 0.2945579755076184 and parameters 
[-2.66255238  2.23342181 -2.12572203 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.64429716  1.14432445
  1.06511119 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Tue Apr  2 00:09:40 2024]  Iteration number: 0 with current cost as 0.36704351422165193 and parameters 
[-2.6991548   2.26637059 -2.12840634 -0.11653101  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18552    -1.06648308  0.67513688  1.14432446
  1.1039725  -1.87354679  0.72965081  2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Tue Apr  2 00:14:13 2024]  Iteration number: 0 with current cost as 0.3310831103680938 and parameters 
[-2.62448912  2.28934466 -2.13155377 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18552    -1.06648308  0.67451669  1.14432446
  1.01056468 -1.87354679  0.72965081  2.8857842  -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Tue Apr  2 00:18:26 2024]  Iteration number: 0 with current cost as 0.32729569619886867 and parameters 
[-2.63851801  2.26884444 -2.13078182 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18552    -1.06648308  0.6710005   1.14432446
  1.03403377 -1.87354679  0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136105]. 
Working on 1.0 fold... 
[Tue Apr  2 00:22:48 2024]  Iteration number: 0 with current cost as 0.40280800028468594 and parameters 
[-2.68432774  2.28019185 -2.12736399 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.69288021  1.14432446
  1.09048497 -1.87354679  0.72965081  2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1278.5710849761963 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Tue Apr  2 00:26:59 2024]  Iteration number: 0 with current cost as 0.2945579755076184 and parameters 
[-2.66255238  2.23342181 -2.12572203 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.64429716  1.14432445
  1.06511119 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Tue Apr  2 00:30:59 2024]  Iteration number: 0 with current cost as 0.36704351422165193 and parameters 
[-2.6991548   2.26637059 -2.12840634 -0.11653101  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18552    -1.06648308  0.67513688  1.14432446
  1.1039725  -1.87354679  0.72965081  2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Tue Apr  2 00:35:34 2024]  Iteration number: 0 with current cost as 0.3310831103680938 and parameters 
[-2.62448912  2.28934466 -2.13155377 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18552    -1.06648308  0.67451669  1.14432446
  1.01056468 -1.87354679  0.72965081  2.8857842  -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Tue Apr  2 00:39:39 2024]  Iteration number: 0 with current cost as 0.32729569619886867 and parameters 
[-2.63851801  2.26884444 -2.13078182 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18552    -1.06648308  0.6710005   1.14432446
  1.03403377 -1.87354679  0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136105]. 
Working on 1.0 fold... 
[Tue Apr  2 00:43:59 2024]  Iteration number: 0 with current cost as 0.40280800028468594 and parameters 
[-2.68432774  2.28019185 -2.12736399 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.69288021  1.14432446
  1.09048497 -1.87354679  0.72965081  2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1270.3437504768372 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Tue Apr  2 00:48:10 2024]  Iteration number: 0 with current cost as 0.2945579755076184 and parameters 
[-2.66255238  2.23342181 -2.12572203 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.64429716  1.14432445
  1.06511119 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Tue Apr  2 00:52:08 2024]  Iteration number: 0 with current cost as 0.36704351422165193 and parameters 
[-2.6991548   2.26637059 -2.12840634 -0.11653101  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18552    -1.06648308  0.67513688  1.14432446
  1.1039725  -1.87354679  0.72965081  2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Tue Apr  2 00:56:40 2024]  Iteration number: 0 with current cost as 0.3310831103680938 and parameters 
[-2.62448912  2.28934466 -2.13155377 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18552    -1.06648308  0.67451669  1.14432446
  1.01056468 -1.87354679  0.72965081  2.8857842  -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Tue Apr  2 01:00:54 2024]  Iteration number: 0 with current cost as 0.32729569619886867 and parameters 
[-2.63851801  2.26884444 -2.13078182 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18552    -1.06648308  0.6710005   1.14432446
  1.03403377 -1.87354679  0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136105]. 
Working on 1.0 fold... 
[Tue Apr  2 01:05:16 2024]  Iteration number: 0 with current cost as 0.40280800028468594 and parameters 
[-2.68432774  2.28019185 -2.12736399 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.69288021  1.14432446
  1.09048497 -1.87354679  0.72965081  2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1276.2468378543854 seconds. 
Discarding model... 

Training complete taking 31817.09809422493 total seconds. 
Now scoring model... 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Scoring complete taking 0.7687520980834961 seconds. 
Saved predicted values as M_Full-Pauli-CRZ_predicted_values.csv
Model scores: {'MSE_train': (2.403321827657704,), 'R2_train': 0.6700058213629705, 'MAE_train': 1.1777487944963017, 'MSE_test': 11.984952528769451, 'R2_test': -0.19478780678972196, 'MAE_test': 2.3105578359017915}. 
Saved model results as M_Full-Pauli-CRZ_results.json. 
