/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/circuits/Encoders.py:12: RuntimeWarning: invalid value encountered in arcsin
  if np.isnan(np.arcsin(features[feature_idx])) or np.isnan(np.arccos(features[feature_idx])):
Loading dataset from /scratch/j/jacobsen/gjones/quad5qubits/quadratic_train.bin... 
Successfully loaded /scratch/j/jacobsen/gjones/quad5qubits/quadratic_train.bin into X and y data. 
Loading dataset from /scratch/j/jacobsen/gjones/quad5qubits/quadratic_test.bin... 
Successfully loaded /scratch/j/jacobsen/gjones/quad5qubits/quadratic_test.bin into X and y data. 
Training model with dataset /scratch/j/jacobsen/gjones/quad5qubits/quadratic_train.bin 
 at time Mon Apr  1 15:44:18 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Apr  1 15:49:12 2024]  Iteration number: 0 with current cost as 0.16568026486602191 and parameters 
[-2.63993171  2.23743459 -2.12427964 -0.11653103  0.55388703 -2.77010902
  3.06858496  2.18960148  1.18551996 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654245  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136107
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Apr  1 15:58:47 2024]  Iteration number: 0 with current cost as 0.17313270197007372 and parameters 
[-2.62073802  2.23743464 -2.12427964 -0.11653103  0.55388704 -2.77010901
  3.06858496  2.18960145  1.18551998 -1.06648312  0.6027151   1.14432443
  1.31029899 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512662  2.83077103 -1.26456712 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Apr  1 16:08:12 2024]  Iteration number: 0 with current cost as 0.1675375044816612 and parameters 
[-2.62635382  2.23743461 -2.12427961 -0.11653105  0.55388705 -2.770109
  3.06858498  2.18960148  1.18551998 -1.06648308  0.60271513  1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654243  0.72897372  1.60512661  2.83077104 -1.26456712 -0.25136105
 -2.39279215 -2.27309774  3.13337152  2.54856956 -0.67550787 -2.69002204]. 
Working on 0.8 fold... 
[Mon Apr  1 16:17:48 2024]  Iteration number: 0 with current cost as 0.13156835156500912 and parameters 
[-2.57114679  2.23743464 -2.12427964 -0.11653101  0.55388708 -2.77010899
  3.068585    2.18960147  1.18552    -1.06648308  0.60271512  1.14432447
  1.310299   -1.87354676  0.72965082  2.88578419 -0.54534333 -0.47522483
 -2.0265424   0.72897371  1.60512664  2.83077105 -1.2645671  -0.25136103
 -2.39279218 -2.27309774  3.13337155  2.5485696  -0.67550785 -2.69002202]. 
Working on 1.0 fold... 
[Mon Apr  1 16:26:30 2024]  Iteration number: 0 with current cost as 0.1482949744651309 and parameters 
[-2.4977156   2.23743464 -2.1242796  -0.11653103  0.55388708 -2.77010897
  3.068585    2.18960149  1.18552002 -1.06648308  0.60271514  1.14432447
  1.31029899 -1.87354676  0.72965084  2.88578423 -0.54534331 -0.47522481
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136101
 -2.39279216 -2.27309774  3.13337157  2.5485696  -0.67550783 -2.69002202]. 
Training complete taking 2757.648596048355 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Apr  1 16:35:06 2024]  Iteration number: 0 with current cost as 0.16568026486602191 and parameters 
[-2.63993171  2.23743459 -2.12427964 -0.11653103  0.55388703 -2.77010902
  3.06858496  2.18960148  1.18551996 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654245  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136107
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Apr  1 16:44:35 2024]  Iteration number: 0 with current cost as 0.17313270197007372 and parameters 
[-2.62073802  2.23743464 -2.12427964 -0.11653103  0.55388704 -2.77010901
  3.06858496  2.18960145  1.18551998 -1.06648312  0.6027151   1.14432443
  1.31029899 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512662  2.83077103 -1.26456712 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Apr  1 16:54:11 2024]  Iteration number: 0 with current cost as 0.1675375044816612 and parameters 
[-2.62635382  2.23743461 -2.12427961 -0.11653105  0.55388705 -2.770109
  3.06858498  2.18960148  1.18551998 -1.06648308  0.60271513  1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654243  0.72897372  1.60512661  2.83077104 -1.26456712 -0.25136105
 -2.39279215 -2.27309774  3.13337152  2.54856956 -0.67550787 -2.69002204]. 
Working on 0.8 fold... 
[Mon Apr  1 17:03:45 2024]  Iteration number: 0 with current cost as 0.13156835156500912 and parameters 
[-2.57114679  2.23743464 -2.12427964 -0.11653101  0.55388708 -2.77010899
  3.068585    2.18960147  1.18552    -1.06648308  0.60271512  1.14432447
  1.310299   -1.87354676  0.72965082  2.88578419 -0.54534333 -0.47522483
 -2.0265424   0.72897371  1.60512664  2.83077105 -1.2645671  -0.25136103
 -2.39279218 -2.27309774  3.13337155  2.5485696  -0.67550785 -2.69002202]. 
Working on 1.0 fold... 
[Mon Apr  1 17:12:18 2024]  Iteration number: 0 with current cost as 0.1482949744651309 and parameters 
[-2.4977156   2.23743464 -2.1242796  -0.11653103  0.55388708 -2.77010897
  3.068585    2.18960149  1.18552002 -1.06648308  0.60271514  1.14432447
  1.31029899 -1.87354676  0.72965084  2.88578423 -0.54534331 -0.47522481
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136101
 -2.39279216 -2.27309774  3.13337157  2.5485696  -0.67550783 -2.69002202]. 
Training complete taking 2754.6946177482605 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Apr  1 17:20:53 2024]  Iteration number: 0 with current cost as 0.16568026486602191 and parameters 
[-2.63993171  2.23743459 -2.12427964 -0.11653103  0.55388703 -2.77010902
  3.06858496  2.18960148  1.18551996 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654245  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136107
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Apr  1 17:30:40 2024]  Iteration number: 0 with current cost as 0.17313270197007372 and parameters 
[-2.62073802  2.23743464 -2.12427964 -0.11653103  0.55388704 -2.77010901
  3.06858496  2.18960145  1.18551998 -1.06648312  0.6027151   1.14432443
  1.31029899 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512662  2.83077103 -1.26456712 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Apr  1 17:40:19 2024]  Iteration number: 0 with current cost as 0.1675375044816612 and parameters 
[-2.62635382  2.23743461 -2.12427961 -0.11653105  0.55388705 -2.770109
  3.06858498  2.18960148  1.18551998 -1.06648308  0.60271513  1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654243  0.72897372  1.60512661  2.83077104 -1.26456712 -0.25136105
 -2.39279215 -2.27309774  3.13337152  2.54856956 -0.67550787 -2.69002204]. 
Working on 0.8 fold... 
[Mon Apr  1 17:49:56 2024]  Iteration number: 0 with current cost as 0.13156835156500912 and parameters 
[-2.57114679  2.23743464 -2.12427964 -0.11653101  0.55388708 -2.77010899
  3.068585    2.18960147  1.18552    -1.06648308  0.60271512  1.14432447
  1.310299   -1.87354676  0.72965082  2.88578419 -0.54534333 -0.47522483
 -2.0265424   0.72897371  1.60512664  2.83077105 -1.2645671  -0.25136103
 -2.39279218 -2.27309774  3.13337155  2.5485696  -0.67550785 -2.69002202]. 
Working on 1.0 fold... 
[Mon Apr  1 17:58:39 2024]  Iteration number: 0 with current cost as 0.1482949744651309 and parameters 
[-2.4977156   2.23743464 -2.1242796  -0.11653103  0.55388708 -2.77010897
  3.068585    2.18960149  1.18552002 -1.06648308  0.60271514  1.14432447
  1.31029899 -1.87354676  0.72965084  2.88578423 -0.54534331 -0.47522481
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136101
 -2.39279216 -2.27309774  3.13337157  2.5485696  -0.67550783 -2.69002202]. 
Training complete taking 2797.871974468231 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Mon Apr  1 18:07:36 2024]  Iteration number: 0 with current cost as 0.16568026486602191 and parameters 
[-2.63993171  2.23743459 -2.12427964 -0.11653103  0.55388703 -2.77010902
  3.06858496  2.18960148  1.18551996 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654245  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136107
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Apr  1 18:17:22 2024]  Iteration number: 0 with current cost as 0.17313270197007372 and parameters 
[-2.62073802  2.23743464 -2.12427964 -0.11653103  0.55388704 -2.77010901
  3.06858496  2.18960145  1.18551998 -1.06648312  0.6027151   1.14432443
  1.31029899 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512662  2.83077103 -1.26456712 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Apr  1 18:26:56 2024]  Iteration number: 0 with current cost as 0.1675375044816612 and parameters 
[-2.62635382  2.23743461 -2.12427961 -0.11653105  0.55388705 -2.770109
  3.06858498  2.18960148  1.18551998 -1.06648308  0.60271513  1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654243  0.72897372  1.60512661  2.83077104 -1.26456712 -0.25136105
 -2.39279215 -2.27309774  3.13337152  2.54856956 -0.67550787 -2.69002204]. 
Working on 0.8 fold... 
[Mon Apr  1 18:36:40 2024]  Iteration number: 0 with current cost as 0.13156835156500912 and parameters 
[-2.57114679  2.23743464 -2.12427964 -0.11653101  0.55388708 -2.77010899
  3.068585    2.18960147  1.18552    -1.06648308  0.60271512  1.14432447
  1.310299   -1.87354676  0.72965082  2.88578419 -0.54534333 -0.47522483
 -2.0265424   0.72897371  1.60512664  2.83077105 -1.2645671  -0.25136103
 -2.39279218 -2.27309774  3.13337155  2.5485696  -0.67550785 -2.69002202]. 
Working on 1.0 fold... 
[Mon Apr  1 18:45:25 2024]  Iteration number: 0 with current cost as 0.1482949744651309 and parameters 
[-2.4977156   2.23743464 -2.1242796  -0.11653103  0.55388708 -2.77010897
  3.068585    2.18960149  1.18552002 -1.06648308  0.60271514  1.14432447
  1.31029899 -1.87354676  0.72965084  2.88578423 -0.54534331 -0.47522481
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136101
 -2.39279216 -2.27309774  3.13337157  2.5485696  -0.67550783 -2.69002202]. 
Training complete taking 2790.9459636211395 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Mon Apr  1 18:54:09 2024]  Iteration number: 0 with current cost as 0.16568026486602191 and parameters 
[-2.63993171  2.23743459 -2.12427964 -0.11653103  0.55388703 -2.77010902
  3.06858496  2.18960148  1.18551996 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654245  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136107
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Apr  1 19:03:34 2024]  Iteration number: 0 with current cost as 0.17313270197007372 and parameters 
[-2.62073802  2.23743464 -2.12427964 -0.11653103  0.55388704 -2.77010901
  3.06858496  2.18960145  1.18551998 -1.06648312  0.6027151   1.14432443
  1.31029899 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512662  2.83077103 -1.26456712 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Apr  1 19:13:03 2024]  Iteration number: 0 with current cost as 0.1675375044816612 and parameters 
[-2.62635382  2.23743461 -2.12427961 -0.11653105  0.55388705 -2.770109
  3.06858498  2.18960148  1.18551998 -1.06648308  0.60271513  1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654243  0.72897372  1.60512661  2.83077104 -1.26456712 -0.25136105
 -2.39279215 -2.27309774  3.13337152  2.54856956 -0.67550787 -2.69002204]. 
Working on 0.8 fold... 
[Mon Apr  1 19:22:32 2024]  Iteration number: 0 with current cost as 0.13156835156500912 and parameters 
[-2.57114679  2.23743464 -2.12427964 -0.11653101  0.55388708 -2.77010899
  3.068585    2.18960147  1.18552    -1.06648308  0.60271512  1.14432447
  1.310299   -1.87354676  0.72965082  2.88578419 -0.54534333 -0.47522483
 -2.0265424   0.72897371  1.60512664  2.83077105 -1.2645671  -0.25136103
 -2.39279218 -2.27309774  3.13337155  2.5485696  -0.67550785 -2.69002202]. 
Working on 1.0 fold... 
[Mon Apr  1 19:31:22 2024]  Iteration number: 0 with current cost as 0.1482949744651309 and parameters 
[-2.4977156   2.23743464 -2.1242796  -0.11653103  0.55388708 -2.77010897
  3.068585    2.18960149  1.18552002 -1.06648308  0.60271514  1.14432447
  1.31029899 -1.87354676  0.72965084  2.88578423 -0.54534331 -0.47522481
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136101
 -2.39279216 -2.27309774  3.13337157  2.5485696  -0.67550783 -2.69002202]. 
Training complete taking 2748.300915002823 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Apr  1 19:39:49 2024]  Iteration number: 0 with current cost as 0.16568026486602191 and parameters 
[-2.63993171  2.23743459 -2.12427964 -0.11653103  0.55388703 -2.77010902
  3.06858496  2.18960148  1.18551996 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654245  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136107
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Apr  1 19:49:21 2024]  Iteration number: 0 with current cost as 0.17313270197007372 and parameters 
[-2.62073802  2.23743464 -2.12427964 -0.11653103  0.55388704 -2.77010901
  3.06858496  2.18960145  1.18551998 -1.06648312  0.6027151   1.14432443
  1.31029899 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512662  2.83077103 -1.26456712 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Apr  1 19:58:57 2024]  Iteration number: 0 with current cost as 0.1675375044816612 and parameters 
[-2.62635382  2.23743461 -2.12427961 -0.11653105  0.55388705 -2.770109
  3.06858498  2.18960148  1.18551998 -1.06648308  0.60271513  1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654243  0.72897372  1.60512661  2.83077104 -1.26456712 -0.25136105
 -2.39279215 -2.27309774  3.13337152  2.54856956 -0.67550787 -2.69002204]. 
Working on 0.8 fold... 
[Mon Apr  1 20:08:40 2024]  Iteration number: 0 with current cost as 0.13156835156500912 and parameters 
[-2.57114679  2.23743464 -2.12427964 -0.11653101  0.55388708 -2.77010899
  3.068585    2.18960147  1.18552    -1.06648308  0.60271512  1.14432447
  1.310299   -1.87354676  0.72965082  2.88578419 -0.54534333 -0.47522483
 -2.0265424   0.72897371  1.60512664  2.83077105 -1.2645671  -0.25136103
 -2.39279218 -2.27309774  3.13337155  2.5485696  -0.67550785 -2.69002202]. 
Working on 1.0 fold... 
[Mon Apr  1 20:17:13 2024]  Iteration number: 0 with current cost as 0.1482949744651309 and parameters 
[-2.4977156   2.23743464 -2.1242796  -0.11653103  0.55388708 -2.77010897
  3.068585    2.18960149  1.18552002 -1.06648308  0.60271514  1.14432447
  1.31029899 -1.87354676  0.72965084  2.88578423 -0.54534331 -0.47522481
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136101
 -2.39279216 -2.27309774  3.13337157  2.5485696  -0.67550783 -2.69002202]. 
Training complete taking 2751.0632236003876 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Apr  1 20:25:46 2024]  Iteration number: 0 with current cost as 0.16568026486602191 and parameters 
[-2.63993171  2.23743459 -2.12427964 -0.11653103  0.55388703 -2.77010902
  3.06858496  2.18960148  1.18551996 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654245  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136107
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Apr  1 20:35:27 2024]  Iteration number: 0 with current cost as 0.17313270197007372 and parameters 
[-2.62073802  2.23743464 -2.12427964 -0.11653103  0.55388704 -2.77010901
  3.06858496  2.18960145  1.18551998 -1.06648312  0.6027151   1.14432443
  1.31029899 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512662  2.83077103 -1.26456712 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Apr  1 20:45:07 2024]  Iteration number: 0 with current cost as 0.1675375044816612 and parameters 
[-2.62635382  2.23743461 -2.12427961 -0.11653105  0.55388705 -2.770109
  3.06858498  2.18960148  1.18551998 -1.06648308  0.60271513  1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654243  0.72897372  1.60512661  2.83077104 -1.26456712 -0.25136105
 -2.39279215 -2.27309774  3.13337152  2.54856956 -0.67550787 -2.69002204]. 
Working on 0.8 fold... 
[Mon Apr  1 20:54:55 2024]  Iteration number: 0 with current cost as 0.13156835156500912 and parameters 
[-2.57114679  2.23743464 -2.12427964 -0.11653101  0.55388708 -2.77010899
  3.068585    2.18960147  1.18552    -1.06648308  0.60271512  1.14432447
  1.310299   -1.87354676  0.72965082  2.88578419 -0.54534333 -0.47522483
 -2.0265424   0.72897371  1.60512664  2.83077105 -1.2645671  -0.25136103
 -2.39279218 -2.27309774  3.13337155  2.5485696  -0.67550785 -2.69002202]. 
Working on 1.0 fold... 
[Mon Apr  1 21:03:42 2024]  Iteration number: 0 with current cost as 0.1482949744651309 and parameters 
[-2.4977156   2.23743464 -2.1242796  -0.11653103  0.55388708 -2.77010897
  3.068585    2.18960149  1.18552002 -1.06648308  0.60271514  1.14432447
  1.31029899 -1.87354676  0.72965084  2.88578423 -0.54534331 -0.47522481
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136101
 -2.39279216 -2.27309774  3.13337157  2.5485696  -0.67550783 -2.69002202]. 
Training complete taking 2811.246817588806 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Apr  1 21:12:36 2024]  Iteration number: 0 with current cost as 0.16568026486602191 and parameters 
[-2.63993171  2.23743459 -2.12427964 -0.11653103  0.55388703 -2.77010902
  3.06858496  2.18960148  1.18551996 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654245  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136107
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Apr  1 21:22:09 2024]  Iteration number: 0 with current cost as 0.17313270197007372 and parameters 
[-2.62073802  2.23743464 -2.12427964 -0.11653103  0.55388704 -2.77010901
  3.06858496  2.18960145  1.18551998 -1.06648312  0.6027151   1.14432443
  1.31029899 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512662  2.83077103 -1.26456712 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Apr  1 21:31:53 2024]  Iteration number: 0 with current cost as 0.1675375044816612 and parameters 
[-2.62635382  2.23743461 -2.12427961 -0.11653105  0.55388705 -2.770109
  3.06858498  2.18960148  1.18551998 -1.06648308  0.60271513  1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654243  0.72897372  1.60512661  2.83077104 -1.26456712 -0.25136105
 -2.39279215 -2.27309774  3.13337152  2.54856956 -0.67550787 -2.69002204]. 
Working on 0.8 fold... 
[Mon Apr  1 21:41:31 2024]  Iteration number: 0 with current cost as 0.13156835156500912 and parameters 
[-2.57114679  2.23743464 -2.12427964 -0.11653101  0.55388708 -2.77010899
  3.068585    2.18960147  1.18552    -1.06648308  0.60271512  1.14432447
  1.310299   -1.87354676  0.72965082  2.88578419 -0.54534333 -0.47522483
 -2.0265424   0.72897371  1.60512664  2.83077105 -1.2645671  -0.25136103
 -2.39279218 -2.27309774  3.13337155  2.5485696  -0.67550785 -2.69002202]. 
Working on 1.0 fold... 
[Mon Apr  1 21:50:17 2024]  Iteration number: 0 with current cost as 0.1482949744651309 and parameters 
[-2.4977156   2.23743464 -2.1242796  -0.11653103  0.55388708 -2.77010897
  3.068585    2.18960149  1.18552002 -1.06648308  0.60271514  1.14432447
  1.31029899 -1.87354676  0.72965084  2.88578423 -0.54534331 -0.47522481
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136101
 -2.39279216 -2.27309774  3.13337157  2.5485696  -0.67550783 -2.69002202]. 
Training complete taking 2776.9513697624207 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Mon Apr  1 21:58:55 2024]  Iteration number: 0 with current cost as 0.16568026486602191 and parameters 
[-2.63993171  2.23743459 -2.12427964 -0.11653103  0.55388703 -2.77010902
  3.06858496  2.18960148  1.18551996 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654245  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136107
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Apr  1 22:08:39 2024]  Iteration number: 0 with current cost as 0.17313270197007372 and parameters 
[-2.62073802  2.23743464 -2.12427964 -0.11653103  0.55388704 -2.77010901
  3.06858496  2.18960145  1.18551998 -1.06648312  0.6027151   1.14432443
  1.31029899 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512662  2.83077103 -1.26456712 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Apr  1 22:18:31 2024]  Iteration number: 0 with current cost as 0.1675375044816612 and parameters 
[-2.62635382  2.23743461 -2.12427961 -0.11653105  0.55388705 -2.770109
  3.06858498  2.18960148  1.18551998 -1.06648308  0.60271513  1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654243  0.72897372  1.60512661  2.83077104 -1.26456712 -0.25136105
 -2.39279215 -2.27309774  3.13337152  2.54856956 -0.67550787 -2.69002204]. 
Working on 0.8 fold... 
[Mon Apr  1 22:28:51 2024]  Iteration number: 0 with current cost as 0.13156835156500912 and parameters 
[-2.57114679  2.23743464 -2.12427964 -0.11653101  0.55388708 -2.77010899
  3.068585    2.18960147  1.18552    -1.06648308  0.60271512  1.14432447
  1.310299   -1.87354676  0.72965082  2.88578419 -0.54534333 -0.47522483
 -2.0265424   0.72897371  1.60512664  2.83077105 -1.2645671  -0.25136103
 -2.39279218 -2.27309774  3.13337155  2.5485696  -0.67550785 -2.69002202]. 
Working on 1.0 fold... 
[Mon Apr  1 22:37:51 2024]  Iteration number: 0 with current cost as 0.1482949744651309 and parameters 
[-2.4977156   2.23743464 -2.1242796  -0.11653103  0.55388708 -2.77010897
  3.068585    2.18960149  1.18552002 -1.06648308  0.60271514  1.14432447
  1.31029899 -1.87354676  0.72965084  2.88578423 -0.54534331 -0.47522481
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136101
 -2.39279216 -2.27309774  3.13337157  2.5485696  -0.67550783 -2.69002202]. 
Training complete taking 2869.783595085144 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Mon Apr  1 22:46:59 2024]  Iteration number: 0 with current cost as 0.16568026486602191 and parameters 
[-2.63993171  2.23743459 -2.12427964 -0.11653103  0.55388703 -2.77010902
  3.06858496  2.18960148  1.18551996 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654245  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136107
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Apr  1 22:57:21 2024]  Iteration number: 0 with current cost as 0.17313270197007372 and parameters 
[-2.62073802  2.23743464 -2.12427964 -0.11653103  0.55388704 -2.77010901
  3.06858496  2.18960145  1.18551998 -1.06648312  0.6027151   1.14432443
  1.31029899 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512662  2.83077103 -1.26456712 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Apr  1 23:07:17 2024]  Iteration number: 0 with current cost as 0.1675375044816612 and parameters 
[-2.62635382  2.23743461 -2.12427961 -0.11653105  0.55388705 -2.770109
  3.06858498  2.18960148  1.18551998 -1.06648308  0.60271513  1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654243  0.72897372  1.60512661  2.83077104 -1.26456712 -0.25136105
 -2.39279215 -2.27309774  3.13337152  2.54856956 -0.67550787 -2.69002204]. 
Working on 0.8 fold... 
[Mon Apr  1 23:17:11 2024]  Iteration number: 0 with current cost as 0.13156835156500912 and parameters 
[-2.57114679  2.23743464 -2.12427964 -0.11653101  0.55388708 -2.77010899
  3.068585    2.18960147  1.18552    -1.06648308  0.60271512  1.14432447
  1.310299   -1.87354676  0.72965082  2.88578419 -0.54534333 -0.47522483
 -2.0265424   0.72897371  1.60512664  2.83077105 -1.2645671  -0.25136103
 -2.39279218 -2.27309774  3.13337155  2.5485696  -0.67550785 -2.69002202]. 
Working on 1.0 fold... 
[Mon Apr  1 23:26:02 2024]  Iteration number: 0 with current cost as 0.1482949744651309 and parameters 
[-2.4977156   2.23743464 -2.1242796  -0.11653103  0.55388708 -2.77010897
  3.068585    2.18960149  1.18552002 -1.06648308  0.60271514  1.14432447
  1.31029899 -1.87354676  0.72965084  2.88578423 -0.54534331 -0.47522481
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136101
 -2.39279216 -2.27309774  3.13337157  2.5485696  -0.67550783 -2.69002202]. 
Training complete taking 2881.5959770679474 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Apr  1 23:35:00 2024]  Iteration number: 0 with current cost as 0.16568026486602191 and parameters 
[-2.63993171  2.23743459 -2.12427964 -0.11653103  0.55388703 -2.77010902
  3.06858496  2.18960148  1.18551996 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654245  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136107
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Apr  1 23:44:39 2024]  Iteration number: 0 with current cost as 0.17313270197007372 and parameters 
[-2.62073802  2.23743464 -2.12427964 -0.11653103  0.55388704 -2.77010901
  3.06858496  2.18960145  1.18551998 -1.06648312  0.6027151   1.14432443
  1.31029899 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512662  2.83077103 -1.26456712 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Apr  1 23:54:23 2024]  Iteration number: 0 with current cost as 0.1675375044816612 and parameters 
[-2.62635382  2.23743461 -2.12427961 -0.11653105  0.55388705 -2.770109
  3.06858498  2.18960148  1.18551998 -1.06648308  0.60271513  1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654243  0.72897372  1.60512661  2.83077104 -1.26456712 -0.25136105
 -2.39279215 -2.27309774  3.13337152  2.54856956 -0.67550787 -2.69002204]. 
Working on 0.8 fold... 
[Tue Apr  2 00:04:13 2024]  Iteration number: 0 with current cost as 0.13156835156500912 and parameters 
[-2.57114679  2.23743464 -2.12427964 -0.11653101  0.55388708 -2.77010899
  3.068585    2.18960147  1.18552    -1.06648308  0.60271512  1.14432447
  1.310299   -1.87354676  0.72965082  2.88578419 -0.54534333 -0.47522483
 -2.0265424   0.72897371  1.60512664  2.83077105 -1.2645671  -0.25136103
 -2.39279218 -2.27309774  3.13337155  2.5485696  -0.67550785 -2.69002202]. 
Working on 1.0 fold... 
[Tue Apr  2 00:13:02 2024]  Iteration number: 0 with current cost as 0.1482949744651309 and parameters 
[-2.4977156   2.23743464 -2.1242796  -0.11653103  0.55388708 -2.77010897
  3.068585    2.18960149  1.18552002 -1.06648308  0.60271514  1.14432447
  1.31029899 -1.87354676  0.72965084  2.88578423 -0.54534331 -0.47522481
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136101
 -2.39279216 -2.27309774  3.13337157  2.5485696  -0.67550783 -2.69002202]. 
Training complete taking 2822.123044729233 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Tue Apr  2 00:22:04 2024]  Iteration number: 0 with current cost as 0.16568026486602191 and parameters 
[-2.63993171  2.23743459 -2.12427964 -0.11653103  0.55388703 -2.77010902
  3.06858496  2.18960148  1.18551996 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654245  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136107
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Tue Apr  2 00:32:18 2024]  Iteration number: 0 with current cost as 0.17313270197007372 and parameters 
[-2.62073802  2.23743464 -2.12427964 -0.11653103  0.55388704 -2.77010901
  3.06858496  2.18960145  1.18551998 -1.06648312  0.6027151   1.14432443
  1.31029899 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512662  2.83077103 -1.26456712 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Tue Apr  2 00:42:28 2024]  Iteration number: 0 with current cost as 0.1675375044816612 and parameters 
[-2.62635382  2.23743461 -2.12427961 -0.11653105  0.55388705 -2.770109
  3.06858498  2.18960148  1.18551998 -1.06648308  0.60271513  1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654243  0.72897372  1.60512661  2.83077104 -1.26456712 -0.25136105
 -2.39279215 -2.27309774  3.13337152  2.54856956 -0.67550787 -2.69002204]. 
Working on 0.8 fold... 
[Tue Apr  2 00:52:32 2024]  Iteration number: 0 with current cost as 0.13156835156500912 and parameters 
[-2.57114679  2.23743464 -2.12427964 -0.11653101  0.55388708 -2.77010899
  3.068585    2.18960147  1.18552    -1.06648308  0.60271512  1.14432447
  1.310299   -1.87354676  0.72965082  2.88578419 -0.54534333 -0.47522483
 -2.0265424   0.72897371  1.60512664  2.83077105 -1.2645671  -0.25136103
 -2.39279218 -2.27309774  3.13337155  2.5485696  -0.67550785 -2.69002202]. 
Working on 1.0 fold... 
[Tue Apr  2 01:01:41 2024]  Iteration number: 0 with current cost as 0.1482949744651309 and parameters 
[-2.4977156   2.23743464 -2.1242796  -0.11653103  0.55388708 -2.77010897
  3.068585    2.18960149  1.18552002 -1.06648308  0.60271514  1.14432447
  1.31029899 -1.87354676  0.72965084  2.88578423 -0.54534331 -0.47522481
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136101
 -2.39279216 -2.27309774  3.13337157  2.5485696  -0.67550783 -2.69002202]. 
Training complete taking 2919.0578536987305 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Tue Apr  2 01:10:38 2024]  Iteration number: 0 with current cost as 0.16568026486602191 and parameters 
[-2.63993171  2.23743459 -2.12427964 -0.11653103  0.55388703 -2.77010902
  3.06858496  2.18960148  1.18551996 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654245  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136107
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Tue Apr  2 01:20:41 2024]  Iteration number: 0 with current cost as 0.17313270197007372 and parameters 
[-2.62073802  2.23743464 -2.12427964 -0.11653103  0.55388704 -2.77010901
  3.06858496  2.18960145  1.18551998 -1.06648312  0.6027151   1.14432443
  1.31029899 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512662  2.83077103 -1.26456712 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Tue Apr  2 01:30:11 2024]  Iteration number: 0 with current cost as 0.1675375044816612 and parameters 
[-2.62635382  2.23743461 -2.12427961 -0.11653105  0.55388705 -2.770109
  3.06858498  2.18960148  1.18551998 -1.06648308  0.60271513  1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654243  0.72897372  1.60512661  2.83077104 -1.26456712 -0.25136105
 -2.39279215 -2.27309774  3.13337152  2.54856956 -0.67550787 -2.69002204]. 
Working on 0.8 fold... 
[Tue Apr  2 01:39:35 2024]  Iteration number: 0 with current cost as 0.13156835156500912 and parameters 
[-2.57114679  2.23743464 -2.12427964 -0.11653101  0.55388708 -2.77010899
  3.068585    2.18960147  1.18552    -1.06648308  0.60271512  1.14432447
  1.310299   -1.87354676  0.72965082  2.88578419 -0.54534333 -0.47522483
 -2.0265424   0.72897371  1.60512664  2.83077105 -1.2645671  -0.25136103
 -2.39279218 -2.27309774  3.13337155  2.5485696  -0.67550785 -2.69002202]. 
Working on 1.0 fold... 
[Tue Apr  2 01:48:05 2024]  Iteration number: 0 with current cost as 0.1482949744651309 and parameters 
[-2.4977156   2.23743464 -2.1242796  -0.11653103  0.55388708 -2.77010897
  3.068585    2.18960149  1.18552002 -1.06648308  0.60271514  1.14432447
  1.31029899 -1.87354676  0.72965084  2.88578423 -0.54534331 -0.47522481
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136101
 -2.39279216 -2.27309774  3.13337157  2.5485696  -0.67550783 -2.69002202]. 
Training complete taking 2770.846022605896 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Tue Apr  2 01:56:32 2024]  Iteration number: 0 with current cost as 0.16568026486602191 and parameters 
[-2.63993171  2.23743459 -2.12427964 -0.11653103  0.55388703 -2.77010902
  3.06858496  2.18960148  1.18551996 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654245  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136107
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Tue Apr  2 02:05:51 2024]  Iteration number: 0 with current cost as 0.17313270197007372 and parameters 
[-2.62073802  2.23743464 -2.12427964 -0.11653103  0.55388704 -2.77010901
  3.06858496  2.18960145  1.18551998 -1.06648312  0.6027151   1.14432443
  1.31029899 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512662  2.83077103 -1.26456712 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Tue Apr  2 02:15:03 2024]  Iteration number: 0 with current cost as 0.1675375044816612 and parameters 
[-2.62635382  2.23743461 -2.12427961 -0.11653105  0.55388705 -2.770109
  3.06858498  2.18960148  1.18551998 -1.06648308  0.60271513  1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654243  0.72897372  1.60512661  2.83077104 -1.26456712 -0.25136105
 -2.39279215 -2.27309774  3.13337152  2.54856956 -0.67550787 -2.69002204]. 
Working on 0.8 fold... 
[Tue Apr  2 02:24:27 2024]  Iteration number: 0 with current cost as 0.13156835156500912 and parameters 
[-2.57114679  2.23743464 -2.12427964 -0.11653101  0.55388708 -2.77010899
  3.068585    2.18960147  1.18552    -1.06648308  0.60271512  1.14432447
  1.310299   -1.87354676  0.72965082  2.88578419 -0.54534333 -0.47522483
 -2.0265424   0.72897371  1.60512664  2.83077105 -1.2645671  -0.25136103
 -2.39279218 -2.27309774  3.13337155  2.5485696  -0.67550785 -2.69002202]. 
Working on 1.0 fold... 
[Tue Apr  2 02:33:01 2024]  Iteration number: 0 with current cost as 0.1482949744651309 and parameters 
[-2.4977156   2.23743464 -2.1242796  -0.11653103  0.55388708 -2.77010897
  3.068585    2.18960149  1.18552002 -1.06648308  0.60271514  1.14432447
  1.31029899 -1.87354676  0.72965084  2.88578423 -0.54534331 -0.47522481
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136101
 -2.39279216 -2.27309774  3.13337157  2.5485696  -0.67550783 -2.69002202]. 
Training complete taking 2695.002954006195 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Tue Apr  2 02:41:23 2024]  Iteration number: 0 with current cost as 0.16568026486602191 and parameters 
[-2.63993171  2.23743459 -2.12427964 -0.11653103  0.55388703 -2.77010902
  3.06858496  2.18960148  1.18551996 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654245  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136107
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Tue Apr  2 02:50:48 2024]  Iteration number: 0 with current cost as 0.17313270197007372 and parameters 
[-2.62073802  2.23743464 -2.12427964 -0.11653103  0.55388704 -2.77010901
  3.06858496  2.18960145  1.18551998 -1.06648312  0.6027151   1.14432443
  1.31029899 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512662  2.83077103 -1.26456712 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Tue Apr  2 03:00:17 2024]  Iteration number: 0 with current cost as 0.1675375044816612 and parameters 
[-2.62635382  2.23743461 -2.12427961 -0.11653105  0.55388705 -2.770109
  3.06858498  2.18960148  1.18551998 -1.06648308  0.60271513  1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654243  0.72897372  1.60512661  2.83077104 -1.26456712 -0.25136105
 -2.39279215 -2.27309774  3.13337152  2.54856956 -0.67550787 -2.69002204]. 
Working on 0.8 fold... 
[Tue Apr  2 03:09:33 2024]  Iteration number: 0 with current cost as 0.13156835156500912 and parameters 
[-2.57114679  2.23743464 -2.12427964 -0.11653101  0.55388708 -2.77010899
  3.068585    2.18960147  1.18552    -1.06648308  0.60271512  1.14432447
  1.310299   -1.87354676  0.72965082  2.88578419 -0.54534333 -0.47522483
 -2.0265424   0.72897371  1.60512664  2.83077105 -1.2645671  -0.25136103
 -2.39279218 -2.27309774  3.13337155  2.5485696  -0.67550785 -2.69002202]. 
Working on 1.0 fold... 
[Tue Apr  2 03:17:52 2024]  Iteration number: 0 with current cost as 0.1482949744651309 and parameters 
[-2.4977156   2.23743464 -2.1242796  -0.11653103  0.55388708 -2.77010897
  3.068585    2.18960149  1.18552002 -1.06648308  0.60271514  1.14432447
  1.31029899 -1.87354676  0.72965084  2.88578423 -0.54534331 -0.47522481
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136101
 -2.39279216 -2.27309774  3.13337157  2.5485696  -0.67550783 -2.69002202]. 
Training complete taking 2691.321140766144 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Tue Apr  2 03:26:14 2024]  Iteration number: 0 with current cost as 0.16568026486602191 and parameters 
[-2.63993171  2.23743459 -2.12427964 -0.11653103  0.55388703 -2.77010902
  3.06858496  2.18960148  1.18551996 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654245  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136107
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Tue Apr  2 03:35:28 2024]  Iteration number: 0 with current cost as 0.17313270197007372 and parameters 
[-2.62073802  2.23743464 -2.12427964 -0.11653103  0.55388704 -2.77010901
  3.06858496  2.18960145  1.18551998 -1.06648312  0.6027151   1.14432443
  1.31029899 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512662  2.83077103 -1.26456712 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Tue Apr  2 03:44:45 2024]  Iteration number: 0 with current cost as 0.1675375044816612 and parameters 
[-2.62635382  2.23743461 -2.12427961 -0.11653105  0.55388705 -2.770109
  3.06858498  2.18960148  1.18551998 -1.06648308  0.60271513  1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654243  0.72897372  1.60512661  2.83077104 -1.26456712 -0.25136105
 -2.39279215 -2.27309774  3.13337152  2.54856956 -0.67550787 -2.69002204]. 
Working on 0.8 fold... 
[Tue Apr  2 03:54:04 2024]  Iteration number: 0 with current cost as 0.13156835156500912 and parameters 
[-2.57114679  2.23743464 -2.12427964 -0.11653101  0.55388708 -2.77010899
  3.068585    2.18960147  1.18552    -1.06648308  0.60271512  1.14432447
  1.310299   -1.87354676  0.72965082  2.88578419 -0.54534333 -0.47522483
 -2.0265424   0.72897371  1.60512664  2.83077105 -1.2645671  -0.25136103
 -2.39279218 -2.27309774  3.13337155  2.5485696  -0.67550785 -2.69002202]. 
Working on 1.0 fold... 
[Tue Apr  2 04:02:33 2024]  Iteration number: 0 with current cost as 0.1482949744651309 and parameters 
[-2.4977156   2.23743464 -2.1242796  -0.11653103  0.55388708 -2.77010897
  3.068585    2.18960149  1.18552002 -1.06648308  0.60271514  1.14432447
  1.31029899 -1.87354676  0.72965084  2.88578423 -0.54534331 -0.47522481
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136101
 -2.39279216 -2.27309774  3.13337157  2.5485696  -0.67550783 -2.69002202]. 
Training complete taking 2685.149647951126 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Tue Apr  2 04:11:02 2024]  Iteration number: 0 with current cost as 0.16568026486602191 and parameters 
[-2.63993171  2.23743459 -2.12427964 -0.11653103  0.55388703 -2.77010902
  3.06858496  2.18960148  1.18551996 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654245  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136107
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Tue Apr  2 04:20:19 2024]  Iteration number: 0 with current cost as 0.17313270197007372 and parameters 
[-2.62073802  2.23743464 -2.12427964 -0.11653103  0.55388704 -2.77010901
  3.06858496  2.18960145  1.18551998 -1.06648312  0.6027151   1.14432443
  1.31029899 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512662  2.83077103 -1.26456712 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Tue Apr  2 04:29:50 2024]  Iteration number: 0 with current cost as 0.1675375044816612 and parameters 
[-2.62635382  2.23743461 -2.12427961 -0.11653105  0.55388705 -2.770109
  3.06858498  2.18960148  1.18551998 -1.06648308  0.60271513  1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654243  0.72897372  1.60512661  2.83077104 -1.26456712 -0.25136105
 -2.39279215 -2.27309774  3.13337152  2.54856956 -0.67550787 -2.69002204]. 
Working on 0.8 fold... 
[Tue Apr  2 04:39:12 2024]  Iteration number: 0 with current cost as 0.13156835156500912 and parameters 
[-2.57114679  2.23743464 -2.12427964 -0.11653101  0.55388708 -2.77010899
  3.068585    2.18960147  1.18552    -1.06648308  0.60271512  1.14432447
  1.310299   -1.87354676  0.72965082  2.88578419 -0.54534333 -0.47522483
 -2.0265424   0.72897371  1.60512664  2.83077105 -1.2645671  -0.25136103
 -2.39279218 -2.27309774  3.13337155  2.5485696  -0.67550785 -2.69002202]. 
Working on 1.0 fold... 
[Tue Apr  2 04:47:33 2024]  Iteration number: 0 with current cost as 0.1482949744651309 and parameters 
[-2.4977156   2.23743464 -2.1242796  -0.11653103  0.55388708 -2.77010897
  3.068585    2.18960149  1.18552002 -1.06648308  0.60271514  1.14432447
  1.31029899 -1.87354676  0.72965084  2.88578423 -0.54534331 -0.47522481
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136101
 -2.39279216 -2.27309774  3.13337157  2.5485696  -0.67550783 -2.69002202]. 
Training complete taking 2706.7149019241333 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Tue Apr  2 04:56:08 2024]  Iteration number: 0 with current cost as 0.16568026486602191 and parameters 
[-2.63993171  2.23743459 -2.12427964 -0.11653103  0.55388703 -2.77010902
  3.06858496  2.18960148  1.18551996 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654245  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136107
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Tue Apr  2 05:05:28 2024]  Iteration number: 0 with current cost as 0.17313270197007372 and parameters 
[-2.62073802  2.23743464 -2.12427964 -0.11653103  0.55388704 -2.77010901
  3.06858496  2.18960145  1.18551998 -1.06648312  0.6027151   1.14432443
  1.31029899 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512662  2.83077103 -1.26456712 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Tue Apr  2 05:15:36 2024]  Iteration number: 0 with current cost as 0.1675375044816612 and parameters 
[-2.62635382  2.23743461 -2.12427961 -0.11653105  0.55388705 -2.770109
  3.06858498  2.18960148  1.18551998 -1.06648308  0.60271513  1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654243  0.72897372  1.60512661  2.83077104 -1.26456712 -0.25136105
 -2.39279215 -2.27309774  3.13337152  2.54856956 -0.67550787 -2.69002204]. 
Working on 0.8 fold... 
[Tue Apr  2 05:25:37 2024]  Iteration number: 0 with current cost as 0.13156835156500912 and parameters 
[-2.57114679  2.23743464 -2.12427964 -0.11653101  0.55388708 -2.77010899
  3.068585    2.18960147  1.18552    -1.06648308  0.60271512  1.14432447
  1.310299   -1.87354676  0.72965082  2.88578419 -0.54534333 -0.47522483
 -2.0265424   0.72897371  1.60512664  2.83077105 -1.2645671  -0.25136103
 -2.39279218 -2.27309774  3.13337155  2.5485696  -0.67550785 -2.69002202]. 
Working on 1.0 fold... 
[Tue Apr  2 05:34:35 2024]  Iteration number: 0 with current cost as 0.1482949744651309 and parameters 
[-2.4977156   2.23743464 -2.1242796  -0.11653103  0.55388708 -2.77010897
  3.068585    2.18960149  1.18552002 -1.06648308  0.60271514  1.14432447
  1.31029899 -1.87354676  0.72965084  2.88578423 -0.54534331 -0.47522481
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136101
 -2.39279216 -2.27309774  3.13337157  2.5485696  -0.67550783 -2.69002202]. 
Training complete taking 2823.3385541439056 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Tue Apr  2 05:43:21 2024]  Iteration number: 0 with current cost as 0.16568026486602191 and parameters 
[-2.63993171  2.23743459 -2.12427964 -0.11653103  0.55388703 -2.77010902
  3.06858496  2.18960148  1.18551996 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654245  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136107
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Tue Apr  2 05:53:19 2024]  Iteration number: 0 with current cost as 0.17313270197007372 and parameters 
[-2.62073802  2.23743464 -2.12427964 -0.11653103  0.55388704 -2.77010901
  3.06858496  2.18960145  1.18551998 -1.06648312  0.6027151   1.14432443
  1.31029899 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512662  2.83077103 -1.26456712 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Tue Apr  2 06:03:13 2024]  Iteration number: 0 with current cost as 0.1675375044816612 and parameters 
[-2.62635382  2.23743461 -2.12427961 -0.11653105  0.55388705 -2.770109
  3.06858498  2.18960148  1.18551998 -1.06648308  0.60271513  1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654243  0.72897372  1.60512661  2.83077104 -1.26456712 -0.25136105
 -2.39279215 -2.27309774  3.13337152  2.54856956 -0.67550787 -2.69002204]. 
Working on 0.8 fold... 
[Tue Apr  2 06:13:01 2024]  Iteration number: 0 with current cost as 0.13156835156500912 and parameters 
[-2.57114679  2.23743464 -2.12427964 -0.11653101  0.55388708 -2.77010899
  3.068585    2.18960147  1.18552    -1.06648308  0.60271512  1.14432447
  1.310299   -1.87354676  0.72965082  2.88578419 -0.54534333 -0.47522483
 -2.0265424   0.72897371  1.60512664  2.83077105 -1.2645671  -0.25136103
 -2.39279218 -2.27309774  3.13337155  2.5485696  -0.67550785 -2.69002202]. 
Working on 1.0 fold... 
[Tue Apr  2 06:21:47 2024]  Iteration number: 0 with current cost as 0.1482949744651309 and parameters 
[-2.4977156   2.23743464 -2.1242796  -0.11653103  0.55388708 -2.77010897
  3.068585    2.18960149  1.18552002 -1.06648308  0.60271514  1.14432447
  1.31029899 -1.87354676  0.72965084  2.88578423 -0.54534331 -0.47522481
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136101
 -2.39279216 -2.27309774  3.13337157  2.5485696  -0.67550783 -2.69002202]. 
Training complete taking 2837.871302843094 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Tue Apr  2 06:30:51 2024]  Iteration number: 0 with current cost as 0.16568026486602191 and parameters 
[-2.63993171  2.23743459 -2.12427964 -0.11653103  0.55388703 -2.77010902
  3.06858496  2.18960148  1.18551996 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654245  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136107
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Tue Apr  2 06:40:37 2024]  Iteration number: 0 with current cost as 0.17313270197007372 and parameters 
[-2.62073802  2.23743464 -2.12427964 -0.11653103  0.55388704 -2.77010901
  3.06858496  2.18960145  1.18551998 -1.06648312  0.6027151   1.14432443
  1.31029899 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512662  2.83077103 -1.26456712 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Tue Apr  2 06:50:33 2024]  Iteration number: 0 with current cost as 0.1675375044816612 and parameters 
[-2.62635382  2.23743461 -2.12427961 -0.11653105  0.55388705 -2.770109
  3.06858498  2.18960148  1.18551998 -1.06648308  0.60271513  1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654243  0.72897372  1.60512661  2.83077104 -1.26456712 -0.25136105
 -2.39279215 -2.27309774  3.13337152  2.54856956 -0.67550787 -2.69002204]. 
Working on 0.8 fold... 
[Tue Apr  2 07:00:13 2024]  Iteration number: 0 with current cost as 0.13156835156500912 and parameters 
[-2.57114679  2.23743464 -2.12427964 -0.11653101  0.55388708 -2.77010899
  3.068585    2.18960147  1.18552    -1.06648308  0.60271512  1.14432447
  1.310299   -1.87354676  0.72965082  2.88578419 -0.54534333 -0.47522483
 -2.0265424   0.72897371  1.60512664  2.83077105 -1.2645671  -0.25136103
 -2.39279218 -2.27309774  3.13337155  2.5485696  -0.67550785 -2.69002202]. 
Working on 1.0 fold... 
[Tue Apr  2 07:09:01 2024]  Iteration number: 0 with current cost as 0.1482949744651309 and parameters 
[-2.4977156   2.23743464 -2.1242796  -0.11653103  0.55388708 -2.77010897
  3.068585    2.18960149  1.18552002 -1.06648308  0.60271514  1.14432447
  1.31029899 -1.87354676  0.72965084  2.88578423 -0.54534331 -0.47522481
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136101
 -2.39279216 -2.27309774  3.13337157  2.5485696  -0.67550783 -2.69002202]. 
Training complete taking 2829.2944564819336 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Tue Apr  2 07:18:01 2024]  Iteration number: 0 with current cost as 0.16568026486602191 and parameters 
[-2.63993171  2.23743459 -2.12427964 -0.11653103  0.55388703 -2.77010902
  3.06858496  2.18960148  1.18551996 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654245  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136107
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Tue Apr  2 07:27:48 2024]  Iteration number: 0 with current cost as 0.17313270197007372 and parameters 
[-2.62073802  2.23743464 -2.12427964 -0.11653103  0.55388704 -2.77010901
  3.06858496  2.18960145  1.18551998 -1.06648312  0.6027151   1.14432443
  1.31029899 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512662  2.83077103 -1.26456712 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Tue Apr  2 07:37:33 2024]  Iteration number: 0 with current cost as 0.1675375044816612 and parameters 
[-2.62635382  2.23743461 -2.12427961 -0.11653105  0.55388705 -2.770109
  3.06858498  2.18960148  1.18551998 -1.06648308  0.60271513  1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654243  0.72897372  1.60512661  2.83077104 -1.26456712 -0.25136105
 -2.39279215 -2.27309774  3.13337152  2.54856956 -0.67550787 -2.69002204]. 
Working on 0.8 fold... 
[Tue Apr  2 07:47:28 2024]  Iteration number: 0 with current cost as 0.13156835156500912 and parameters 
[-2.57114679  2.23743464 -2.12427964 -0.11653101  0.55388708 -2.77010899
  3.068585    2.18960147  1.18552    -1.06648308  0.60271512  1.14432447
  1.310299   -1.87354676  0.72965082  2.88578419 -0.54534333 -0.47522483
 -2.0265424   0.72897371  1.60512664  2.83077105 -1.2645671  -0.25136103
 -2.39279218 -2.27309774  3.13337155  2.5485696  -0.67550785 -2.69002202]. 
Working on 1.0 fold... 
[Tue Apr  2 07:56:18 2024]  Iteration number: 0 with current cost as 0.1482949744651309 and parameters 
[-2.4977156   2.23743464 -2.1242796  -0.11653103  0.55388708 -2.77010897
  3.068585    2.18960149  1.18552002 -1.06648308  0.60271514  1.14432447
  1.31029899 -1.87354676  0.72965084  2.88578423 -0.54534331 -0.47522481
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136101
 -2.39279216 -2.27309774  3.13337157  2.5485696  -0.67550783 -2.69002202]. 
Training complete taking 2842.016014814377 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Tue Apr  2 08:05:24 2024]  Iteration number: 0 with current cost as 0.16568026486602191 and parameters 
[-2.63993171  2.23743459 -2.12427964 -0.11653103  0.55388703 -2.77010902
  3.06858496  2.18960148  1.18551996 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654245  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136107
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Tue Apr  2 08:15:16 2024]  Iteration number: 0 with current cost as 0.17313270197007372 and parameters 
[-2.62073802  2.23743464 -2.12427964 -0.11653103  0.55388704 -2.77010901
  3.06858496  2.18960145  1.18551998 -1.06648312  0.6027151   1.14432443
  1.31029899 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512662  2.83077103 -1.26456712 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Tue Apr  2 08:25:05 2024]  Iteration number: 0 with current cost as 0.1675375044816612 and parameters 
[-2.62635382  2.23743461 -2.12427961 -0.11653105  0.55388705 -2.770109
  3.06858498  2.18960148  1.18551998 -1.06648308  0.60271513  1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654243  0.72897372  1.60512661  2.83077104 -1.26456712 -0.25136105
 -2.39279215 -2.27309774  3.13337152  2.54856956 -0.67550787 -2.69002204]. 
Working on 0.8 fold... 
[Tue Apr  2 08:34:46 2024]  Iteration number: 0 with current cost as 0.13156835156500912 and parameters 
[-2.57114679  2.23743464 -2.12427964 -0.11653101  0.55388708 -2.77010899
  3.068585    2.18960147  1.18552    -1.06648308  0.60271512  1.14432447
  1.310299   -1.87354676  0.72965082  2.88578419 -0.54534333 -0.47522483
 -2.0265424   0.72897371  1.60512664  2.83077105 -1.2645671  -0.25136103
 -2.39279218 -2.27309774  3.13337155  2.5485696  -0.67550785 -2.69002202]. 
Working on 1.0 fold... 
[Tue Apr  2 08:43:38 2024]  Iteration number: 0 with current cost as 0.1482949744651309 and parameters 
[-2.4977156   2.23743464 -2.1242796  -0.11653103  0.55388708 -2.77010897
  3.068585    2.18960149  1.18552002 -1.06648308  0.60271514  1.14432447
  1.31029899 -1.87354676  0.72965084  2.88578423 -0.54534331 -0.47522481
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136101
 -2.39279216 -2.27309774  3.13337157  2.5485696  -0.67550783 -2.69002202]. 
Training complete taking 2833.5639264583588 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Tue Apr  2 08:52:28 2024]  Iteration number: 0 with current cost as 0.16568026486602191 and parameters 
[-2.63993171  2.23743459 -2.12427964 -0.11653103  0.55388703 -2.77010902
  3.06858496  2.18960148  1.18551996 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654245  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136107
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Tue Apr  2 09:02:30 2024]  Iteration number: 0 with current cost as 0.17313270197007372 and parameters 
[-2.62073802  2.23743464 -2.12427964 -0.11653103  0.55388704 -2.77010901
  3.06858496  2.18960145  1.18551998 -1.06648312  0.6027151   1.14432443
  1.31029899 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512662  2.83077103 -1.26456712 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Tue Apr  2 09:12:30 2024]  Iteration number: 0 with current cost as 0.1675375044816612 and parameters 
[-2.62635382  2.23743461 -2.12427961 -0.11653105  0.55388705 -2.770109
  3.06858498  2.18960148  1.18551998 -1.06648308  0.60271513  1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654243  0.72897372  1.60512661  2.83077104 -1.26456712 -0.25136105
 -2.39279215 -2.27309774  3.13337152  2.54856956 -0.67550787 -2.69002204]. 
Working on 0.8 fold... 
[Tue Apr  2 09:22:24 2024]  Iteration number: 0 with current cost as 0.13156835156500912 and parameters 
[-2.57114679  2.23743464 -2.12427964 -0.11653101  0.55388708 -2.77010899
  3.068585    2.18960147  1.18552    -1.06648308  0.60271512  1.14432447
  1.310299   -1.87354676  0.72965082  2.88578419 -0.54534333 -0.47522483
 -2.0265424   0.72897371  1.60512664  2.83077105 -1.2645671  -0.25136103
 -2.39279218 -2.27309774  3.13337155  2.5485696  -0.67550785 -2.69002202]. 
Working on 1.0 fold... 
[Tue Apr  2 09:31:37 2024]  Iteration number: 0 with current cost as 0.1482949744651309 and parameters 
[-2.4977156   2.23743464 -2.1242796  -0.11653103  0.55388708 -2.77010897
  3.068585    2.18960149  1.18552002 -1.06648308  0.60271514  1.14432447
  1.31029899 -1.87354676  0.72965084  2.88578423 -0.54534331 -0.47522481
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136101
 -2.39279216 -2.27309774  3.13337157  2.5485696  -0.67550783 -2.69002202]. 
Training complete taking 2885.5389251708984 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Tue Apr  2 09:40:42 2024]  Iteration number: 0 with current cost as 0.16568026486602191 and parameters 
[-2.63993171  2.23743459 -2.12427964 -0.11653103  0.55388703 -2.77010902
  3.06858496  2.18960148  1.18551996 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654245  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136107
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Tue Apr  2 09:50:46 2024]  Iteration number: 0 with current cost as 0.17313270197007372 and parameters 
[-2.62073802  2.23743464 -2.12427964 -0.11653103  0.55388704 -2.77010901
  3.06858496  2.18960145  1.18551998 -1.06648312  0.6027151   1.14432443
  1.31029899 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512662  2.83077103 -1.26456712 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Tue Apr  2 10:00:30 2024]  Iteration number: 0 with current cost as 0.1675375044816612 and parameters 
[-2.62635382  2.23743461 -2.12427961 -0.11653105  0.55388705 -2.770109
  3.06858498  2.18960148  1.18551998 -1.06648308  0.60271513  1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654243  0.72897372  1.60512661  2.83077104 -1.26456712 -0.25136105
 -2.39279215 -2.27309774  3.13337152  2.54856956 -0.67550787 -2.69002204]. 
Working on 0.8 fold... 
[Tue Apr  2 10:10:28 2024]  Iteration number: 0 with current cost as 0.13156835156500912 and parameters 
[-2.57114679  2.23743464 -2.12427964 -0.11653101  0.55388708 -2.77010899
  3.068585    2.18960147  1.18552    -1.06648308  0.60271512  1.14432447
  1.310299   -1.87354676  0.72965082  2.88578419 -0.54534333 -0.47522483
 -2.0265424   0.72897371  1.60512664  2.83077105 -1.2645671  -0.25136103
 -2.39279218 -2.27309774  3.13337155  2.5485696  -0.67550785 -2.69002202]. 
Working on 1.0 fold... 
[Tue Apr  2 10:19:22 2024]  Iteration number: 0 with current cost as 0.1482949744651309 and parameters 
[-2.4977156   2.23743464 -2.1242796  -0.11653103  0.55388708 -2.77010897
  3.068585    2.18960149  1.18552002 -1.06648308  0.60271514  1.14432447
  1.31029899 -1.87354676  0.72965084  2.88578423 -0.54534331 -0.47522481
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136101
 -2.39279216 -2.27309774  3.13337157  2.5485696  -0.67550783 -2.69002202]. 
Training complete taking 2853.0508646965027 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Tue Apr  2 10:28:17 2024]  Iteration number: 0 with current cost as 0.16568026486602191 and parameters 
[-2.63993171  2.23743459 -2.12427964 -0.11653103  0.55388703 -2.77010902
  3.06858496  2.18960148  1.18551996 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654245  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136107
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Tue Apr  2 10:38:30 2024]  Iteration number: 0 with current cost as 0.17313270197007372 and parameters 
[-2.62073802  2.23743464 -2.12427964 -0.11653103  0.55388704 -2.77010901
  3.06858496  2.18960145  1.18551998 -1.06648312  0.6027151   1.14432443
  1.31029899 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512662  2.83077103 -1.26456712 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Tue Apr  2 10:48:41 2024]  Iteration number: 0 with current cost as 0.1675375044816612 and parameters 
[-2.62635382  2.23743461 -2.12427961 -0.11653105  0.55388705 -2.770109
  3.06858498  2.18960148  1.18551998 -1.06648308  0.60271513  1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654243  0.72897372  1.60512661  2.83077104 -1.26456712 -0.25136105
 -2.39279215 -2.27309774  3.13337152  2.54856956 -0.67550787 -2.69002204]. 
Working on 0.8 fold... 
[Tue Apr  2 10:58:19 2024]  Iteration number: 0 with current cost as 0.13156835156500912 and parameters 
[-2.57114679  2.23743464 -2.12427964 -0.11653101  0.55388708 -2.77010899
  3.068585    2.18960147  1.18552    -1.06648308  0.60271512  1.14432447
  1.310299   -1.87354676  0.72965082  2.88578419 -0.54534333 -0.47522483
 -2.0265424   0.72897371  1.60512664  2.83077105 -1.2645671  -0.25136103
 -2.39279218 -2.27309774  3.13337155  2.5485696  -0.67550785 -2.69002202]. 
Working on 1.0 fold... 
[Tue Apr  2 11:07:33 2024]  Iteration number: 0 with current cost as 0.1482949744651309 and parameters 
[-2.4977156   2.23743464 -2.1242796  -0.11653103  0.55388708 -2.77010897
  3.068585    2.18960149  1.18552002 -1.06648308  0.60271514  1.14432447
  1.31029899 -1.87354676  0.72965084  2.88578423 -0.54534331 -0.47522481
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136101
 -2.39279216 -2.27309774  3.13337157  2.5485696  -0.67550783 -2.69002202]. 
Training complete taking 2890.4509127140045 seconds. 
Discarding model... 

Training complete taking 70025.44426560402 total seconds. 
Now scoring model... 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Scoring complete taking 3.0275325775146484 seconds. 
Saved predicted values as M-M-CZ_Full-CRX_predicted_values.csv
Model scores: {'MSE_train': (3.4614219201854617,), 'R2_train': 0.524720713504668, 'MAE_train': 1.405538477924619, 'MSE_test': 15.2799557855674, 'R2_test': -0.5232688504237617, 'MAE_test': 2.617093479400039}. 
Saved model results as M-M-CZ_Full-CRX_results.json. 
