/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /home/gjones/scratch/quad5qubits/quadratic_train.bin... 
Successfully loaded /home/gjones/scratch/quad5qubits/quadratic_train.bin into X and y data. 
Loading dataset from /home/gjones/scratch/quad5qubits/quadratic_test.bin... 
Successfully loaded /home/gjones/scratch/quad5qubits/quadratic_test.bin into X and y data. 
Training model with dataset /home/gjones/scratch/quad5qubits/quadratic_train.bin 
 at time Wed Mar 27 23:15:43 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Wed Mar 27 23:17:07 2024]  Iteration number: 0 with current cost as 0.4107398892048182 and parameters 
[-3.16517157  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858499  2.18960145  1.18551999 -1.06648308  0.6027151   1.14432446
  1.31029898 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512663  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309775  3.13337155  2.54856959 -0.67550788 -2.69002202]. 
Working on 0.4 fold... 
[Wed Mar 27 23:20:33 2024]  Iteration number: 0 with current cost as 0.38180867765402016 and parameters 
[-3.13598875  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Wed Mar 27 23:24:00 2024]  Iteration number: 0 with current cost as 0.4369194066226436 and parameters 
[-3.09378384  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Wed Mar 27 23:27:26 2024]  Iteration number: 0 with current cost as 0.40720349410079637 and parameters 
[-3.07437254  2.23743464 -2.12427963 -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Wed Mar 27 23:30:54 2024]  Iteration number: 0 with current cost as 0.36310551551615655 and parameters 
[-3.05187505  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1034.9553894996643 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Wed Mar 27 23:34:21 2024]  Iteration number: 0 with current cost as 0.4107398892048182 and parameters 
[-3.16517157  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858499  2.18960145  1.18551999 -1.06648308  0.6027151   1.14432446
  1.31029898 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512663  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309775  3.13337155  2.54856959 -0.67550788 -2.69002202]. 
Working on 0.4 fold... 
[Wed Mar 27 23:37:48 2024]  Iteration number: 0 with current cost as 0.38180867765402016 and parameters 
[-3.13598875  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Wed Mar 27 23:41:14 2024]  Iteration number: 0 with current cost as 0.4369194066226436 and parameters 
[-3.09378384  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Wed Mar 27 23:44:39 2024]  Iteration number: 0 with current cost as 0.40720349410079637 and parameters 
[-3.07437254  2.23743464 -2.12427963 -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Wed Mar 27 23:48:07 2024]  Iteration number: 0 with current cost as 0.36310551551615655 and parameters 
[-3.05187505  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1032.3270907402039 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Wed Mar 27 23:51:32 2024]  Iteration number: 0 with current cost as 0.4107398892048182 and parameters 
[-3.16517157  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858499  2.18960145  1.18551999 -1.06648308  0.6027151   1.14432446
  1.31029898 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512663  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309775  3.13337155  2.54856959 -0.67550788 -2.69002202]. 
Working on 0.4 fold... 
[Wed Mar 27 23:54:57 2024]  Iteration number: 0 with current cost as 0.38180867765402016 and parameters 
[-3.13598875  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Wed Mar 27 23:58:23 2024]  Iteration number: 0 with current cost as 0.4369194066226436 and parameters 
[-3.09378384  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Mar 28 00:01:48 2024]  Iteration number: 0 with current cost as 0.40720349410079637 and parameters 
[-3.07437254  2.23743464 -2.12427963 -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Mar 28 00:05:15 2024]  Iteration number: 0 with current cost as 0.36310551551615655 and parameters 
[-3.05187505  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1027.3402652740479 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Thu Mar 28 00:08:40 2024]  Iteration number: 0 with current cost as 0.4107398892048182 and parameters 
[-3.16517157  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858499  2.18960145  1.18551999 -1.06648308  0.6027151   1.14432446
  1.31029898 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512663  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309775  3.13337155  2.54856959 -0.67550788 -2.69002202]. 
Working on 0.4 fold... 
[Thu Mar 28 00:12:06 2024]  Iteration number: 0 with current cost as 0.38180867765402016 and parameters 
[-3.13598875  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Mar 28 00:15:31 2024]  Iteration number: 0 with current cost as 0.4369194066226436 and parameters 
[-3.09378384  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Mar 28 00:18:56 2024]  Iteration number: 0 with current cost as 0.40720349410079637 and parameters 
[-3.07437254  2.23743464 -2.12427963 -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Mar 28 00:22:22 2024]  Iteration number: 0 with current cost as 0.36310551551615655 and parameters 
[-3.05187505  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1026.8610577583313 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Thu Mar 28 00:25:47 2024]  Iteration number: 0 with current cost as 0.4107398892048182 and parameters 
[-3.16517157  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858499  2.18960145  1.18551999 -1.06648308  0.6027151   1.14432446
  1.31029898 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512663  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309775  3.13337155  2.54856959 -0.67550788 -2.69002202]. 
Working on 0.4 fold... 
[Thu Mar 28 00:29:12 2024]  Iteration number: 0 with current cost as 0.38180867765402016 and parameters 
[-3.13598875  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Mar 28 00:32:37 2024]  Iteration number: 0 with current cost as 0.4369194066226436 and parameters 
[-3.09378384  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Mar 28 00:36:03 2024]  Iteration number: 0 with current cost as 0.40720349410079637 and parameters 
[-3.07437254  2.23743464 -2.12427963 -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Mar 28 00:39:28 2024]  Iteration number: 0 with current cost as 0.36310551551615655 and parameters 
[-3.05187505  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1026.3935768604279 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Mar 28 00:42:53 2024]  Iteration number: 0 with current cost as 0.4107398892048182 and parameters 
[-3.16517157  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858499  2.18960145  1.18551999 -1.06648308  0.6027151   1.14432446
  1.31029898 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512663  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309775  3.13337155  2.54856959 -0.67550788 -2.69002202]. 
Working on 0.4 fold... 
[Thu Mar 28 00:46:18 2024]  Iteration number: 0 with current cost as 0.38180867765402016 and parameters 
[-3.13598875  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Mar 28 00:49:44 2024]  Iteration number: 0 with current cost as 0.4369194066226436 and parameters 
[-3.09378384  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Mar 28 00:53:10 2024]  Iteration number: 0 with current cost as 0.40720349410079637 and parameters 
[-3.07437254  2.23743464 -2.12427963 -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Mar 28 00:56:35 2024]  Iteration number: 0 with current cost as 0.36310551551615655 and parameters 
[-3.05187505  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1027.1743566989899 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Mar 28 01:00:00 2024]  Iteration number: 0 with current cost as 0.4107398892048182 and parameters 
[-3.16517157  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858499  2.18960145  1.18551999 -1.06648308  0.6027151   1.14432446
  1.31029898 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512663  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309775  3.13337155  2.54856959 -0.67550788 -2.69002202]. 
Working on 0.4 fold... 
[Thu Mar 28 01:03:26 2024]  Iteration number: 0 with current cost as 0.38180867765402016 and parameters 
[-3.13598875  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Mar 28 01:06:50 2024]  Iteration number: 0 with current cost as 0.4369194066226436 and parameters 
[-3.09378384  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Mar 28 01:10:16 2024]  Iteration number: 0 with current cost as 0.40720349410079637 and parameters 
[-3.07437254  2.23743464 -2.12427963 -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Mar 28 01:13:41 2024]  Iteration number: 0 with current cost as 0.36310551551615655 and parameters 
[-3.05187505  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1025.2222559452057 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Mar 28 01:17:06 2024]  Iteration number: 0 with current cost as 0.4107398892048182 and parameters 
[-3.16517157  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858499  2.18960145  1.18551999 -1.06648308  0.6027151   1.14432446
  1.31029898 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512663  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309775  3.13337155  2.54856959 -0.67550788 -2.69002202]. 
Working on 0.4 fold... 
[Thu Mar 28 01:20:31 2024]  Iteration number: 0 with current cost as 0.38180867765402016 and parameters 
[-3.13598875  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Mar 28 01:23:56 2024]  Iteration number: 0 with current cost as 0.4369194066226436 and parameters 
[-3.09378384  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Mar 28 01:27:21 2024]  Iteration number: 0 with current cost as 0.40720349410079637 and parameters 
[-3.07437254  2.23743464 -2.12427963 -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Mar 28 01:30:45 2024]  Iteration number: 0 with current cost as 0.36310551551615655 and parameters 
[-3.05187505  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1025.1723415851593 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Thu Mar 28 01:34:10 2024]  Iteration number: 0 with current cost as 0.4107398892048182 and parameters 
[-3.16517157  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858499  2.18960145  1.18551999 -1.06648308  0.6027151   1.14432446
  1.31029898 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512663  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309775  3.13337155  2.54856959 -0.67550788 -2.69002202]. 
Working on 0.4 fold... 
[Thu Mar 28 01:37:36 2024]  Iteration number: 0 with current cost as 0.38180867765402016 and parameters 
[-3.13598875  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Mar 28 01:41:01 2024]  Iteration number: 0 with current cost as 0.4369194066226436 and parameters 
[-3.09378384  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Mar 28 01:44:25 2024]  Iteration number: 0 with current cost as 0.40720349410079637 and parameters 
[-3.07437254  2.23743464 -2.12427963 -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Mar 28 01:47:53 2024]  Iteration number: 0 with current cost as 0.36310551551615655 and parameters 
[-3.05187505  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1027.9473745822906 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Thu Mar 28 01:51:19 2024]  Iteration number: 0 with current cost as 0.4107398892048182 and parameters 
[-3.16517157  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858499  2.18960145  1.18551999 -1.06648308  0.6027151   1.14432446
  1.31029898 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512663  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309775  3.13337155  2.54856959 -0.67550788 -2.69002202]. 
Working on 0.4 fold... 
[Thu Mar 28 01:54:45 2024]  Iteration number: 0 with current cost as 0.38180867765402016 and parameters 
[-3.13598875  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Mar 28 01:58:10 2024]  Iteration number: 0 with current cost as 0.4369194066226436 and parameters 
[-3.09378384  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Mar 28 02:01:35 2024]  Iteration number: 0 with current cost as 0.40720349410079637 and parameters 
[-3.07437254  2.23743464 -2.12427963 -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Mar 28 02:05:01 2024]  Iteration number: 0 with current cost as 0.36310551551615655 and parameters 
[-3.05187505  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1026.846398115158 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Mar 28 02:08:26 2024]  Iteration number: 0 with current cost as 0.4107398892048182 and parameters 
[-3.16517157  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858499  2.18960145  1.18551999 -1.06648308  0.6027151   1.14432446
  1.31029898 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512663  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309775  3.13337155  2.54856959 -0.67550788 -2.69002202]. 
Working on 0.4 fold... 
[Thu Mar 28 02:11:51 2024]  Iteration number: 0 with current cost as 0.38180867765402016 and parameters 
[-3.13598875  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Mar 28 02:15:16 2024]  Iteration number: 0 with current cost as 0.4369194066226436 and parameters 
[-3.09378384  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Mar 28 02:18:41 2024]  Iteration number: 0 with current cost as 0.40720349410079637 and parameters 
[-3.07437254  2.23743464 -2.12427963 -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Mar 28 02:22:06 2024]  Iteration number: 0 with current cost as 0.36310551551615655 and parameters 
[-3.05187505  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1026.0659809112549 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Mar 28 02:25:32 2024]  Iteration number: 0 with current cost as 0.4107398892048182 and parameters 
[-3.16517157  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858499  2.18960145  1.18551999 -1.06648308  0.6027151   1.14432446
  1.31029898 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512663  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309775  3.13337155  2.54856959 -0.67550788 -2.69002202]. 
Working on 0.4 fold... 
[Thu Mar 28 02:28:58 2024]  Iteration number: 0 with current cost as 0.38180867765402016 and parameters 
[-3.13598875  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Mar 28 02:32:23 2024]  Iteration number: 0 with current cost as 0.4369194066226436 and parameters 
[-3.09378384  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Mar 28 02:35:48 2024]  Iteration number: 0 with current cost as 0.40720349410079637 and parameters 
[-3.07437254  2.23743464 -2.12427963 -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Mar 28 02:39:13 2024]  Iteration number: 0 with current cost as 0.36310551551615655 and parameters 
[-3.05187505  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1026.947467803955 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Mar 28 02:42:39 2024]  Iteration number: 0 with current cost as 0.4107398892048182 and parameters 
[-3.16517157  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858499  2.18960145  1.18551999 -1.06648308  0.6027151   1.14432446
  1.31029898 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512663  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309775  3.13337155  2.54856959 -0.67550788 -2.69002202]. 
Working on 0.4 fold... 
[Thu Mar 28 02:46:04 2024]  Iteration number: 0 with current cost as 0.38180867765402016 and parameters 
[-3.13598875  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Mar 28 02:49:29 2024]  Iteration number: 0 with current cost as 0.4369194066226436 and parameters 
[-3.09378384  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Mar 28 02:52:54 2024]  Iteration number: 0 with current cost as 0.40720349410079637 and parameters 
[-3.07437254  2.23743464 -2.12427963 -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Mar 28 02:56:19 2024]  Iteration number: 0 with current cost as 0.36310551551615655 and parameters 
[-3.05187505  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1026.4685366153717 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Mar 28 02:59:45 2024]  Iteration number: 0 with current cost as 0.4107398892048182 and parameters 
[-3.16517157  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858499  2.18960145  1.18551999 -1.06648308  0.6027151   1.14432446
  1.31029898 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512663  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309775  3.13337155  2.54856959 -0.67550788 -2.69002202]. 
Working on 0.4 fold... 
[Thu Mar 28 03:03:10 2024]  Iteration number: 0 with current cost as 0.38180867765402016 and parameters 
[-3.13598875  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Mar 28 03:06:35 2024]  Iteration number: 0 with current cost as 0.4369194066226436 and parameters 
[-3.09378384  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Mar 28 03:10:00 2024]  Iteration number: 0 with current cost as 0.40720349410079637 and parameters 
[-3.07437254  2.23743464 -2.12427963 -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Mar 28 03:13:26 2024]  Iteration number: 0 with current cost as 0.36310551551615655 and parameters 
[-3.05187505  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1025.8508858680725 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Mar 28 03:16:51 2024]  Iteration number: 0 with current cost as 0.4107398892048182 and parameters 
[-3.16517157  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858499  2.18960145  1.18551999 -1.06648308  0.6027151   1.14432446
  1.31029898 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512663  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309775  3.13337155  2.54856959 -0.67550788 -2.69002202]. 
Working on 0.4 fold... 
[Thu Mar 28 03:20:16 2024]  Iteration number: 0 with current cost as 0.38180867765402016 and parameters 
[-3.13598875  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Mar 28 03:23:41 2024]  Iteration number: 0 with current cost as 0.4369194066226436 and parameters 
[-3.09378384  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Mar 28 03:27:06 2024]  Iteration number: 0 with current cost as 0.40720349410079637 and parameters 
[-3.07437254  2.23743464 -2.12427963 -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Mar 28 03:30:32 2024]  Iteration number: 0 with current cost as 0.36310551551615655 and parameters 
[-3.05187505  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1025.769936323166 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Mar 28 03:33:57 2024]  Iteration number: 0 with current cost as 0.4107398892048182 and parameters 
[-3.16517157  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858499  2.18960145  1.18551999 -1.06648308  0.6027151   1.14432446
  1.31029898 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512663  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309775  3.13337155  2.54856959 -0.67550788 -2.69002202]. 
Working on 0.4 fold... 
[Thu Mar 28 03:37:24 2024]  Iteration number: 0 with current cost as 0.38180867765402016 and parameters 
[-3.13598875  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Mar 28 03:40:50 2024]  Iteration number: 0 with current cost as 0.4369194066226436 and parameters 
[-3.09378384  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Mar 28 03:44:15 2024]  Iteration number: 0 with current cost as 0.40720349410079637 and parameters 
[-3.07437254  2.23743464 -2.12427963 -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Mar 28 03:47:43 2024]  Iteration number: 0 with current cost as 0.36310551551615655 and parameters 
[-3.05187505  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1031.7021188735962 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Mar 28 03:51:08 2024]  Iteration number: 0 with current cost as 0.4107398892048182 and parameters 
[-3.16517157  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858499  2.18960145  1.18551999 -1.06648308  0.6027151   1.14432446
  1.31029898 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512663  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309775  3.13337155  2.54856959 -0.67550788 -2.69002202]. 
Working on 0.4 fold... 
[Thu Mar 28 03:54:33 2024]  Iteration number: 0 with current cost as 0.38180867765402016 and parameters 
[-3.13598875  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Mar 28 03:57:58 2024]  Iteration number: 0 with current cost as 0.4369194066226436 and parameters 
[-3.09378384  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Mar 28 04:01:23 2024]  Iteration number: 0 with current cost as 0.40720349410079637 and parameters 
[-3.07437254  2.23743464 -2.12427963 -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Mar 28 04:04:48 2024]  Iteration number: 0 with current cost as 0.36310551551615655 and parameters 
[-3.05187505  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1024.960974931717 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Mar 28 04:08:13 2024]  Iteration number: 0 with current cost as 0.4107398892048182 and parameters 
[-3.16517157  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858499  2.18960145  1.18551999 -1.06648308  0.6027151   1.14432446
  1.31029898 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512663  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309775  3.13337155  2.54856959 -0.67550788 -2.69002202]. 
Working on 0.4 fold... 
[Thu Mar 28 04:11:38 2024]  Iteration number: 0 with current cost as 0.38180867765402016 and parameters 
[-3.13598875  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Mar 28 04:15:04 2024]  Iteration number: 0 with current cost as 0.4369194066226436 and parameters 
[-3.09378384  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Mar 28 04:18:29 2024]  Iteration number: 0 with current cost as 0.40720349410079637 and parameters 
[-3.07437254  2.23743464 -2.12427963 -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Mar 28 04:21:54 2024]  Iteration number: 0 with current cost as 0.36310551551615655 and parameters 
[-3.05187505  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1026.0122618675232 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Mar 28 04:25:19 2024]  Iteration number: 0 with current cost as 0.4107398892048182 and parameters 
[-3.16517157  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858499  2.18960145  1.18551999 -1.06648308  0.6027151   1.14432446
  1.31029898 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512663  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309775  3.13337155  2.54856959 -0.67550788 -2.69002202]. 
Working on 0.4 fold... 
[Thu Mar 28 04:28:44 2024]  Iteration number: 0 with current cost as 0.38180867765402016 and parameters 
[-3.13598875  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Mar 28 04:32:10 2024]  Iteration number: 0 with current cost as 0.4369194066226436 and parameters 
[-3.09378384  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Mar 28 04:35:35 2024]  Iteration number: 0 with current cost as 0.40720349410079637 and parameters 
[-3.07437254  2.23743464 -2.12427963 -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Mar 28 04:39:00 2024]  Iteration number: 0 with current cost as 0.36310551551615655 and parameters 
[-3.05187505  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1025.9082367420197 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Mar 28 04:42:25 2024]  Iteration number: 0 with current cost as 0.4107398892048182 and parameters 
[-3.16517157  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858499  2.18960145  1.18551999 -1.06648308  0.6027151   1.14432446
  1.31029898 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512663  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309775  3.13337155  2.54856959 -0.67550788 -2.69002202]. 
Working on 0.4 fold... 
[Thu Mar 28 04:45:50 2024]  Iteration number: 0 with current cost as 0.38180867765402016 and parameters 
[-3.13598875  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Mar 28 04:49:15 2024]  Iteration number: 0 with current cost as 0.4369194066226436 and parameters 
[-3.09378384  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Mar 28 04:52:41 2024]  Iteration number: 0 with current cost as 0.40720349410079637 and parameters 
[-3.07437254  2.23743464 -2.12427963 -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Mar 28 04:56:06 2024]  Iteration number: 0 with current cost as 0.36310551551615655 and parameters 
[-3.05187505  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1025.4149177074432 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Mar 28 04:59:30 2024]  Iteration number: 0 with current cost as 0.4107398892048182 and parameters 
[-3.16517157  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858499  2.18960145  1.18551999 -1.06648308  0.6027151   1.14432446
  1.31029898 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512663  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309775  3.13337155  2.54856959 -0.67550788 -2.69002202]. 
Working on 0.4 fold... 
[Thu Mar 28 05:02:55 2024]  Iteration number: 0 with current cost as 0.38180867765402016 and parameters 
[-3.13598875  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Mar 28 05:06:20 2024]  Iteration number: 0 with current cost as 0.4369194066226436 and parameters 
[-3.09378384  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Mar 28 05:09:45 2024]  Iteration number: 0 with current cost as 0.40720349410079637 and parameters 
[-3.07437254  2.23743464 -2.12427963 -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Mar 28 05:13:11 2024]  Iteration number: 0 with current cost as 0.36310551551615655 and parameters 
[-3.05187505  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1025.4866807460785 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Mar 28 05:16:36 2024]  Iteration number: 0 with current cost as 0.4107398892048182 and parameters 
[-3.16517157  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858499  2.18960145  1.18551999 -1.06648308  0.6027151   1.14432446
  1.31029898 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512663  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309775  3.13337155  2.54856959 -0.67550788 -2.69002202]. 
Working on 0.4 fold... 
[Thu Mar 28 05:20:01 2024]  Iteration number: 0 with current cost as 0.38180867765402016 and parameters 
[-3.13598875  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Mar 28 05:23:26 2024]  Iteration number: 0 with current cost as 0.4369194066226436 and parameters 
[-3.09378384  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Mar 28 05:26:51 2024]  Iteration number: 0 with current cost as 0.40720349410079637 and parameters 
[-3.07437254  2.23743464 -2.12427963 -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Mar 28 05:30:16 2024]  Iteration number: 0 with current cost as 0.36310551551615655 and parameters 
[-3.05187505  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1024.5630567073822 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Mar 28 05:33:40 2024]  Iteration number: 0 with current cost as 0.4107398892048182 and parameters 
[-3.16517157  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858499  2.18960145  1.18551999 -1.06648308  0.6027151   1.14432446
  1.31029898 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512663  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309775  3.13337155  2.54856959 -0.67550788 -2.69002202]. 
Working on 0.4 fold... 
[Thu Mar 28 05:37:05 2024]  Iteration number: 0 with current cost as 0.38180867765402016 and parameters 
[-3.13598875  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Mar 28 05:40:31 2024]  Iteration number: 0 with current cost as 0.4369194066226436 and parameters 
[-3.09378384  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Mar 28 05:43:55 2024]  Iteration number: 0 with current cost as 0.40720349410079637 and parameters 
[-3.07437254  2.23743464 -2.12427963 -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Mar 28 05:47:22 2024]  Iteration number: 0 with current cost as 0.36310551551615655 and parameters 
[-3.05187505  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1028.305404663086 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Thu Mar 28 05:50:49 2024]  Iteration number: 0 with current cost as 0.4107398892048182 and parameters 
[-3.16517157  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858499  2.18960145  1.18551999 -1.06648308  0.6027151   1.14432446
  1.31029898 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512663  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309775  3.13337155  2.54856959 -0.67550788 -2.69002202]. 
Working on 0.4 fold... 
[Thu Mar 28 05:54:14 2024]  Iteration number: 0 with current cost as 0.38180867765402016 and parameters 
[-3.13598875  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Mar 28 05:57:40 2024]  Iteration number: 0 with current cost as 0.4369194066226436 and parameters 
[-3.09378384  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Mar 28 06:01:05 2024]  Iteration number: 0 with current cost as 0.40720349410079637 and parameters 
[-3.07437254  2.23743464 -2.12427963 -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Mar 28 06:04:30 2024]  Iteration number: 0 with current cost as 0.36310551551615655 and parameters 
[-3.05187505  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1026.2933325767517 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Thu Mar 28 06:07:55 2024]  Iteration number: 0 with current cost as 0.4107398892048182 and parameters 
[-3.16517157  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858499  2.18960145  1.18551999 -1.06648308  0.6027151   1.14432446
  1.31029898 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512663  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309775  3.13337155  2.54856959 -0.67550788 -2.69002202]. 
Working on 0.4 fold... 
[Thu Mar 28 06:11:20 2024]  Iteration number: 0 with current cost as 0.38180867765402016 and parameters 
[-3.13598875  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Mar 28 06:14:46 2024]  Iteration number: 0 with current cost as 0.4369194066226436 and parameters 
[-3.09378384  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Mar 28 06:18:12 2024]  Iteration number: 0 with current cost as 0.40720349410079637 and parameters 
[-3.07437254  2.23743464 -2.12427963 -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Mar 28 06:21:37 2024]  Iteration number: 0 with current cost as 0.36310551551615655 and parameters 
[-3.05187505  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1027.1506035327911 seconds. 
Discarding model... 

Training complete taking 25677.14138364792 total seconds. 
Now scoring model... 
Scoring complete taking 2.3240065574645996 seconds. 
Saved predicted values as IQP_Full-CRZ_predicted_values.csv
Model scores: {'MSE_train': (8.713806856652536,), 'R2_train': -0.19647127711782963, 'MAE_train': 2.627965950214261, 'MSE_test': 10.186406788793283, 'R2_test': -0.015489598063503696, 'MAE_test': 2.7994175383091218}. 
Saved model results as IQP_Full-CRZ_results.json. 
