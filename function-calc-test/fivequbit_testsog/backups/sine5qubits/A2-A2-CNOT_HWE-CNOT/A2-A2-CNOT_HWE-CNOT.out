/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /scratch/j/jacobsen/gjones/sine5qubits/sine_train.bin... 
Successfully loaded /scratch/j/jacobsen/gjones/sine5qubits/sine_train.bin into X and y data. 
Loading dataset from /scratch/j/jacobsen/gjones/sine5qubits/sine_test.bin... 
Successfully loaded /scratch/j/jacobsen/gjones/sine5qubits/sine_test.bin into X and y data. 
Training model with dataset /scratch/j/jacobsen/gjones/sine5qubits/sine_train.bin 
 at time Thu Apr  4 21:37:45 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 21:38:02 2024]  Iteration number: 0 with current cost as 0.22930133696983296 and parameters 
[-2.90318349  2.23743462 -2.12427964 -0.10149013  0.46190389 -2.89845243
  3.49376842  1.53211429  0.82696178 -1.3734468   0.81173336  0.83705606
  1.27397532 -2.07218913  0.14035223]. 
[Thu Apr  4 21:44:02 2024]  Iteration number: 50 with current cost as 0.08343057626883282 and parameters 
[-2.90318431  2.23743413 -2.12427978 -0.90800818 -0.8840039  -0.89851592
  5.02080153  0.3004745  -0.63417651 -0.71981656  1.22019161  1.99959206
  2.55195046 -2.40591419  1.16971741]. 
Working on 0.4 fold... 
[Thu Apr  4 21:46:00 2024]  Iteration number: 0 with current cost as 0.23448062944776937 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.16499735  0.48511526 -2.93627473
  3.46866313  1.52507975  0.87424827 -1.3703845   0.89456474  0.86667119
  1.31894041 -2.04067536  0.10235463]. 
Working on 0.6 fold... 
[Thu Apr  4 21:50:44 2024]  Iteration number: 0 with current cost as 0.3387438803124553 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.14446776  0.53846358 -2.8274426
  3.13905147  2.04730835  1.14507481 -1.13703832  0.6515629   1.07395094
  1.31391968 -1.9074027   0.5963043 ]. 
[Thu Apr  4 21:55:59 2024]  Iteration number: 50 with current cost as 0.08677003031586851 and parameters 
[-2.90318299  2.23743503 -2.12427939 -0.32757738  1.54487519 -4.43879614
  2.8371232   1.89126359  1.68645101 -2.04439625 -0.80850654  1.30706256
  4.23568489 -2.45112886  2.49421718]. 
Working on 0.8 fold... 
[Thu Apr  4 21:58:33 2024]  Iteration number: 0 with current cost as 0.2040107522153226 and parameters 
[-2.90318345  2.23743464 -2.1242796  -0.10141192  0.45987249 -2.90158548
  3.57791405  1.39994962  0.75716634 -1.34984792  0.80744484  0.8643616
  1.27261328 -2.11053401  0.00764771]. 
[Thu Apr  4 22:03:20 2024]  Iteration number: 50 with current cost as 0.07683496576511009 and parameters 
[-2.90318391  2.23743484 -2.12427929  1.37515312 -1.35162693 -1.67737014
  7.01279204  0.53620401 -2.50758207  0.00842131  1.81713026  1.77817348
  4.4195452  -1.91319279  2.26760412]. 
Working on 1.0 fold... 
[Thu Apr  4 22:03:39 2024]  Iteration number: 0 with current cost as 0.25096250772166334 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.0139779   0.46746413 -2.78672857
  3.49107865  1.64779575  0.76571836 -1.35334597  0.85479454  0.87491963
  1.27074113 -2.04781193  0.23846906]. 
Training complete taking 1815.5127818584442 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 22:08:19 2024]  Iteration number: 0 with current cost as 0.22930133696983296 and parameters 
[-2.90318349  2.23743462 -2.12427964 -0.10149013  0.46190389 -2.89845243
  3.49376842  1.53211429  0.82696178 -1.3734468   0.81173336  0.83705606
  1.27397532 -2.07218913  0.14035223]. 
[Thu Apr  4 22:14:15 2024]  Iteration number: 50 with current cost as 0.08343057626883282 and parameters 
[-2.90318431  2.23743413 -2.12427978 -0.90800818 -0.8840039  -0.89851592
  5.02080153  0.3004745  -0.63417651 -0.71981656  1.22019161  1.99959206
  2.55195046 -2.40591419  1.16971741]. 
Working on 0.4 fold... 
[Thu Apr  4 22:16:13 2024]  Iteration number: 0 with current cost as 0.23448062944776937 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.16499735  0.48511526 -2.93627473
  3.46866313  1.52507975  0.87424827 -1.3703845   0.89456474  0.86667119
  1.31894041 -2.04067536  0.10235463]. 
Working on 0.6 fold... 
[Thu Apr  4 22:21:26 2024]  Iteration number: 0 with current cost as 0.3387438803124553 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.14446776  0.53846358 -2.8274426
  3.13905147  2.04730835  1.14507481 -1.13703832  0.6515629   1.07395094
  1.31391968 -1.9074027   0.5963043 ]. 
[Thu Apr  4 22:26:49 2024]  Iteration number: 50 with current cost as 0.08677003031586851 and parameters 
[-2.90318299  2.23743503 -2.12427939 -0.32757738  1.54487519 -4.43879614
  2.8371232   1.89126359  1.68645101 -2.04439625 -0.80850654  1.30706256
  4.23568489 -2.45112886  2.49421718]. 
Working on 0.8 fold... 
[Thu Apr  4 22:29:21 2024]  Iteration number: 0 with current cost as 0.2040107522153226 and parameters 
[-2.90318345  2.23743464 -2.1242796  -0.10141192  0.45987249 -2.90158548
  3.57791405  1.39994962  0.75716634 -1.34984792  0.80744484  0.8643616
  1.27261328 -2.11053401  0.00764771]. 
[Thu Apr  4 22:34:04 2024]  Iteration number: 50 with current cost as 0.07683496576511009 and parameters 
[-2.90318391  2.23743484 -2.12427929  1.37515312 -1.35162693 -1.67737014
  7.01279204  0.53620401 -2.50758207  0.00842131  1.81713026  1.77817348
  4.4195452  -1.91319279  2.26760412]. 
Working on 1.0 fold... 
[Thu Apr  4 22:34:24 2024]  Iteration number: 0 with current cost as 0.25096250772166334 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.0139779   0.46746413 -2.78672857
  3.49107865  1.64779575  0.76571836 -1.35334597  0.85479454  0.87491963
  1.27074113 -2.04781193  0.23846906]. 
Training complete taking 1852.815213918686 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 22:39:11 2024]  Iteration number: 0 with current cost as 0.22930133696983296 and parameters 
[-2.90318349  2.23743462 -2.12427964 -0.10149013  0.46190389 -2.89845243
  3.49376842  1.53211429  0.82696178 -1.3734468   0.81173336  0.83705606
  1.27397532 -2.07218913  0.14035223]. 
[Thu Apr  4 22:45:20 2024]  Iteration number: 50 with current cost as 0.08343057626883282 and parameters 
[-2.90318431  2.23743413 -2.12427978 -0.90800818 -0.8840039  -0.89851592
  5.02080153  0.3004745  -0.63417651 -0.71981656  1.22019161  1.99959206
  2.55195046 -2.40591419  1.16971741]. 
Working on 0.4 fold... 
[Thu Apr  4 22:47:18 2024]  Iteration number: 0 with current cost as 0.23448062944776937 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.16499735  0.48511526 -2.93627473
  3.46866313  1.52507975  0.87424827 -1.3703845   0.89456474  0.86667119
  1.31894041 -2.04067536  0.10235463]. 
Working on 0.6 fold... 
[Thu Apr  4 22:51:58 2024]  Iteration number: 0 with current cost as 0.3387438803124553 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.14446776  0.53846358 -2.8274426
  3.13905147  2.04730835  1.14507481 -1.13703832  0.6515629   1.07395094
  1.31391968 -1.9074027   0.5963043 ]. 
[Thu Apr  4 22:57:00 2024]  Iteration number: 50 with current cost as 0.08677003031586851 and parameters 
[-2.90318299  2.23743503 -2.12427939 -0.32757738  1.54487519 -4.43879614
  2.8371232   1.89126359  1.68645101 -2.04439625 -0.80850654  1.30706256
  4.23568489 -2.45112886  2.49421718]. 
Working on 0.8 fold... 
[Thu Apr  4 22:59:44 2024]  Iteration number: 0 with current cost as 0.2040107522153226 and parameters 
[-2.90318345  2.23743464 -2.1242796  -0.10141192  0.45987249 -2.90158548
  3.57791405  1.39994962  0.75716634 -1.34984792  0.80744484  0.8643616
  1.27261328 -2.11053401  0.00764771]. 
[Thu Apr  4 23:04:24 2024]  Iteration number: 50 with current cost as 0.07683496576511009 and parameters 
[-2.90318391  2.23743484 -2.12427929  1.37515312 -1.35162693 -1.67737014
  7.01279204  0.53620401 -2.50758207  0.00842131  1.81713026  1.77817348
  4.4195452  -1.91319279  2.26760412]. 
Working on 1.0 fold... 
[Thu Apr  4 23:04:42 2024]  Iteration number: 0 with current cost as 0.25096250772166334 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.0139779   0.46746413 -2.78672857
  3.49107865  1.64779575  0.76571836 -1.35334597  0.85479454  0.87491963
  1.27074113 -2.04781193  0.23846906]. 
Training complete taking 1812.5224313735962 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 23:09:24 2024]  Iteration number: 0 with current cost as 0.22930133696983296 and parameters 
[-2.90318349  2.23743462 -2.12427964 -0.10149013  0.46190389 -2.89845243
  3.49376842  1.53211429  0.82696178 -1.3734468   0.81173336  0.83705606
  1.27397532 -2.07218913  0.14035223]. 
[Thu Apr  4 23:15:31 2024]  Iteration number: 50 with current cost as 0.08343057626883282 and parameters 
[-2.90318431  2.23743413 -2.12427978 -0.90800818 -0.8840039  -0.89851592
  5.02080153  0.3004745  -0.63417651 -0.71981656  1.22019161  1.99959206
  2.55195046 -2.40591419  1.16971741]. 
Working on 0.4 fold... 
[Thu Apr  4 23:17:30 2024]  Iteration number: 0 with current cost as 0.23448062944776937 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.16499735  0.48511526 -2.93627473
  3.46866313  1.52507975  0.87424827 -1.3703845   0.89456474  0.86667119
  1.31894041 -2.04067536  0.10235463]. 
Working on 0.6 fold... 
[Thu Apr  4 23:22:25 2024]  Iteration number: 0 with current cost as 0.3387438803124553 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.14446776  0.53846358 -2.8274426
  3.13905147  2.04730835  1.14507481 -1.13703832  0.6515629   1.07395094
  1.31391968 -1.9074027   0.5963043 ]. 
[Thu Apr  4 23:27:29 2024]  Iteration number: 50 with current cost as 0.08677003031586851 and parameters 
[-2.90318299  2.23743503 -2.12427939 -0.32757738  1.54487519 -4.43879614
  2.8371232   1.89126359  1.68645101 -2.04439625 -0.80850654  1.30706256
  4.23568489 -2.45112886  2.49421718]. 
Working on 0.8 fold... 
[Thu Apr  4 23:30:29 2024]  Iteration number: 0 with current cost as 0.2040107522153226 and parameters 
[-2.90318345  2.23743464 -2.1242796  -0.10141192  0.45987249 -2.90158548
  3.57791405  1.39994962  0.75716634 -1.34984792  0.80744484  0.8643616
  1.27261328 -2.11053401  0.00764771]. 
[Thu Apr  4 23:35:14 2024]  Iteration number: 50 with current cost as 0.07683496576511009 and parameters 
[-2.90318391  2.23743484 -2.12427929  1.37515312 -1.35162693 -1.67737014
  7.01279204  0.53620401 -2.50758207  0.00842131  1.81713026  1.77817348
  4.4195452  -1.91319279  2.26760412]. 
Working on 1.0 fold... 
[Thu Apr  4 23:35:30 2024]  Iteration number: 0 with current cost as 0.25096250772166334 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.0139779   0.46746413 -2.78672857
  3.49107865  1.64779575  0.76571836 -1.35334597  0.85479454  0.87491963
  1.27074113 -2.04781193  0.23846906]. 
Training complete taking 1846.8478033542633 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 23:40:09 2024]  Iteration number: 0 with current cost as 0.22930133696983296 and parameters 
[-2.90318349  2.23743462 -2.12427964 -0.10149013  0.46190389 -2.89845243
  3.49376842  1.53211429  0.82696178 -1.3734468   0.81173336  0.83705606
  1.27397532 -2.07218913  0.14035223]. 
[Thu Apr  4 23:46:03 2024]  Iteration number: 50 with current cost as 0.08343057626883282 and parameters 
[-2.90318431  2.23743413 -2.12427978 -0.90800818 -0.8840039  -0.89851592
  5.02080153  0.3004745  -0.63417651 -0.71981656  1.22019161  1.99959206
  2.55195046 -2.40591419  1.16971741]. 
Working on 0.4 fold... 
[Thu Apr  4 23:48:01 2024]  Iteration number: 0 with current cost as 0.23448062944776937 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.16499735  0.48511526 -2.93627473
  3.46866313  1.52507975  0.87424827 -1.3703845   0.89456474  0.86667119
  1.31894041 -2.04067536  0.10235463]. 
Working on 0.6 fold... 
[Thu Apr  4 23:52:41 2024]  Iteration number: 0 with current cost as 0.3387438803124553 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.14446776  0.53846358 -2.8274426
  3.13905147  2.04730835  1.14507481 -1.13703832  0.6515629   1.07395094
  1.31391968 -1.9074027   0.5963043 ]. 
[Thu Apr  4 23:57:52 2024]  Iteration number: 50 with current cost as 0.08677003031586851 and parameters 
[-2.90318299  2.23743503 -2.12427939 -0.32757738  1.54487519 -4.43879614
  2.8371232   1.89126359  1.68645101 -2.04439625 -0.80850654  1.30706256
  4.23568489 -2.45112886  2.49421718]. 
Working on 0.8 fold... 
[Fri Apr  5 00:00:23 2024]  Iteration number: 0 with current cost as 0.2040107522153226 and parameters 
[-2.90318345  2.23743464 -2.1242796  -0.10141192  0.45987249 -2.90158548
  3.57791405  1.39994962  0.75716634 -1.34984792  0.80744484  0.8643616
  1.27261328 -2.11053401  0.00764771]. 
[Fri Apr  5 00:05:01 2024]  Iteration number: 50 with current cost as 0.07683496576511009 and parameters 
[-2.90318391  2.23743484 -2.12427929  1.37515312 -1.35162693 -1.67737014
  7.01279204  0.53620401 -2.50758207  0.00842131  1.81713026  1.77817348
  4.4195452  -1.91319279  2.26760412]. 
Working on 1.0 fold... 
[Fri Apr  5 00:05:19 2024]  Iteration number: 0 with current cost as 0.25096250772166334 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.0139779   0.46746413 -2.78672857
  3.49107865  1.64779575  0.76571836 -1.35334597  0.85479454  0.87491963
  1.27074113 -2.04781193  0.23846906]. 
Training complete taking 1786.8092150688171 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 00:09:57 2024]  Iteration number: 0 with current cost as 0.22930133696983296 and parameters 
[-2.90318349  2.23743462 -2.12427964 -0.10149013  0.46190389 -2.89845243
  3.49376842  1.53211429  0.82696178 -1.3734468   0.81173336  0.83705606
  1.27397532 -2.07218913  0.14035223]. 
[Fri Apr  5 00:15:52 2024]  Iteration number: 50 with current cost as 0.08343057626883282 and parameters 
[-2.90318431  2.23743413 -2.12427978 -0.90800818 -0.8840039  -0.89851592
  5.02080153  0.3004745  -0.63417651 -0.71981656  1.22019161  1.99959206
  2.55195046 -2.40591419  1.16971741]. 
Working on 0.4 fold... 
[Fri Apr  5 00:17:50 2024]  Iteration number: 0 with current cost as 0.23448062944776937 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.16499735  0.48511526 -2.93627473
  3.46866313  1.52507975  0.87424827 -1.3703845   0.89456474  0.86667119
  1.31894041 -2.04067536  0.10235463]. 
Working on 0.6 fold... 
[Fri Apr  5 00:22:31 2024]  Iteration number: 0 with current cost as 0.3387438803124553 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.14446776  0.53846358 -2.8274426
  3.13905147  2.04730835  1.14507481 -1.13703832  0.6515629   1.07395094
  1.31391968 -1.9074027   0.5963043 ]. 
[Fri Apr  5 00:27:35 2024]  Iteration number: 50 with current cost as 0.08677003031586851 and parameters 
[-2.90318299  2.23743503 -2.12427939 -0.32757738  1.54487519 -4.43879614
  2.8371232   1.89126359  1.68645101 -2.04439625 -0.80850654  1.30706256
  4.23568489 -2.45112886  2.49421718]. 
Working on 0.8 fold... 
[Fri Apr  5 00:30:17 2024]  Iteration number: 0 with current cost as 0.2040107522153226 and parameters 
[-2.90318345  2.23743464 -2.1242796  -0.10141192  0.45987249 -2.90158548
  3.57791405  1.39994962  0.75716634 -1.34984792  0.80744484  0.8643616
  1.27261328 -2.11053401  0.00764771]. 
[Fri Apr  5 00:34:57 2024]  Iteration number: 50 with current cost as 0.07683496576511009 and parameters 
[-2.90318391  2.23743484 -2.12427929  1.37515312 -1.35162693 -1.67737014
  7.01279204  0.53620401 -2.50758207  0.00842131  1.81713026  1.77817348
  4.4195452  -1.91319279  2.26760412]. 
Working on 1.0 fold... 
[Fri Apr  5 00:35:13 2024]  Iteration number: 0 with current cost as 0.25096250772166334 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.0139779   0.46746413 -2.78672857
  3.49107865  1.64779575  0.76571836 -1.35334597  0.85479454  0.87491963
  1.27074113 -2.04781193  0.23846906]. 
Training complete taking 1796.6361966133118 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 00:39:54 2024]  Iteration number: 0 with current cost as 0.22930133696983296 and parameters 
[-2.90318349  2.23743462 -2.12427964 -0.10149013  0.46190389 -2.89845243
  3.49376842  1.53211429  0.82696178 -1.3734468   0.81173336  0.83705606
  1.27397532 -2.07218913  0.14035223]. 
[Fri Apr  5 00:45:51 2024]  Iteration number: 50 with current cost as 0.08343057626883282 and parameters 
[-2.90318431  2.23743413 -2.12427978 -0.90800818 -0.8840039  -0.89851592
  5.02080153  0.3004745  -0.63417651 -0.71981656  1.22019161  1.99959206
  2.55195046 -2.40591419  1.16971741]. 
Working on 0.4 fold... 
[Fri Apr  5 00:47:50 2024]  Iteration number: 0 with current cost as 0.23448062944776937 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.16499735  0.48511526 -2.93627473
  3.46866313  1.52507975  0.87424827 -1.3703845   0.89456474  0.86667119
  1.31894041 -2.04067536  0.10235463]. 
Working on 0.6 fold... 
[Fri Apr  5 00:52:28 2024]  Iteration number: 0 with current cost as 0.3387438803124553 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.14446776  0.53846358 -2.8274426
  3.13905147  2.04730835  1.14507481 -1.13703832  0.6515629   1.07395094
  1.31391968 -1.9074027   0.5963043 ]. 
[Fri Apr  5 00:57:31 2024]  Iteration number: 50 with current cost as 0.08677003031586851 and parameters 
[-2.90318299  2.23743503 -2.12427939 -0.32757738  1.54487519 -4.43879614
  2.8371232   1.89126359  1.68645101 -2.04439625 -0.80850654  1.30706256
  4.23568489 -2.45112886  2.49421718]. 
Working on 0.8 fold... 
[Fri Apr  5 01:00:02 2024]  Iteration number: 0 with current cost as 0.2040107522153226 and parameters 
[-2.90318345  2.23743464 -2.1242796  -0.10141192  0.45987249 -2.90158548
  3.57791405  1.39994962  0.75716634 -1.34984792  0.80744484  0.8643616
  1.27261328 -2.11053401  0.00764771]. 
[Fri Apr  5 01:04:52 2024]  Iteration number: 50 with current cost as 0.07683496576511009 and parameters 
[-2.90318391  2.23743484 -2.12427929  1.37515312 -1.35162693 -1.67737014
  7.01279204  0.53620401 -2.50758207  0.00842131  1.81713026  1.77817348
  4.4195452  -1.91319279  2.26760412]. 
Working on 1.0 fold... 
[Fri Apr  5 01:05:09 2024]  Iteration number: 0 with current cost as 0.25096250772166334 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.0139779   0.46746413 -2.78672857
  3.49107865  1.64779575  0.76571836 -1.35334597  0.85479454  0.87491963
  1.27074113 -2.04781193  0.23846906]. 
Training complete taking 1797.2444014549255 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 01:09:51 2024]  Iteration number: 0 with current cost as 0.22930133696983296 and parameters 
[-2.90318349  2.23743462 -2.12427964 -0.10149013  0.46190389 -2.89845243
  3.49376842  1.53211429  0.82696178 -1.3734468   0.81173336  0.83705606
  1.27397532 -2.07218913  0.14035223]. 
[Fri Apr  5 01:15:46 2024]  Iteration number: 50 with current cost as 0.08343057626883282 and parameters 
[-2.90318431  2.23743413 -2.12427978 -0.90800818 -0.8840039  -0.89851592
  5.02080153  0.3004745  -0.63417651 -0.71981656  1.22019161  1.99959206
  2.55195046 -2.40591419  1.16971741]. 
Working on 0.4 fold... 
[Fri Apr  5 01:17:44 2024]  Iteration number: 0 with current cost as 0.23448062944776937 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.16499735  0.48511526 -2.93627473
  3.46866313  1.52507975  0.87424827 -1.3703845   0.89456474  0.86667119
  1.31894041 -2.04067536  0.10235463]. 
Working on 0.6 fold... 
[Fri Apr  5 01:22:26 2024]  Iteration number: 0 with current cost as 0.3387438803124553 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.14446776  0.53846358 -2.8274426
  3.13905147  2.04730835  1.14507481 -1.13703832  0.6515629   1.07395094
  1.31391968 -1.9074027   0.5963043 ]. 
[Fri Apr  5 01:27:29 2024]  Iteration number: 50 with current cost as 0.08677003031586851 and parameters 
[-2.90318299  2.23743503 -2.12427939 -0.32757738  1.54487519 -4.43879614
  2.8371232   1.89126359  1.68645101 -2.04439625 -0.80850654  1.30706256
  4.23568489 -2.45112886  2.49421718]. 
Working on 0.8 fold... 
[Fri Apr  5 01:30:01 2024]  Iteration number: 0 with current cost as 0.2040107522153226 and parameters 
[-2.90318345  2.23743464 -2.1242796  -0.10141192  0.45987249 -2.90158548
  3.57791405  1.39994962  0.75716634 -1.34984792  0.80744484  0.8643616
  1.27261328 -2.11053401  0.00764771]. 
[Fri Apr  5 01:34:50 2024]  Iteration number: 50 with current cost as 0.07683496576511009 and parameters 
[-2.90318391  2.23743484 -2.12427929  1.37515312 -1.35162693 -1.67737014
  7.01279204  0.53620401 -2.50758207  0.00842131  1.81713026  1.77817348
  4.4195452  -1.91319279  2.26760412]. 
Working on 1.0 fold... 
[Fri Apr  5 01:35:07 2024]  Iteration number: 0 with current cost as 0.25096250772166334 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.0139779   0.46746413 -2.78672857
  3.49107865  1.64779575  0.76571836 -1.35334597  0.85479454  0.87491963
  1.27074113 -2.04781193  0.23846906]. 
Training complete taking 1795.0330457687378 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 01:39:45 2024]  Iteration number: 0 with current cost as 0.22930133696983296 and parameters 
[-2.90318349  2.23743462 -2.12427964 -0.10149013  0.46190389 -2.89845243
  3.49376842  1.53211429  0.82696178 -1.3734468   0.81173336  0.83705606
  1.27397532 -2.07218913  0.14035223]. 
[Fri Apr  5 01:45:41 2024]  Iteration number: 50 with current cost as 0.08343057626883282 and parameters 
[-2.90318431  2.23743413 -2.12427978 -0.90800818 -0.8840039  -0.89851592
  5.02080153  0.3004745  -0.63417651 -0.71981656  1.22019161  1.99959206
  2.55195046 -2.40591419  1.16971741]. 
Working on 0.4 fold... 
[Fri Apr  5 01:47:39 2024]  Iteration number: 0 with current cost as 0.23448062944776937 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.16499735  0.48511526 -2.93627473
  3.46866313  1.52507975  0.87424827 -1.3703845   0.89456474  0.86667119
  1.31894041 -2.04067536  0.10235463]. 
Working on 0.6 fold... 
[Fri Apr  5 01:52:23 2024]  Iteration number: 0 with current cost as 0.3387438803124553 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.14446776  0.53846358 -2.8274426
  3.13905147  2.04730835  1.14507481 -1.13703832  0.6515629   1.07395094
  1.31391968 -1.9074027   0.5963043 ]. 
[Fri Apr  5 01:57:41 2024]  Iteration number: 50 with current cost as 0.08677003031586851 and parameters 
[-2.90318299  2.23743503 -2.12427939 -0.32757738  1.54487519 -4.43879614
  2.8371232   1.89126359  1.68645101 -2.04439625 -0.80850654  1.30706256
  4.23568489 -2.45112886  2.49421718]. 
Working on 0.8 fold... 
[Fri Apr  5 02:00:12 2024]  Iteration number: 0 with current cost as 0.2040107522153226 and parameters 
[-2.90318345  2.23743464 -2.1242796  -0.10141192  0.45987249 -2.90158548
  3.57791405  1.39994962  0.75716634 -1.34984792  0.80744484  0.8643616
  1.27261328 -2.11053401  0.00764771]. 
[Fri Apr  5 02:04:55 2024]  Iteration number: 50 with current cost as 0.07683496576511009 and parameters 
[-2.90318391  2.23743484 -2.12427929  1.37515312 -1.35162693 -1.67737014
  7.01279204  0.53620401 -2.50758207  0.00842131  1.81713026  1.77817348
  4.4195452  -1.91319279  2.26760412]. 
Working on 1.0 fold... 
[Fri Apr  5 02:05:11 2024]  Iteration number: 0 with current cost as 0.25096250772166334 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.0139779   0.46746413 -2.78672857
  3.49107865  1.64779575  0.76571836 -1.35334597  0.85479454  0.87491963
  1.27074113 -2.04781193  0.23846906]. 
Training complete taking 1817.2774231433868 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 02:10:05 2024]  Iteration number: 0 with current cost as 0.22930133696983296 and parameters 
[-2.90318349  2.23743462 -2.12427964 -0.10149013  0.46190389 -2.89845243
  3.49376842  1.53211429  0.82696178 -1.3734468   0.81173336  0.83705606
  1.27397532 -2.07218913  0.14035223]. 
[Fri Apr  5 02:16:32 2024]  Iteration number: 50 with current cost as 0.08343057626883282 and parameters 
[-2.90318431  2.23743413 -2.12427978 -0.90800818 -0.8840039  -0.89851592
  5.02080153  0.3004745  -0.63417651 -0.71981656  1.22019161  1.99959206
  2.55195046 -2.40591419  1.16971741]. 
Working on 0.4 fold... 
[Fri Apr  5 02:18:38 2024]  Iteration number: 0 with current cost as 0.23448062944776937 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.16499735  0.48511526 -2.93627473
  3.46866313  1.52507975  0.87424827 -1.3703845   0.89456474  0.86667119
  1.31894041 -2.04067536  0.10235463]. 
Working on 0.6 fold... 
[Fri Apr  5 02:23:17 2024]  Iteration number: 0 with current cost as 0.3387438803124553 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.14446776  0.53846358 -2.8274426
  3.13905147  2.04730835  1.14507481 -1.13703832  0.6515629   1.07395094
  1.31391968 -1.9074027   0.5963043 ]. 
[Fri Apr  5 02:28:30 2024]  Iteration number: 50 with current cost as 0.08677003031586851 and parameters 
[-2.90318299  2.23743503 -2.12427939 -0.32757738  1.54487519 -4.43879614
  2.8371232   1.89126359  1.68645101 -2.04439625 -0.80850654  1.30706256
  4.23568489 -2.45112886  2.49421718]. 
Working on 0.8 fold... 
[Fri Apr  5 02:31:04 2024]  Iteration number: 0 with current cost as 0.2040107522153226 and parameters 
[-2.90318345  2.23743464 -2.1242796  -0.10141192  0.45987249 -2.90158548
  3.57791405  1.39994962  0.75716634 -1.34984792  0.80744484  0.8643616
  1.27261328 -2.11053401  0.00764771]. 
[Fri Apr  5 02:35:44 2024]  Iteration number: 50 with current cost as 0.07683496576511009 and parameters 
[-2.90318391  2.23743484 -2.12427929  1.37515312 -1.35162693 -1.67737014
  7.01279204  0.53620401 -2.50758207  0.00842131  1.81713026  1.77817348
  4.4195452  -1.91319279  2.26760412]. 
Working on 1.0 fold... 
[Fri Apr  5 02:36:02 2024]  Iteration number: 0 with current cost as 0.25096250772166334 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.0139779   0.46746413 -2.78672857
  3.49107865  1.64779575  0.76571836 -1.35334597  0.85479454  0.87491963
  1.27074113 -2.04781193  0.23846906]. 
Training complete taking 1841.4359691143036 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 02:40:43 2024]  Iteration number: 0 with current cost as 0.22930133696983296 and parameters 
[-2.90318349  2.23743462 -2.12427964 -0.10149013  0.46190389 -2.89845243
  3.49376842  1.53211429  0.82696178 -1.3734468   0.81173336  0.83705606
  1.27397532 -2.07218913  0.14035223]. 
[Fri Apr  5 02:46:37 2024]  Iteration number: 50 with current cost as 0.08343057626883282 and parameters 
[-2.90318431  2.23743413 -2.12427978 -0.90800818 -0.8840039  -0.89851592
  5.02080153  0.3004745  -0.63417651 -0.71981656  1.22019161  1.99959206
  2.55195046 -2.40591419  1.16971741]. 
Working on 0.4 fold... 
[Fri Apr  5 02:48:36 2024]  Iteration number: 0 with current cost as 0.23448062944776937 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.16499735  0.48511526 -2.93627473
  3.46866313  1.52507975  0.87424827 -1.3703845   0.89456474  0.86667119
  1.31894041 -2.04067536  0.10235463]. 
Working on 0.6 fold... 
[Fri Apr  5 02:53:17 2024]  Iteration number: 0 with current cost as 0.3387438803124553 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.14446776  0.53846358 -2.8274426
  3.13905147  2.04730835  1.14507481 -1.13703832  0.6515629   1.07395094
  1.31391968 -1.9074027   0.5963043 ]. 
[Fri Apr  5 02:58:35 2024]  Iteration number: 50 with current cost as 0.08677003031586851 and parameters 
[-2.90318299  2.23743503 -2.12427939 -0.32757738  1.54487519 -4.43879614
  2.8371232   1.89126359  1.68645101 -2.04439625 -0.80850654  1.30706256
  4.23568489 -2.45112886  2.49421718]. 
Working on 0.8 fold... 
[Fri Apr  5 03:01:10 2024]  Iteration number: 0 with current cost as 0.2040107522153226 and parameters 
[-2.90318345  2.23743464 -2.1242796  -0.10141192  0.45987249 -2.90158548
  3.57791405  1.39994962  0.75716634 -1.34984792  0.80744484  0.8643616
  1.27261328 -2.11053401  0.00764771]. 
[Fri Apr  5 03:05:52 2024]  Iteration number: 50 with current cost as 0.07683496576511009 and parameters 
[-2.90318391  2.23743484 -2.12427929  1.37515312 -1.35162693 -1.67737014
  7.01279204  0.53620401 -2.50758207  0.00842131  1.81713026  1.77817348
  4.4195452  -1.91319279  2.26760412]. 
Working on 1.0 fold... 
[Fri Apr  5 03:06:09 2024]  Iteration number: 0 with current cost as 0.25096250772166334 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.0139779   0.46746413 -2.78672857
  3.49107865  1.64779575  0.76571836 -1.35334597  0.85479454  0.87491963
  1.27074113 -2.04781193  0.23846906]. 
Training complete taking 1806.1569974422455 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 03:10:50 2024]  Iteration number: 0 with current cost as 0.22930133696983296 and parameters 
[-2.90318349  2.23743462 -2.12427964 -0.10149013  0.46190389 -2.89845243
  3.49376842  1.53211429  0.82696178 -1.3734468   0.81173336  0.83705606
  1.27397532 -2.07218913  0.14035223]. 
[Fri Apr  5 03:16:52 2024]  Iteration number: 50 with current cost as 0.08343057626883282 and parameters 
[-2.90318431  2.23743413 -2.12427978 -0.90800818 -0.8840039  -0.89851592
  5.02080153  0.3004745  -0.63417651 -0.71981656  1.22019161  1.99959206
  2.55195046 -2.40591419  1.16971741]. 
Working on 0.4 fold... 
[Fri Apr  5 03:18:49 2024]  Iteration number: 0 with current cost as 0.23448062944776937 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.16499735  0.48511526 -2.93627473
  3.46866313  1.52507975  0.87424827 -1.3703845   0.89456474  0.86667119
  1.31894041 -2.04067536  0.10235463]. 
Working on 0.6 fold... 
[Fri Apr  5 03:23:49 2024]  Iteration number: 0 with current cost as 0.3387438803124553 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.14446776  0.53846358 -2.8274426
  3.13905147  2.04730835  1.14507481 -1.13703832  0.6515629   1.07395094
  1.31391968 -1.9074027   0.5963043 ]. 
[Fri Apr  5 03:28:51 2024]  Iteration number: 50 with current cost as 0.08677003031586851 and parameters 
[-2.90318299  2.23743503 -2.12427939 -0.32757738  1.54487519 -4.43879614
  2.8371232   1.89126359  1.68645101 -2.04439625 -0.80850654  1.30706256
  4.23568489 -2.45112886  2.49421718]. 
Working on 0.8 fold... 
[Fri Apr  5 03:31:24 2024]  Iteration number: 0 with current cost as 0.2040107522153226 and parameters 
[-2.90318345  2.23743464 -2.1242796  -0.10141192  0.45987249 -2.90158548
  3.57791405  1.39994962  0.75716634 -1.34984792  0.80744484  0.8643616
  1.27261328 -2.11053401  0.00764771]. 
[Fri Apr  5 03:36:12 2024]  Iteration number: 50 with current cost as 0.07683496576511009 and parameters 
[-2.90318391  2.23743484 -2.12427929  1.37515312 -1.35162693 -1.67737014
  7.01279204  0.53620401 -2.50758207  0.00842131  1.81713026  1.77817348
  4.4195452  -1.91319279  2.26760412]. 
Working on 1.0 fold... 
[Fri Apr  5 03:36:30 2024]  Iteration number: 0 with current cost as 0.25096250772166334 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.0139779   0.46746413 -2.78672857
  3.49107865  1.64779575  0.76571836 -1.35334597  0.85479454  0.87491963
  1.27074113 -2.04781193  0.23846906]. 
Training complete taking 1864.3415541648865 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 03:41:54 2024]  Iteration number: 0 with current cost as 0.22930133696983296 and parameters 
[-2.90318349  2.23743462 -2.12427964 -0.10149013  0.46190389 -2.89845243
  3.49376842  1.53211429  0.82696178 -1.3734468   0.81173336  0.83705606
  1.27397532 -2.07218913  0.14035223]. 
[Fri Apr  5 03:47:49 2024]  Iteration number: 50 with current cost as 0.08343057626883282 and parameters 
[-2.90318431  2.23743413 -2.12427978 -0.90800818 -0.8840039  -0.89851592
  5.02080153  0.3004745  -0.63417651 -0.71981656  1.22019161  1.99959206
  2.55195046 -2.40591419  1.16971741]. 
Working on 0.4 fold... 
[Fri Apr  5 03:49:47 2024]  Iteration number: 0 with current cost as 0.23448062944776937 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.16499735  0.48511526 -2.93627473
  3.46866313  1.52507975  0.87424827 -1.3703845   0.89456474  0.86667119
  1.31894041 -2.04067536  0.10235463]. 
Working on 0.6 fold... 
[Fri Apr  5 03:54:30 2024]  Iteration number: 0 with current cost as 0.3387438803124553 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.14446776  0.53846358 -2.8274426
  3.13905147  2.04730835  1.14507481 -1.13703832  0.6515629   1.07395094
  1.31391968 -1.9074027   0.5963043 ]. 
[Fri Apr  5 03:59:33 2024]  Iteration number: 50 with current cost as 0.08677003031586851 and parameters 
[-2.90318299  2.23743503 -2.12427939 -0.32757738  1.54487519 -4.43879614
  2.8371232   1.89126359  1.68645101 -2.04439625 -0.80850654  1.30706256
  4.23568489 -2.45112886  2.49421718]. 
Working on 0.8 fold... 
[Fri Apr  5 04:02:05 2024]  Iteration number: 0 with current cost as 0.2040107522153226 and parameters 
[-2.90318345  2.23743464 -2.1242796  -0.10141192  0.45987249 -2.90158548
  3.57791405  1.39994962  0.75716634 -1.34984792  0.80744484  0.8643616
  1.27261328 -2.11053401  0.00764771]. 
[Fri Apr  5 04:06:44 2024]  Iteration number: 50 with current cost as 0.07683496576511009 and parameters 
[-2.90318391  2.23743484 -2.12427929  1.37515312 -1.35162693 -1.67737014
  7.01279204  0.53620401 -2.50758207  0.00842131  1.81713026  1.77817348
  4.4195452  -1.91319279  2.26760412]. 
Working on 1.0 fold... 
[Fri Apr  5 04:07:02 2024]  Iteration number: 0 with current cost as 0.25096250772166334 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.0139779   0.46746413 -2.78672857
  3.49107865  1.64779575  0.76571836 -1.35334597  0.85479454  0.87491963
  1.27074113 -2.04781193  0.23846906]. 
Training complete taking 1786.4453947544098 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 04:11:42 2024]  Iteration number: 0 with current cost as 0.22930133696983296 and parameters 
[-2.90318349  2.23743462 -2.12427964 -0.10149013  0.46190389 -2.89845243
  3.49376842  1.53211429  0.82696178 -1.3734468   0.81173336  0.83705606
  1.27397532 -2.07218913  0.14035223]. 
[Fri Apr  5 04:17:38 2024]  Iteration number: 50 with current cost as 0.08343057626883282 and parameters 
[-2.90318431  2.23743413 -2.12427978 -0.90800818 -0.8840039  -0.89851592
  5.02080153  0.3004745  -0.63417651 -0.71981656  1.22019161  1.99959206
  2.55195046 -2.40591419  1.16971741]. 
Working on 0.4 fold... 
[Fri Apr  5 04:19:36 2024]  Iteration number: 0 with current cost as 0.23448062944776937 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.16499735  0.48511526 -2.93627473
  3.46866313  1.52507975  0.87424827 -1.3703845   0.89456474  0.86667119
  1.31894041 -2.04067536  0.10235463]. 
Working on 0.6 fold... 
[Fri Apr  5 04:24:16 2024]  Iteration number: 0 with current cost as 0.3387438803124553 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.14446776  0.53846358 -2.8274426
  3.13905147  2.04730835  1.14507481 -1.13703832  0.6515629   1.07395094
  1.31391968 -1.9074027   0.5963043 ]. 
[Fri Apr  5 04:29:34 2024]  Iteration number: 50 with current cost as 0.08677003031586851 and parameters 
[-2.90318299  2.23743503 -2.12427939 -0.32757738  1.54487519 -4.43879614
  2.8371232   1.89126359  1.68645101 -2.04439625 -0.80850654  1.30706256
  4.23568489 -2.45112886  2.49421718]. 
Working on 0.8 fold... 
[Fri Apr  5 04:32:05 2024]  Iteration number: 0 with current cost as 0.2040107522153226 and parameters 
[-2.90318345  2.23743464 -2.1242796  -0.10141192  0.45987249 -2.90158548
  3.57791405  1.39994962  0.75716634 -1.34984792  0.80744484  0.8643616
  1.27261328 -2.11053401  0.00764771]. 
[Fri Apr  5 04:37:07 2024]  Iteration number: 50 with current cost as 0.07683496576511009 and parameters 
[-2.90318391  2.23743484 -2.12427929  1.37515312 -1.35162693 -1.67737014
  7.01279204  0.53620401 -2.50758207  0.00842131  1.81713026  1.77817348
  4.4195452  -1.91319279  2.26760412]. 
Working on 1.0 fold... 
[Fri Apr  5 04:37:23 2024]  Iteration number: 0 with current cost as 0.25096250772166334 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.0139779   0.46746413 -2.78672857
  3.49107865  1.64779575  0.76571836 -1.35334597  0.85479454  0.87491963
  1.27074113 -2.04781193  0.23846906]. 
Training complete taking 1829.3285460472107 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 04:42:11 2024]  Iteration number: 0 with current cost as 0.22930133696983296 and parameters 
[-2.90318349  2.23743462 -2.12427964 -0.10149013  0.46190389 -2.89845243
  3.49376842  1.53211429  0.82696178 -1.3734468   0.81173336  0.83705606
  1.27397532 -2.07218913  0.14035223]. 
[Fri Apr  5 04:48:03 2024]  Iteration number: 50 with current cost as 0.08343057626883282 and parameters 
[-2.90318431  2.23743413 -2.12427978 -0.90800818 -0.8840039  -0.89851592
  5.02080153  0.3004745  -0.63417651 -0.71981656  1.22019161  1.99959206
  2.55195046 -2.40591419  1.16971741]. 
Working on 0.4 fold... 
[Fri Apr  5 04:49:59 2024]  Iteration number: 0 with current cost as 0.23448062944776937 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.16499735  0.48511526 -2.93627473
  3.46866313  1.52507975  0.87424827 -1.3703845   0.89456474  0.86667119
  1.31894041 -2.04067536  0.10235463]. 
Working on 0.6 fold... 
[Fri Apr  5 04:54:39 2024]  Iteration number: 0 with current cost as 0.3387438803124553 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.14446776  0.53846358 -2.8274426
  3.13905147  2.04730835  1.14507481 -1.13703832  0.6515629   1.07395094
  1.31391968 -1.9074027   0.5963043 ]. 
[Fri Apr  5 04:59:41 2024]  Iteration number: 50 with current cost as 0.08677003031586851 and parameters 
[-2.90318299  2.23743503 -2.12427939 -0.32757738  1.54487519 -4.43879614
  2.8371232   1.89126359  1.68645101 -2.04439625 -0.80850654  1.30706256
  4.23568489 -2.45112886  2.49421718]. 
Working on 0.8 fold... 
[Fri Apr  5 05:02:14 2024]  Iteration number: 0 with current cost as 0.2040107522153226 and parameters 
[-2.90318345  2.23743464 -2.1242796  -0.10141192  0.45987249 -2.90158548
  3.57791405  1.39994962  0.75716634 -1.34984792  0.80744484  0.8643616
  1.27261328 -2.11053401  0.00764771]. 
[Fri Apr  5 05:07:04 2024]  Iteration number: 50 with current cost as 0.07683496576511009 and parameters 
[-2.90318391  2.23743484 -2.12427929  1.37515312 -1.35162693 -1.67737014
  7.01279204  0.53620401 -2.50758207  0.00842131  1.81713026  1.77817348
  4.4195452  -1.91319279  2.26760412]. 
Working on 1.0 fold... 
[Fri Apr  5 05:07:20 2024]  Iteration number: 0 with current cost as 0.25096250772166334 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.0139779   0.46746413 -2.78672857
  3.49107865  1.64779575  0.76571836 -1.35334597  0.85479454  0.87491963
  1.27074113 -2.04781193  0.23846906]. 
Training complete taking 1789.5275452136993 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 05:11:59 2024]  Iteration number: 0 with current cost as 0.22930133696983296 and parameters 
[-2.90318349  2.23743462 -2.12427964 -0.10149013  0.46190389 -2.89845243
  3.49376842  1.53211429  0.82696178 -1.3734468   0.81173336  0.83705606
  1.27397532 -2.07218913  0.14035223]. 
[Fri Apr  5 05:17:54 2024]  Iteration number: 50 with current cost as 0.08343057626883282 and parameters 
[-2.90318431  2.23743413 -2.12427978 -0.90800818 -0.8840039  -0.89851592
  5.02080153  0.3004745  -0.63417651 -0.71981656  1.22019161  1.99959206
  2.55195046 -2.40591419  1.16971741]. 
Working on 0.4 fold... 
[Fri Apr  5 05:19:52 2024]  Iteration number: 0 with current cost as 0.23448062944776937 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.16499735  0.48511526 -2.93627473
  3.46866313  1.52507975  0.87424827 -1.3703845   0.89456474  0.86667119
  1.31894041 -2.04067536  0.10235463]. 
Working on 0.6 fold... 
[Fri Apr  5 05:24:45 2024]  Iteration number: 0 with current cost as 0.3387438803124553 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.14446776  0.53846358 -2.8274426
  3.13905147  2.04730835  1.14507481 -1.13703832  0.6515629   1.07395094
  1.31391968 -1.9074027   0.5963043 ]. 
[Fri Apr  5 05:29:52 2024]  Iteration number: 50 with current cost as 0.08677003031586851 and parameters 
[-2.90318299  2.23743503 -2.12427939 -0.32757738  1.54487519 -4.43879614
  2.8371232   1.89126359  1.68645101 -2.04439625 -0.80850654  1.30706256
  4.23568489 -2.45112886  2.49421718]. 
Working on 0.8 fold... 
[Fri Apr  5 05:32:28 2024]  Iteration number: 0 with current cost as 0.2040107522153226 and parameters 
[-2.90318345  2.23743464 -2.1242796  -0.10141192  0.45987249 -2.90158548
  3.57791405  1.39994962  0.75716634 -1.34984792  0.80744484  0.8643616
  1.27261328 -2.11053401  0.00764771]. 
[Fri Apr  5 05:37:17 2024]  Iteration number: 50 with current cost as 0.07683496576511009 and parameters 
[-2.90318391  2.23743484 -2.12427929  1.37515312 -1.35162693 -1.67737014
  7.01279204  0.53620401 -2.50758207  0.00842131  1.81713026  1.77817348
  4.4195452  -1.91319279  2.26760412]. 
Working on 1.0 fold... 
[Fri Apr  5 05:37:35 2024]  Iteration number: 0 with current cost as 0.25096250772166334 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.0139779   0.46746413 -2.78672857
  3.49107865  1.64779575  0.76571836 -1.35334597  0.85479454  0.87491963
  1.27074113 -2.04781193  0.23846906]. 
Training complete taking 1812.7695503234863 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 05:42:14 2024]  Iteration number: 0 with current cost as 0.22930133696983296 and parameters 
[-2.90318349  2.23743462 -2.12427964 -0.10149013  0.46190389 -2.89845243
  3.49376842  1.53211429  0.82696178 -1.3734468   0.81173336  0.83705606
  1.27397532 -2.07218913  0.14035223]. 
[Fri Apr  5 05:48:12 2024]  Iteration number: 50 with current cost as 0.08343057626883282 and parameters 
[-2.90318431  2.23743413 -2.12427978 -0.90800818 -0.8840039  -0.89851592
  5.02080153  0.3004745  -0.63417651 -0.71981656  1.22019161  1.99959206
  2.55195046 -2.40591419  1.16971741]. 
Working on 0.4 fold... 
[Fri Apr  5 05:50:14 2024]  Iteration number: 0 with current cost as 0.23448062944776937 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.16499735  0.48511526 -2.93627473
  3.46866313  1.52507975  0.87424827 -1.3703845   0.89456474  0.86667119
  1.31894041 -2.04067536  0.10235463]. 
Working on 0.6 fold... 
[Fri Apr  5 05:54:54 2024]  Iteration number: 0 with current cost as 0.3387438803124553 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.14446776  0.53846358 -2.8274426
  3.13905147  2.04730835  1.14507481 -1.13703832  0.6515629   1.07395094
  1.31391968 -1.9074027   0.5963043 ]. 
[Fri Apr  5 05:59:58 2024]  Iteration number: 50 with current cost as 0.08677003031586851 and parameters 
[-2.90318299  2.23743503 -2.12427939 -0.32757738  1.54487519 -4.43879614
  2.8371232   1.89126359  1.68645101 -2.04439625 -0.80850654  1.30706256
  4.23568489 -2.45112886  2.49421718]. 
Working on 0.8 fold... 
[Fri Apr  5 06:02:32 2024]  Iteration number: 0 with current cost as 0.2040107522153226 and parameters 
[-2.90318345  2.23743464 -2.1242796  -0.10141192  0.45987249 -2.90158548
  3.57791405  1.39994962  0.75716634 -1.34984792  0.80744484  0.8643616
  1.27261328 -2.11053401  0.00764771]. 
[Fri Apr  5 06:07:15 2024]  Iteration number: 50 with current cost as 0.07683496576511009 and parameters 
[-2.90318391  2.23743484 -2.12427929  1.37515312 -1.35162693 -1.67737014
  7.01279204  0.53620401 -2.50758207  0.00842131  1.81713026  1.77817348
  4.4195452  -1.91319279  2.26760412]. 
Working on 1.0 fold... 
[Fri Apr  5 06:07:31 2024]  Iteration number: 0 with current cost as 0.25096250772166334 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.0139779   0.46746413 -2.78672857
  3.49107865  1.64779575  0.76571836 -1.35334597  0.85479454  0.87491963
  1.27074113 -2.04781193  0.23846906]. 
Training complete taking 1798.7709023952484 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 06:12:12 2024]  Iteration number: 0 with current cost as 0.22930133696983296 and parameters 
[-2.90318349  2.23743462 -2.12427964 -0.10149013  0.46190389 -2.89845243
  3.49376842  1.53211429  0.82696178 -1.3734468   0.81173336  0.83705606
  1.27397532 -2.07218913  0.14035223]. 
[Fri Apr  5 06:18:10 2024]  Iteration number: 50 with current cost as 0.08343057626883282 and parameters 
[-2.90318431  2.23743413 -2.12427978 -0.90800818 -0.8840039  -0.89851592
  5.02080153  0.3004745  -0.63417651 -0.71981656  1.22019161  1.99959206
  2.55195046 -2.40591419  1.16971741]. 
Working on 0.4 fold... 
[Fri Apr  5 06:20:07 2024]  Iteration number: 0 with current cost as 0.23448062944776937 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.16499735  0.48511526 -2.93627473
  3.46866313  1.52507975  0.87424827 -1.3703845   0.89456474  0.86667119
  1.31894041 -2.04067536  0.10235463]. 
Working on 0.6 fold... 
[Fri Apr  5 06:24:54 2024]  Iteration number: 0 with current cost as 0.3387438803124553 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.14446776  0.53846358 -2.8274426
  3.13905147  2.04730835  1.14507481 -1.13703832  0.6515629   1.07395094
  1.31391968 -1.9074027   0.5963043 ]. 
[Fri Apr  5 06:30:13 2024]  Iteration number: 50 with current cost as 0.08677003031586851 and parameters 
[-2.90318299  2.23743503 -2.12427939 -0.32757738  1.54487519 -4.43879614
  2.8371232   1.89126359  1.68645101 -2.04439625 -0.80850654  1.30706256
  4.23568489 -2.45112886  2.49421718]. 
Working on 0.8 fold... 
[Fri Apr  5 06:32:44 2024]  Iteration number: 0 with current cost as 0.2040107522153226 and parameters 
[-2.90318345  2.23743464 -2.1242796  -0.10141192  0.45987249 -2.90158548
  3.57791405  1.39994962  0.75716634 -1.34984792  0.80744484  0.8643616
  1.27261328 -2.11053401  0.00764771]. 
[Fri Apr  5 06:37:24 2024]  Iteration number: 50 with current cost as 0.07683496576511009 and parameters 
[-2.90318391  2.23743484 -2.12427929  1.37515312 -1.35162693 -1.67737014
  7.01279204  0.53620401 -2.50758207  0.00842131  1.81713026  1.77817348
  4.4195452  -1.91319279  2.26760412]. 
Working on 1.0 fold... 
[Fri Apr  5 06:37:42 2024]  Iteration number: 0 with current cost as 0.25096250772166334 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.0139779   0.46746413 -2.78672857
  3.49107865  1.64779575  0.76571836 -1.35334597  0.85479454  0.87491963
  1.27074113 -2.04781193  0.23846906]. 
Training complete taking 1808.9896848201752 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 06:42:21 2024]  Iteration number: 0 with current cost as 0.22930133696983296 and parameters 
[-2.90318349  2.23743462 -2.12427964 -0.10149013  0.46190389 -2.89845243
  3.49376842  1.53211429  0.82696178 -1.3734468   0.81173336  0.83705606
  1.27397532 -2.07218913  0.14035223]. 
[Fri Apr  5 06:48:16 2024]  Iteration number: 50 with current cost as 0.08343057626883282 and parameters 
[-2.90318431  2.23743413 -2.12427978 -0.90800818 -0.8840039  -0.89851592
  5.02080153  0.3004745  -0.63417651 -0.71981656  1.22019161  1.99959206
  2.55195046 -2.40591419  1.16971741]. 
Working on 0.4 fold... 
[Fri Apr  5 06:50:14 2024]  Iteration number: 0 with current cost as 0.23448062944776937 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.16499735  0.48511526 -2.93627473
  3.46866313  1.52507975  0.87424827 -1.3703845   0.89456474  0.86667119
  1.31894041 -2.04067536  0.10235463]. 
Working on 0.6 fold... 
[Fri Apr  5 06:54:57 2024]  Iteration number: 0 with current cost as 0.3387438803124553 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.14446776  0.53846358 -2.8274426
  3.13905147  2.04730835  1.14507481 -1.13703832  0.6515629   1.07395094
  1.31391968 -1.9074027   0.5963043 ]. 
[Fri Apr  5 07:00:10 2024]  Iteration number: 50 with current cost as 0.08677003031586851 and parameters 
[-2.90318299  2.23743503 -2.12427939 -0.32757738  1.54487519 -4.43879614
  2.8371232   1.89126359  1.68645101 -2.04439625 -0.80850654  1.30706256
  4.23568489 -2.45112886  2.49421718]. 
Working on 0.8 fold... 
[Fri Apr  5 07:02:44 2024]  Iteration number: 0 with current cost as 0.2040107522153226 and parameters 
[-2.90318345  2.23743464 -2.1242796  -0.10141192  0.45987249 -2.90158548
  3.57791405  1.39994962  0.75716634 -1.34984792  0.80744484  0.8643616
  1.27261328 -2.11053401  0.00764771]. 
[Fri Apr  5 07:07:24 2024]  Iteration number: 50 with current cost as 0.07683496576511009 and parameters 
[-2.90318391  2.23743484 -2.12427929  1.37515312 -1.35162693 -1.67737014
  7.01279204  0.53620401 -2.50758207  0.00842131  1.81713026  1.77817348
  4.4195452  -1.91319279  2.26760412]. 
Working on 1.0 fold... 
[Fri Apr  5 07:07:41 2024]  Iteration number: 0 with current cost as 0.25096250772166334 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.0139779   0.46746413 -2.78672857
  3.49107865  1.64779575  0.76571836 -1.35334597  0.85479454  0.87491963
  1.27074113 -2.04781193  0.23846906]. 
Training complete taking 1799.8111264705658 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 07:12:21 2024]  Iteration number: 0 with current cost as 0.22930133696983296 and parameters 
[-2.90318349  2.23743462 -2.12427964 -0.10149013  0.46190389 -2.89845243
  3.49376842  1.53211429  0.82696178 -1.3734468   0.81173336  0.83705606
  1.27397532 -2.07218913  0.14035223]. 
[Fri Apr  5 07:18:33 2024]  Iteration number: 50 with current cost as 0.08343057626883282 and parameters 
[-2.90318431  2.23743413 -2.12427978 -0.90800818 -0.8840039  -0.89851592
  5.02080153  0.3004745  -0.63417651 -0.71981656  1.22019161  1.99959206
  2.55195046 -2.40591419  1.16971741]. 
Working on 0.4 fold... 
[Fri Apr  5 07:20:30 2024]  Iteration number: 0 with current cost as 0.23448062944776937 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.16499735  0.48511526 -2.93627473
  3.46866313  1.52507975  0.87424827 -1.3703845   0.89456474  0.86667119
  1.31894041 -2.04067536  0.10235463]. 
Working on 0.6 fold... 
[Fri Apr  5 07:25:15 2024]  Iteration number: 0 with current cost as 0.3387438803124553 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.14446776  0.53846358 -2.8274426
  3.13905147  2.04730835  1.14507481 -1.13703832  0.6515629   1.07395094
  1.31391968 -1.9074027   0.5963043 ]. 
[Fri Apr  5 07:30:24 2024]  Iteration number: 50 with current cost as 0.08677003031586851 and parameters 
[-2.90318299  2.23743503 -2.12427939 -0.32757738  1.54487519 -4.43879614
  2.8371232   1.89126359  1.68645101 -2.04439625 -0.80850654  1.30706256
  4.23568489 -2.45112886  2.49421718]. 
Working on 0.8 fold... 
[Fri Apr  5 07:33:08 2024]  Iteration number: 0 with current cost as 0.2040107522153226 and parameters 
[-2.90318345  2.23743464 -2.1242796  -0.10141192  0.45987249 -2.90158548
  3.57791405  1.39994962  0.75716634 -1.34984792  0.80744484  0.8643616
  1.27261328 -2.11053401  0.00764771]. 
[Fri Apr  5 07:37:51 2024]  Iteration number: 50 with current cost as 0.07683496576511009 and parameters 
[-2.90318391  2.23743484 -2.12427929  1.37515312 -1.35162693 -1.67737014
  7.01279204  0.53620401 -2.50758207  0.00842131  1.81713026  1.77817348
  4.4195452  -1.91319279  2.26760412]. 
Working on 1.0 fold... 
[Fri Apr  5 07:38:07 2024]  Iteration number: 0 with current cost as 0.25096250772166334 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.0139779   0.46746413 -2.78672857
  3.49107865  1.64779575  0.76571836 -1.35334597  0.85479454  0.87491963
  1.27074113 -2.04781193  0.23846906]. 
Training complete taking 1828.1966111660004 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 07:42:49 2024]  Iteration number: 0 with current cost as 0.22930133696983296 and parameters 
[-2.90318349  2.23743462 -2.12427964 -0.10149013  0.46190389 -2.89845243
  3.49376842  1.53211429  0.82696178 -1.3734468   0.81173336  0.83705606
  1.27397532 -2.07218913  0.14035223]. 
[Fri Apr  5 07:48:46 2024]  Iteration number: 50 with current cost as 0.08343057626883282 and parameters 
[-2.90318431  2.23743413 -2.12427978 -0.90800818 -0.8840039  -0.89851592
  5.02080153  0.3004745  -0.63417651 -0.71981656  1.22019161  1.99959206
  2.55195046 -2.40591419  1.16971741]. 
Working on 0.4 fold... 
[Fri Apr  5 07:50:44 2024]  Iteration number: 0 with current cost as 0.23448062944776937 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.16499735  0.48511526 -2.93627473
  3.46866313  1.52507975  0.87424827 -1.3703845   0.89456474  0.86667119
  1.31894041 -2.04067536  0.10235463]. 
Working on 0.6 fold... 
[Fri Apr  5 07:55:37 2024]  Iteration number: 0 with current cost as 0.3387438803124553 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.14446776  0.53846358 -2.8274426
  3.13905147  2.04730835  1.14507481 -1.13703832  0.6515629   1.07395094
  1.31391968 -1.9074027   0.5963043 ]. 
[Fri Apr  5 08:00:51 2024]  Iteration number: 50 with current cost as 0.08677003031586851 and parameters 
[-2.90318299  2.23743503 -2.12427939 -0.32757738  1.54487519 -4.43879614
  2.8371232   1.89126359  1.68645101 -2.04439625 -0.80850654  1.30706256
  4.23568489 -2.45112886  2.49421718]. 
Working on 0.8 fold... 
[Fri Apr  5 08:03:22 2024]  Iteration number: 0 with current cost as 0.2040107522153226 and parameters 
[-2.90318345  2.23743464 -2.1242796  -0.10141192  0.45987249 -2.90158548
  3.57791405  1.39994962  0.75716634 -1.34984792  0.80744484  0.8643616
  1.27261328 -2.11053401  0.00764771]. 
[Fri Apr  5 08:08:08 2024]  Iteration number: 50 with current cost as 0.07683496576511009 and parameters 
[-2.90318391  2.23743484 -2.12427929  1.37515312 -1.35162693 -1.67737014
  7.01279204  0.53620401 -2.50758207  0.00842131  1.81713026  1.77817348
  4.4195452  -1.91319279  2.26760412]. 
Working on 1.0 fold... 
[Fri Apr  5 08:08:26 2024]  Iteration number: 0 with current cost as 0.25096250772166334 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.0139779   0.46746413 -2.78672857
  3.49107865  1.64779575  0.76571836 -1.35334597  0.85479454  0.87491963
  1.27074113 -2.04781193  0.23846906]. 
Training complete taking 1817.952112197876 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 08:13:06 2024]  Iteration number: 0 with current cost as 0.22930133696983296 and parameters 
[-2.90318349  2.23743462 -2.12427964 -0.10149013  0.46190389 -2.89845243
  3.49376842  1.53211429  0.82696178 -1.3734468   0.81173336  0.83705606
  1.27397532 -2.07218913  0.14035223]. 
[Fri Apr  5 08:19:24 2024]  Iteration number: 50 with current cost as 0.08343057626883282 and parameters 
[-2.90318431  2.23743413 -2.12427978 -0.90800818 -0.8840039  -0.89851592
  5.02080153  0.3004745  -0.63417651 -0.71981656  1.22019161  1.99959206
  2.55195046 -2.40591419  1.16971741]. 
Working on 0.4 fold... 
[Fri Apr  5 08:21:22 2024]  Iteration number: 0 with current cost as 0.23448062944776937 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.16499735  0.48511526 -2.93627473
  3.46866313  1.52507975  0.87424827 -1.3703845   0.89456474  0.86667119
  1.31894041 -2.04067536  0.10235463]. 
Working on 0.6 fold... 
[Fri Apr  5 08:26:25 2024]  Iteration number: 0 with current cost as 0.3387438803124553 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.14446776  0.53846358 -2.8274426
  3.13905147  2.04730835  1.14507481 -1.13703832  0.6515629   1.07395094
  1.31391968 -1.9074027   0.5963043 ]. 
[Fri Apr  5 08:31:31 2024]  Iteration number: 50 with current cost as 0.08677003031586851 and parameters 
[-2.90318299  2.23743503 -2.12427939 -0.32757738  1.54487519 -4.43879614
  2.8371232   1.89126359  1.68645101 -2.04439625 -0.80850654  1.30706256
  4.23568489 -2.45112886  2.49421718]. 
Working on 0.8 fold... 
[Fri Apr  5 08:34:03 2024]  Iteration number: 0 with current cost as 0.2040107522153226 and parameters 
[-2.90318345  2.23743464 -2.1242796  -0.10141192  0.45987249 -2.90158548
  3.57791405  1.39994962  0.75716634 -1.34984792  0.80744484  0.8643616
  1.27261328 -2.11053401  0.00764771]. 
[Fri Apr  5 08:38:42 2024]  Iteration number: 50 with current cost as 0.07683496576511009 and parameters 
[-2.90318391  2.23743484 -2.12427929  1.37515312 -1.35162693 -1.67737014
  7.01279204  0.53620401 -2.50758207  0.00842131  1.81713026  1.77817348
  4.4195452  -1.91319279  2.26760412]. 
Working on 1.0 fold... 
[Fri Apr  5 08:38:59 2024]  Iteration number: 0 with current cost as 0.25096250772166334 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.0139779   0.46746413 -2.78672857
  3.49107865  1.64779575  0.76571836 -1.35334597  0.85479454  0.87491963
  1.27074113 -2.04781193  0.23846906]. 
Training complete taking 1833.156336069107 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 08:43:39 2024]  Iteration number: 0 with current cost as 0.22930133696983296 and parameters 
[-2.90318349  2.23743462 -2.12427964 -0.10149013  0.46190389 -2.89845243
  3.49376842  1.53211429  0.82696178 -1.3734468   0.81173336  0.83705606
  1.27397532 -2.07218913  0.14035223]. 
[Fri Apr  5 08:49:32 2024]  Iteration number: 50 with current cost as 0.08343057626883282 and parameters 
[-2.90318431  2.23743413 -2.12427978 -0.90800818 -0.8840039  -0.89851592
  5.02080153  0.3004745  -0.63417651 -0.71981656  1.22019161  1.99959206
  2.55195046 -2.40591419  1.16971741]. 
Working on 0.4 fold... 
[Fri Apr  5 08:51:30 2024]  Iteration number: 0 with current cost as 0.23448062944776937 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.16499735  0.48511526 -2.93627473
  3.46866313  1.52507975  0.87424827 -1.3703845   0.89456474  0.86667119
  1.31894041 -2.04067536  0.10235463]. 
Working on 0.6 fold... 
[Fri Apr  5 08:56:09 2024]  Iteration number: 0 with current cost as 0.3387438803124553 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.14446776  0.53846358 -2.8274426
  3.13905147  2.04730835  1.14507481 -1.13703832  0.6515629   1.07395094
  1.31391968 -1.9074027   0.5963043 ]. 
[Fri Apr  5 09:01:13 2024]  Iteration number: 50 with current cost as 0.08677003031586851 and parameters 
[-2.90318299  2.23743503 -2.12427939 -0.32757738  1.54487519 -4.43879614
  2.8371232   1.89126359  1.68645101 -2.04439625 -0.80850654  1.30706256
  4.23568489 -2.45112886  2.49421718]. 
Working on 0.8 fold... 
[Fri Apr  5 09:03:45 2024]  Iteration number: 0 with current cost as 0.2040107522153226 and parameters 
[-2.90318345  2.23743464 -2.1242796  -0.10141192  0.45987249 -2.90158548
  3.57791405  1.39994962  0.75716634 -1.34984792  0.80744484  0.8643616
  1.27261328 -2.11053401  0.00764771]. 
[Fri Apr  5 09:08:28 2024]  Iteration number: 50 with current cost as 0.07683496576511009 and parameters 
[-2.90318391  2.23743484 -2.12427929  1.37515312 -1.35162693 -1.67737014
  7.01279204  0.53620401 -2.50758207  0.00842131  1.81713026  1.77817348
  4.4195452  -1.91319279  2.26760412]. 
Working on 1.0 fold... 
[Fri Apr  5 09:08:44 2024]  Iteration number: 0 with current cost as 0.25096250772166334 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.0139779   0.46746413 -2.78672857
  3.49107865  1.64779575  0.76571836 -1.35334597  0.85479454  0.87491963
  1.27074113 -2.04781193  0.23846906]. 
Training complete taking 1791.0767662525177 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 09:13:32 2024]  Iteration number: 0 with current cost as 0.22930133696983296 and parameters 
[-2.90318349  2.23743462 -2.12427964 -0.10149013  0.46190389 -2.89845243
  3.49376842  1.53211429  0.82696178 -1.3734468   0.81173336  0.83705606
  1.27397532 -2.07218913  0.14035223]. 
[Fri Apr  5 09:19:30 2024]  Iteration number: 50 with current cost as 0.08343057626883282 and parameters 
[-2.90318431  2.23743413 -2.12427978 -0.90800818 -0.8840039  -0.89851592
  5.02080153  0.3004745  -0.63417651 -0.71981656  1.22019161  1.99959206
  2.55195046 -2.40591419  1.16971741]. 
Working on 0.4 fold... 
[Fri Apr  5 09:21:28 2024]  Iteration number: 0 with current cost as 0.23448062944776937 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.16499735  0.48511526 -2.93627473
  3.46866313  1.52507975  0.87424827 -1.3703845   0.89456474  0.86667119
  1.31894041 -2.04067536  0.10235463]. 
Working on 0.6 fold... 
[Fri Apr  5 09:26:10 2024]  Iteration number: 0 with current cost as 0.3387438803124553 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.14446776  0.53846358 -2.8274426
  3.13905147  2.04730835  1.14507481 -1.13703832  0.6515629   1.07395094
  1.31391968 -1.9074027   0.5963043 ]. 
[Fri Apr  5 09:31:13 2024]  Iteration number: 50 with current cost as 0.08677003031586851 and parameters 
[-2.90318299  2.23743503 -2.12427939 -0.32757738  1.54487519 -4.43879614
  2.8371232   1.89126359  1.68645101 -2.04439625 -0.80850654  1.30706256
  4.23568489 -2.45112886  2.49421718]. 
Working on 0.8 fold... 
[Fri Apr  5 09:33:59 2024]  Iteration number: 0 with current cost as 0.2040107522153226 and parameters 
[-2.90318345  2.23743464 -2.1242796  -0.10141192  0.45987249 -2.90158548
  3.57791405  1.39994962  0.75716634 -1.34984792  0.80744484  0.8643616
  1.27261328 -2.11053401  0.00764771]. 
[Fri Apr  5 09:38:40 2024]  Iteration number: 50 with current cost as 0.07683496576511009 and parameters 
[-2.90318391  2.23743484 -2.12427929  1.37515312 -1.35162693 -1.67737014
  7.01279204  0.53620401 -2.50758207  0.00842131  1.81713026  1.77817348
  4.4195452  -1.91319279  2.26760412]. 
Working on 1.0 fold... 
[Fri Apr  5 09:38:58 2024]  Iteration number: 0 with current cost as 0.25096250772166334 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.0139779   0.46746413 -2.78672857
  3.49107865  1.64779575  0.76571836 -1.35334597  0.85479454  0.87491963
  1.27074113 -2.04781193  0.23846906]. 
Training complete taking 1809.088175535202 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 09:43:39 2024]  Iteration number: 0 with current cost as 0.22930133696983296 and parameters 
[-2.90318349  2.23743462 -2.12427964 -0.10149013  0.46190389 -2.89845243
  3.49376842  1.53211429  0.82696178 -1.3734468   0.81173336  0.83705606
  1.27397532 -2.07218913  0.14035223]. 
[Fri Apr  5 09:49:33 2024]  Iteration number: 50 with current cost as 0.08343057626883282 and parameters 
[-2.90318431  2.23743413 -2.12427978 -0.90800818 -0.8840039  -0.89851592
  5.02080153  0.3004745  -0.63417651 -0.71981656  1.22019161  1.99959206
  2.55195046 -2.40591419  1.16971741]. 
Working on 0.4 fold... 
[Fri Apr  5 09:51:31 2024]  Iteration number: 0 with current cost as 0.23448062944776937 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.16499735  0.48511526 -2.93627473
  3.46866313  1.52507975  0.87424827 -1.3703845   0.89456474  0.86667119
  1.31894041 -2.04067536  0.10235463]. 
Working on 0.6 fold... 
[Fri Apr  5 09:56:11 2024]  Iteration number: 0 with current cost as 0.3387438803124553 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.14446776  0.53846358 -2.8274426
  3.13905147  2.04730835  1.14507481 -1.13703832  0.6515629   1.07395094
  1.31391968 -1.9074027   0.5963043 ]. 
[Fri Apr  5 10:01:14 2024]  Iteration number: 50 with current cost as 0.08677003031586851 and parameters 
[-2.90318299  2.23743503 -2.12427939 -0.32757738  1.54487519 -4.43879614
  2.8371232   1.89126359  1.68645101 -2.04439625 -0.80850654  1.30706256
  4.23568489 -2.45112886  2.49421718]. 
Working on 0.8 fold... 
[Fri Apr  5 10:03:46 2024]  Iteration number: 0 with current cost as 0.2040107522153226 and parameters 
[-2.90318345  2.23743464 -2.1242796  -0.10141192  0.45987249 -2.90158548
  3.57791405  1.39994962  0.75716634 -1.34984792  0.80744484  0.8643616
  1.27261328 -2.11053401  0.00764771]. 
[Fri Apr  5 10:08:26 2024]  Iteration number: 50 with current cost as 0.07683496576511009 and parameters 
[-2.90318391  2.23743484 -2.12427929  1.37515312 -1.35162693 -1.67737014
  7.01279204  0.53620401 -2.50758207  0.00842131  1.81713026  1.77817348
  4.4195452  -1.91319279  2.26760412]. 
Working on 1.0 fold... 
[Fri Apr  5 10:08:43 2024]  Iteration number: 0 with current cost as 0.25096250772166334 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.0139779   0.46746413 -2.78672857
  3.49107865  1.64779575  0.76571836 -1.35334597  0.85479454  0.87491963
  1.27074113 -2.04781193  0.23846906]. 
Training complete taking 1783.2570135593414 seconds. 
Discarding model... 

Training complete taking 45321.00486445427 total seconds. 
Now scoring model... 
Scoring complete taking 0.8682763576507568 seconds. 
Saved predicted values as A2-A2-CNOT_HWE-CNOT_predicted_values.csv
Model scores: {'MSE_train': (0.13167999335871836,), 'R2_train': 0.7377941334055889, 'MAE_train': 0.3034404580389869, 'MSE_test': 0.16968326055138938, 'R2_test': 0.6835466664059049, 'MAE_test': 0.3609568528456106}. 
Saved model results as A2-A2-CNOT_HWE-CNOT_results.json. 
