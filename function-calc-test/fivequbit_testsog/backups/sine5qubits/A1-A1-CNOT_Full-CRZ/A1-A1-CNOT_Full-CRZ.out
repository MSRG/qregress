/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /scratch/j/jacobsen/gjones/sine5qubits/sine_train.bin... 
Successfully loaded /scratch/j/jacobsen/gjones/sine5qubits/sine_train.bin into X and y data. 
Loading dataset from /scratch/j/jacobsen/gjones/sine5qubits/sine_test.bin... 
Successfully loaded /scratch/j/jacobsen/gjones/sine5qubits/sine_test.bin into X and y data. 
Training model with dataset /scratch/j/jacobsen/gjones/sine5qubits/sine_train.bin 
 at time Thu Apr  4 21:37:45 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 21:40:45 2024]  Iteration number: 0 with current cost as 0.6752914842561337 and parameters 
[-0.07783359  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010916
  3.06858479  2.18960126  1.18552008 -1.06648327  0.60271491  1.14432426
  1.31029899 -1.8735468   0.72965061  2.885784   -0.54534335 -0.47522485
 -2.0265425   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 21:51:20 2024]  Iteration number: 0 with current cost as 0.30564058391695037 and parameters 
[ 1.45292768  2.23743464 -2.12427947 -0.11653103  0.55388708 -2.77010931
  3.06858498  2.18960145  1.18552015 -1.06648308  0.6027151   1.14432428
  1.31029899 -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522468
 -2.0265424   0.72897386  1.60512664  2.83077107 -1.2645671  -0.25136088
 -2.39279201 -2.27309774  3.13337155  2.54856975 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 21:56:56 2024]  Iteration number: 0 with current cost as 0.31224371193036243 and parameters 
[ 1.38271467  2.23743464 -2.12427948 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.18960145  1.18552014 -1.06648324  0.60271479  1.14432429
  1.31029899 -1.87354664  0.7296508   2.88578404 -0.54534335 -0.4752247
 -2.0265424   0.7289737   1.60512664  2.83077091 -1.26456694 -0.25136105
 -2.39279218 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 22:03:23 2024]  Iteration number: 0 with current cost as 0.2985578379552156 and parameters 
[ 1.33710438  2.23743464 -2.12427964 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029883 -1.8735468   0.72965049  2.88578388 -0.54534351 -0.47522485
 -2.02654256  0.72897354  1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279234 -2.2730979   3.13337124  2.54856943 -0.67550803 -2.69002233]. 
Working on 1.0 fold... 
[Thu Apr  4 22:09:46 2024]  Iteration number: 0 with current cost as 0.3032998345453765 and parameters 
[ 1.35806187  2.23743448 -2.12427964 -0.11653103  0.55388693 -2.77010928
  3.06858483  2.1896013   1.18551998 -1.06648324  0.60271495  1.1443243
  1.31029899 -1.8735468   0.7296505   2.88578389 -0.5453435  -0.47522485
 -2.02654256  0.7289737   1.60512664  2.83077092 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.1333714   2.54856958 -0.67550787 -2.69002217]. 
Training complete taking 2138.748898744583 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 22:16:13 2024]  Iteration number: 0 with current cost as 0.6752914842561337 and parameters 
[-0.07783359  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010916
  3.06858479  2.18960126  1.18552008 -1.06648327  0.60271491  1.14432426
  1.31029899 -1.8735468   0.72965061  2.885784   -0.54534335 -0.47522485
 -2.0265425   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 22:26:33 2024]  Iteration number: 0 with current cost as 0.30564058391695037 and parameters 
[ 1.45292768  2.23743464 -2.12427947 -0.11653103  0.55388708 -2.77010931
  3.06858498  2.18960145  1.18552015 -1.06648308  0.6027151   1.14432428
  1.31029899 -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522468
 -2.0265424   0.72897386  1.60512664  2.83077107 -1.2645671  -0.25136088
 -2.39279201 -2.27309774  3.13337155  2.54856975 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 22:31:58 2024]  Iteration number: 0 with current cost as 0.31224371193036243 and parameters 
[ 1.38271467  2.23743464 -2.12427948 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.18960145  1.18552014 -1.06648324  0.60271479  1.14432429
  1.31029899 -1.87354664  0.7296508   2.88578404 -0.54534335 -0.4752247
 -2.0265424   0.7289737   1.60512664  2.83077091 -1.26456694 -0.25136105
 -2.39279218 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 22:38:11 2024]  Iteration number: 0 with current cost as 0.2985578379552156 and parameters 
[ 1.33710438  2.23743464 -2.12427964 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029883 -1.8735468   0.72965049  2.88578388 -0.54534351 -0.47522485
 -2.02654256  0.72897354  1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279234 -2.2730979   3.13337124  2.54856943 -0.67550803 -2.69002233]. 
Working on 1.0 fold... 
[Thu Apr  4 22:44:23 2024]  Iteration number: 0 with current cost as 0.3032998345453765 and parameters 
[ 1.35806187  2.23743448 -2.12427964 -0.11653103  0.55388693 -2.77010928
  3.06858483  2.1896013   1.18551998 -1.06648324  0.60271495  1.1443243
  1.31029899 -1.8735468   0.7296505   2.88578389 -0.5453435  -0.47522485
 -2.02654256  0.7289737   1.60512664  2.83077092 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.1333714   2.54856958 -0.67550787 -2.69002217]. 
Training complete taking 2076.5355262756348 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 22:50:56 2024]  Iteration number: 0 with current cost as 0.6752914842561337 and parameters 
[-0.07783359  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010916
  3.06858479  2.18960126  1.18552008 -1.06648327  0.60271491  1.14432426
  1.31029899 -1.8735468   0.72965061  2.885784   -0.54534335 -0.47522485
 -2.0265425   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 23:01:59 2024]  Iteration number: 0 with current cost as 0.30564058391695037 and parameters 
[ 1.45292768  2.23743464 -2.12427947 -0.11653103  0.55388708 -2.77010931
  3.06858498  2.18960145  1.18552015 -1.06648308  0.6027151   1.14432428
  1.31029899 -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522468
 -2.0265424   0.72897386  1.60512664  2.83077107 -1.2645671  -0.25136088
 -2.39279201 -2.27309774  3.13337155  2.54856975 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 23:07:43 2024]  Iteration number: 0 with current cost as 0.31224371193036243 and parameters 
[ 1.38271467  2.23743464 -2.12427948 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.18960145  1.18552014 -1.06648324  0.60271479  1.14432429
  1.31029899 -1.87354664  0.7296508   2.88578404 -0.54534335 -0.4752247
 -2.0265424   0.7289737   1.60512664  2.83077091 -1.26456694 -0.25136105
 -2.39279218 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 23:14:12 2024]  Iteration number: 0 with current cost as 0.2985578379552156 and parameters 
[ 1.33710438  2.23743464 -2.12427964 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029883 -1.8735468   0.72965049  2.88578388 -0.54534351 -0.47522485
 -2.02654256  0.72897354  1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279234 -2.2730979   3.13337124  2.54856943 -0.67550803 -2.69002233]. 
Working on 1.0 fold... 
[Thu Apr  4 23:20:36 2024]  Iteration number: 0 with current cost as 0.3032998345453765 and parameters 
[ 1.35806187  2.23743448 -2.12427964 -0.11653103  0.55388693 -2.77010928
  3.06858483  2.1896013   1.18551998 -1.06648324  0.60271495  1.1443243
  1.31029899 -1.8735468   0.7296505   2.88578389 -0.5453435  -0.47522485
 -2.02654256  0.7289737   1.60512664  2.83077092 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.1333714   2.54856958 -0.67550787 -2.69002217]. 
Training complete taking 2165.9744522571564 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 23:26:57 2024]  Iteration number: 0 with current cost as 0.6752914842561337 and parameters 
[-0.07783359  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010916
  3.06858479  2.18960126  1.18552008 -1.06648327  0.60271491  1.14432426
  1.31029899 -1.8735468   0.72965061  2.885784   -0.54534335 -0.47522485
 -2.0265425   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 23:37:38 2024]  Iteration number: 0 with current cost as 0.30564058391695037 and parameters 
[ 1.45292768  2.23743464 -2.12427947 -0.11653103  0.55388708 -2.77010931
  3.06858498  2.18960145  1.18552015 -1.06648308  0.6027151   1.14432428
  1.31029899 -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522468
 -2.0265424   0.72897386  1.60512664  2.83077107 -1.2645671  -0.25136088
 -2.39279201 -2.27309774  3.13337155  2.54856975 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 23:43:30 2024]  Iteration number: 0 with current cost as 0.31224371193036243 and parameters 
[ 1.38271467  2.23743464 -2.12427948 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.18960145  1.18552014 -1.06648324  0.60271479  1.14432429
  1.31029899 -1.87354664  0.7296508   2.88578404 -0.54534335 -0.4752247
 -2.0265424   0.7289737   1.60512664  2.83077091 -1.26456694 -0.25136105
 -2.39279218 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 23:50:09 2024]  Iteration number: 0 with current cost as 0.2985578379552156 and parameters 
[ 1.33710438  2.23743464 -2.12427964 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029883 -1.8735468   0.72965049  2.88578388 -0.54534351 -0.47522485
 -2.02654256  0.72897354  1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279234 -2.2730979   3.13337124  2.54856943 -0.67550803 -2.69002233]. 
Working on 1.0 fold... 
[Thu Apr  4 23:56:35 2024]  Iteration number: 0 with current cost as 0.3032998345453765 and parameters 
[ 1.35806187  2.23743448 -2.12427964 -0.11653103  0.55388693 -2.77010928
  3.06858483  2.1896013   1.18551998 -1.06648324  0.60271495  1.1443243
  1.31029899 -1.8735468   0.7296505   2.88578389 -0.5453435  -0.47522485
 -2.02654256  0.7289737   1.60512664  2.83077092 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.1333714   2.54856958 -0.67550787 -2.69002217]. 
Training complete taking 2159.803249359131 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 00:03:06 2024]  Iteration number: 0 with current cost as 0.6752914842561337 and parameters 
[-0.07783359  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010916
  3.06858479  2.18960126  1.18552008 -1.06648327  0.60271491  1.14432426
  1.31029899 -1.8735468   0.72965061  2.885784   -0.54534335 -0.47522485
 -2.0265425   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 00:13:49 2024]  Iteration number: 0 with current cost as 0.30564058391695037 and parameters 
[ 1.45292768  2.23743464 -2.12427947 -0.11653103  0.55388708 -2.77010931
  3.06858498  2.18960145  1.18552015 -1.06648308  0.6027151   1.14432428
  1.31029899 -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522468
 -2.0265424   0.72897386  1.60512664  2.83077107 -1.2645671  -0.25136088
 -2.39279201 -2.27309774  3.13337155  2.54856975 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 00:19:22 2024]  Iteration number: 0 with current cost as 0.31224371193036243 and parameters 
[ 1.38271467  2.23743464 -2.12427948 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.18960145  1.18552014 -1.06648324  0.60271479  1.14432429
  1.31029899 -1.87354664  0.7296508   2.88578404 -0.54534335 -0.4752247
 -2.0265424   0.7289737   1.60512664  2.83077091 -1.26456694 -0.25136105
 -2.39279218 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 00:25:50 2024]  Iteration number: 0 with current cost as 0.2985578379552156 and parameters 
[ 1.33710438  2.23743464 -2.12427964 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029883 -1.8735468   0.72965049  2.88578388 -0.54534351 -0.47522485
 -2.02654256  0.72897354  1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279234 -2.2730979   3.13337124  2.54856943 -0.67550803 -2.69002233]. 
Working on 1.0 fold... 
[Fri Apr  5 00:32:11 2024]  Iteration number: 0 with current cost as 0.3032998345453765 and parameters 
[ 1.35806187  2.23743448 -2.12427964 -0.11653103  0.55388693 -2.77010928
  3.06858483  2.1896013   1.18551998 -1.06648324  0.60271495  1.1443243
  1.31029899 -1.8735468   0.7296505   2.88578389 -0.5453435  -0.47522485
 -2.02654256  0.7289737   1.60512664  2.83077092 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.1333714   2.54856958 -0.67550787 -2.69002217]. 
Training complete taking 2145.6164240837097 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 00:38:42 2024]  Iteration number: 0 with current cost as 0.6752914842561337 and parameters 
[-0.07783359  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010916
  3.06858479  2.18960126  1.18552008 -1.06648327  0.60271491  1.14432426
  1.31029899 -1.8735468   0.72965061  2.885784   -0.54534335 -0.47522485
 -2.0265425   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 00:49:25 2024]  Iteration number: 0 with current cost as 0.30564058391695037 and parameters 
[ 1.45292768  2.23743464 -2.12427947 -0.11653103  0.55388708 -2.77010931
  3.06858498  2.18960145  1.18552015 -1.06648308  0.6027151   1.14432428
  1.31029899 -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522468
 -2.0265424   0.72897386  1.60512664  2.83077107 -1.2645671  -0.25136088
 -2.39279201 -2.27309774  3.13337155  2.54856975 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 00:55:08 2024]  Iteration number: 0 with current cost as 0.31224371193036243 and parameters 
[ 1.38271467  2.23743464 -2.12427948 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.18960145  1.18552014 -1.06648324  0.60271479  1.14432429
  1.31029899 -1.87354664  0.7296508   2.88578404 -0.54534335 -0.4752247
 -2.0265424   0.7289737   1.60512664  2.83077091 -1.26456694 -0.25136105
 -2.39279218 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 01:01:30 2024]  Iteration number: 0 with current cost as 0.2985578379552156 and parameters 
[ 1.33710438  2.23743464 -2.12427964 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029883 -1.8735468   0.72965049  2.88578388 -0.54534351 -0.47522485
 -2.02654256  0.72897354  1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279234 -2.2730979   3.13337124  2.54856943 -0.67550803 -2.69002233]. 
Working on 1.0 fold... 
[Fri Apr  5 01:07:43 2024]  Iteration number: 0 with current cost as 0.3032998345453765 and parameters 
[ 1.35806187  2.23743448 -2.12427964 -0.11653103  0.55388693 -2.77010928
  3.06858483  2.1896013   1.18551998 -1.06648324  0.60271495  1.1443243
  1.31029899 -1.8735468   0.7296505   2.88578389 -0.5453435  -0.47522485
 -2.02654256  0.7289737   1.60512664  2.83077092 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.1333714   2.54856958 -0.67550787 -2.69002217]. 
Training complete taking 2120.2467131614685 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 01:13:59 2024]  Iteration number: 0 with current cost as 0.6752914842561337 and parameters 
[-0.07783359  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010916
  3.06858479  2.18960126  1.18552008 -1.06648327  0.60271491  1.14432426
  1.31029899 -1.8735468   0.72965061  2.885784   -0.54534335 -0.47522485
 -2.0265425   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 01:24:26 2024]  Iteration number: 0 with current cost as 0.30564058391695037 and parameters 
[ 1.45292768  2.23743464 -2.12427947 -0.11653103  0.55388708 -2.77010931
  3.06858498  2.18960145  1.18552015 -1.06648308  0.6027151   1.14432428
  1.31029899 -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522468
 -2.0265424   0.72897386  1.60512664  2.83077107 -1.2645671  -0.25136088
 -2.39279201 -2.27309774  3.13337155  2.54856975 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 01:29:59 2024]  Iteration number: 0 with current cost as 0.31224371193036243 and parameters 
[ 1.38271467  2.23743464 -2.12427948 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.18960145  1.18552014 -1.06648324  0.60271479  1.14432429
  1.31029899 -1.87354664  0.7296508   2.88578404 -0.54534335 -0.4752247
 -2.0265424   0.7289737   1.60512664  2.83077091 -1.26456694 -0.25136105
 -2.39279218 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 01:36:26 2024]  Iteration number: 0 with current cost as 0.2985578379552156 and parameters 
[ 1.33710438  2.23743464 -2.12427964 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029883 -1.8735468   0.72965049  2.88578388 -0.54534351 -0.47522485
 -2.02654256  0.72897354  1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279234 -2.2730979   3.13337124  2.54856943 -0.67550803 -2.69002233]. 
Working on 1.0 fold... 
[Fri Apr  5 01:43:09 2024]  Iteration number: 0 with current cost as 0.3032998345453765 and parameters 
[ 1.35806187  2.23743448 -2.12427964 -0.11653103  0.55388693 -2.77010928
  3.06858483  2.1896013   1.18551998 -1.06648324  0.60271495  1.1443243
  1.31029899 -1.8735468   0.7296505   2.88578389 -0.5453435  -0.47522485
 -2.02654256  0.7289737   1.60512664  2.83077092 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.1333714   2.54856958 -0.67550787 -2.69002217]. 
Training complete taking 2143.3841433525085 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 01:50:09 2024]  Iteration number: 0 with current cost as 0.6752914842561337 and parameters 
[-0.07783359  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010916
  3.06858479  2.18960126  1.18552008 -1.06648327  0.60271491  1.14432426
  1.31029899 -1.8735468   0.72965061  2.885784   -0.54534335 -0.47522485
 -2.0265425   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 02:01:28 2024]  Iteration number: 0 with current cost as 0.30564058391695037 and parameters 
[ 1.45292768  2.23743464 -2.12427947 -0.11653103  0.55388708 -2.77010931
  3.06858498  2.18960145  1.18552015 -1.06648308  0.6027151   1.14432428
  1.31029899 -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522468
 -2.0265424   0.72897386  1.60512664  2.83077107 -1.2645671  -0.25136088
 -2.39279201 -2.27309774  3.13337155  2.54856975 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 02:07:20 2024]  Iteration number: 0 with current cost as 0.31224371193036243 and parameters 
[ 1.38271467  2.23743464 -2.12427948 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.18960145  1.18552014 -1.06648324  0.60271479  1.14432429
  1.31029899 -1.87354664  0.7296508   2.88578404 -0.54534335 -0.4752247
 -2.0265424   0.7289737   1.60512664  2.83077091 -1.26456694 -0.25136105
 -2.39279218 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 02:13:58 2024]  Iteration number: 0 with current cost as 0.2985578379552156 and parameters 
[ 1.33710438  2.23743464 -2.12427964 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029883 -1.8735468   0.72965049  2.88578388 -0.54534351 -0.47522485
 -2.02654256  0.72897354  1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279234 -2.2730979   3.13337124  2.54856943 -0.67550803 -2.69002233]. 
Working on 1.0 fold... 
[Fri Apr  5 02:20:57 2024]  Iteration number: 0 with current cost as 0.3032998345453765 and parameters 
[ 1.35806187  2.23743448 -2.12427964 -0.11653103  0.55388693 -2.77010928
  3.06858483  2.1896013   1.18551998 -1.06648324  0.60271495  1.1443243
  1.31029899 -1.8735468   0.7296505   2.88578389 -0.5453435  -0.47522485
 -2.02654256  0.7289737   1.60512664  2.83077092 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.1333714   2.54856958 -0.67550787 -2.69002217]. 
Training complete taking 2252.3755917549133 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 02:27:23 2024]  Iteration number: 0 with current cost as 0.6752914842561337 and parameters 
[-0.07783359  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010916
  3.06858479  2.18960126  1.18552008 -1.06648327  0.60271491  1.14432426
  1.31029899 -1.8735468   0.72965061  2.885784   -0.54534335 -0.47522485
 -2.0265425   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 02:38:53 2024]  Iteration number: 0 with current cost as 0.30564058391695037 and parameters 
[ 1.45292768  2.23743464 -2.12427947 -0.11653103  0.55388708 -2.77010931
  3.06858498  2.18960145  1.18552015 -1.06648308  0.6027151   1.14432428
  1.31029899 -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522468
 -2.0265424   0.72897386  1.60512664  2.83077107 -1.2645671  -0.25136088
 -2.39279201 -2.27309774  3.13337155  2.54856975 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 02:44:59 2024]  Iteration number: 0 with current cost as 0.31224371193036243 and parameters 
[ 1.38271467  2.23743464 -2.12427948 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.18960145  1.18552014 -1.06648324  0.60271479  1.14432429
  1.31029899 -1.87354664  0.7296508   2.88578404 -0.54534335 -0.4752247
 -2.0265424   0.7289737   1.60512664  2.83077091 -1.26456694 -0.25136105
 -2.39279218 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 02:51:49 2024]  Iteration number: 0 with current cost as 0.2985578379552156 and parameters 
[ 1.33710438  2.23743464 -2.12427964 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029883 -1.8735468   0.72965049  2.88578388 -0.54534351 -0.47522485
 -2.02654256  0.72897354  1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279234 -2.2730979   3.13337124  2.54856943 -0.67550803 -2.69002233]. 
Working on 1.0 fold... 
[Fri Apr  5 02:58:24 2024]  Iteration number: 0 with current cost as 0.3032998345453765 and parameters 
[ 1.35806187  2.23743448 -2.12427964 -0.11653103  0.55388693 -2.77010928
  3.06858483  2.1896013   1.18551998 -1.06648324  0.60271495  1.1443243
  1.31029899 -1.8735468   0.7296505   2.88578389 -0.5453435  -0.47522485
 -2.02654256  0.7289737   1.60512664  2.83077092 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.1333714   2.54856958 -0.67550787 -2.69002217]. 
Training complete taking 2252.3593122959137 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 03:04:52 2024]  Iteration number: 0 with current cost as 0.6752914842561337 and parameters 
[-0.07783359  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010916
  3.06858479  2.18960126  1.18552008 -1.06648327  0.60271491  1.14432426
  1.31029899 -1.8735468   0.72965061  2.885784   -0.54534335 -0.47522485
 -2.0265425   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 03:16:27 2024]  Iteration number: 0 with current cost as 0.30564058391695037 and parameters 
[ 1.45292768  2.23743464 -2.12427947 -0.11653103  0.55388708 -2.77010931
  3.06858498  2.18960145  1.18552015 -1.06648308  0.6027151   1.14432428
  1.31029899 -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522468
 -2.0265424   0.72897386  1.60512664  2.83077107 -1.2645671  -0.25136088
 -2.39279201 -2.27309774  3.13337155  2.54856975 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 03:22:25 2024]  Iteration number: 0 with current cost as 0.31224371193036243 and parameters 
[ 1.38271467  2.23743464 -2.12427948 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.18960145  1.18552014 -1.06648324  0.60271479  1.14432429
  1.31029899 -1.87354664  0.7296508   2.88578404 -0.54534335 -0.4752247
 -2.0265424   0.7289737   1.60512664  2.83077091 -1.26456694 -0.25136105
 -2.39279218 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 03:29:20 2024]  Iteration number: 0 with current cost as 0.2985578379552156 and parameters 
[ 1.33710438  2.23743464 -2.12427964 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029883 -1.8735468   0.72965049  2.88578388 -0.54534351 -0.47522485
 -2.02654256  0.72897354  1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279234 -2.2730979   3.13337124  2.54856943 -0.67550803 -2.69002233]. 
Working on 1.0 fold... 
[Fri Apr  5 03:36:03 2024]  Iteration number: 0 with current cost as 0.3032998345453765 and parameters 
[ 1.35806187  2.23743448 -2.12427964 -0.11653103  0.55388693 -2.77010928
  3.06858483  2.1896013   1.18551998 -1.06648324  0.60271495  1.1443243
  1.31029899 -1.8735468   0.7296505   2.88578389 -0.5453435  -0.47522485
 -2.02654256  0.7289737   1.60512664  2.83077092 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.1333714   2.54856958 -0.67550787 -2.69002217]. 
Training complete taking 2263.69220662117 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 03:42:34 2024]  Iteration number: 0 with current cost as 0.6752914842561337 and parameters 
[-0.07783359  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010916
  3.06858479  2.18960126  1.18552008 -1.06648327  0.60271491  1.14432426
  1.31029899 -1.8735468   0.72965061  2.885784   -0.54534335 -0.47522485
 -2.0265425   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 03:53:36 2024]  Iteration number: 0 with current cost as 0.30564058391695037 and parameters 
[ 1.45292768  2.23743464 -2.12427947 -0.11653103  0.55388708 -2.77010931
  3.06858498  2.18960145  1.18552015 -1.06648308  0.6027151   1.14432428
  1.31029899 -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522468
 -2.0265424   0.72897386  1.60512664  2.83077107 -1.2645671  -0.25136088
 -2.39279201 -2.27309774  3.13337155  2.54856975 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 03:59:36 2024]  Iteration number: 0 with current cost as 0.31224371193036243 and parameters 
[ 1.38271467  2.23743464 -2.12427948 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.18960145  1.18552014 -1.06648324  0.60271479  1.14432429
  1.31029899 -1.87354664  0.7296508   2.88578404 -0.54534335 -0.4752247
 -2.0265424   0.7289737   1.60512664  2.83077091 -1.26456694 -0.25136105
 -2.39279218 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 04:06:13 2024]  Iteration number: 0 with current cost as 0.2985578379552156 and parameters 
[ 1.33710438  2.23743464 -2.12427964 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029883 -1.8735468   0.72965049  2.88578388 -0.54534351 -0.47522485
 -2.02654256  0.72897354  1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279234 -2.2730979   3.13337124  2.54856943 -0.67550803 -2.69002233]. 
Working on 1.0 fold... 
[Fri Apr  5 04:13:08 2024]  Iteration number: 0 with current cost as 0.3032998345453765 and parameters 
[ 1.35806187  2.23743448 -2.12427964 -0.11653103  0.55388693 -2.77010928
  3.06858483  2.1896013   1.18551998 -1.06648324  0.60271495  1.1443243
  1.31029899 -1.8735468   0.7296505   2.88578389 -0.5453435  -0.47522485
 -2.02654256  0.7289737   1.60512664  2.83077092 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.1333714   2.54856958 -0.67550787 -2.69002217]. 
Training complete taking 2225.090303182602 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 04:19:45 2024]  Iteration number: 0 with current cost as 0.6752914842561337 and parameters 
[-0.07783359  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010916
  3.06858479  2.18960126  1.18552008 -1.06648327  0.60271491  1.14432426
  1.31029899 -1.8735468   0.72965061  2.885784   -0.54534335 -0.47522485
 -2.0265425   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 04:31:21 2024]  Iteration number: 0 with current cost as 0.30564058391695037 and parameters 
[ 1.45292768  2.23743464 -2.12427947 -0.11653103  0.55388708 -2.77010931
  3.06858498  2.18960145  1.18552015 -1.06648308  0.6027151   1.14432428
  1.31029899 -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522468
 -2.0265424   0.72897386  1.60512664  2.83077107 -1.2645671  -0.25136088
 -2.39279201 -2.27309774  3.13337155  2.54856975 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 04:37:34 2024]  Iteration number: 0 with current cost as 0.31224371193036243 and parameters 
[ 1.38271467  2.23743464 -2.12427948 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.18960145  1.18552014 -1.06648324  0.60271479  1.14432429
  1.31029899 -1.87354664  0.7296508   2.88578404 -0.54534335 -0.4752247
 -2.0265424   0.7289737   1.60512664  2.83077091 -1.26456694 -0.25136105
 -2.39279218 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 04:44:22 2024]  Iteration number: 0 with current cost as 0.2985578379552156 and parameters 
[ 1.33710438  2.23743464 -2.12427964 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029883 -1.8735468   0.72965049  2.88578388 -0.54534351 -0.47522485
 -2.02654256  0.72897354  1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279234 -2.2730979   3.13337124  2.54856943 -0.67550803 -2.69002233]. 
Working on 1.0 fold... 
[Fri Apr  5 04:51:25 2024]  Iteration number: 0 with current cost as 0.3032998345453765 and parameters 
[ 1.35806187  2.23743448 -2.12427964 -0.11653103  0.55388693 -2.77010928
  3.06858483  2.1896013   1.18551998 -1.06648324  0.60271495  1.1443243
  1.31029899 -1.8735468   0.7296505   2.88578389 -0.5453435  -0.47522485
 -2.02654256  0.7289737   1.60512664  2.83077092 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.1333714   2.54856958 -0.67550787 -2.69002217]. 
Training complete taking 2303.4868807792664 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 04:58:24 2024]  Iteration number: 0 with current cost as 0.6752914842561337 and parameters 
[-0.07783359  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010916
  3.06858479  2.18960126  1.18552008 -1.06648327  0.60271491  1.14432426
  1.31029899 -1.8735468   0.72965061  2.885784   -0.54534335 -0.47522485
 -2.0265425   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 05:09:28 2024]  Iteration number: 0 with current cost as 0.30564058391695037 and parameters 
[ 1.45292768  2.23743464 -2.12427947 -0.11653103  0.55388708 -2.77010931
  3.06858498  2.18960145  1.18552015 -1.06648308  0.6027151   1.14432428
  1.31029899 -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522468
 -2.0265424   0.72897386  1.60512664  2.83077107 -1.2645671  -0.25136088
 -2.39279201 -2.27309774  3.13337155  2.54856975 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 05:15:22 2024]  Iteration number: 0 with current cost as 0.31224371193036243 and parameters 
[ 1.38271467  2.23743464 -2.12427948 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.18960145  1.18552014 -1.06648324  0.60271479  1.14432429
  1.31029899 -1.87354664  0.7296508   2.88578404 -0.54534335 -0.4752247
 -2.0265424   0.7289737   1.60512664  2.83077091 -1.26456694 -0.25136105
 -2.39279218 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 05:22:06 2024]  Iteration number: 0 with current cost as 0.2985578379552156 and parameters 
[ 1.33710438  2.23743464 -2.12427964 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029883 -1.8735468   0.72965049  2.88578388 -0.54534351 -0.47522485
 -2.02654256  0.72897354  1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279234 -2.2730979   3.13337124  2.54856943 -0.67550803 -2.69002233]. 
Working on 1.0 fold... 
[Fri Apr  5 05:28:39 2024]  Iteration number: 0 with current cost as 0.3032998345453765 and parameters 
[ 1.35806187  2.23743448 -2.12427964 -0.11653103  0.55388693 -2.77010928
  3.06858483  2.1896013   1.18551998 -1.06648324  0.60271495  1.1443243
  1.31029899 -1.8735468   0.7296505   2.88578389 -0.5453435  -0.47522485
 -2.02654256  0.7289737   1.60512664  2.83077092 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.1333714   2.54856958 -0.67550787 -2.69002217]. 
Training complete taking 2221.571304798126 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 05:35:09 2024]  Iteration number: 0 with current cost as 0.6752914842561337 and parameters 
[-0.07783359  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010916
  3.06858479  2.18960126  1.18552008 -1.06648327  0.60271491  1.14432426
  1.31029899 -1.8735468   0.72965061  2.885784   -0.54534335 -0.47522485
 -2.0265425   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 05:46:03 2024]  Iteration number: 0 with current cost as 0.30564058391695037 and parameters 
[ 1.45292768  2.23743464 -2.12427947 -0.11653103  0.55388708 -2.77010931
  3.06858498  2.18960145  1.18552015 -1.06648308  0.6027151   1.14432428
  1.31029899 -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522468
 -2.0265424   0.72897386  1.60512664  2.83077107 -1.2645671  -0.25136088
 -2.39279201 -2.27309774  3.13337155  2.54856975 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 05:51:45 2024]  Iteration number: 0 with current cost as 0.31224371193036243 and parameters 
[ 1.38271467  2.23743464 -2.12427948 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.18960145  1.18552014 -1.06648324  0.60271479  1.14432429
  1.31029899 -1.87354664  0.7296508   2.88578404 -0.54534335 -0.4752247
 -2.0265424   0.7289737   1.60512664  2.83077091 -1.26456694 -0.25136105
 -2.39279218 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 05:58:05 2024]  Iteration number: 0 with current cost as 0.2985578379552156 and parameters 
[ 1.33710438  2.23743464 -2.12427964 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029883 -1.8735468   0.72965049  2.88578388 -0.54534351 -0.47522485
 -2.02654256  0.72897354  1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279234 -2.2730979   3.13337124  2.54856943 -0.67550803 -2.69002233]. 
Working on 1.0 fold... 
[Fri Apr  5 06:04:38 2024]  Iteration number: 0 with current cost as 0.3032998345453765 and parameters 
[ 1.35806187  2.23743448 -2.12427964 -0.11653103  0.55388693 -2.77010928
  3.06858483  2.1896013   1.18551998 -1.06648324  0.60271495  1.1443243
  1.31029899 -1.8735468   0.7296505   2.88578389 -0.5453435  -0.47522485
 -2.02654256  0.7289737   1.60512664  2.83077092 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.1333714   2.54856958 -0.67550787 -2.69002217]. 
Training complete taking 2176.444341659546 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 06:11:23 2024]  Iteration number: 0 with current cost as 0.6752914842561337 and parameters 
[-0.07783359  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010916
  3.06858479  2.18960126  1.18552008 -1.06648327  0.60271491  1.14432426
  1.31029899 -1.8735468   0.72965061  2.885784   -0.54534335 -0.47522485
 -2.0265425   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 06:22:08 2024]  Iteration number: 0 with current cost as 0.30564058391695037 and parameters 
[ 1.45292768  2.23743464 -2.12427947 -0.11653103  0.55388708 -2.77010931
  3.06858498  2.18960145  1.18552015 -1.06648308  0.6027151   1.14432428
  1.31029899 -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522468
 -2.0265424   0.72897386  1.60512664  2.83077107 -1.2645671  -0.25136088
 -2.39279201 -2.27309774  3.13337155  2.54856975 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 06:27:55 2024]  Iteration number: 0 with current cost as 0.31224371193036243 and parameters 
[ 1.38271467  2.23743464 -2.12427948 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.18960145  1.18552014 -1.06648324  0.60271479  1.14432429
  1.31029899 -1.87354664  0.7296508   2.88578404 -0.54534335 -0.4752247
 -2.0265424   0.7289737   1.60512664  2.83077091 -1.26456694 -0.25136105
 -2.39279218 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 06:34:20 2024]  Iteration number: 0 with current cost as 0.2985578379552156 and parameters 
[ 1.33710438  2.23743464 -2.12427964 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029883 -1.8735468   0.72965049  2.88578388 -0.54534351 -0.47522485
 -2.02654256  0.72897354  1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279234 -2.2730979   3.13337124  2.54856943 -0.67550803 -2.69002233]. 
Working on 1.0 fold... 
[Fri Apr  5 06:40:52 2024]  Iteration number: 0 with current cost as 0.3032998345453765 and parameters 
[ 1.35806187  2.23743448 -2.12427964 -0.11653103  0.55388693 -2.77010928
  3.06858483  2.1896013   1.18551998 -1.06648324  0.60271495  1.1443243
  1.31029899 -1.8735468   0.7296505   2.88578389 -0.5453435  -0.47522485
 -2.02654256  0.7289737   1.60512664  2.83077092 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.1333714   2.54856958 -0.67550787 -2.69002217]. 
Training complete taking 2162.124575614929 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 06:47:25 2024]  Iteration number: 0 with current cost as 0.6752914842561337 and parameters 
[-0.07783359  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010916
  3.06858479  2.18960126  1.18552008 -1.06648327  0.60271491  1.14432426
  1.31029899 -1.8735468   0.72965061  2.885784   -0.54534335 -0.47522485
 -2.0265425   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 06:58:05 2024]  Iteration number: 0 with current cost as 0.30564058391695037 and parameters 
[ 1.45292768  2.23743464 -2.12427947 -0.11653103  0.55388708 -2.77010931
  3.06858498  2.18960145  1.18552015 -1.06648308  0.6027151   1.14432428
  1.31029899 -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522468
 -2.0265424   0.72897386  1.60512664  2.83077107 -1.2645671  -0.25136088
 -2.39279201 -2.27309774  3.13337155  2.54856975 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 07:03:46 2024]  Iteration number: 0 with current cost as 0.31224371193036243 and parameters 
[ 1.38271467  2.23743464 -2.12427948 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.18960145  1.18552014 -1.06648324  0.60271479  1.14432429
  1.31029899 -1.87354664  0.7296508   2.88578404 -0.54534335 -0.4752247
 -2.0265424   0.7289737   1.60512664  2.83077091 -1.26456694 -0.25136105
 -2.39279218 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 07:10:19 2024]  Iteration number: 0 with current cost as 0.2985578379552156 and parameters 
[ 1.33710438  2.23743464 -2.12427964 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029883 -1.8735468   0.72965049  2.88578388 -0.54534351 -0.47522485
 -2.02654256  0.72897354  1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279234 -2.2730979   3.13337124  2.54856943 -0.67550803 -2.69002233]. 
Working on 1.0 fold... 
[Fri Apr  5 07:16:53 2024]  Iteration number: 0 with current cost as 0.3032998345453765 and parameters 
[ 1.35806187  2.23743448 -2.12427964 -0.11653103  0.55388693 -2.77010928
  3.06858483  2.1896013   1.18551998 -1.06648324  0.60271495  1.1443243
  1.31029899 -1.8735468   0.7296505   2.88578389 -0.5453435  -0.47522485
 -2.02654256  0.7289737   1.60512664  2.83077092 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.1333714   2.54856958 -0.67550787 -2.69002217]. 
Training complete taking 2149.6361796855927 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 07:23:14 2024]  Iteration number: 0 with current cost as 0.6752914842561337 and parameters 
[-0.07783359  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010916
  3.06858479  2.18960126  1.18552008 -1.06648327  0.60271491  1.14432426
  1.31029899 -1.8735468   0.72965061  2.885784   -0.54534335 -0.47522485
 -2.0265425   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 07:34:08 2024]  Iteration number: 0 with current cost as 0.30564058391695037 and parameters 
[ 1.45292768  2.23743464 -2.12427947 -0.11653103  0.55388708 -2.77010931
  3.06858498  2.18960145  1.18552015 -1.06648308  0.6027151   1.14432428
  1.31029899 -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522468
 -2.0265424   0.72897386  1.60512664  2.83077107 -1.2645671  -0.25136088
 -2.39279201 -2.27309774  3.13337155  2.54856975 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 07:39:56 2024]  Iteration number: 0 with current cost as 0.31224371193036243 and parameters 
[ 1.38271467  2.23743464 -2.12427948 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.18960145  1.18552014 -1.06648324  0.60271479  1.14432429
  1.31029899 -1.87354664  0.7296508   2.88578404 -0.54534335 -0.4752247
 -2.0265424   0.7289737   1.60512664  2.83077091 -1.26456694 -0.25136105
 -2.39279218 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 07:46:32 2024]  Iteration number: 0 with current cost as 0.2985578379552156 and parameters 
[ 1.33710438  2.23743464 -2.12427964 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029883 -1.8735468   0.72965049  2.88578388 -0.54534351 -0.47522485
 -2.02654256  0.72897354  1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279234 -2.2730979   3.13337124  2.54856943 -0.67550803 -2.69002233]. 
Working on 1.0 fold... 
[Fri Apr  5 07:52:58 2024]  Iteration number: 0 with current cost as 0.3032998345453765 and parameters 
[ 1.35806187  2.23743448 -2.12427964 -0.11653103  0.55388693 -2.77010928
  3.06858483  2.1896013   1.18551998 -1.06648324  0.60271495  1.1443243
  1.31029899 -1.8735468   0.7296505   2.88578389 -0.5453435  -0.47522485
 -2.02654256  0.7289737   1.60512664  2.83077092 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.1333714   2.54856958 -0.67550787 -2.69002217]. 
Training complete taking 2163.4158611297607 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 07:59:31 2024]  Iteration number: 0 with current cost as 0.6752914842561337 and parameters 
[-0.07783359  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010916
  3.06858479  2.18960126  1.18552008 -1.06648327  0.60271491  1.14432426
  1.31029899 -1.8735468   0.72965061  2.885784   -0.54534335 -0.47522485
 -2.0265425   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 08:10:39 2024]  Iteration number: 0 with current cost as 0.30564058391695037 and parameters 
[ 1.45292768  2.23743464 -2.12427947 -0.11653103  0.55388708 -2.77010931
  3.06858498  2.18960145  1.18552015 -1.06648308  0.6027151   1.14432428
  1.31029899 -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522468
 -2.0265424   0.72897386  1.60512664  2.83077107 -1.2645671  -0.25136088
 -2.39279201 -2.27309774  3.13337155  2.54856975 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 08:16:39 2024]  Iteration number: 0 with current cost as 0.31224371193036243 and parameters 
[ 1.38271467  2.23743464 -2.12427948 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.18960145  1.18552014 -1.06648324  0.60271479  1.14432429
  1.31029899 -1.87354664  0.7296508   2.88578404 -0.54534335 -0.4752247
 -2.0265424   0.7289737   1.60512664  2.83077091 -1.26456694 -0.25136105
 -2.39279218 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 08:23:15 2024]  Iteration number: 0 with current cost as 0.2985578379552156 and parameters 
[ 1.33710438  2.23743464 -2.12427964 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029883 -1.8735468   0.72965049  2.88578388 -0.54534351 -0.47522485
 -2.02654256  0.72897354  1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279234 -2.2730979   3.13337124  2.54856943 -0.67550803 -2.69002233]. 
Working on 1.0 fold... 
[Fri Apr  5 08:30:09 2024]  Iteration number: 0 with current cost as 0.3032998345453765 and parameters 
[ 1.35806187  2.23743448 -2.12427964 -0.11653103  0.55388693 -2.77010928
  3.06858483  2.1896013   1.18551998 -1.06648324  0.60271495  1.1443243
  1.31029899 -1.8735468   0.7296505   2.88578389 -0.5453435  -0.47522485
 -2.02654256  0.7289737   1.60512664  2.83077092 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.1333714   2.54856958 -0.67550787 -2.69002217]. 
Training complete taking 2246.35089635849 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 08:36:46 2024]  Iteration number: 0 with current cost as 0.6752914842561337 and parameters 
[-0.07783359  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010916
  3.06858479  2.18960126  1.18552008 -1.06648327  0.60271491  1.14432426
  1.31029899 -1.8735468   0.72965061  2.885784   -0.54534335 -0.47522485
 -2.0265425   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 08:47:34 2024]  Iteration number: 0 with current cost as 0.30564058391695037 and parameters 
[ 1.45292768  2.23743464 -2.12427947 -0.11653103  0.55388708 -2.77010931
  3.06858498  2.18960145  1.18552015 -1.06648308  0.6027151   1.14432428
  1.31029899 -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522468
 -2.0265424   0.72897386  1.60512664  2.83077107 -1.2645671  -0.25136088
 -2.39279201 -2.27309774  3.13337155  2.54856975 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 08:53:18 2024]  Iteration number: 0 with current cost as 0.31224371193036243 and parameters 
[ 1.38271467  2.23743464 -2.12427948 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.18960145  1.18552014 -1.06648324  0.60271479  1.14432429
  1.31029899 -1.87354664  0.7296508   2.88578404 -0.54534335 -0.4752247
 -2.0265424   0.7289737   1.60512664  2.83077091 -1.26456694 -0.25136105
 -2.39279218 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 08:59:47 2024]  Iteration number: 0 with current cost as 0.2985578379552156 and parameters 
[ 1.33710438  2.23743464 -2.12427964 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029883 -1.8735468   0.72965049  2.88578388 -0.54534351 -0.47522485
 -2.02654256  0.72897354  1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279234 -2.2730979   3.13337124  2.54856943 -0.67550803 -2.69002233]. 
Working on 1.0 fold... 
[Fri Apr  5 09:06:24 2024]  Iteration number: 0 with current cost as 0.3032998345453765 and parameters 
[ 1.35806187  2.23743448 -2.12427964 -0.11653103  0.55388693 -2.77010928
  3.06858483  2.1896013   1.18551998 -1.06648324  0.60271495  1.1443243
  1.31029899 -1.8735468   0.7296505   2.88578389 -0.5453435  -0.47522485
 -2.02654256  0.7289737   1.60512664  2.83077092 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.1333714   2.54856958 -0.67550787 -2.69002217]. 
Training complete taking 2168.8720440864563 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 09:12:55 2024]  Iteration number: 0 with current cost as 0.6752914842561337 and parameters 
[-0.07783359  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010916
  3.06858479  2.18960126  1.18552008 -1.06648327  0.60271491  1.14432426
  1.31029899 -1.8735468   0.72965061  2.885784   -0.54534335 -0.47522485
 -2.0265425   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 09:23:38 2024]  Iteration number: 0 with current cost as 0.30564058391695037 and parameters 
[ 1.45292768  2.23743464 -2.12427947 -0.11653103  0.55388708 -2.77010931
  3.06858498  2.18960145  1.18552015 -1.06648308  0.6027151   1.14432428
  1.31029899 -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522468
 -2.0265424   0.72897386  1.60512664  2.83077107 -1.2645671  -0.25136088
 -2.39279201 -2.27309774  3.13337155  2.54856975 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 09:29:40 2024]  Iteration number: 0 with current cost as 0.31224371193036243 and parameters 
[ 1.38271467  2.23743464 -2.12427948 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.18960145  1.18552014 -1.06648324  0.60271479  1.14432429
  1.31029899 -1.87354664  0.7296508   2.88578404 -0.54534335 -0.4752247
 -2.0265424   0.7289737   1.60512664  2.83077091 -1.26456694 -0.25136105
 -2.39279218 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 09:36:11 2024]  Iteration number: 0 with current cost as 0.2985578379552156 and parameters 
[ 1.33710438  2.23743464 -2.12427964 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029883 -1.8735468   0.72965049  2.88578388 -0.54534351 -0.47522485
 -2.02654256  0.72897354  1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279234 -2.2730979   3.13337124  2.54856943 -0.67550803 -2.69002233]. 
Working on 1.0 fold... 
[Fri Apr  5 09:42:54 2024]  Iteration number: 0 with current cost as 0.3032998345453765 and parameters 
[ 1.35806187  2.23743448 -2.12427964 -0.11653103  0.55388693 -2.77010928
  3.06858483  2.1896013   1.18551998 -1.06648324  0.60271495  1.1443243
  1.31029899 -1.8735468   0.7296505   2.88578389 -0.5453435  -0.47522485
 -2.02654256  0.7289737   1.60512664  2.83077092 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.1333714   2.54856958 -0.67550787 -2.69002217]. 
Training complete taking 2202.494955778122 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 09:49:52 2024]  Iteration number: 0 with current cost as 0.6752914842561337 and parameters 
[-0.07783359  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010916
  3.06858479  2.18960126  1.18552008 -1.06648327  0.60271491  1.14432426
  1.31029899 -1.8735468   0.72965061  2.885784   -0.54534335 -0.47522485
 -2.0265425   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 10:01:28 2024]  Iteration number: 0 with current cost as 0.30564058391695037 and parameters 
[ 1.45292768  2.23743464 -2.12427947 -0.11653103  0.55388708 -2.77010931
  3.06858498  2.18960145  1.18552015 -1.06648308  0.6027151   1.14432428
  1.31029899 -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522468
 -2.0265424   0.72897386  1.60512664  2.83077107 -1.2645671  -0.25136088
 -2.39279201 -2.27309774  3.13337155  2.54856975 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 10:07:56 2024]  Iteration number: 0 with current cost as 0.31224371193036243 and parameters 
[ 1.38271467  2.23743464 -2.12427948 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.18960145  1.18552014 -1.06648324  0.60271479  1.14432429
  1.31029899 -1.87354664  0.7296508   2.88578404 -0.54534335 -0.4752247
 -2.0265424   0.7289737   1.60512664  2.83077091 -1.26456694 -0.25136105
 -2.39279218 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 10:14:52 2024]  Iteration number: 0 with current cost as 0.2985578379552156 and parameters 
[ 1.33710438  2.23743464 -2.12427964 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029883 -1.8735468   0.72965049  2.88578388 -0.54534351 -0.47522485
 -2.02654256  0.72897354  1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279234 -2.2730979   3.13337124  2.54856943 -0.67550803 -2.69002233]. 
Working on 1.0 fold... 
[Fri Apr  5 10:21:31 2024]  Iteration number: 0 with current cost as 0.3032998345453765 and parameters 
[ 1.35806187  2.23743448 -2.12427964 -0.11653103  0.55388693 -2.77010928
  3.06858483  2.1896013   1.18551998 -1.06648324  0.60271495  1.1443243
  1.31029899 -1.8735468   0.7296505   2.88578389 -0.5453435  -0.47522485
 -2.02654256  0.7289737   1.60512664  2.83077092 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.1333714   2.54856958 -0.67550787 -2.69002217]. 
Training complete taking 2306.042958498001 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 10:28:11 2024]  Iteration number: 0 with current cost as 0.6752914842561337 and parameters 
[-0.07783359  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010916
  3.06858479  2.18960126  1.18552008 -1.06648327  0.60271491  1.14432426
  1.31029899 -1.8735468   0.72965061  2.885784   -0.54534335 -0.47522485
 -2.0265425   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 10:39:19 2024]  Iteration number: 0 with current cost as 0.30564058391695037 and parameters 
[ 1.45292768  2.23743464 -2.12427947 -0.11653103  0.55388708 -2.77010931
  3.06858498  2.18960145  1.18552015 -1.06648308  0.6027151   1.14432428
  1.31029899 -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522468
 -2.0265424   0.72897386  1.60512664  2.83077107 -1.2645671  -0.25136088
 -2.39279201 -2.27309774  3.13337155  2.54856975 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 10:45:12 2024]  Iteration number: 0 with current cost as 0.31224371193036243 and parameters 
[ 1.38271467  2.23743464 -2.12427948 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.18960145  1.18552014 -1.06648324  0.60271479  1.14432429
  1.31029899 -1.87354664  0.7296508   2.88578404 -0.54534335 -0.4752247
 -2.0265424   0.7289737   1.60512664  2.83077091 -1.26456694 -0.25136105
 -2.39279218 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 10:51:42 2024]  Iteration number: 0 with current cost as 0.2985578379552156 and parameters 
[ 1.33710438  2.23743464 -2.12427964 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029883 -1.8735468   0.72965049  2.88578388 -0.54534351 -0.47522485
 -2.02654256  0.72897354  1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279234 -2.2730979   3.13337124  2.54856943 -0.67550803 -2.69002233]. 
Working on 1.0 fold... 
[Fri Apr  5 10:58:09 2024]  Iteration number: 0 with current cost as 0.3032998345453765 and parameters 
[ 1.35806187  2.23743448 -2.12427964 -0.11653103  0.55388693 -2.77010928
  3.06858483  2.1896013   1.18551998 -1.06648324  0.60271495  1.1443243
  1.31029899 -1.8735468   0.7296505   2.88578389 -0.5453435  -0.47522485
 -2.02654256  0.7289737   1.60512664  2.83077092 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.1333714   2.54856958 -0.67550787 -2.69002217]. 
Training complete taking 2207.2593524456024 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 11:04:47 2024]  Iteration number: 0 with current cost as 0.6752914842561337 and parameters 
[-0.07783359  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010916
  3.06858479  2.18960126  1.18552008 -1.06648327  0.60271491  1.14432426
  1.31029899 -1.8735468   0.72965061  2.885784   -0.54534335 -0.47522485
 -2.0265425   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 11:15:43 2024]  Iteration number: 0 with current cost as 0.30564058391695037 and parameters 
[ 1.45292768  2.23743464 -2.12427947 -0.11653103  0.55388708 -2.77010931
  3.06858498  2.18960145  1.18552015 -1.06648308  0.6027151   1.14432428
  1.31029899 -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522468
 -2.0265424   0.72897386  1.60512664  2.83077107 -1.2645671  -0.25136088
 -2.39279201 -2.27309774  3.13337155  2.54856975 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 11:21:46 2024]  Iteration number: 0 with current cost as 0.31224371193036243 and parameters 
[ 1.38271467  2.23743464 -2.12427948 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.18960145  1.18552014 -1.06648324  0.60271479  1.14432429
  1.31029899 -1.87354664  0.7296508   2.88578404 -0.54534335 -0.4752247
 -2.0265424   0.7289737   1.60512664  2.83077091 -1.26456694 -0.25136105
 -2.39279218 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 11:28:10 2024]  Iteration number: 0 with current cost as 0.2985578379552156 and parameters 
[ 1.33710438  2.23743464 -2.12427964 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029883 -1.8735468   0.72965049  2.88578388 -0.54534351 -0.47522485
 -2.02654256  0.72897354  1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279234 -2.2730979   3.13337124  2.54856943 -0.67550803 -2.69002233]. 
Working on 1.0 fold... 
[Fri Apr  5 11:34:46 2024]  Iteration number: 0 with current cost as 0.3032998345453765 and parameters 
[ 1.35806187  2.23743448 -2.12427964 -0.11653103  0.55388693 -2.77010928
  3.06858483  2.1896013   1.18551998 -1.06648324  0.60271495  1.1443243
  1.31029899 -1.8735468   0.7296505   2.88578389 -0.5453435  -0.47522485
 -2.02654256  0.7289737   1.60512664  2.83077092 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.1333714   2.54856958 -0.67550787 -2.69002217]. 
Training complete taking 2185.6795122623444 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 11:41:23 2024]  Iteration number: 0 with current cost as 0.6752914842561337 and parameters 
[-0.07783359  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010916
  3.06858479  2.18960126  1.18552008 -1.06648327  0.60271491  1.14432426
  1.31029899 -1.8735468   0.72965061  2.885784   -0.54534335 -0.47522485
 -2.0265425   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 11:52:48 2024]  Iteration number: 0 with current cost as 0.30564058391695037 and parameters 
[ 1.45292768  2.23743464 -2.12427947 -0.11653103  0.55388708 -2.77010931
  3.06858498  2.18960145  1.18552015 -1.06648308  0.6027151   1.14432428
  1.31029899 -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522468
 -2.0265424   0.72897386  1.60512664  2.83077107 -1.2645671  -0.25136088
 -2.39279201 -2.27309774  3.13337155  2.54856975 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 11:58:24 2024]  Iteration number: 0 with current cost as 0.31224371193036243 and parameters 
[ 1.38271467  2.23743464 -2.12427948 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.18960145  1.18552014 -1.06648324  0.60271479  1.14432429
  1.31029899 -1.87354664  0.7296508   2.88578404 -0.54534335 -0.4752247
 -2.0265424   0.7289737   1.60512664  2.83077091 -1.26456694 -0.25136105
 -2.39279218 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 12:05:03 2024]  Iteration number: 0 with current cost as 0.2985578379552156 and parameters 
[ 1.33710438  2.23743464 -2.12427964 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029883 -1.8735468   0.72965049  2.88578388 -0.54534351 -0.47522485
 -2.02654256  0.72897354  1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279234 -2.2730979   3.13337124  2.54856943 -0.67550803 -2.69002233]. 
Working on 1.0 fold... 
[Fri Apr  5 12:11:30 2024]  Iteration number: 0 with current cost as 0.3032998345453765 and parameters 
[ 1.35806187  2.23743448 -2.12427964 -0.11653103  0.55388693 -2.77010928
  3.06858483  2.1896013   1.18551998 -1.06648324  0.60271495  1.1443243
  1.31029899 -1.8735468   0.7296505   2.88578389 -0.5453435  -0.47522485
 -2.02654256  0.7289737   1.60512664  2.83077092 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.1333714   2.54856958 -0.67550787 -2.69002217]. 
Training complete taking 2213.5138623714447 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 12:18:18 2024]  Iteration number: 0 with current cost as 0.6752914842561337 and parameters 
[-0.07783359  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010916
  3.06858479  2.18960126  1.18552008 -1.06648327  0.60271491  1.14432426
  1.31029899 -1.8735468   0.72965061  2.885784   -0.54534335 -0.47522485
 -2.0265425   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 12:30:11 2024]  Iteration number: 0 with current cost as 0.30564058391695037 and parameters 
[ 1.45292768  2.23743464 -2.12427947 -0.11653103  0.55388708 -2.77010931
  3.06858498  2.18960145  1.18552015 -1.06648308  0.6027151   1.14432428
  1.31029899 -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522468
 -2.0265424   0.72897386  1.60512664  2.83077107 -1.2645671  -0.25136088
 -2.39279201 -2.27309774  3.13337155  2.54856975 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 12:36:26 2024]  Iteration number: 0 with current cost as 0.31224371193036243 and parameters 
[ 1.38271467  2.23743464 -2.12427948 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.18960145  1.18552014 -1.06648324  0.60271479  1.14432429
  1.31029899 -1.87354664  0.7296508   2.88578404 -0.54534335 -0.4752247
 -2.0265424   0.7289737   1.60512664  2.83077091 -1.26456694 -0.25136105
 -2.39279218 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 12:43:17 2024]  Iteration number: 0 with current cost as 0.2985578379552156 and parameters 
[ 1.33710438  2.23743464 -2.12427964 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029883 -1.8735468   0.72965049  2.88578388 -0.54534351 -0.47522485
 -2.02654256  0.72897354  1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279234 -2.2730979   3.13337124  2.54856943 -0.67550803 -2.69002233]. 
Working on 1.0 fold... 
[Fri Apr  5 12:49:57 2024]  Iteration number: 0 with current cost as 0.3032998345453765 and parameters 
[ 1.35806187  2.23743448 -2.12427964 -0.11653103  0.55388693 -2.77010928
  3.06858483  2.1896013   1.18551998 -1.06648324  0.60271495  1.1443243
  1.31029899 -1.8735468   0.7296505   2.88578389 -0.5453435  -0.47522485
 -2.02654256  0.7289737   1.60512664  2.83077092 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.1333714   2.54856958 -0.67550787 -2.69002217]. 
Training complete taking 2302.181562900543 seconds. 
Discarding model... 

Training complete taking 54952.90179800987 total seconds. 
Now scoring model... 
Scoring complete taking 2.7399511337280273 seconds. 
Saved predicted values as A1-A1-CNOT_Full-CRZ_predicted_values.csv
Model scores: {'MSE_train': (0.5000432171860141,), 'R2_train': 0.004296235497681211, 'MAE_train': 0.6162022730962173, 'MSE_test': 0.540605205554071, 'R2_test': -0.008209760349904949, 'MAE_test': 0.6648572309218637}. 
Saved model results as A1-A1-CNOT_Full-CRZ_results.json. 
