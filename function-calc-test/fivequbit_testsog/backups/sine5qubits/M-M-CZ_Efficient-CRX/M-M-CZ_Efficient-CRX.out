/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /scratch/j/jacobsen/gjones/sine5qubits/sine_train.bin... 
Successfully loaded /scratch/j/jacobsen/gjones/sine5qubits/sine_train.bin into X and y data. 
Loading dataset from /scratch/j/jacobsen/gjones/sine5qubits/sine_test.bin... 
Successfully loaded /scratch/j/jacobsen/gjones/sine5qubits/sine_test.bin into X and y data. 
Training model with dataset /scratch/j/jacobsen/gjones/sine5qubits/sine_train.bin 
 at time Thu Apr  4 22:03:30 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 22:06:11 2024]  Iteration number: 0 with current cost as 0.4980691193908029 and parameters 
[-1.59746767  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010897
  3.06858481  2.18960145  1.18551998 -1.06648326  0.6027151   1.14432445
  1.31029899 -1.87354645]. 
Working on 0.4 fold... 
[Thu Apr  4 22:10:33 2024]  Iteration number: 0 with current cost as 0.4990018843125928 and parameters 
[-1.70535715  2.23743464 -2.12427939 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.1896017   1.18552023 -1.06648308  0.60271585  1.14432495
  1.31029923 -1.87354606]. 
Working on 0.6 fold... 
[Thu Apr  4 22:16:19 2024]  Iteration number: 0 with current cost as 0.15865953808949992 and parameters 
[ 1.60727036  2.23743464 -2.12427888 -0.11653178  0.55388708 -2.77010897
  3.06858423  2.18960221  1.18552074 -1.06648308  0.60271586  1.1443252
  1.31029974 -1.87354454]. 
Working on 0.8 fold... 
[Thu Apr  4 22:20:51 2024]  Iteration number: 0 with current cost as 0.48691723272705184 and parameters 
[-1.62060706  2.23743439 -2.12427939 -0.11653078  0.55388708 -2.77010922
  3.0685845   2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354631]. 
Working on 1.0 fold... 
[Thu Apr  4 22:25:37 2024]  Iteration number: 0 with current cost as 0.15191354640306573 and parameters 
[ 1.54067827  2.2374339  -2.1242789  -0.11653029  0.55388708 -2.77010971
  3.06858278  2.18960145  1.18552072 -1.06648382  0.6027151   1.14432445
  1.31029972 -1.87354533]. 
Training complete taking 1462.4674229621887 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 22:30:29 2024]  Iteration number: 0 with current cost as 0.4980691193908029 and parameters 
[-1.59746767  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010897
  3.06858481  2.18960145  1.18551998 -1.06648326  0.6027151   1.14432445
  1.31029899 -1.87354645]. 
Working on 0.4 fold... 
[Thu Apr  4 22:34:52 2024]  Iteration number: 0 with current cost as 0.4990018843125928 and parameters 
[-1.70535715  2.23743464 -2.12427939 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.1896017   1.18552023 -1.06648308  0.60271585  1.14432495
  1.31029923 -1.87354606]. 
Working on 0.6 fold... 
[Thu Apr  4 22:40:46 2024]  Iteration number: 0 with current cost as 0.15865953808949992 and parameters 
[ 1.60727036  2.23743464 -2.12427888 -0.11653178  0.55388708 -2.77010897
  3.06858423  2.18960221  1.18552074 -1.06648308  0.60271586  1.1443252
  1.31029974 -1.87354454]. 
Working on 0.8 fold... 
[Thu Apr  4 22:45:03 2024]  Iteration number: 0 with current cost as 0.48691723272705184 and parameters 
[-1.62060706  2.23743439 -2.12427939 -0.11653078  0.55388708 -2.77010922
  3.0685845   2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354631]. 
Working on 1.0 fold... 
[Thu Apr  4 22:49:51 2024]  Iteration number: 0 with current cost as 0.15191354640306573 and parameters 
[ 1.54067827  2.2374339  -2.1242789  -0.11653029  0.55388708 -2.77010971
  3.06858278  2.18960145  1.18552072 -1.06648382  0.6027151   1.14432445
  1.31029972 -1.87354533]. 
Training complete taking 1449.091965675354 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 22:54:50 2024]  Iteration number: 0 with current cost as 0.4980691193908029 and parameters 
[-1.59746767  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010897
  3.06858481  2.18960145  1.18551998 -1.06648326  0.6027151   1.14432445
  1.31029899 -1.87354645]. 
Working on 0.4 fold... 
[Thu Apr  4 22:59:18 2024]  Iteration number: 0 with current cost as 0.4990018843125928 and parameters 
[-1.70535715  2.23743464 -2.12427939 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.1896017   1.18552023 -1.06648308  0.60271585  1.14432495
  1.31029923 -1.87354606]. 
Working on 0.6 fold... 
[Thu Apr  4 23:05:15 2024]  Iteration number: 0 with current cost as 0.15865953808949992 and parameters 
[ 1.60727036  2.23743464 -2.12427888 -0.11653178  0.55388708 -2.77010897
  3.06858423  2.18960221  1.18552074 -1.06648308  0.60271586  1.1443252
  1.31029974 -1.87354454]. 
Working on 0.8 fold... 
[Thu Apr  4 23:09:38 2024]  Iteration number: 0 with current cost as 0.48691723272705184 and parameters 
[-1.62060706  2.23743439 -2.12427939 -0.11653078  0.55388708 -2.77010922
  3.0685845   2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354631]. 
Working on 1.0 fold... 
[Thu Apr  4 23:14:40 2024]  Iteration number: 0 with current cost as 0.15191354640306573 and parameters 
[ 1.54067827  2.2374339  -2.1242789  -0.11653029  0.55388708 -2.77010971
  3.06858278  2.18960145  1.18552072 -1.06648382  0.6027151   1.14432445
  1.31029972 -1.87354533]. 
Training complete taking 1499.6787691116333 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 23:19:38 2024]  Iteration number: 0 with current cost as 0.4980691193908029 and parameters 
[-1.59746767  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010897
  3.06858481  2.18960145  1.18551998 -1.06648326  0.6027151   1.14432445
  1.31029899 -1.87354645]. 
Working on 0.4 fold... 
[Thu Apr  4 23:24:07 2024]  Iteration number: 0 with current cost as 0.4990018843125928 and parameters 
[-1.70535715  2.23743464 -2.12427939 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.1896017   1.18552023 -1.06648308  0.60271585  1.14432495
  1.31029923 -1.87354606]. 
Working on 0.6 fold... 
[Thu Apr  4 23:30:08 2024]  Iteration number: 0 with current cost as 0.15865953808949992 and parameters 
[ 1.60727036  2.23743464 -2.12427888 -0.11653178  0.55388708 -2.77010897
  3.06858423  2.18960221  1.18552074 -1.06648308  0.60271586  1.1443252
  1.31029974 -1.87354454]. 
Working on 0.8 fold... 
[Thu Apr  4 23:34:38 2024]  Iteration number: 0 with current cost as 0.48691723272705184 and parameters 
[-1.62060706  2.23743439 -2.12427939 -0.11653078  0.55388708 -2.77010922
  3.0685845   2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354631]. 
Working on 1.0 fold... 
[Thu Apr  4 23:39:31 2024]  Iteration number: 0 with current cost as 0.15191354640306573 and parameters 
[ 1.54067827  2.2374339  -2.1242789  -0.11653029  0.55388708 -2.77010971
  3.06858278  2.18960145  1.18552072 -1.06648382  0.6027151   1.14432445
  1.31029972 -1.87354533]. 
Training complete taking 1491.8352808952332 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 23:44:36 2024]  Iteration number: 0 with current cost as 0.4980691193908029 and parameters 
[-1.59746767  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010897
  3.06858481  2.18960145  1.18551998 -1.06648326  0.6027151   1.14432445
  1.31029899 -1.87354645]. 
Working on 0.4 fold... 
[Thu Apr  4 23:49:18 2024]  Iteration number: 0 with current cost as 0.4990018843125928 and parameters 
[-1.70535715  2.23743464 -2.12427939 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.1896017   1.18552023 -1.06648308  0.60271585  1.14432495
  1.31029923 -1.87354606]. 
Working on 0.6 fold... 
[Thu Apr  4 23:55:11 2024]  Iteration number: 0 with current cost as 0.15865953808949992 and parameters 
[ 1.60727036  2.23743464 -2.12427888 -0.11653178  0.55388708 -2.77010897
  3.06858423  2.18960221  1.18552074 -1.06648308  0.60271586  1.1443252
  1.31029974 -1.87354454]. 
Working on 0.8 fold... 
[Thu Apr  4 23:59:52 2024]  Iteration number: 0 with current cost as 0.48691723272705184 and parameters 
[-1.62060706  2.23743439 -2.12427939 -0.11653078  0.55388708 -2.77010922
  3.0685845   2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354631]. 
Working on 1.0 fold... 
[Fri Apr  5 00:04:51 2024]  Iteration number: 0 with current cost as 0.15191354640306573 and parameters 
[ 1.54067827  2.2374339  -2.1242789  -0.11653029  0.55388708 -2.77010971
  3.06858278  2.18960145  1.18552072 -1.06648382  0.6027151   1.14432445
  1.31029972 -1.87354533]. 
Training complete taking 1516.182942390442 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 00:10:05 2024]  Iteration number: 0 with current cost as 0.4980691193908029 and parameters 
[-1.59746767  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010897
  3.06858481  2.18960145  1.18551998 -1.06648326  0.6027151   1.14432445
  1.31029899 -1.87354645]. 
Working on 0.4 fold... 
[Fri Apr  5 00:14:33 2024]  Iteration number: 0 with current cost as 0.4990018843125928 and parameters 
[-1.70535715  2.23743464 -2.12427939 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.1896017   1.18552023 -1.06648308  0.60271585  1.14432495
  1.31029923 -1.87354606]. 
Working on 0.6 fold... 
[Fri Apr  5 00:20:36 2024]  Iteration number: 0 with current cost as 0.15865953808949992 and parameters 
[ 1.60727036  2.23743464 -2.12427888 -0.11653178  0.55388708 -2.77010897
  3.06858423  2.18960221  1.18552074 -1.06648308  0.60271586  1.1443252
  1.31029974 -1.87354454]. 
Working on 0.8 fold... 
[Fri Apr  5 00:25:03 2024]  Iteration number: 0 with current cost as 0.48691723272705184 and parameters 
[-1.62060706  2.23743439 -2.12427939 -0.11653078  0.55388708 -2.77010922
  3.0685845   2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354631]. 
Working on 1.0 fold... 
[Fri Apr  5 00:30:03 2024]  Iteration number: 0 with current cost as 0.15191354640306573 and parameters 
[ 1.54067827  2.2374339  -2.1242789  -0.11653029  0.55388708 -2.77010971
  3.06858278  2.18960145  1.18552072 -1.06648382  0.6027151   1.14432445
  1.31029972 -1.87354533]. 
Training complete taking 1513.1209602355957 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 00:35:14 2024]  Iteration number: 0 with current cost as 0.4980691193908029 and parameters 
[-1.59746767  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010897
  3.06858481  2.18960145  1.18551998 -1.06648326  0.6027151   1.14432445
  1.31029899 -1.87354645]. 
Working on 0.4 fold... 
[Fri Apr  5 00:39:43 2024]  Iteration number: 0 with current cost as 0.4990018843125928 and parameters 
[-1.70535715  2.23743464 -2.12427939 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.1896017   1.18552023 -1.06648308  0.60271585  1.14432495
  1.31029923 -1.87354606]. 
Working on 0.6 fold... 
[Fri Apr  5 00:45:29 2024]  Iteration number: 0 with current cost as 0.15865953808949992 and parameters 
[ 1.60727036  2.23743464 -2.12427888 -0.11653178  0.55388708 -2.77010897
  3.06858423  2.18960221  1.18552074 -1.06648308  0.60271586  1.1443252
  1.31029974 -1.87354454]. 
Working on 0.8 fold... 
[Fri Apr  5 00:49:53 2024]  Iteration number: 0 with current cost as 0.48691723272705184 and parameters 
[-1.62060706  2.23743439 -2.12427939 -0.11653078  0.55388708 -2.77010922
  3.0685845   2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354631]. 
Working on 1.0 fold... 
[Fri Apr  5 00:54:50 2024]  Iteration number: 0 with current cost as 0.15191354640306573 and parameters 
[ 1.54067827  2.2374339  -2.1242789  -0.11653029  0.55388708 -2.77010971
  3.06858278  2.18960145  1.18552072 -1.06648382  0.6027151   1.14432445
  1.31029972 -1.87354533]. 
Training complete taking 1486.6847655773163 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 00:59:57 2024]  Iteration number: 0 with current cost as 0.4980691193908029 and parameters 
[-1.59746767  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010897
  3.06858481  2.18960145  1.18551998 -1.06648326  0.6027151   1.14432445
  1.31029899 -1.87354645]. 
Working on 0.4 fold... 
[Fri Apr  5 01:04:47 2024]  Iteration number: 0 with current cost as 0.4990018843125928 and parameters 
[-1.70535715  2.23743464 -2.12427939 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.1896017   1.18552023 -1.06648308  0.60271585  1.14432495
  1.31029923 -1.87354606]. 
Working on 0.6 fold... 
[Fri Apr  5 01:10:57 2024]  Iteration number: 0 with current cost as 0.15865953808949992 and parameters 
[ 1.60727036  2.23743464 -2.12427888 -0.11653178  0.55388708 -2.77010897
  3.06858423  2.18960221  1.18552074 -1.06648308  0.60271586  1.1443252
  1.31029974 -1.87354454]. 
Working on 0.8 fold... 
[Fri Apr  5 01:15:46 2024]  Iteration number: 0 with current cost as 0.48691723272705184 and parameters 
[-1.62060706  2.23743439 -2.12427939 -0.11653078  0.55388708 -2.77010922
  3.0685845   2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354631]. 
Working on 1.0 fold... 
[Fri Apr  5 01:20:55 2024]  Iteration number: 0 with current cost as 0.15191354640306573 and parameters 
[ 1.54067827  2.2374339  -2.1242789  -0.11653029  0.55388708 -2.77010971
  3.06858278  2.18960145  1.18552072 -1.06648382  0.6027151   1.14432445
  1.31029972 -1.87354533]. 
Training complete taking 1566.158173084259 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 01:25:58 2024]  Iteration number: 0 with current cost as 0.4980691193908029 and parameters 
[-1.59746767  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010897
  3.06858481  2.18960145  1.18551998 -1.06648326  0.6027151   1.14432445
  1.31029899 -1.87354645]. 
Working on 0.4 fold... 
[Fri Apr  5 01:30:24 2024]  Iteration number: 0 with current cost as 0.4990018843125928 and parameters 
[-1.70535715  2.23743464 -2.12427939 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.1896017   1.18552023 -1.06648308  0.60271585  1.14432495
  1.31029923 -1.87354606]. 
Working on 0.6 fold... 
[Fri Apr  5 01:36:22 2024]  Iteration number: 0 with current cost as 0.15865953808949992 and parameters 
[ 1.60727036  2.23743464 -2.12427888 -0.11653178  0.55388708 -2.77010897
  3.06858423  2.18960221  1.18552074 -1.06648308  0.60271586  1.1443252
  1.31029974 -1.87354454]. 
Working on 0.8 fold... 
[Fri Apr  5 01:41:06 2024]  Iteration number: 0 with current cost as 0.48691723272705184 and parameters 
[-1.62060706  2.23743439 -2.12427939 -0.11653078  0.55388708 -2.77010922
  3.0685845   2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354631]. 
Working on 1.0 fold... 
[Fri Apr  5 01:46:18 2024]  Iteration number: 0 with current cost as 0.15191354640306573 and parameters 
[ 1.54067827  2.2374339  -2.1242789  -0.11653029  0.55388708 -2.77010971
  3.06858278  2.18960145  1.18552072 -1.06648382  0.6027151   1.14432445
  1.31029972 -1.87354533]. 
Training complete taking 1523.9771826267242 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 01:51:30 2024]  Iteration number: 0 with current cost as 0.4980691193908029 and parameters 
[-1.59746767  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010897
  3.06858481  2.18960145  1.18551998 -1.06648326  0.6027151   1.14432445
  1.31029899 -1.87354645]. 
Working on 0.4 fold... 
[Fri Apr  5 01:56:06 2024]  Iteration number: 0 with current cost as 0.4990018843125928 and parameters 
[-1.70535715  2.23743464 -2.12427939 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.1896017   1.18552023 -1.06648308  0.60271585  1.14432495
  1.31029923 -1.87354606]. 
Working on 0.6 fold... 
[Fri Apr  5 02:02:08 2024]  Iteration number: 0 with current cost as 0.15865953808949992 and parameters 
[ 1.60727036  2.23743464 -2.12427888 -0.11653178  0.55388708 -2.77010897
  3.06858423  2.18960221  1.18552074 -1.06648308  0.60271586  1.1443252
  1.31029974 -1.87354454]. 
Working on 0.8 fold... 
[Fri Apr  5 02:06:53 2024]  Iteration number: 0 with current cost as 0.48691723272705184 and parameters 
[-1.62060706  2.23743439 -2.12427939 -0.11653078  0.55388708 -2.77010922
  3.0685845   2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354631]. 
Working on 1.0 fold... 
[Fri Apr  5 02:12:10 2024]  Iteration number: 0 with current cost as 0.15191354640306573 and parameters 
[ 1.54067827  2.2374339  -2.1242789  -0.11653029  0.55388708 -2.77010971
  3.06858278  2.18960145  1.18552072 -1.06648382  0.6027151   1.14432445
  1.31029972 -1.87354533]. 
Training complete taking 1552.6247203350067 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 02:17:10 2024]  Iteration number: 0 with current cost as 0.4980691193908029 and parameters 
[-1.59746767  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010897
  3.06858481  2.18960145  1.18551998 -1.06648326  0.6027151   1.14432445
  1.31029899 -1.87354645]. 
Working on 0.4 fold... 
[Fri Apr  5 02:21:44 2024]  Iteration number: 0 with current cost as 0.4990018843125928 and parameters 
[-1.70535715  2.23743464 -2.12427939 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.1896017   1.18552023 -1.06648308  0.60271585  1.14432495
  1.31029923 -1.87354606]. 
Working on 0.6 fold... 
[Fri Apr  5 02:27:50 2024]  Iteration number: 0 with current cost as 0.15865953808949992 and parameters 
[ 1.60727036  2.23743464 -2.12427888 -0.11653178  0.55388708 -2.77010897
  3.06858423  2.18960221  1.18552074 -1.06648308  0.60271586  1.1443252
  1.31029974 -1.87354454]. 
Working on 0.8 fold... 
[Fri Apr  5 02:32:26 2024]  Iteration number: 0 with current cost as 0.48691723272705184 and parameters 
[-1.62060706  2.23743439 -2.12427939 -0.11653078  0.55388708 -2.77010922
  3.0685845   2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354631]. 
Working on 1.0 fold... 
[Fri Apr  5 02:37:14 2024]  Iteration number: 0 with current cost as 0.15191354640306573 and parameters 
[ 1.54067827  2.2374339  -2.1242789  -0.11653029  0.55388708 -2.77010971
  3.06858278  2.18960145  1.18552072 -1.06648382  0.6027151   1.14432445
  1.31029972 -1.87354533]. 
Training complete taking 1498.4915554523468 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 02:42:12 2024]  Iteration number: 0 with current cost as 0.4980691193908029 and parameters 
[-1.59746767  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010897
  3.06858481  2.18960145  1.18551998 -1.06648326  0.6027151   1.14432445
  1.31029899 -1.87354645]. 
Working on 0.4 fold... 
[Fri Apr  5 02:47:07 2024]  Iteration number: 0 with current cost as 0.4990018843125928 and parameters 
[-1.70535715  2.23743464 -2.12427939 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.1896017   1.18552023 -1.06648308  0.60271585  1.14432495
  1.31029923 -1.87354606]. 
Working on 0.6 fold... 
[Fri Apr  5 02:53:16 2024]  Iteration number: 0 with current cost as 0.15865953808949992 and parameters 
[ 1.60727036  2.23743464 -2.12427888 -0.11653178  0.55388708 -2.77010897
  3.06858423  2.18960221  1.18552074 -1.06648308  0.60271586  1.1443252
  1.31029974 -1.87354454]. 
Working on 0.8 fold... 
[Fri Apr  5 02:57:51 2024]  Iteration number: 0 with current cost as 0.48691723272705184 and parameters 
[-1.62060706  2.23743439 -2.12427939 -0.11653078  0.55388708 -2.77010922
  3.0685845   2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354631]. 
Working on 1.0 fold... 
[Fri Apr  5 03:02:35 2024]  Iteration number: 0 with current cost as 0.15191354640306573 and parameters 
[ 1.54067827  2.2374339  -2.1242789  -0.11653029  0.55388708 -2.77010971
  3.06858278  2.18960145  1.18552072 -1.06648382  0.6027151   1.14432445
  1.31029972 -1.87354533]. 
Training complete taking 1514.0705480575562 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 03:07:16 2024]  Iteration number: 0 with current cost as 0.4980691193908029 and parameters 
[-1.59746767  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010897
  3.06858481  2.18960145  1.18551998 -1.06648326  0.6027151   1.14432445
  1.31029899 -1.87354645]. 
Working on 0.4 fold... 
[Fri Apr  5 03:11:35 2024]  Iteration number: 0 with current cost as 0.4990018843125928 and parameters 
[-1.70535715  2.23743464 -2.12427939 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.1896017   1.18552023 -1.06648308  0.60271585  1.14432495
  1.31029923 -1.87354606]. 
Working on 0.6 fold... 
[Fri Apr  5 03:17:10 2024]  Iteration number: 0 with current cost as 0.15865953808949992 and parameters 
[ 1.60727036  2.23743464 -2.12427888 -0.11653178  0.55388708 -2.77010897
  3.06858423  2.18960221  1.18552074 -1.06648308  0.60271586  1.1443252
  1.31029974 -1.87354454]. 
Working on 0.8 fold... 
[Fri Apr  5 03:21:20 2024]  Iteration number: 0 with current cost as 0.48691723272705184 and parameters 
[-1.62060706  2.23743439 -2.12427939 -0.11653078  0.55388708 -2.77010922
  3.0685845   2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354631]. 
Working on 1.0 fold... 
[Fri Apr  5 03:25:52 2024]  Iteration number: 0 with current cost as 0.15191354640306573 and parameters 
[ 1.54067827  2.2374339  -2.1242789  -0.11653029  0.55388708 -2.77010971
  3.06858278  2.18960145  1.18552072 -1.06648382  0.6027151   1.14432445
  1.31029972 -1.87354533]. 
Training complete taking 1397.8578896522522 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 03:30:35 2024]  Iteration number: 0 with current cost as 0.4980691193908029 and parameters 
[-1.59746767  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010897
  3.06858481  2.18960145  1.18551998 -1.06648326  0.6027151   1.14432445
  1.31029899 -1.87354645]. 
Working on 0.4 fold... 
[Fri Apr  5 03:35:00 2024]  Iteration number: 0 with current cost as 0.4990018843125928 and parameters 
[-1.70535715  2.23743464 -2.12427939 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.1896017   1.18552023 -1.06648308  0.60271585  1.14432495
  1.31029923 -1.87354606]. 
Working on 0.6 fold... 
[Fri Apr  5 03:40:43 2024]  Iteration number: 0 with current cost as 0.15865953808949992 and parameters 
[ 1.60727036  2.23743464 -2.12427888 -0.11653178  0.55388708 -2.77010897
  3.06858423  2.18960221  1.18552074 -1.06648308  0.60271586  1.1443252
  1.31029974 -1.87354454]. 
Working on 0.8 fold... 
[Fri Apr  5 03:45:04 2024]  Iteration number: 0 with current cost as 0.48691723272705184 and parameters 
[-1.62060706  2.23743439 -2.12427939 -0.11653078  0.55388708 -2.77010922
  3.0685845   2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354631]. 
Working on 1.0 fold... 
[Fri Apr  5 03:49:49 2024]  Iteration number: 0 with current cost as 0.15191354640306573 and parameters 
[ 1.54067827  2.2374339  -2.1242789  -0.11653029  0.55388708 -2.77010971
  3.06858278  2.18960145  1.18552072 -1.06648382  0.6027151   1.14432445
  1.31029972 -1.87354533]. 
Training complete taking 1440.0279932022095 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 03:54:37 2024]  Iteration number: 0 with current cost as 0.4980691193908029 and parameters 
[-1.59746767  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010897
  3.06858481  2.18960145  1.18551998 -1.06648326  0.6027151   1.14432445
  1.31029899 -1.87354645]. 
Working on 0.4 fold... 
[Fri Apr  5 03:58:57 2024]  Iteration number: 0 with current cost as 0.4990018843125928 and parameters 
[-1.70535715  2.23743464 -2.12427939 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.1896017   1.18552023 -1.06648308  0.60271585  1.14432495
  1.31029923 -1.87354606]. 
Working on 0.6 fold... 
[Fri Apr  5 04:04:28 2024]  Iteration number: 0 with current cost as 0.15865953808949992 and parameters 
[ 1.60727036  2.23743464 -2.12427888 -0.11653178  0.55388708 -2.77010897
  3.06858423  2.18960221  1.18552074 -1.06648308  0.60271586  1.1443252
  1.31029974 -1.87354454]. 
Working on 0.8 fold... 
[Fri Apr  5 04:09:07 2024]  Iteration number: 0 with current cost as 0.48691723272705184 and parameters 
[-1.62060706  2.23743439 -2.12427939 -0.11653078  0.55388708 -2.77010922
  3.0685845   2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354631]. 
Working on 1.0 fold... 
[Fri Apr  5 04:13:47 2024]  Iteration number: 0 with current cost as 0.15191354640306573 and parameters 
[ 1.54067827  2.2374339  -2.1242789  -0.11653029  0.55388708 -2.77010971
  3.06858278  2.18960145  1.18552072 -1.06648382  0.6027151   1.14432445
  1.31029972 -1.87354533]. 
Training complete taking 1434.250863790512 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 04:18:32 2024]  Iteration number: 0 with current cost as 0.4980691193908029 and parameters 
[-1.59746767  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010897
  3.06858481  2.18960145  1.18551998 -1.06648326  0.6027151   1.14432445
  1.31029899 -1.87354645]. 
Working on 0.4 fold... 
[Fri Apr  5 04:22:54 2024]  Iteration number: 0 with current cost as 0.4990018843125928 and parameters 
[-1.70535715  2.23743464 -2.12427939 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.1896017   1.18552023 -1.06648308  0.60271585  1.14432495
  1.31029923 -1.87354606]. 
Working on 0.6 fold... 
[Fri Apr  5 04:28:15 2024]  Iteration number: 0 with current cost as 0.15865953808949992 and parameters 
[ 1.60727036  2.23743464 -2.12427888 -0.11653178  0.55388708 -2.77010897
  3.06858423  2.18960221  1.18552074 -1.06648308  0.60271586  1.1443252
  1.31029974 -1.87354454]. 
Working on 0.8 fold... 
[Fri Apr  5 04:32:22 2024]  Iteration number: 0 with current cost as 0.48691723272705184 and parameters 
[-1.62060706  2.23743439 -2.12427939 -0.11653078  0.55388708 -2.77010922
  3.0685845   2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354631]. 
Working on 1.0 fold... 
[Fri Apr  5 04:36:48 2024]  Iteration number: 0 with current cost as 0.15191354640306573 and parameters 
[ 1.54067827  2.2374339  -2.1242789  -0.11653029  0.55388708 -2.77010971
  3.06858278  2.18960145  1.18552072 -1.06648382  0.6027151   1.14432445
  1.31029972 -1.87354533]. 
Training complete taking 1372.2511248588562 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 04:41:19 2024]  Iteration number: 0 with current cost as 0.4980691193908029 and parameters 
[-1.59746767  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010897
  3.06858481  2.18960145  1.18551998 -1.06648326  0.6027151   1.14432445
  1.31029899 -1.87354645]. 
Working on 0.4 fold... 
[Fri Apr  5 04:45:27 2024]  Iteration number: 0 with current cost as 0.4990018843125928 and parameters 
[-1.70535715  2.23743464 -2.12427939 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.1896017   1.18552023 -1.06648308  0.60271585  1.14432495
  1.31029923 -1.87354606]. 
Working on 0.6 fold... 
[Fri Apr  5 04:50:57 2024]  Iteration number: 0 with current cost as 0.15865953808949992 and parameters 
[ 1.60727036  2.23743464 -2.12427888 -0.11653178  0.55388708 -2.77010897
  3.06858423  2.18960221  1.18552074 -1.06648308  0.60271586  1.1443252
  1.31029974 -1.87354454]. 
Working on 0.8 fold... 
[Fri Apr  5 04:55:12 2024]  Iteration number: 0 with current cost as 0.48691723272705184 and parameters 
[-1.62060706  2.23743439 -2.12427939 -0.11653078  0.55388708 -2.77010922
  3.0685845   2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354631]. 
Working on 1.0 fold... 
[Fri Apr  5 04:59:52 2024]  Iteration number: 0 with current cost as 0.15191354640306573 and parameters 
[ 1.54067827  2.2374339  -2.1242789  -0.11653029  0.55388708 -2.77010971
  3.06858278  2.18960145  1.18552072 -1.06648382  0.6027151   1.14432445
  1.31029972 -1.87354533]. 
Training complete taking 1390.2705442905426 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 05:04:28 2024]  Iteration number: 0 with current cost as 0.4980691193908029 and parameters 
[-1.59746767  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010897
  3.06858481  2.18960145  1.18551998 -1.06648326  0.6027151   1.14432445
  1.31029899 -1.87354645]. 
Working on 0.4 fold... 
[Fri Apr  5 05:08:49 2024]  Iteration number: 0 with current cost as 0.4990018843125928 and parameters 
[-1.70535715  2.23743464 -2.12427939 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.1896017   1.18552023 -1.06648308  0.60271585  1.14432495
  1.31029923 -1.87354606]. 
Working on 0.6 fold... 
[Fri Apr  5 05:14:05 2024]  Iteration number: 0 with current cost as 0.15865953808949992 and parameters 
[ 1.60727036  2.23743464 -2.12427888 -0.11653178  0.55388708 -2.77010897
  3.06858423  2.18960221  1.18552074 -1.06648308  0.60271586  1.1443252
  1.31029974 -1.87354454]. 
Working on 0.8 fold... 
[Fri Apr  5 05:18:21 2024]  Iteration number: 0 with current cost as 0.48691723272705184 and parameters 
[-1.62060706  2.23743439 -2.12427939 -0.11653078  0.55388708 -2.77010922
  3.0685845   2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354631]. 
Working on 1.0 fold... 
[Fri Apr  5 05:23:02 2024]  Iteration number: 0 with current cost as 0.15191354640306573 and parameters 
[ 1.54067827  2.2374339  -2.1242789  -0.11653029  0.55388708 -2.77010971
  3.06858278  2.18960145  1.18552072 -1.06648382  0.6027151   1.14432445
  1.31029972 -1.87354533]. 
Training complete taking 1385.0297338962555 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 05:27:29 2024]  Iteration number: 0 with current cost as 0.4980691193908029 and parameters 
[-1.59746767  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010897
  3.06858481  2.18960145  1.18551998 -1.06648326  0.6027151   1.14432445
  1.31029899 -1.87354645]. 
Working on 0.4 fold... 
[Fri Apr  5 05:31:43 2024]  Iteration number: 0 with current cost as 0.4990018843125928 and parameters 
[-1.70535715  2.23743464 -2.12427939 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.1896017   1.18552023 -1.06648308  0.60271585  1.14432495
  1.31029923 -1.87354606]. 
Working on 0.6 fold... 
[Fri Apr  5 05:37:17 2024]  Iteration number: 0 with current cost as 0.15865953808949992 and parameters 
[ 1.60727036  2.23743464 -2.12427888 -0.11653178  0.55388708 -2.77010897
  3.06858423  2.18960221  1.18552074 -1.06648308  0.60271586  1.1443252
  1.31029974 -1.87354454]. 
Working on 0.8 fold... 
[Fri Apr  5 05:41:35 2024]  Iteration number: 0 with current cost as 0.48691723272705184 and parameters 
[-1.62060706  2.23743439 -2.12427939 -0.11653078  0.55388708 -2.77010922
  3.0685845   2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354631]. 
Working on 1.0 fold... 
[Fri Apr  5 05:46:02 2024]  Iteration number: 0 with current cost as 0.15191354640306573 and parameters 
[ 1.54067827  2.2374339  -2.1242789  -0.11653029  0.55388708 -2.77010971
  3.06858278  2.18960145  1.18552072 -1.06648382  0.6027151   1.14432445
  1.31029972 -1.87354533]. 
Training complete taking 1387.4687721729279 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 05:50:44 2024]  Iteration number: 0 with current cost as 0.4980691193908029 and parameters 
[-1.59746767  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010897
  3.06858481  2.18960145  1.18551998 -1.06648326  0.6027151   1.14432445
  1.31029899 -1.87354645]. 
Working on 0.4 fold... 
[Fri Apr  5 05:54:53 2024]  Iteration number: 0 with current cost as 0.4990018843125928 and parameters 
[-1.70535715  2.23743464 -2.12427939 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.1896017   1.18552023 -1.06648308  0.60271585  1.14432495
  1.31029923 -1.87354606]. 
Working on 0.6 fold... 
[Fri Apr  5 06:00:18 2024]  Iteration number: 0 with current cost as 0.15865953808949992 and parameters 
[ 1.60727036  2.23743464 -2.12427888 -0.11653178  0.55388708 -2.77010897
  3.06858423  2.18960221  1.18552074 -1.06648308  0.60271586  1.1443252
  1.31029974 -1.87354454]. 
Working on 0.8 fold... 
[Fri Apr  5 06:04:31 2024]  Iteration number: 0 with current cost as 0.48691723272705184 and parameters 
[-1.62060706  2.23743439 -2.12427939 -0.11653078  0.55388708 -2.77010922
  3.0685845   2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354631]. 
Working on 1.0 fold... 
[Fri Apr  5 06:09:08 2024]  Iteration number: 0 with current cost as 0.15191354640306573 and parameters 
[ 1.54067827  2.2374339  -2.1242789  -0.11653029  0.55388708 -2.77010971
  3.06858278  2.18960145  1.18552072 -1.06648382  0.6027151   1.14432445
  1.31029972 -1.87354533]. 
Training complete taking 1382.3767693042755 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 06:13:52 2024]  Iteration number: 0 with current cost as 0.4980691193908029 and parameters 
[-1.59746767  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010897
  3.06858481  2.18960145  1.18551998 -1.06648326  0.6027151   1.14432445
  1.31029899 -1.87354645]. 
Working on 0.4 fold... 
[Fri Apr  5 06:18:15 2024]  Iteration number: 0 with current cost as 0.4990018843125928 and parameters 
[-1.70535715  2.23743464 -2.12427939 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.1896017   1.18552023 -1.06648308  0.60271585  1.14432495
  1.31029923 -1.87354606]. 
Working on 0.6 fold... 
[Fri Apr  5 06:23:34 2024]  Iteration number: 0 with current cost as 0.15865953808949992 and parameters 
[ 1.60727036  2.23743464 -2.12427888 -0.11653178  0.55388708 -2.77010897
  3.06858423  2.18960221  1.18552074 -1.06648308  0.60271586  1.1443252
  1.31029974 -1.87354454]. 
Working on 0.8 fold... 
[Fri Apr  5 06:28:02 2024]  Iteration number: 0 with current cost as 0.48691723272705184 and parameters 
[-1.62060706  2.23743439 -2.12427939 -0.11653078  0.55388708 -2.77010922
  3.0685845   2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354631]. 
Working on 1.0 fold... 
[Fri Apr  5 06:32:47 2024]  Iteration number: 0 with current cost as 0.15191354640306573 and parameters 
[ 1.54067827  2.2374339  -2.1242789  -0.11653029  0.55388708 -2.77010971
  3.06858278  2.18960145  1.18552072 -1.06648382  0.6027151   1.14432445
  1.31029972 -1.87354533]. 
Training complete taking 1417.8749525547028 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 06:37:21 2024]  Iteration number: 0 with current cost as 0.4980691193908029 and parameters 
[-1.59746767  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010897
  3.06858481  2.18960145  1.18551998 -1.06648326  0.6027151   1.14432445
  1.31029899 -1.87354645]. 
Working on 0.4 fold... 
[Fri Apr  5 06:41:36 2024]  Iteration number: 0 with current cost as 0.4990018843125928 and parameters 
[-1.70535715  2.23743464 -2.12427939 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.1896017   1.18552023 -1.06648308  0.60271585  1.14432495
  1.31029923 -1.87354606]. 
Working on 0.6 fold... 
[Fri Apr  5 06:47:06 2024]  Iteration number: 0 with current cost as 0.15865953808949992 and parameters 
[ 1.60727036  2.23743464 -2.12427888 -0.11653178  0.55388708 -2.77010897
  3.06858423  2.18960221  1.18552074 -1.06648308  0.60271586  1.1443252
  1.31029974 -1.87354454]. 
Working on 0.8 fold... 
[Fri Apr  5 06:51:13 2024]  Iteration number: 0 with current cost as 0.48691723272705184 and parameters 
[-1.62060706  2.23743439 -2.12427939 -0.11653078  0.55388708 -2.77010922
  3.0685845   2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354631]. 
Working on 1.0 fold... 
[Fri Apr  5 06:55:52 2024]  Iteration number: 0 with current cost as 0.15191354640306573 and parameters 
[ 1.54067827  2.2374339  -2.1242789  -0.11653029  0.55388708 -2.77010971
  3.06858278  2.18960145  1.18552072 -1.06648382  0.6027151   1.14432445
  1.31029972 -1.87354533]. 
Training complete taking 1384.624757051468 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 07:00:29 2024]  Iteration number: 0 with current cost as 0.4980691193908029 and parameters 
[-1.59746767  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010897
  3.06858481  2.18960145  1.18551998 -1.06648326  0.6027151   1.14432445
  1.31029899 -1.87354645]. 
Working on 0.4 fold... 
[Fri Apr  5 07:04:45 2024]  Iteration number: 0 with current cost as 0.4990018843125928 and parameters 
[-1.70535715  2.23743464 -2.12427939 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.1896017   1.18552023 -1.06648308  0.60271585  1.14432495
  1.31029923 -1.87354606]. 
Working on 0.6 fold... 
[Fri Apr  5 07:10:31 2024]  Iteration number: 0 with current cost as 0.15865953808949992 and parameters 
[ 1.60727036  2.23743464 -2.12427888 -0.11653178  0.55388708 -2.77010897
  3.06858423  2.18960221  1.18552074 -1.06648308  0.60271586  1.1443252
  1.31029974 -1.87354454]. 
Working on 0.8 fold... 
[Fri Apr  5 07:14:48 2024]  Iteration number: 0 with current cost as 0.48691723272705184 and parameters 
[-1.62060706  2.23743439 -2.12427939 -0.11653078  0.55388708 -2.77010922
  3.0685845   2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354631]. 
Working on 1.0 fold... 
[Fri Apr  5 07:19:26 2024]  Iteration number: 0 with current cost as 0.15191354640306573 and parameters 
[ 1.54067827  2.2374339  -2.1242789  -0.11653029  0.55388708 -2.77010971
  3.06858278  2.18960145  1.18552072 -1.06648382  0.6027151   1.14432445
  1.31029972 -1.87354533]. 
Training complete taking 1418.7483537197113 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 07:24:01 2024]  Iteration number: 0 with current cost as 0.4980691193908029 and parameters 
[-1.59746767  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010897
  3.06858481  2.18960145  1.18551998 -1.06648326  0.6027151   1.14432445
  1.31029899 -1.87354645]. 
Working on 0.4 fold... 
[Fri Apr  5 07:28:18 2024]  Iteration number: 0 with current cost as 0.4990018843125928 and parameters 
[-1.70535715  2.23743464 -2.12427939 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.1896017   1.18552023 -1.06648308  0.60271585  1.14432495
  1.31029923 -1.87354606]. 
Working on 0.6 fold... 
[Fri Apr  5 07:34:02 2024]  Iteration number: 0 with current cost as 0.15865953808949992 and parameters 
[ 1.60727036  2.23743464 -2.12427888 -0.11653178  0.55388708 -2.77010897
  3.06858423  2.18960221  1.18552074 -1.06648308  0.60271586  1.1443252
  1.31029974 -1.87354454]. 
Working on 0.8 fold... 
[Fri Apr  5 07:38:28 2024]  Iteration number: 0 with current cost as 0.48691723272705184 and parameters 
[-1.62060706  2.23743439 -2.12427939 -0.11653078  0.55388708 -2.77010922
  3.0685845   2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354631]. 
Working on 1.0 fold... 
[Fri Apr  5 07:43:18 2024]  Iteration number: 0 with current cost as 0.15191354640306573 and parameters 
[ 1.54067827  2.2374339  -2.1242789  -0.11653029  0.55388708 -2.77010971
  3.06858278  2.18960145  1.18552072 -1.06648382  0.6027151   1.14432445
  1.31029972 -1.87354533]. 
Training complete taking 1428.471033334732 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 07:48:01 2024]  Iteration number: 0 with current cost as 0.4980691193908029 and parameters 
[-1.59746767  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010897
  3.06858481  2.18960145  1.18551998 -1.06648326  0.6027151   1.14432445
  1.31029899 -1.87354645]. 
Working on 0.4 fold... 
[Fri Apr  5 07:52:21 2024]  Iteration number: 0 with current cost as 0.4990018843125928 and parameters 
[-1.70535715  2.23743464 -2.12427939 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.1896017   1.18552023 -1.06648308  0.60271585  1.14432495
  1.31029923 -1.87354606]. 
Working on 0.6 fold... 
[Fri Apr  5 07:58:01 2024]  Iteration number: 0 with current cost as 0.15865953808949992 and parameters 
[ 1.60727036  2.23743464 -2.12427888 -0.11653178  0.55388708 -2.77010897
  3.06858423  2.18960221  1.18552074 -1.06648308  0.60271586  1.1443252
  1.31029974 -1.87354454]. 
Working on 0.8 fold... 
[Fri Apr  5 08:02:16 2024]  Iteration number: 0 with current cost as 0.48691723272705184 and parameters 
[-1.62060706  2.23743439 -2.12427939 -0.11653078  0.55388708 -2.77010922
  3.0685845   2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354631]. 
Working on 1.0 fold... 
[Fri Apr  5 08:07:05 2024]  Iteration number: 0 with current cost as 0.15191354640306573 and parameters 
[ 1.54067827  2.2374339  -2.1242789  -0.11653029  0.55388708 -2.77010971
  3.06858278  2.18960145  1.18552072 -1.06648382  0.6027151   1.14432445
  1.31029972 -1.87354533]. 
Training complete taking 1433.057544708252 seconds. 
Discarding model... 

Training complete taking 36346.69525527954 total seconds. 
Now scoring model... 
Scoring complete taking 3.1203484535217285 seconds. 
Saved predicted values as M-M-CZ_Efficient-CRX_predicted_values.csv
Model scores: {'MSE_train': (0.2597577560603638,), 'R2_train': 0.4827611560787045, 'MAE_train': 0.4330897402320538, 'MSE_test': 0.29390023216657635, 'R2_test': 0.45188636810155847, 'MAE_test': 0.48785120046161035}. 
Saved model results as M-M-CZ_Efficient-CRX_results.json. 
