/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /scratch/j/jacobsen/gjones/sine5qubits/sine_train.bin... 
Successfully loaded /scratch/j/jacobsen/gjones/sine5qubits/sine_train.bin into X and y data. 
Loading dataset from /scratch/j/jacobsen/gjones/sine5qubits/sine_test.bin... 
Successfully loaded /scratch/j/jacobsen/gjones/sine5qubits/sine_test.bin into X and y data. 
Training model with dataset /scratch/j/jacobsen/gjones/sine5qubits/sine_train.bin 
 at time Thu Apr  4 21:37:45 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 21:42:39 2024]  Iteration number: 0 with current cost as 0.35912864343770934 and parameters 
[-1.52651411  2.23743457 -2.1242797  -0.11653109  0.55388701 -2.77010904
  3.06858485  2.18960139  1.18551992 -1.06648322  0.60271504  1.14432445
  1.31029899 -1.8735468   0.72965067  2.88578413 -0.54534342 -0.47522485
 -2.02654247  0.72897363  1.60512664  2.830771   -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856952 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 21:49:43 2024]  Iteration number: 0 with current cost as 0.5920625826664997 and parameters 
[-0.22659192  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858483  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578419 -0.54534351 -0.4752247
 -2.02654256  0.7289737   1.60512664  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856943 -0.67550772 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 21:58:34 2024]  Iteration number: 0 with current cost as 0.6045894314765937 and parameters 
[-0.11002133  2.23743432 -2.12427995 -0.11653118  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432429
  1.31029867 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.72897354  1.60512664  2.83077091 -1.2645671  -0.2513612
 -2.39279218 -2.2730979   3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 22:11:29 2024]  Iteration number: 0 with current cost as 0.3355064298290499 and parameters 
[-1.56013632  2.23743464 -2.12427957 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432452
  1.31029899 -1.87354673  0.72965074  2.88578419 -0.54534335 -0.47522472
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337161  2.54856958 -0.67550781 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 22:19:21 2024]  Iteration number: 0 with current cost as 0.5977001784458996 and parameters 
[-0.10470975  2.23743432 -2.12427964 -0.11653118  0.55388692 -2.77010913
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578388 -0.54534351 -0.47522485
 -2.02654272  0.72897354  1.60512664  2.83077107 -1.2645671  -0.2513612
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2911.6275300979614 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 22:31:08 2024]  Iteration number: 0 with current cost as 0.35912864343770934 and parameters 
[-1.52651411  2.23743457 -2.1242797  -0.11653109  0.55388701 -2.77010904
  3.06858485  2.18960139  1.18551992 -1.06648322  0.60271504  1.14432445
  1.31029899 -1.8735468   0.72965067  2.88578413 -0.54534342 -0.47522485
 -2.02654247  0.72897363  1.60512664  2.830771   -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856952 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 22:37:53 2024]  Iteration number: 0 with current cost as 0.5920625826664997 and parameters 
[-0.22659192  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858483  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578419 -0.54534351 -0.4752247
 -2.02654256  0.7289737   1.60512664  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856943 -0.67550772 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 22:46:46 2024]  Iteration number: 0 with current cost as 0.6045894314765937 and parameters 
[-0.11002133  2.23743432 -2.12427995 -0.11653118  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432429
  1.31029867 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.72897354  1.60512664  2.83077091 -1.2645671  -0.2513612
 -2.39279218 -2.2730979   3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 22:59:38 2024]  Iteration number: 0 with current cost as 0.3355064298290499 and parameters 
[-1.56013632  2.23743464 -2.12427957 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432452
  1.31029899 -1.87354673  0.72965074  2.88578419 -0.54534335 -0.47522472
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337161  2.54856958 -0.67550781 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 23:07:17 2024]  Iteration number: 0 with current cost as 0.5977001784458996 and parameters 
[-0.10470975  2.23743432 -2.12427964 -0.11653118  0.55388692 -2.77010913
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578388 -0.54534351 -0.47522485
 -2.02654272  0.72897354  1.60512664  2.83077107 -1.2645671  -0.2513612
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2868.8844015598297 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 23:19:07 2024]  Iteration number: 0 with current cost as 0.35912864343770934 and parameters 
[-1.52651411  2.23743457 -2.1242797  -0.11653109  0.55388701 -2.77010904
  3.06858485  2.18960139  1.18551992 -1.06648322  0.60271504  1.14432445
  1.31029899 -1.8735468   0.72965067  2.88578413 -0.54534342 -0.47522485
 -2.02654247  0.72897363  1.60512664  2.830771   -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856952 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 23:25:57 2024]  Iteration number: 0 with current cost as 0.5920625826664997 and parameters 
[-0.22659192  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858483  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578419 -0.54534351 -0.4752247
 -2.02654256  0.7289737   1.60512664  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856943 -0.67550772 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 23:34:53 2024]  Iteration number: 0 with current cost as 0.6045894314765937 and parameters 
[-0.11002133  2.23743432 -2.12427995 -0.11653118  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432429
  1.31029867 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.72897354  1.60512664  2.83077091 -1.2645671  -0.2513612
 -2.39279218 -2.2730979   3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 23:47:37 2024]  Iteration number: 0 with current cost as 0.3355064298290499 and parameters 
[-1.56013632  2.23743464 -2.12427957 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432452
  1.31029899 -1.87354673  0.72965074  2.88578419 -0.54534335 -0.47522472
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337161  2.54856958 -0.67550781 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 23:55:33 2024]  Iteration number: 0 with current cost as 0.5977001784458996 and parameters 
[-0.10470975  2.23743432 -2.12427964 -0.11653118  0.55388692 -2.77010913
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578388 -0.54534351 -0.47522485
 -2.02654272  0.72897354  1.60512664  2.83077107 -1.2645671  -0.2513612
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2894.7175080776215 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 00:07:05 2024]  Iteration number: 0 with current cost as 0.35912864343770934 and parameters 
[-1.52651411  2.23743457 -2.1242797  -0.11653109  0.55388701 -2.77010904
  3.06858485  2.18960139  1.18551992 -1.06648322  0.60271504  1.14432445
  1.31029899 -1.8735468   0.72965067  2.88578413 -0.54534342 -0.47522485
 -2.02654247  0.72897363  1.60512664  2.830771   -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856952 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 00:13:47 2024]  Iteration number: 0 with current cost as 0.5920625826664997 and parameters 
[-0.22659192  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858483  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578419 -0.54534351 -0.4752247
 -2.02654256  0.7289737   1.60512664  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856943 -0.67550772 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 00:22:13 2024]  Iteration number: 0 with current cost as 0.6045894314765937 and parameters 
[-0.11002133  2.23743432 -2.12427995 -0.11653118  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432429
  1.31029867 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.72897354  1.60512664  2.83077091 -1.2645671  -0.2513612
 -2.39279218 -2.2730979   3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 00:34:40 2024]  Iteration number: 0 with current cost as 0.3355064298290499 and parameters 
[-1.56013632  2.23743464 -2.12427957 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432452
  1.31029899 -1.87354673  0.72965074  2.88578419 -0.54534335 -0.47522472
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337161  2.54856958 -0.67550781 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 00:42:13 2024]  Iteration number: 0 with current cost as 0.5977001784458996 and parameters 
[-0.10470975  2.23743432 -2.12427964 -0.11653118  0.55388692 -2.77010913
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578388 -0.54534351 -0.47522485
 -2.02654272  0.72897354  1.60512664  2.83077107 -1.2645671  -0.2513612
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2782.4915392398834 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 00:53:24 2024]  Iteration number: 0 with current cost as 0.35912864343770934 and parameters 
[-1.52651411  2.23743457 -2.1242797  -0.11653109  0.55388701 -2.77010904
  3.06858485  2.18960139  1.18551992 -1.06648322  0.60271504  1.14432445
  1.31029899 -1.8735468   0.72965067  2.88578413 -0.54534342 -0.47522485
 -2.02654247  0.72897363  1.60512664  2.830771   -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856952 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 00:59:52 2024]  Iteration number: 0 with current cost as 0.5920625826664997 and parameters 
[-0.22659192  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858483  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578419 -0.54534351 -0.4752247
 -2.02654256  0.7289737   1.60512664  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856943 -0.67550772 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 01:08:10 2024]  Iteration number: 0 with current cost as 0.6045894314765937 and parameters 
[-0.11002133  2.23743432 -2.12427995 -0.11653118  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432429
  1.31029867 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.72897354  1.60512664  2.83077091 -1.2645671  -0.2513612
 -2.39279218 -2.2730979   3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 01:20:22 2024]  Iteration number: 0 with current cost as 0.3355064298290499 and parameters 
[-1.56013632  2.23743464 -2.12427957 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432452
  1.31029899 -1.87354673  0.72965074  2.88578419 -0.54534335 -0.47522472
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337161  2.54856958 -0.67550781 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 01:28:05 2024]  Iteration number: 0 with current cost as 0.5977001784458996 and parameters 
[-0.10470975  2.23743432 -2.12427964 -0.11653118  0.55388692 -2.77010913
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578388 -0.54534351 -0.47522485
 -2.02654272  0.72897354  1.60512664  2.83077107 -1.2645671  -0.2513612
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2761.9183309078217 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 01:39:36 2024]  Iteration number: 0 with current cost as 0.35912864343770934 and parameters 
[-1.52651411  2.23743457 -2.1242797  -0.11653109  0.55388701 -2.77010904
  3.06858485  2.18960139  1.18551992 -1.06648322  0.60271504  1.14432445
  1.31029899 -1.8735468   0.72965067  2.88578413 -0.54534342 -0.47522485
 -2.02654247  0.72897363  1.60512664  2.830771   -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856952 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 01:46:24 2024]  Iteration number: 0 with current cost as 0.5920625826664997 and parameters 
[-0.22659192  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858483  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578419 -0.54534351 -0.4752247
 -2.02654256  0.7289737   1.60512664  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856943 -0.67550772 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 01:55:09 2024]  Iteration number: 0 with current cost as 0.6045894314765937 and parameters 
[-0.11002133  2.23743432 -2.12427995 -0.11653118  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432429
  1.31029867 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.72897354  1.60512664  2.83077091 -1.2645671  -0.2513612
 -2.39279218 -2.2730979   3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 02:07:57 2024]  Iteration number: 0 with current cost as 0.3355064298290499 and parameters 
[-1.56013632  2.23743464 -2.12427957 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432452
  1.31029899 -1.87354673  0.72965074  2.88578419 -0.54534335 -0.47522472
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337161  2.54856958 -0.67550781 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 02:15:50 2024]  Iteration number: 0 with current cost as 0.5977001784458996 and parameters 
[-0.10470975  2.23743432 -2.12427964 -0.11653118  0.55388692 -2.77010913
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578388 -0.54534351 -0.47522485
 -2.02654272  0.72897354  1.60512664  2.83077107 -1.2645671  -0.2513612
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2879.8340554237366 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 02:27:35 2024]  Iteration number: 0 with current cost as 0.35912864343770934 and parameters 
[-1.52651411  2.23743457 -2.1242797  -0.11653109  0.55388701 -2.77010904
  3.06858485  2.18960139  1.18551992 -1.06648322  0.60271504  1.14432445
  1.31029899 -1.8735468   0.72965067  2.88578413 -0.54534342 -0.47522485
 -2.02654247  0.72897363  1.60512664  2.830771   -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856952 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 02:34:23 2024]  Iteration number: 0 with current cost as 0.5920625826664997 and parameters 
[-0.22659192  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858483  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578419 -0.54534351 -0.4752247
 -2.02654256  0.7289737   1.60512664  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856943 -0.67550772 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 02:43:09 2024]  Iteration number: 0 with current cost as 0.6045894314765937 and parameters 
[-0.11002133  2.23743432 -2.12427995 -0.11653118  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432429
  1.31029867 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.72897354  1.60512664  2.83077091 -1.2645671  -0.2513612
 -2.39279218 -2.2730979   3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 02:55:50 2024]  Iteration number: 0 with current cost as 0.3355064298290499 and parameters 
[-1.56013632  2.23743464 -2.12427957 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432452
  1.31029899 -1.87354673  0.72965074  2.88578419 -0.54534335 -0.47522472
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337161  2.54856958 -0.67550781 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 03:03:41 2024]  Iteration number: 0 with current cost as 0.5977001784458996 and parameters 
[-0.10470975  2.23743432 -2.12427964 -0.11653118  0.55388692 -2.77010913
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578388 -0.54534351 -0.47522485
 -2.02654272  0.72897354  1.60512664  2.83077107 -1.2645671  -0.2513612
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2868.478176832199 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 03:15:26 2024]  Iteration number: 0 with current cost as 0.35912864343770934 and parameters 
[-1.52651411  2.23743457 -2.1242797  -0.11653109  0.55388701 -2.77010904
  3.06858485  2.18960139  1.18551992 -1.06648322  0.60271504  1.14432445
  1.31029899 -1.8735468   0.72965067  2.88578413 -0.54534342 -0.47522485
 -2.02654247  0.72897363  1.60512664  2.830771   -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856952 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 03:22:13 2024]  Iteration number: 0 with current cost as 0.5920625826664997 and parameters 
[-0.22659192  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858483  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578419 -0.54534351 -0.4752247
 -2.02654256  0.7289737   1.60512664  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856943 -0.67550772 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 03:31:04 2024]  Iteration number: 0 with current cost as 0.6045894314765937 and parameters 
[-0.11002133  2.23743432 -2.12427995 -0.11653118  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432429
  1.31029867 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.72897354  1.60512664  2.83077091 -1.2645671  -0.2513612
 -2.39279218 -2.2730979   3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 03:43:34 2024]  Iteration number: 0 with current cost as 0.3355064298290499 and parameters 
[-1.56013632  2.23743464 -2.12427957 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432452
  1.31029899 -1.87354673  0.72965074  2.88578419 -0.54534335 -0.47522472
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337161  2.54856958 -0.67550781 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 03:51:10 2024]  Iteration number: 0 with current cost as 0.5977001784458996 and parameters 
[-0.10470975  2.23743432 -2.12427964 -0.11653118  0.55388692 -2.77010913
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578388 -0.54534351 -0.47522485
 -2.02654272  0.72897354  1.60512664  2.83077107 -1.2645671  -0.2513612
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2847.988824367523 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 04:02:55 2024]  Iteration number: 0 with current cost as 0.35912864343770934 and parameters 
[-1.52651411  2.23743457 -2.1242797  -0.11653109  0.55388701 -2.77010904
  3.06858485  2.18960139  1.18551992 -1.06648322  0.60271504  1.14432445
  1.31029899 -1.8735468   0.72965067  2.88578413 -0.54534342 -0.47522485
 -2.02654247  0.72897363  1.60512664  2.830771   -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856952 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 04:09:47 2024]  Iteration number: 0 with current cost as 0.5920625826664997 and parameters 
[-0.22659192  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858483  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578419 -0.54534351 -0.4752247
 -2.02654256  0.7289737   1.60512664  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856943 -0.67550772 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 04:18:36 2024]  Iteration number: 0 with current cost as 0.6045894314765937 and parameters 
[-0.11002133  2.23743432 -2.12427995 -0.11653118  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432429
  1.31029867 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.72897354  1.60512664  2.83077091 -1.2645671  -0.2513612
 -2.39279218 -2.2730979   3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 04:31:28 2024]  Iteration number: 0 with current cost as 0.3355064298290499 and parameters 
[-1.56013632  2.23743464 -2.12427957 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432452
  1.31029899 -1.87354673  0.72965074  2.88578419 -0.54534335 -0.47522472
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337161  2.54856958 -0.67550781 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 04:39:20 2024]  Iteration number: 0 with current cost as 0.5977001784458996 and parameters 
[-0.10470975  2.23743432 -2.12427964 -0.11653118  0.55388692 -2.77010913
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578388 -0.54534351 -0.47522485
 -2.02654272  0.72897354  1.60512664  2.83077107 -1.2645671  -0.2513612
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2879.6149139404297 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 04:50:49 2024]  Iteration number: 0 with current cost as 0.35912864343770934 and parameters 
[-1.52651411  2.23743457 -2.1242797  -0.11653109  0.55388701 -2.77010904
  3.06858485  2.18960139  1.18551992 -1.06648322  0.60271504  1.14432445
  1.31029899 -1.8735468   0.72965067  2.88578413 -0.54534342 -0.47522485
 -2.02654247  0.72897363  1.60512664  2.830771   -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856952 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 04:57:35 2024]  Iteration number: 0 with current cost as 0.5920625826664997 and parameters 
[-0.22659192  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858483  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578419 -0.54534351 -0.4752247
 -2.02654256  0.7289737   1.60512664  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856943 -0.67550772 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 05:06:17 2024]  Iteration number: 0 with current cost as 0.6045894314765937 and parameters 
[-0.11002133  2.23743432 -2.12427995 -0.11653118  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432429
  1.31029867 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.72897354  1.60512664  2.83077091 -1.2645671  -0.2513612
 -2.39279218 -2.2730979   3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 05:18:48 2024]  Iteration number: 0 with current cost as 0.3355064298290499 and parameters 
[-1.56013632  2.23743464 -2.12427957 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432452
  1.31029899 -1.87354673  0.72965074  2.88578419 -0.54534335 -0.47522472
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337161  2.54856958 -0.67550781 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 05:26:43 2024]  Iteration number: 0 with current cost as 0.5977001784458996 and parameters 
[-0.10470975  2.23743432 -2.12427964 -0.11653118  0.55388692 -2.77010913
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578388 -0.54534351 -0.47522485
 -2.02654272  0.72897354  1.60512664  2.83077107 -1.2645671  -0.2513612
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2861.8948180675507 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 05:38:35 2024]  Iteration number: 0 with current cost as 0.35912864343770934 and parameters 
[-1.52651411  2.23743457 -2.1242797  -0.11653109  0.55388701 -2.77010904
  3.06858485  2.18960139  1.18551992 -1.06648322  0.60271504  1.14432445
  1.31029899 -1.8735468   0.72965067  2.88578413 -0.54534342 -0.47522485
 -2.02654247  0.72897363  1.60512664  2.830771   -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856952 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 05:45:31 2024]  Iteration number: 0 with current cost as 0.5920625826664997 and parameters 
[-0.22659192  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858483  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578419 -0.54534351 -0.4752247
 -2.02654256  0.7289737   1.60512664  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856943 -0.67550772 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 05:54:17 2024]  Iteration number: 0 with current cost as 0.6045894314765937 and parameters 
[-0.11002133  2.23743432 -2.12427995 -0.11653118  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432429
  1.31029867 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.72897354  1.60512664  2.83077091 -1.2645671  -0.2513612
 -2.39279218 -2.2730979   3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 06:07:03 2024]  Iteration number: 0 with current cost as 0.3355064298290499 and parameters 
[-1.56013632  2.23743464 -2.12427957 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432452
  1.31029899 -1.87354673  0.72965074  2.88578419 -0.54534335 -0.47522472
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337161  2.54856958 -0.67550781 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 06:14:57 2024]  Iteration number: 0 with current cost as 0.5977001784458996 and parameters 
[-0.10470975  2.23743432 -2.12427964 -0.11653118  0.55388692 -2.77010913
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578388 -0.54534351 -0.47522485
 -2.02654272  0.72897354  1.60512664  2.83077107 -1.2645671  -0.2513612
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2873.519209384918 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 06:26:24 2024]  Iteration number: 0 with current cost as 0.35912864343770934 and parameters 
[-1.52651411  2.23743457 -2.1242797  -0.11653109  0.55388701 -2.77010904
  3.06858485  2.18960139  1.18551992 -1.06648322  0.60271504  1.14432445
  1.31029899 -1.8735468   0.72965067  2.88578413 -0.54534342 -0.47522485
 -2.02654247  0.72897363  1.60512664  2.830771   -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856952 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 06:33:22 2024]  Iteration number: 0 with current cost as 0.5920625826664997 and parameters 
[-0.22659192  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858483  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578419 -0.54534351 -0.4752247
 -2.02654256  0.7289737   1.60512664  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856943 -0.67550772 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 06:42:10 2024]  Iteration number: 0 with current cost as 0.6045894314765937 and parameters 
[-0.11002133  2.23743432 -2.12427995 -0.11653118  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432429
  1.31029867 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.72897354  1.60512664  2.83077091 -1.2645671  -0.2513612
 -2.39279218 -2.2730979   3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 06:55:04 2024]  Iteration number: 0 with current cost as 0.3355064298290499 and parameters 
[-1.56013632  2.23743464 -2.12427957 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432452
  1.31029899 -1.87354673  0.72965074  2.88578419 -0.54534335 -0.47522472
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337161  2.54856958 -0.67550781 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 07:02:59 2024]  Iteration number: 0 with current cost as 0.5977001784458996 and parameters 
[-0.10470975  2.23743432 -2.12427964 -0.11653118  0.55388692 -2.77010913
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578388 -0.54534351 -0.47522485
 -2.02654272  0.72897354  1.60512664  2.83077107 -1.2645671  -0.2513612
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2891.182025909424 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 07:14:48 2024]  Iteration number: 0 with current cost as 0.35912864343770934 and parameters 
[-1.52651411  2.23743457 -2.1242797  -0.11653109  0.55388701 -2.77010904
  3.06858485  2.18960139  1.18551992 -1.06648322  0.60271504  1.14432445
  1.31029899 -1.8735468   0.72965067  2.88578413 -0.54534342 -0.47522485
 -2.02654247  0.72897363  1.60512664  2.830771   -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856952 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 07:21:40 2024]  Iteration number: 0 with current cost as 0.5920625826664997 and parameters 
[-0.22659192  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858483  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578419 -0.54534351 -0.4752247
 -2.02654256  0.7289737   1.60512664  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856943 -0.67550772 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 07:30:14 2024]  Iteration number: 0 with current cost as 0.6045894314765937 and parameters 
[-0.11002133  2.23743432 -2.12427995 -0.11653118  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432429
  1.31029867 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.72897354  1.60512664  2.83077091 -1.2645671  -0.2513612
 -2.39279218 -2.2730979   3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 07:42:59 2024]  Iteration number: 0 with current cost as 0.3355064298290499 and parameters 
[-1.56013632  2.23743464 -2.12427957 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432452
  1.31029899 -1.87354673  0.72965074  2.88578419 -0.54534335 -0.47522472
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337161  2.54856958 -0.67550781 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 07:50:32 2024]  Iteration number: 0 with current cost as 0.5977001784458996 and parameters 
[-0.10470975  2.23743432 -2.12427964 -0.11653118  0.55388692 -2.77010913
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578388 -0.54534351 -0.47522485
 -2.02654272  0.72897354  1.60512664  2.83077107 -1.2645671  -0.2513612
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2850.5649859905243 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 08:02:28 2024]  Iteration number: 0 with current cost as 0.35912864343770934 and parameters 
[-1.52651411  2.23743457 -2.1242797  -0.11653109  0.55388701 -2.77010904
  3.06858485  2.18960139  1.18551992 -1.06648322  0.60271504  1.14432445
  1.31029899 -1.8735468   0.72965067  2.88578413 -0.54534342 -0.47522485
 -2.02654247  0.72897363  1.60512664  2.830771   -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856952 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 08:09:22 2024]  Iteration number: 0 with current cost as 0.5920625826664997 and parameters 
[-0.22659192  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858483  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578419 -0.54534351 -0.4752247
 -2.02654256  0.7289737   1.60512664  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856943 -0.67550772 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 08:18:18 2024]  Iteration number: 0 with current cost as 0.6045894314765937 and parameters 
[-0.11002133  2.23743432 -2.12427995 -0.11653118  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432429
  1.31029867 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.72897354  1.60512664  2.83077091 -1.2645671  -0.2513612
 -2.39279218 -2.2730979   3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 08:30:38 2024]  Iteration number: 0 with current cost as 0.3355064298290499 and parameters 
[-1.56013632  2.23743464 -2.12427957 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432452
  1.31029899 -1.87354673  0.72965074  2.88578419 -0.54534335 -0.47522472
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337161  2.54856958 -0.67550781 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 08:38:04 2024]  Iteration number: 0 with current cost as 0.5977001784458996 and parameters 
[-0.10470975  2.23743432 -2.12427964 -0.11653118  0.55388692 -2.77010913
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578388 -0.54534351 -0.47522485
 -2.02654272  0.72897354  1.60512664  2.83077107 -1.2645671  -0.2513612
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2847.881100177765 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 08:49:23 2024]  Iteration number: 0 with current cost as 0.35912864343770934 and parameters 
[-1.52651411  2.23743457 -2.1242797  -0.11653109  0.55388701 -2.77010904
  3.06858485  2.18960139  1.18551992 -1.06648322  0.60271504  1.14432445
  1.31029899 -1.8735468   0.72965067  2.88578413 -0.54534342 -0.47522485
 -2.02654247  0.72897363  1.60512664  2.830771   -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856952 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 08:55:52 2024]  Iteration number: 0 with current cost as 0.5920625826664997 and parameters 
[-0.22659192  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858483  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578419 -0.54534351 -0.4752247
 -2.02654256  0.7289737   1.60512664  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856943 -0.67550772 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 09:04:11 2024]  Iteration number: 0 with current cost as 0.6045894314765937 and parameters 
[-0.11002133  2.23743432 -2.12427995 -0.11653118  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432429
  1.31029867 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.72897354  1.60512664  2.83077091 -1.2645671  -0.2513612
 -2.39279218 -2.2730979   3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 09:16:41 2024]  Iteration number: 0 with current cost as 0.3355064298290499 and parameters 
[-1.56013632  2.23743464 -2.12427957 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432452
  1.31029899 -1.87354673  0.72965074  2.88578419 -0.54534335 -0.47522472
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337161  2.54856958 -0.67550781 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 09:24:25 2024]  Iteration number: 0 with current cost as 0.5977001784458996 and parameters 
[-0.10470975  2.23743432 -2.12427964 -0.11653118  0.55388692 -2.77010913
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578388 -0.54534351 -0.47522485
 -2.02654272  0.72897354  1.60512664  2.83077107 -1.2645671  -0.2513612
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2775.79891037941 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 09:35:54 2024]  Iteration number: 0 with current cost as 0.35912864343770934 and parameters 
[-1.52651411  2.23743457 -2.1242797  -0.11653109  0.55388701 -2.77010904
  3.06858485  2.18960139  1.18551992 -1.06648322  0.60271504  1.14432445
  1.31029899 -1.8735468   0.72965067  2.88578413 -0.54534342 -0.47522485
 -2.02654247  0.72897363  1.60512664  2.830771   -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856952 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 09:42:53 2024]  Iteration number: 0 with current cost as 0.5920625826664997 and parameters 
[-0.22659192  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858483  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578419 -0.54534351 -0.4752247
 -2.02654256  0.7289737   1.60512664  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856943 -0.67550772 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 09:51:47 2024]  Iteration number: 0 with current cost as 0.6045894314765937 and parameters 
[-0.11002133  2.23743432 -2.12427995 -0.11653118  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432429
  1.31029867 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.72897354  1.60512664  2.83077091 -1.2645671  -0.2513612
 -2.39279218 -2.2730979   3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 10:04:38 2024]  Iteration number: 0 with current cost as 0.3355064298290499 and parameters 
[-1.56013632  2.23743464 -2.12427957 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432452
  1.31029899 -1.87354673  0.72965074  2.88578419 -0.54534335 -0.47522472
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337161  2.54856958 -0.67550781 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 10:12:39 2024]  Iteration number: 0 with current cost as 0.5977001784458996 and parameters 
[-0.10470975  2.23743432 -2.12427964 -0.11653118  0.55388692 -2.77010913
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578388 -0.54534351 -0.47522485
 -2.02654272  0.72897354  1.60512664  2.83077107 -1.2645671  -0.2513612
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2911.587922811508 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 10:24:18 2024]  Iteration number: 0 with current cost as 0.35912864343770934 and parameters 
[-1.52651411  2.23743457 -2.1242797  -0.11653109  0.55388701 -2.77010904
  3.06858485  2.18960139  1.18551992 -1.06648322  0.60271504  1.14432445
  1.31029899 -1.8735468   0.72965067  2.88578413 -0.54534342 -0.47522485
 -2.02654247  0.72897363  1.60512664  2.830771   -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856952 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 10:30:58 2024]  Iteration number: 0 with current cost as 0.5920625826664997 and parameters 
[-0.22659192  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858483  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578419 -0.54534351 -0.4752247
 -2.02654256  0.7289737   1.60512664  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856943 -0.67550772 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 10:39:31 2024]  Iteration number: 0 with current cost as 0.6045894314765937 and parameters 
[-0.11002133  2.23743432 -2.12427995 -0.11653118  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432429
  1.31029867 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.72897354  1.60512664  2.83077091 -1.2645671  -0.2513612
 -2.39279218 -2.2730979   3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 10:51:26 2024]  Iteration number: 0 with current cost as 0.3355064298290499 and parameters 
[-1.56013632  2.23743464 -2.12427957 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432452
  1.31029899 -1.87354673  0.72965074  2.88578419 -0.54534335 -0.47522472
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337161  2.54856958 -0.67550781 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 10:59:04 2024]  Iteration number: 0 with current cost as 0.5977001784458996 and parameters 
[-0.10470975  2.23743432 -2.12427964 -0.11653118  0.55388692 -2.77010913
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578388 -0.54534351 -0.47522485
 -2.02654272  0.72897354  1.60512664  2.83077107 -1.2645671  -0.2513612
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2796.1807873249054 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 11:10:41 2024]  Iteration number: 0 with current cost as 0.35912864343770934 and parameters 
[-1.52651411  2.23743457 -2.1242797  -0.11653109  0.55388701 -2.77010904
  3.06858485  2.18960139  1.18551992 -1.06648322  0.60271504  1.14432445
  1.31029899 -1.8735468   0.72965067  2.88578413 -0.54534342 -0.47522485
 -2.02654247  0.72897363  1.60512664  2.830771   -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856952 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 11:17:25 2024]  Iteration number: 0 with current cost as 0.5920625826664997 and parameters 
[-0.22659192  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858483  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578419 -0.54534351 -0.4752247
 -2.02654256  0.7289737   1.60512664  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856943 -0.67550772 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 11:25:55 2024]  Iteration number: 0 with current cost as 0.6045894314765937 and parameters 
[-0.11002133  2.23743432 -2.12427995 -0.11653118  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432429
  1.31029867 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.72897354  1.60512664  2.83077091 -1.2645671  -0.2513612
 -2.39279218 -2.2730979   3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 11:38:23 2024]  Iteration number: 0 with current cost as 0.3355064298290499 and parameters 
[-1.56013632  2.23743464 -2.12427957 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432452
  1.31029899 -1.87354673  0.72965074  2.88578419 -0.54534335 -0.47522472
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337161  2.54856958 -0.67550781 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 11:45:58 2024]  Iteration number: 0 with current cost as 0.5977001784458996 and parameters 
[-0.10470975  2.23743432 -2.12427964 -0.11653118  0.55388692 -2.77010913
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578388 -0.54534351 -0.47522485
 -2.02654272  0.72897354  1.60512664  2.83077107 -1.2645671  -0.2513612
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2797.413709640503 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 11:57:24 2024]  Iteration number: 0 with current cost as 0.35912864343770934 and parameters 
[-1.52651411  2.23743457 -2.1242797  -0.11653109  0.55388701 -2.77010904
  3.06858485  2.18960139  1.18551992 -1.06648322  0.60271504  1.14432445
  1.31029899 -1.8735468   0.72965067  2.88578413 -0.54534342 -0.47522485
 -2.02654247  0.72897363  1.60512664  2.830771   -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856952 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 12:04:05 2024]  Iteration number: 0 with current cost as 0.5920625826664997 and parameters 
[-0.22659192  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858483  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578419 -0.54534351 -0.4752247
 -2.02654256  0.7289737   1.60512664  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856943 -0.67550772 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 12:12:16 2024]  Iteration number: 0 with current cost as 0.6045894314765937 and parameters 
[-0.11002133  2.23743432 -2.12427995 -0.11653118  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432429
  1.31029867 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.72897354  1.60512664  2.83077091 -1.2645671  -0.2513612
 -2.39279218 -2.2730979   3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 12:24:41 2024]  Iteration number: 0 with current cost as 0.3355064298290499 and parameters 
[-1.56013632  2.23743464 -2.12427957 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432452
  1.31029899 -1.87354673  0.72965074  2.88578419 -0.54534335 -0.47522472
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337161  2.54856958 -0.67550781 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 12:32:15 2024]  Iteration number: 0 with current cost as 0.5977001784458996 and parameters 
[-0.10470975  2.23743432 -2.12427964 -0.11653118  0.55388692 -2.77010913
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578388 -0.54534351 -0.47522485
 -2.02654272  0.72897354  1.60512664  2.83077107 -1.2645671  -0.2513612
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2765.8334410190582 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 12:43:29 2024]  Iteration number: 0 with current cost as 0.35912864343770934 and parameters 
[-1.52651411  2.23743457 -2.1242797  -0.11653109  0.55388701 -2.77010904
  3.06858485  2.18960139  1.18551992 -1.06648322  0.60271504  1.14432445
  1.31029899 -1.8735468   0.72965067  2.88578413 -0.54534342 -0.47522485
 -2.02654247  0.72897363  1.60512664  2.830771   -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856952 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 12:49:54 2024]  Iteration number: 0 with current cost as 0.5920625826664997 and parameters 
[-0.22659192  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858483  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578419 -0.54534351 -0.4752247
 -2.02654256  0.7289737   1.60512664  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856943 -0.67550772 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 12:58:27 2024]  Iteration number: 0 with current cost as 0.6045894314765937 and parameters 
[-0.11002133  2.23743432 -2.12427995 -0.11653118  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432429
  1.31029867 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.72897354  1.60512664  2.83077091 -1.2645671  -0.2513612
 -2.39279218 -2.2730979   3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 13:10:33 2024]  Iteration number: 0 with current cost as 0.3355064298290499 and parameters 
[-1.56013632  2.23743464 -2.12427957 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432452
  1.31029899 -1.87354673  0.72965074  2.88578419 -0.54534335 -0.47522472
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337161  2.54856958 -0.67550781 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 13:17:58 2024]  Iteration number: 0 with current cost as 0.5977001784458996 and parameters 
[-0.10470975  2.23743432 -2.12427964 -0.11653118  0.55388692 -2.77010913
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578388 -0.54534351 -0.47522485
 -2.02654272  0.72897354  1.60512664  2.83077107 -1.2645671  -0.2513612
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2753.3287234306335 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 13:29:21 2024]  Iteration number: 0 with current cost as 0.35912864343770934 and parameters 
[-1.52651411  2.23743457 -2.1242797  -0.11653109  0.55388701 -2.77010904
  3.06858485  2.18960139  1.18551992 -1.06648322  0.60271504  1.14432445
  1.31029899 -1.8735468   0.72965067  2.88578413 -0.54534342 -0.47522485
 -2.02654247  0.72897363  1.60512664  2.830771   -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856952 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 13:36:06 2024]  Iteration number: 0 with current cost as 0.5920625826664997 and parameters 
[-0.22659192  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858483  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578419 -0.54534351 -0.4752247
 -2.02654256  0.7289737   1.60512664  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856943 -0.67550772 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 13:44:29 2024]  Iteration number: 0 with current cost as 0.6045894314765937 and parameters 
[-0.11002133  2.23743432 -2.12427995 -0.11653118  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432429
  1.31029867 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.72897354  1.60512664  2.83077091 -1.2645671  -0.2513612
 -2.39279218 -2.2730979   3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 13:56:44 2024]  Iteration number: 0 with current cost as 0.3355064298290499 and parameters 
[-1.56013632  2.23743464 -2.12427957 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432452
  1.31029899 -1.87354673  0.72965074  2.88578419 -0.54534335 -0.47522472
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337161  2.54856958 -0.67550781 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 14:04:15 2024]  Iteration number: 0 with current cost as 0.5977001784458996 and parameters 
[-0.10470975  2.23743432 -2.12427964 -0.11653118  0.55388692 -2.77010913
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578388 -0.54534351 -0.47522485
 -2.02654272  0.72897354  1.60512664  2.83077107 -1.2645671  -0.2513612
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2769.7321903705597 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 14:15:35 2024]  Iteration number: 0 with current cost as 0.35912864343770934 and parameters 
[-1.52651411  2.23743457 -2.1242797  -0.11653109  0.55388701 -2.77010904
  3.06858485  2.18960139  1.18551992 -1.06648322  0.60271504  1.14432445
  1.31029899 -1.8735468   0.72965067  2.88578413 -0.54534342 -0.47522485
 -2.02654247  0.72897363  1.60512664  2.830771   -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856952 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 14:22:11 2024]  Iteration number: 0 with current cost as 0.5920625826664997 and parameters 
[-0.22659192  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858483  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578419 -0.54534351 -0.4752247
 -2.02654256  0.7289737   1.60512664  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856943 -0.67550772 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 14:30:34 2024]  Iteration number: 0 with current cost as 0.6045894314765937 and parameters 
[-0.11002133  2.23743432 -2.12427995 -0.11653118  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432429
  1.31029867 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.72897354  1.60512664  2.83077091 -1.2645671  -0.2513612
 -2.39279218 -2.2730979   3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 14:42:43 2024]  Iteration number: 0 with current cost as 0.3355064298290499 and parameters 
[-1.56013632  2.23743464 -2.12427957 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432452
  1.31029899 -1.87354673  0.72965074  2.88578419 -0.54534335 -0.47522472
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337161  2.54856958 -0.67550781 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 14:50:09 2024]  Iteration number: 0 with current cost as 0.5977001784458996 and parameters 
[-0.10470975  2.23743432 -2.12427964 -0.11653118  0.55388692 -2.77010913
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578388 -0.54534351 -0.47522485
 -2.02654272  0.72897354  1.60512664  2.83077107 -1.2645671  -0.2513612
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2757.243415594101 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 15:02:07 2024]  Iteration number: 0 with current cost as 0.35912864343770934 and parameters 
[-1.52651411  2.23743457 -2.1242797  -0.11653109  0.55388701 -2.77010904
  3.06858485  2.18960139  1.18551992 -1.06648322  0.60271504  1.14432445
  1.31029899 -1.8735468   0.72965067  2.88578413 -0.54534342 -0.47522485
 -2.02654247  0.72897363  1.60512664  2.830771   -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856952 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 15:09:07 2024]  Iteration number: 0 with current cost as 0.5920625826664997 and parameters 
[-0.22659192  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858483  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578419 -0.54534351 -0.4752247
 -2.02654256  0.7289737   1.60512664  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856943 -0.67550772 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 15:18:08 2024]  Iteration number: 0 with current cost as 0.6045894314765937 and parameters 
[-0.11002133  2.23743432 -2.12427995 -0.11653118  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432429
  1.31029867 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.72897354  1.60512664  2.83077091 -1.2645671  -0.2513612
 -2.39279218 -2.2730979   3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 15:31:14 2024]  Iteration number: 0 with current cost as 0.3355064298290499 and parameters 
[-1.56013632  2.23743464 -2.12427957 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432452
  1.31029899 -1.87354673  0.72965074  2.88578419 -0.54534335 -0.47522472
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337161  2.54856958 -0.67550781 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 15:38:58 2024]  Iteration number: 0 with current cost as 0.5977001784458996 and parameters 
[-0.10470975  2.23743432 -2.12427964 -0.11653118  0.55388692 -2.77010913
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578388 -0.54534351 -0.47522485
 -2.02654272  0.72897354  1.60512664  2.83077107 -1.2645671  -0.2513612
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2965.804941177368 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 15:51:22 2024]  Iteration number: 0 with current cost as 0.35912864343770934 and parameters 
[-1.52651411  2.23743457 -2.1242797  -0.11653109  0.55388701 -2.77010904
  3.06858485  2.18960139  1.18551992 -1.06648322  0.60271504  1.14432445
  1.31029899 -1.8735468   0.72965067  2.88578413 -0.54534342 -0.47522485
 -2.02654247  0.72897363  1.60512664  2.830771   -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856952 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 15:58:22 2024]  Iteration number: 0 with current cost as 0.5920625826664997 and parameters 
[-0.22659192  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858483  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578419 -0.54534351 -0.4752247
 -2.02654256  0.7289737   1.60512664  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856943 -0.67550772 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 16:07:12 2024]  Iteration number: 0 with current cost as 0.6045894314765937 and parameters 
[-0.11002133  2.23743432 -2.12427995 -0.11653118  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432429
  1.31029867 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.72897354  1.60512664  2.83077091 -1.2645671  -0.2513612
 -2.39279218 -2.2730979   3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 16:20:06 2024]  Iteration number: 0 with current cost as 0.3355064298290499 and parameters 
[-1.56013632  2.23743464 -2.12427957 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432452
  1.31029899 -1.87354673  0.72965074  2.88578419 -0.54534335 -0.47522472
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337161  2.54856958 -0.67550781 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 16:27:13 2024]  Iteration number: 0 with current cost as 0.5977001784458996 and parameters 
[-0.10470975  2.23743432 -2.12427964 -0.11653118  0.55388692 -2.77010913
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578388 -0.54534351 -0.47522485
 -2.02654272  0.72897354  1.60512664  2.83077107 -1.2645671  -0.2513612
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2859.2267451286316 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 16:38:54 2024]  Iteration number: 0 with current cost as 0.35912864343770934 and parameters 
[-1.52651411  2.23743457 -2.1242797  -0.11653109  0.55388701 -2.77010904
  3.06858485  2.18960139  1.18551992 -1.06648322  0.60271504  1.14432445
  1.31029899 -1.8735468   0.72965067  2.88578413 -0.54534342 -0.47522485
 -2.02654247  0.72897363  1.60512664  2.830771   -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856952 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 16:45:26 2024]  Iteration number: 0 with current cost as 0.5920625826664997 and parameters 
[-0.22659192  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858483  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578419 -0.54534351 -0.4752247
 -2.02654256  0.7289737   1.60512664  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856943 -0.67550772 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 16:54:17 2024]  Iteration number: 0 with current cost as 0.6045894314765937 and parameters 
[-0.11002133  2.23743432 -2.12427995 -0.11653118  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432429
  1.31029867 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.72897354  1.60512664  2.83077091 -1.2645671  -0.2513612
 -2.39279218 -2.2730979   3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 17:06:38 2024]  Iteration number: 0 with current cost as 0.3355064298290499 and parameters 
[-1.56013632  2.23743464 -2.12427957 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432452
  1.31029899 -1.87354673  0.72965074  2.88578419 -0.54534335 -0.47522472
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337161  2.54856958 -0.67550781 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 17:14:20 2024]  Iteration number: 0 with current cost as 0.5977001784458996 and parameters 
[-0.10470975  2.23743432 -2.12427964 -0.11653118  0.55388692 -2.77010913
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578388 -0.54534351 -0.47522485
 -2.02654272  0.72897354  1.60512664  2.83077107 -1.2645671  -0.2513612
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2809.5315504074097 seconds. 
Discarding model... 

Training complete taking 70982.28043055534 total seconds. 
Now scoring model... 
Scoring complete taking 3.2344954013824463 seconds. 
Saved predicted values as M-A2-CNOT_Full-CRZ_predicted_values.csv
Model scores: {'MSE_train': (0.6029836177966987,), 'R2_train': -0.2006823361230725, 'MAE_train': 0.681277914825985, 'MSE_test': 0.6329974013228202, 'R2_test': -0.18051796714702828, 'MAE_test': 0.7433181771052026}. 
Saved model results as M-A2-CNOT_Full-CRZ_results.json. 
