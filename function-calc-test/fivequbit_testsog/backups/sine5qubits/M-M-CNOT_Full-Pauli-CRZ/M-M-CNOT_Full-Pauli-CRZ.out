/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /scratch/j/jacobsen/gjones/sine5qubits/sine_train.bin... 
Successfully loaded /scratch/j/jacobsen/gjones/sine5qubits/sine_train.bin into X and y data. 
Loading dataset from /scratch/j/jacobsen/gjones/sine5qubits/sine_test.bin... 
Successfully loaded /scratch/j/jacobsen/gjones/sine5qubits/sine_test.bin into X and y data. 
Training model with dataset /scratch/j/jacobsen/gjones/sine5qubits/sine_train.bin 
 at time Thu Apr  4 21:37:45 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 21:38:06 2024]  Iteration number: 0 with current cost as 0.3240662658895387 and parameters 
[-3.3141811   2.23624957 -2.09218479 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.56278673  1.14432446
  1.79362613 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Thu Apr  4 21:48:38 2024]  Iteration number: 50 with current cost as 0.20324097213927872 and parameters 
[-3.72742901  1.70522636  0.71941543 -0.11653036  0.5538889  -2.77010862
  3.06858516  2.18960275  1.18551851 -1.06648284  2.14092072  1.14432654
  1.58314769 -1.87354535  0.72965309  2.88578483 -0.54534289 -0.47522254
 -2.02654286  0.72897376  1.60512697  2.83077093 -1.26456488 -0.25135922]. 
Working on 0.4 fold... 
[Thu Apr  4 21:50:31 2024]  Iteration number: 0 with current cost as 0.3224777053385867 and parameters 
[-3.27519046  2.25709375 -2.09148432 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.56933536  1.14432445
  1.74788225 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Thu Apr  4 22:00:42 2024]  Iteration number: 50 with current cost as 0.20486308703930756 and parameters 
[-3.72397172  1.64578053  0.69862092 -0.11653194  0.55388787 -2.7701095
  3.06858426  2.18960166  1.18551946 -1.06648446  2.2095001   1.14432376
  1.59413582 -1.87354633  0.72965025  2.88578522 -0.54534353 -0.47522442
 -2.02654308  0.72897283  1.6051259   2.83077052 -1.26456708 -0.25136165]. 
Working on 0.6 fold... 
[Thu Apr  4 22:01:55 2024]  Iteration number: 0 with current cost as 0.32513488215811104 and parameters 
[-3.27523253  2.26676962 -2.0891852  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57166946  1.14432445
  1.74642117 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 22:08:40 2024]  Iteration number: 0 with current cost as 0.30581830611818633 and parameters 
[-3.27475338  2.22493854 -2.10197661 -0.11653103  0.55388707 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.56515227  1.14432445
  1.75486206 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 22:18:10 2024]  Iteration number: 0 with current cost as 0.3187300152283721 and parameters 
[-3.30557073  2.24363227 -2.08772003 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.56181803  1.14432446
  1.77986359 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2996.1599414348602 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 22:28:02 2024]  Iteration number: 0 with current cost as 0.3240662658895387 and parameters 
[-3.3141811   2.23624957 -2.09218479 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.56278673  1.14432446
  1.79362613 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Thu Apr  4 22:38:11 2024]  Iteration number: 50 with current cost as 0.20324097213927872 and parameters 
[-3.72742901  1.70522636  0.71941543 -0.11653036  0.5538889  -2.77010862
  3.06858516  2.18960275  1.18551851 -1.06648284  2.14092072  1.14432654
  1.58314769 -1.87354535  0.72965309  2.88578483 -0.54534289 -0.47522254
 -2.02654286  0.72897376  1.60512697  2.83077093 -1.26456488 -0.25135922]. 
Working on 0.4 fold... 
[Thu Apr  4 22:40:05 2024]  Iteration number: 0 with current cost as 0.3224777053385867 and parameters 
[-3.27519046  2.25709375 -2.09148432 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.56933536  1.14432445
  1.74788225 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Thu Apr  4 22:50:13 2024]  Iteration number: 50 with current cost as 0.20486308703930756 and parameters 
[-3.72397172  1.64578053  0.69862092 -0.11653194  0.55388787 -2.7701095
  3.06858426  2.18960166  1.18551946 -1.06648446  2.2095001   1.14432376
  1.59413582 -1.87354633  0.72965025  2.88578522 -0.54534353 -0.47522442
 -2.02654308  0.72897283  1.6051259   2.83077052 -1.26456708 -0.25136165]. 
Working on 0.6 fold... 
[Thu Apr  4 22:51:28 2024]  Iteration number: 0 with current cost as 0.32513488215811104 and parameters 
[-3.27523253  2.26676962 -2.0891852  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57166946  1.14432445
  1.74642117 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 22:58:11 2024]  Iteration number: 0 with current cost as 0.30581830611818633 and parameters 
[-3.27475338  2.22493854 -2.10197661 -0.11653103  0.55388707 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.56515227  1.14432445
  1.75486206 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 23:07:28 2024]  Iteration number: 0 with current cost as 0.3187300152283721 and parameters 
[-3.30557073  2.24363227 -2.08772003 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.56181803  1.14432446
  1.77986359 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2959.2376878261566 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 23:17:20 2024]  Iteration number: 0 with current cost as 0.3240662658895387 and parameters 
[-3.3141811   2.23624957 -2.09218479 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.56278673  1.14432446
  1.79362613 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Thu Apr  4 23:27:37 2024]  Iteration number: 50 with current cost as 0.20324097213927872 and parameters 
[-3.72742901  1.70522636  0.71941543 -0.11653036  0.5538889  -2.77010862
  3.06858516  2.18960275  1.18551851 -1.06648284  2.14092072  1.14432654
  1.58314769 -1.87354535  0.72965309  2.88578483 -0.54534289 -0.47522254
 -2.02654286  0.72897376  1.60512697  2.83077093 -1.26456488 -0.25135922]. 
Working on 0.4 fold... 
[Thu Apr  4 23:29:31 2024]  Iteration number: 0 with current cost as 0.3224777053385867 and parameters 
[-3.27519046  2.25709375 -2.09148432 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.56933536  1.14432445
  1.74788225 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Thu Apr  4 23:39:46 2024]  Iteration number: 50 with current cost as 0.20486308703930756 and parameters 
[-3.72397172  1.64578053  0.69862092 -0.11653194  0.55388787 -2.7701095
  3.06858426  2.18960166  1.18551946 -1.06648446  2.2095001   1.14432376
  1.59413582 -1.87354633  0.72965025  2.88578522 -0.54534353 -0.47522442
 -2.02654308  0.72897283  1.6051259   2.83077052 -1.26456708 -0.25136165]. 
Working on 0.6 fold... 
[Thu Apr  4 23:40:59 2024]  Iteration number: 0 with current cost as 0.32513488215811104 and parameters 
[-3.27523253  2.26676962 -2.0891852  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57166946  1.14432445
  1.74642117 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 23:47:46 2024]  Iteration number: 0 with current cost as 0.30581830611818633 and parameters 
[-3.27475338  2.22493854 -2.10197661 -0.11653103  0.55388707 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.56515227  1.14432445
  1.75486206 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 23:57:12 2024]  Iteration number: 0 with current cost as 0.3187300152283721 and parameters 
[-3.30557073  2.24363227 -2.08772003 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.56181803  1.14432446
  1.77986359 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2981.2337460517883 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 00:07:11 2024]  Iteration number: 0 with current cost as 0.3240662658895387 and parameters 
[-3.3141811   2.23624957 -2.09218479 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.56278673  1.14432446
  1.79362613 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Fri Apr  5 00:17:25 2024]  Iteration number: 50 with current cost as 0.20324097213927872 and parameters 
[-3.72742901  1.70522636  0.71941543 -0.11653036  0.5538889  -2.77010862
  3.06858516  2.18960275  1.18551851 -1.06648284  2.14092072  1.14432654
  1.58314769 -1.87354535  0.72965309  2.88578483 -0.54534289 -0.47522254
 -2.02654286  0.72897376  1.60512697  2.83077093 -1.26456488 -0.25135922]. 
Working on 0.4 fold... 
[Fri Apr  5 00:19:18 2024]  Iteration number: 0 with current cost as 0.3224777053385867 and parameters 
[-3.27519046  2.25709375 -2.09148432 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.56933536  1.14432445
  1.74788225 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Fri Apr  5 00:29:37 2024]  Iteration number: 50 with current cost as 0.20486308703930756 and parameters 
[-3.72397172  1.64578053  0.69862092 -0.11653194  0.55388787 -2.7701095
  3.06858426  2.18960166  1.18551946 -1.06648446  2.2095001   1.14432376
  1.59413582 -1.87354633  0.72965025  2.88578522 -0.54534353 -0.47522442
 -2.02654308  0.72897283  1.6051259   2.83077052 -1.26456708 -0.25136165]. 
Working on 0.6 fold... 
[Fri Apr  5 00:31:01 2024]  Iteration number: 0 with current cost as 0.32513488215811104 and parameters 
[-3.27523253  2.26676962 -2.0891852  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57166946  1.14432445
  1.74642117 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Fri Apr  5 00:37:49 2024]  Iteration number: 0 with current cost as 0.30581830611818633 and parameters 
[-3.27475338  2.22493854 -2.10197661 -0.11653103  0.55388707 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.56515227  1.14432445
  1.75486206 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 00:47:05 2024]  Iteration number: 0 with current cost as 0.3187300152283721 and parameters 
[-3.30557073  2.24363227 -2.08772003 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.56181803  1.14432446
  1.77986359 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 3025.188003063202 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 00:57:41 2024]  Iteration number: 0 with current cost as 0.3240662658895387 and parameters 
[-3.3141811   2.23624957 -2.09218479 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.56278673  1.14432446
  1.79362613 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Fri Apr  5 01:07:37 2024]  Iteration number: 50 with current cost as 0.20324097213927872 and parameters 
[-3.72742901  1.70522636  0.71941543 -0.11653036  0.5538889  -2.77010862
  3.06858516  2.18960275  1.18551851 -1.06648284  2.14092072  1.14432654
  1.58314769 -1.87354535  0.72965309  2.88578483 -0.54534289 -0.47522254
 -2.02654286  0.72897376  1.60512697  2.83077093 -1.26456488 -0.25135922]. 
Working on 0.4 fold... 
[Fri Apr  5 01:09:28 2024]  Iteration number: 0 with current cost as 0.3224777053385867 and parameters 
[-3.27519046  2.25709375 -2.09148432 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.56933536  1.14432445
  1.74788225 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Fri Apr  5 01:19:36 2024]  Iteration number: 50 with current cost as 0.20486308703930756 and parameters 
[-3.72397172  1.64578053  0.69862092 -0.11653194  0.55388787 -2.7701095
  3.06858426  2.18960166  1.18551946 -1.06648446  2.2095001   1.14432376
  1.59413582 -1.87354633  0.72965025  2.88578522 -0.54534353 -0.47522442
 -2.02654308  0.72897283  1.6051259   2.83077052 -1.26456708 -0.25136165]. 
Working on 0.6 fold... 
[Fri Apr  5 01:20:46 2024]  Iteration number: 0 with current cost as 0.32513488215811104 and parameters 
[-3.27523253  2.26676962 -2.0891852  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57166946  1.14432445
  1.74642117 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Fri Apr  5 01:27:29 2024]  Iteration number: 0 with current cost as 0.30581830611818633 and parameters 
[-3.27475338  2.22493854 -2.10197661 -0.11653103  0.55388707 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.56515227  1.14432445
  1.75486206 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 01:36:51 2024]  Iteration number: 0 with current cost as 0.3187300152283721 and parameters 
[-3.30557073  2.24363227 -2.08772003 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.56181803  1.14432446
  1.77986359 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2966.7533571720123 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 01:46:54 2024]  Iteration number: 0 with current cost as 0.3240662658895387 and parameters 
[-3.3141811   2.23624957 -2.09218479 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.56278673  1.14432446
  1.79362613 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Fri Apr  5 01:57:00 2024]  Iteration number: 50 with current cost as 0.20324097213927872 and parameters 
[-3.72742901  1.70522636  0.71941543 -0.11653036  0.5538889  -2.77010862
  3.06858516  2.18960275  1.18551851 -1.06648284  2.14092072  1.14432654
  1.58314769 -1.87354535  0.72965309  2.88578483 -0.54534289 -0.47522254
 -2.02654286  0.72897376  1.60512697  2.83077093 -1.26456488 -0.25135922]. 
Working on 0.4 fold... 
[Fri Apr  5 01:58:53 2024]  Iteration number: 0 with current cost as 0.3224777053385867 and parameters 
[-3.27519046  2.25709375 -2.09148432 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.56933536  1.14432445
  1.74788225 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Fri Apr  5 02:09:01 2024]  Iteration number: 50 with current cost as 0.20486308703930756 and parameters 
[-3.72397172  1.64578053  0.69862092 -0.11653194  0.55388787 -2.7701095
  3.06858426  2.18960166  1.18551946 -1.06648446  2.2095001   1.14432376
  1.59413582 -1.87354633  0.72965025  2.88578522 -0.54534353 -0.47522442
 -2.02654308  0.72897283  1.6051259   2.83077052 -1.26456708 -0.25136165]. 
Working on 0.6 fold... 
[Fri Apr  5 02:10:18 2024]  Iteration number: 0 with current cost as 0.32513488215811104 and parameters 
[-3.27523253  2.26676962 -2.0891852  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57166946  1.14432445
  1.74642117 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Fri Apr  5 02:16:59 2024]  Iteration number: 0 with current cost as 0.30581830611818633 and parameters 
[-3.27475338  2.22493854 -2.10197661 -0.11653103  0.55388707 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.56515227  1.14432445
  1.75486206 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 02:26:12 2024]  Iteration number: 0 with current cost as 0.3187300152283721 and parameters 
[-3.30557073  2.24363227 -2.08772003 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.56181803  1.14432446
  1.77986359 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2948.2291526794434 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 02:36:02 2024]  Iteration number: 0 with current cost as 0.3240662658895387 and parameters 
[-3.3141811   2.23624957 -2.09218479 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.56278673  1.14432446
  1.79362613 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Fri Apr  5 02:46:03 2024]  Iteration number: 50 with current cost as 0.20324097213927872 and parameters 
[-3.72742901  1.70522636  0.71941543 -0.11653036  0.5538889  -2.77010862
  3.06858516  2.18960275  1.18551851 -1.06648284  2.14092072  1.14432654
  1.58314769 -1.87354535  0.72965309  2.88578483 -0.54534289 -0.47522254
 -2.02654286  0.72897376  1.60512697  2.83077093 -1.26456488 -0.25135922]. 
Working on 0.4 fold... 
[Fri Apr  5 02:48:00 2024]  Iteration number: 0 with current cost as 0.3224777053385867 and parameters 
[-3.27519046  2.25709375 -2.09148432 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.56933536  1.14432445
  1.74788225 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Fri Apr  5 02:58:08 2024]  Iteration number: 50 with current cost as 0.20486308703930756 and parameters 
[-3.72397172  1.64578053  0.69862092 -0.11653194  0.55388787 -2.7701095
  3.06858426  2.18960166  1.18551946 -1.06648446  2.2095001   1.14432376
  1.59413582 -1.87354633  0.72965025  2.88578522 -0.54534353 -0.47522442
 -2.02654308  0.72897283  1.6051259   2.83077052 -1.26456708 -0.25136165]. 
Working on 0.6 fold... 
[Fri Apr  5 02:59:23 2024]  Iteration number: 0 with current cost as 0.32513488215811104 and parameters 
[-3.27523253  2.26676962 -2.0891852  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57166946  1.14432445
  1.74642117 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Fri Apr  5 03:06:12 2024]  Iteration number: 0 with current cost as 0.30581830611818633 and parameters 
[-3.27475338  2.22493854 -2.10197661 -0.11653103  0.55388707 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.56515227  1.14432445
  1.75486206 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 03:15:42 2024]  Iteration number: 0 with current cost as 0.3187300152283721 and parameters 
[-3.30557073  2.24363227 -2.08772003 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.56181803  1.14432446
  1.77986359 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2971.76385641098 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 03:25:33 2024]  Iteration number: 0 with current cost as 0.3240662658895387 and parameters 
[-3.3141811   2.23624957 -2.09218479 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.56278673  1.14432446
  1.79362613 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Fri Apr  5 03:35:32 2024]  Iteration number: 50 with current cost as 0.20324097213927872 and parameters 
[-3.72742901  1.70522636  0.71941543 -0.11653036  0.5538889  -2.77010862
  3.06858516  2.18960275  1.18551851 -1.06648284  2.14092072  1.14432654
  1.58314769 -1.87354535  0.72965309  2.88578483 -0.54534289 -0.47522254
 -2.02654286  0.72897376  1.60512697  2.83077093 -1.26456488 -0.25135922]. 
Working on 0.4 fold... 
[Fri Apr  5 03:37:31 2024]  Iteration number: 0 with current cost as 0.3224777053385867 and parameters 
[-3.27519046  2.25709375 -2.09148432 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.56933536  1.14432445
  1.74788225 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Fri Apr  5 03:47:46 2024]  Iteration number: 50 with current cost as 0.20486308703930756 and parameters 
[-3.72397172  1.64578053  0.69862092 -0.11653194  0.55388787 -2.7701095
  3.06858426  2.18960166  1.18551946 -1.06648446  2.2095001   1.14432376
  1.59413582 -1.87354633  0.72965025  2.88578522 -0.54534353 -0.47522442
 -2.02654308  0.72897283  1.6051259   2.83077052 -1.26456708 -0.25136165]. 
Working on 0.6 fold... 
[Fri Apr  5 03:48:57 2024]  Iteration number: 0 with current cost as 0.32513488215811104 and parameters 
[-3.27523253  2.26676962 -2.0891852  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57166946  1.14432445
  1.74642117 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Fri Apr  5 03:55:39 2024]  Iteration number: 0 with current cost as 0.30581830611818633 and parameters 
[-3.27475338  2.22493854 -2.10197661 -0.11653103  0.55388707 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.56515227  1.14432445
  1.75486206 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 04:04:55 2024]  Iteration number: 0 with current cost as 0.3187300152283721 and parameters 
[-3.30557073  2.24363227 -2.08772003 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.56181803  1.14432446
  1.77986359 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2946.17986702919 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 04:14:39 2024]  Iteration number: 0 with current cost as 0.3240662658895387 and parameters 
[-3.3141811   2.23624957 -2.09218479 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.56278673  1.14432446
  1.79362613 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Fri Apr  5 04:24:41 2024]  Iteration number: 50 with current cost as 0.20324097213927872 and parameters 
[-3.72742901  1.70522636  0.71941543 -0.11653036  0.5538889  -2.77010862
  3.06858516  2.18960275  1.18551851 -1.06648284  2.14092072  1.14432654
  1.58314769 -1.87354535  0.72965309  2.88578483 -0.54534289 -0.47522254
 -2.02654286  0.72897376  1.60512697  2.83077093 -1.26456488 -0.25135922]. 
Working on 0.4 fold... 
[Fri Apr  5 04:26:34 2024]  Iteration number: 0 with current cost as 0.3224777053385867 and parameters 
[-3.27519046  2.25709375 -2.09148432 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.56933536  1.14432445
  1.74788225 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Fri Apr  5 04:36:38 2024]  Iteration number: 50 with current cost as 0.20486308703930756 and parameters 
[-3.72397172  1.64578053  0.69862092 -0.11653194  0.55388787 -2.7701095
  3.06858426  2.18960166  1.18551946 -1.06648446  2.2095001   1.14432376
  1.59413582 -1.87354633  0.72965025  2.88578522 -0.54534353 -0.47522442
 -2.02654308  0.72897283  1.6051259   2.83077052 -1.26456708 -0.25136165]. 
Working on 0.6 fold... 
[Fri Apr  5 04:37:51 2024]  Iteration number: 0 with current cost as 0.32513488215811104 and parameters 
[-3.27523253  2.26676962 -2.0891852  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57166946  1.14432445
  1.74642117 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Fri Apr  5 04:44:32 2024]  Iteration number: 0 with current cost as 0.30581830611818633 and parameters 
[-3.27475338  2.22493854 -2.10197661 -0.11653103  0.55388707 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.56515227  1.14432445
  1.75486206 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 04:53:48 2024]  Iteration number: 0 with current cost as 0.3187300152283721 and parameters 
[-3.30557073  2.24363227 -2.08772003 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.56181803  1.14432446
  1.77986359 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2938.9112107753754 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 05:03:39 2024]  Iteration number: 0 with current cost as 0.3240662658895387 and parameters 
[-3.3141811   2.23624957 -2.09218479 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.56278673  1.14432446
  1.79362613 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Fri Apr  5 05:13:45 2024]  Iteration number: 50 with current cost as 0.20324097213927872 and parameters 
[-3.72742901  1.70522636  0.71941543 -0.11653036  0.5538889  -2.77010862
  3.06858516  2.18960275  1.18551851 -1.06648284  2.14092072  1.14432654
  1.58314769 -1.87354535  0.72965309  2.88578483 -0.54534289 -0.47522254
 -2.02654286  0.72897376  1.60512697  2.83077093 -1.26456488 -0.25135922]. 
Working on 0.4 fold... 
[Fri Apr  5 05:15:38 2024]  Iteration number: 0 with current cost as 0.3224777053385867 and parameters 
[-3.27519046  2.25709375 -2.09148432 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.56933536  1.14432445
  1.74788225 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Fri Apr  5 05:25:45 2024]  Iteration number: 50 with current cost as 0.20486308703930756 and parameters 
[-3.72397172  1.64578053  0.69862092 -0.11653194  0.55388787 -2.7701095
  3.06858426  2.18960166  1.18551946 -1.06648446  2.2095001   1.14432376
  1.59413582 -1.87354633  0.72965025  2.88578522 -0.54534353 -0.47522442
 -2.02654308  0.72897283  1.6051259   2.83077052 -1.26456708 -0.25136165]. 
Working on 0.6 fold... 
[Fri Apr  5 05:26:57 2024]  Iteration number: 0 with current cost as 0.32513488215811104 and parameters 
[-3.27523253  2.26676962 -2.0891852  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57166946  1.14432445
  1.74642117 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Fri Apr  5 05:33:45 2024]  Iteration number: 0 with current cost as 0.30581830611818633 and parameters 
[-3.27475338  2.22493854 -2.10197661 -0.11653103  0.55388707 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.56515227  1.14432445
  1.75486206 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 05:43:01 2024]  Iteration number: 0 with current cost as 0.3187300152283721 and parameters 
[-3.30557073  2.24363227 -2.08772003 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.56181803  1.14432446
  1.77986359 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2947.138658761978 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 05:52:46 2024]  Iteration number: 0 with current cost as 0.3240662658895387 and parameters 
[-3.3141811   2.23624957 -2.09218479 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.56278673  1.14432446
  1.79362613 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Fri Apr  5 06:02:41 2024]  Iteration number: 50 with current cost as 0.20324097213927872 and parameters 
[-3.72742901  1.70522636  0.71941543 -0.11653036  0.5538889  -2.77010862
  3.06858516  2.18960275  1.18551851 -1.06648284  2.14092072  1.14432654
  1.58314769 -1.87354535  0.72965309  2.88578483 -0.54534289 -0.47522254
 -2.02654286  0.72897376  1.60512697  2.83077093 -1.26456488 -0.25135922]. 
Working on 0.4 fold... 
[Fri Apr  5 06:04:35 2024]  Iteration number: 0 with current cost as 0.3224777053385867 and parameters 
[-3.27519046  2.25709375 -2.09148432 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.56933536  1.14432445
  1.74788225 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Fri Apr  5 06:14:39 2024]  Iteration number: 50 with current cost as 0.20486308703930756 and parameters 
[-3.72397172  1.64578053  0.69862092 -0.11653194  0.55388787 -2.7701095
  3.06858426  2.18960166  1.18551946 -1.06648446  2.2095001   1.14432376
  1.59413582 -1.87354633  0.72965025  2.88578522 -0.54534353 -0.47522442
 -2.02654308  0.72897283  1.6051259   2.83077052 -1.26456708 -0.25136165]. 
Working on 0.6 fold... 
[Fri Apr  5 06:15:50 2024]  Iteration number: 0 with current cost as 0.32513488215811104 and parameters 
[-3.27523253  2.26676962 -2.0891852  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57166946  1.14432445
  1.74642117 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Fri Apr  5 06:22:29 2024]  Iteration number: 0 with current cost as 0.30581830611818633 and parameters 
[-3.27475338  2.22493854 -2.10197661 -0.11653103  0.55388707 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.56515227  1.14432445
  1.75486206 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 06:31:42 2024]  Iteration number: 0 with current cost as 0.3187300152283721 and parameters 
[-3.30557073  2.24363227 -2.08772003 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.56181803  1.14432446
  1.77986359 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2922.745575904846 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 06:41:30 2024]  Iteration number: 0 with current cost as 0.3240662658895387 and parameters 
[-3.3141811   2.23624957 -2.09218479 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.56278673  1.14432446
  1.79362613 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Fri Apr  5 06:51:27 2024]  Iteration number: 50 with current cost as 0.20324097213927872 and parameters 
[-3.72742901  1.70522636  0.71941543 -0.11653036  0.5538889  -2.77010862
  3.06858516  2.18960275  1.18551851 -1.06648284  2.14092072  1.14432654
  1.58314769 -1.87354535  0.72965309  2.88578483 -0.54534289 -0.47522254
 -2.02654286  0.72897376  1.60512697  2.83077093 -1.26456488 -0.25135922]. 
Working on 0.4 fold... 
[Fri Apr  5 06:53:21 2024]  Iteration number: 0 with current cost as 0.3224777053385867 and parameters 
[-3.27519046  2.25709375 -2.09148432 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.56933536  1.14432445
  1.74788225 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Fri Apr  5 07:04:20 2024]  Iteration number: 50 with current cost as 0.20486308703930756 and parameters 
[-3.72397172  1.64578053  0.69862092 -0.11653194  0.55388787 -2.7701095
  3.06858426  2.18960166  1.18551946 -1.06648446  2.2095001   1.14432376
  1.59413582 -1.87354633  0.72965025  2.88578522 -0.54534353 -0.47522442
 -2.02654308  0.72897283  1.6051259   2.83077052 -1.26456708 -0.25136165]. 
Working on 0.6 fold... 
[Fri Apr  5 07:05:33 2024]  Iteration number: 0 with current cost as 0.32513488215811104 and parameters 
[-3.27523253  2.26676962 -2.0891852  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57166946  1.14432445
  1.74642117 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Fri Apr  5 07:12:18 2024]  Iteration number: 0 with current cost as 0.30581830611818633 and parameters 
[-3.27475338  2.22493854 -2.10197661 -0.11653103  0.55388707 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.56515227  1.14432445
  1.75486206 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 07:21:31 2024]  Iteration number: 0 with current cost as 0.3187300152283721 and parameters 
[-3.30557073  2.24363227 -2.08772003 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.56181803  1.14432446
  1.77986359 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2985.7199857234955 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 07:31:16 2024]  Iteration number: 0 with current cost as 0.3240662658895387 and parameters 
[-3.3141811   2.23624957 -2.09218479 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.56278673  1.14432446
  1.79362613 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Fri Apr  5 07:41:41 2024]  Iteration number: 50 with current cost as 0.20324097213927872 and parameters 
[-3.72742901  1.70522636  0.71941543 -0.11653036  0.5538889  -2.77010862
  3.06858516  2.18960275  1.18551851 -1.06648284  2.14092072  1.14432654
  1.58314769 -1.87354535  0.72965309  2.88578483 -0.54534289 -0.47522254
 -2.02654286  0.72897376  1.60512697  2.83077093 -1.26456488 -0.25135922]. 
Working on 0.4 fold... 
[Fri Apr  5 07:43:35 2024]  Iteration number: 0 with current cost as 0.3224777053385867 and parameters 
[-3.27519046  2.25709375 -2.09148432 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.56933536  1.14432445
  1.74788225 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Fri Apr  5 07:53:44 2024]  Iteration number: 50 with current cost as 0.20486308703930756 and parameters 
[-3.72397172  1.64578053  0.69862092 -0.11653194  0.55388787 -2.7701095
  3.06858426  2.18960166  1.18551946 -1.06648446  2.2095001   1.14432376
  1.59413582 -1.87354633  0.72965025  2.88578522 -0.54534353 -0.47522442
 -2.02654308  0.72897283  1.6051259   2.83077052 -1.26456708 -0.25136165]. 
Working on 0.6 fold... 
[Fri Apr  5 07:54:55 2024]  Iteration number: 0 with current cost as 0.32513488215811104 and parameters 
[-3.27523253  2.26676962 -2.0891852  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57166946  1.14432445
  1.74642117 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Fri Apr  5 08:01:37 2024]  Iteration number: 0 with current cost as 0.30581830611818633 and parameters 
[-3.27475338  2.22493854 -2.10197661 -0.11653103  0.55388707 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.56515227  1.14432445
  1.75486206 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 08:10:52 2024]  Iteration number: 0 with current cost as 0.3187300152283721 and parameters 
[-3.30557073  2.24363227 -2.08772003 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.56181803  1.14432446
  1.77986359 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2970.2425258159637 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 08:20:46 2024]  Iteration number: 0 with current cost as 0.3240662658895387 and parameters 
[-3.3141811   2.23624957 -2.09218479 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.56278673  1.14432446
  1.79362613 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Fri Apr  5 08:31:08 2024]  Iteration number: 50 with current cost as 0.20324097213927872 and parameters 
[-3.72742901  1.70522636  0.71941543 -0.11653036  0.5538889  -2.77010862
  3.06858516  2.18960275  1.18551851 -1.06648284  2.14092072  1.14432654
  1.58314769 -1.87354535  0.72965309  2.88578483 -0.54534289 -0.47522254
 -2.02654286  0.72897376  1.60512697  2.83077093 -1.26456488 -0.25135922]. 
Working on 0.4 fold... 
[Fri Apr  5 08:33:02 2024]  Iteration number: 0 with current cost as 0.3224777053385867 and parameters 
[-3.27519046  2.25709375 -2.09148432 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.56933536  1.14432445
  1.74788225 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Fri Apr  5 08:43:14 2024]  Iteration number: 50 with current cost as 0.20486308703930756 and parameters 
[-3.72397172  1.64578053  0.69862092 -0.11653194  0.55388787 -2.7701095
  3.06858426  2.18960166  1.18551946 -1.06648446  2.2095001   1.14432376
  1.59413582 -1.87354633  0.72965025  2.88578522 -0.54534353 -0.47522442
 -2.02654308  0.72897283  1.6051259   2.83077052 -1.26456708 -0.25136165]. 
Working on 0.6 fold... 
[Fri Apr  5 08:44:27 2024]  Iteration number: 0 with current cost as 0.32513488215811104 and parameters 
[-3.27523253  2.26676962 -2.0891852  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57166946  1.14432445
  1.74642117 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Fri Apr  5 08:51:15 2024]  Iteration number: 0 with current cost as 0.30581830611818633 and parameters 
[-3.27475338  2.22493854 -2.10197661 -0.11653103  0.55388707 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.56515227  1.14432445
  1.75486206 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 09:00:44 2024]  Iteration number: 0 with current cost as 0.3187300152283721 and parameters 
[-3.30557073  2.24363227 -2.08772003 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.56181803  1.14432446
  1.77986359 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 3003.626483440399 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 09:10:49 2024]  Iteration number: 0 with current cost as 0.3240662658895387 and parameters 
[-3.3141811   2.23624957 -2.09218479 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.56278673  1.14432446
  1.79362613 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Fri Apr  5 09:20:43 2024]  Iteration number: 50 with current cost as 0.20324097213927872 and parameters 
[-3.72742901  1.70522636  0.71941543 -0.11653036  0.5538889  -2.77010862
  3.06858516  2.18960275  1.18551851 -1.06648284  2.14092072  1.14432654
  1.58314769 -1.87354535  0.72965309  2.88578483 -0.54534289 -0.47522254
 -2.02654286  0.72897376  1.60512697  2.83077093 -1.26456488 -0.25135922]. 
Working on 0.4 fold... 
[Fri Apr  5 09:22:37 2024]  Iteration number: 0 with current cost as 0.3224777053385867 and parameters 
[-3.27519046  2.25709375 -2.09148432 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.56933536  1.14432445
  1.74788225 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Fri Apr  5 09:32:43 2024]  Iteration number: 50 with current cost as 0.20486308703930756 and parameters 
[-3.72397172  1.64578053  0.69862092 -0.11653194  0.55388787 -2.7701095
  3.06858426  2.18960166  1.18551946 -1.06648446  2.2095001   1.14432376
  1.59413582 -1.87354633  0.72965025  2.88578522 -0.54534353 -0.47522442
 -2.02654308  0.72897283  1.6051259   2.83077052 -1.26456708 -0.25136165]. 
Working on 0.6 fold... 
[Fri Apr  5 09:33:55 2024]  Iteration number: 0 with current cost as 0.32513488215811104 and parameters 
[-3.27523253  2.26676962 -2.0891852  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57166946  1.14432445
  1.74642117 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Fri Apr  5 09:41:14 2024]  Iteration number: 0 with current cost as 0.30581830611818633 and parameters 
[-3.27475338  2.22493854 -2.10197661 -0.11653103  0.55388707 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.56515227  1.14432445
  1.75486206 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 09:50:29 2024]  Iteration number: 0 with current cost as 0.3187300152283721 and parameters 
[-3.30557073  2.24363227 -2.08772003 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.56181803  1.14432446
  1.77986359 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2970.682048559189 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 10:00:20 2024]  Iteration number: 0 with current cost as 0.3240662658895387 and parameters 
[-3.3141811   2.23624957 -2.09218479 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.56278673  1.14432446
  1.79362613 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Fri Apr  5 10:10:50 2024]  Iteration number: 50 with current cost as 0.20324097213927872 and parameters 
[-3.72742901  1.70522636  0.71941543 -0.11653036  0.5538889  -2.77010862
  3.06858516  2.18960275  1.18551851 -1.06648284  2.14092072  1.14432654
  1.58314769 -1.87354535  0.72965309  2.88578483 -0.54534289 -0.47522254
 -2.02654286  0.72897376  1.60512697  2.83077093 -1.26456488 -0.25135922]. 
Working on 0.4 fold... 
[Fri Apr  5 10:12:44 2024]  Iteration number: 0 with current cost as 0.3224777053385867 and parameters 
[-3.27519046  2.25709375 -2.09148432 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.56933536  1.14432445
  1.74788225 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Fri Apr  5 10:22:49 2024]  Iteration number: 50 with current cost as 0.20486308703930756 and parameters 
[-3.72397172  1.64578053  0.69862092 -0.11653194  0.55388787 -2.7701095
  3.06858426  2.18960166  1.18551946 -1.06648446  2.2095001   1.14432376
  1.59413582 -1.87354633  0.72965025  2.88578522 -0.54534353 -0.47522442
 -2.02654308  0.72897283  1.6051259   2.83077052 -1.26456708 -0.25136165]. 
Working on 0.6 fold... 
[Fri Apr  5 10:24:00 2024]  Iteration number: 0 with current cost as 0.32513488215811104 and parameters 
[-3.27523253  2.26676962 -2.0891852  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57166946  1.14432445
  1.74642117 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Fri Apr  5 10:30:41 2024]  Iteration number: 0 with current cost as 0.30581830611818633 and parameters 
[-3.27475338  2.22493854 -2.10197661 -0.11653103  0.55388707 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.56515227  1.14432445
  1.75486206 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 10:39:58 2024]  Iteration number: 0 with current cost as 0.3187300152283721 and parameters 
[-3.30557073  2.24363227 -2.08772003 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.56181803  1.14432446
  1.77986359 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2964.445857524872 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 10:49:44 2024]  Iteration number: 0 with current cost as 0.3240662658895387 and parameters 
[-3.3141811   2.23624957 -2.09218479 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.56278673  1.14432446
  1.79362613 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Fri Apr  5 10:59:40 2024]  Iteration number: 50 with current cost as 0.20324097213927872 and parameters 
[-3.72742901  1.70522636  0.71941543 -0.11653036  0.5538889  -2.77010862
  3.06858516  2.18960275  1.18551851 -1.06648284  2.14092072  1.14432654
  1.58314769 -1.87354535  0.72965309  2.88578483 -0.54534289 -0.47522254
 -2.02654286  0.72897376  1.60512697  2.83077093 -1.26456488 -0.25135922]. 
Working on 0.4 fold... 
[Fri Apr  5 11:01:32 2024]  Iteration number: 0 with current cost as 0.3224777053385867 and parameters 
[-3.27519046  2.25709375 -2.09148432 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.56933536  1.14432445
  1.74788225 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Fri Apr  5 11:11:36 2024]  Iteration number: 50 with current cost as 0.20486308703930756 and parameters 
[-3.72397172  1.64578053  0.69862092 -0.11653194  0.55388787 -2.7701095
  3.06858426  2.18960166  1.18551946 -1.06648446  2.2095001   1.14432376
  1.59413582 -1.87354633  0.72965025  2.88578522 -0.54534353 -0.47522442
 -2.02654308  0.72897283  1.6051259   2.83077052 -1.26456708 -0.25136165]. 
Working on 0.6 fold... 
[Fri Apr  5 11:12:49 2024]  Iteration number: 0 with current cost as 0.32513488215811104 and parameters 
[-3.27523253  2.26676962 -2.0891852  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57166946  1.14432445
  1.74642117 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Fri Apr  5 11:19:25 2024]  Iteration number: 0 with current cost as 0.30581830611818633 and parameters 
[-3.27475338  2.22493854 -2.10197661 -0.11653103  0.55388707 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.56515227  1.14432445
  1.75486206 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 11:28:39 2024]  Iteration number: 0 with current cost as 0.3187300152283721 and parameters 
[-3.30557073  2.24363227 -2.08772003 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.56181803  1.14432446
  1.77986359 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2920.5627756118774 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 11:38:25 2024]  Iteration number: 0 with current cost as 0.3240662658895387 and parameters 
[-3.3141811   2.23624957 -2.09218479 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.56278673  1.14432446
  1.79362613 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Fri Apr  5 11:48:21 2024]  Iteration number: 50 with current cost as 0.20324097213927872 and parameters 
[-3.72742901  1.70522636  0.71941543 -0.11653036  0.5538889  -2.77010862
  3.06858516  2.18960275  1.18551851 -1.06648284  2.14092072  1.14432654
  1.58314769 -1.87354535  0.72965309  2.88578483 -0.54534289 -0.47522254
 -2.02654286  0.72897376  1.60512697  2.83077093 -1.26456488 -0.25135922]. 
Working on 0.4 fold... 
[Fri Apr  5 11:50:14 2024]  Iteration number: 0 with current cost as 0.3224777053385867 and parameters 
[-3.27519046  2.25709375 -2.09148432 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.56933536  1.14432445
  1.74788225 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Fri Apr  5 12:00:57 2024]  Iteration number: 50 with current cost as 0.20486308703930756 and parameters 
[-3.72397172  1.64578053  0.69862092 -0.11653194  0.55388787 -2.7701095
  3.06858426  2.18960166  1.18551946 -1.06648446  2.2095001   1.14432376
  1.59413582 -1.87354633  0.72965025  2.88578522 -0.54534353 -0.47522442
 -2.02654308  0.72897283  1.6051259   2.83077052 -1.26456708 -0.25136165]. 
Working on 0.6 fold... 
[Fri Apr  5 12:02:09 2024]  Iteration number: 0 with current cost as 0.32513488215811104 and parameters 
[-3.27523253  2.26676962 -2.0891852  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57166946  1.14432445
  1.74642117 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Fri Apr  5 12:09:21 2024]  Iteration number: 0 with current cost as 0.30581830611818633 and parameters 
[-3.27475338  2.22493854 -2.10197661 -0.11653103  0.55388707 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.56515227  1.14432445
  1.75486206 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 12:18:37 2024]  Iteration number: 0 with current cost as 0.3187300152283721 and parameters 
[-3.30557073  2.24363227 -2.08772003 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.56181803  1.14432446
  1.77986359 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 3026.7310943603516 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 12:28:58 2024]  Iteration number: 0 with current cost as 0.3240662658895387 and parameters 
[-3.3141811   2.23624957 -2.09218479 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.56278673  1.14432446
  1.79362613 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Fri Apr  5 12:39:11 2024]  Iteration number: 50 with current cost as 0.20324097213927872 and parameters 
[-3.72742901  1.70522636  0.71941543 -0.11653036  0.5538889  -2.77010862
  3.06858516  2.18960275  1.18551851 -1.06648284  2.14092072  1.14432654
  1.58314769 -1.87354535  0.72965309  2.88578483 -0.54534289 -0.47522254
 -2.02654286  0.72897376  1.60512697  2.83077093 -1.26456488 -0.25135922]. 
Working on 0.4 fold... 
[Fri Apr  5 12:41:09 2024]  Iteration number: 0 with current cost as 0.3224777053385867 and parameters 
[-3.27519046  2.25709375 -2.09148432 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.56933536  1.14432445
  1.74788225 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Fri Apr  5 12:51:16 2024]  Iteration number: 50 with current cost as 0.20486308703930756 and parameters 
[-3.72397172  1.64578053  0.69862092 -0.11653194  0.55388787 -2.7701095
  3.06858426  2.18960166  1.18551946 -1.06648446  2.2095001   1.14432376
  1.59413582 -1.87354633  0.72965025  2.88578522 -0.54534353 -0.47522442
 -2.02654308  0.72897283  1.6051259   2.83077052 -1.26456708 -0.25136165]. 
Working on 0.6 fold... 
[Fri Apr  5 12:52:28 2024]  Iteration number: 0 with current cost as 0.32513488215811104 and parameters 
[-3.27523253  2.26676962 -2.0891852  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57166946  1.14432445
  1.74642117 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Fri Apr  5 12:59:08 2024]  Iteration number: 0 with current cost as 0.30581830611818633 and parameters 
[-3.27475338  2.22493854 -2.10197661 -0.11653103  0.55388707 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.56515227  1.14432445
  1.75486206 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 13:08:21 2024]  Iteration number: 0 with current cost as 0.3187300152283721 and parameters 
[-3.30557073  2.24363227 -2.08772003 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.56181803  1.14432446
  1.77986359 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2952.3124701976776 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 13:18:04 2024]  Iteration number: 0 with current cost as 0.3240662658895387 and parameters 
[-3.3141811   2.23624957 -2.09218479 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.56278673  1.14432446
  1.79362613 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Fri Apr  5 13:28:08 2024]  Iteration number: 50 with current cost as 0.20324097213927872 and parameters 
[-3.72742901  1.70522636  0.71941543 -0.11653036  0.5538889  -2.77010862
  3.06858516  2.18960275  1.18551851 -1.06648284  2.14092072  1.14432654
  1.58314769 -1.87354535  0.72965309  2.88578483 -0.54534289 -0.47522254
 -2.02654286  0.72897376  1.60512697  2.83077093 -1.26456488 -0.25135922]. 
Working on 0.4 fold... 
[Fri Apr  5 13:30:01 2024]  Iteration number: 0 with current cost as 0.3224777053385867 and parameters 
[-3.27519046  2.25709375 -2.09148432 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.56933536  1.14432445
  1.74788225 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Fri Apr  5 13:40:08 2024]  Iteration number: 50 with current cost as 0.20486308703930756 and parameters 
[-3.72397172  1.64578053  0.69862092 -0.11653194  0.55388787 -2.7701095
  3.06858426  2.18960166  1.18551946 -1.06648446  2.2095001   1.14432376
  1.59413582 -1.87354633  0.72965025  2.88578522 -0.54534353 -0.47522442
 -2.02654308  0.72897283  1.6051259   2.83077052 -1.26456708 -0.25136165]. 
Working on 0.6 fold... 
[Fri Apr  5 13:41:21 2024]  Iteration number: 0 with current cost as 0.32513488215811104 and parameters 
[-3.27523253  2.26676962 -2.0891852  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57166946  1.14432445
  1.74642117 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Fri Apr  5 13:48:01 2024]  Iteration number: 0 with current cost as 0.30581830611818633 and parameters 
[-3.27475338  2.22493854 -2.10197661 -0.11653103  0.55388707 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.56515227  1.14432445
  1.75486206 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 13:57:26 2024]  Iteration number: 0 with current cost as 0.3187300152283721 and parameters 
[-3.30557073  2.24363227 -2.08772003 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.56181803  1.14432446
  1.77986359 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2962.9349904060364 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 14:07:27 2024]  Iteration number: 0 with current cost as 0.3240662658895387 and parameters 
[-3.3141811   2.23624957 -2.09218479 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.56278673  1.14432446
  1.79362613 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Fri Apr  5 14:17:47 2024]  Iteration number: 50 with current cost as 0.20324097213927872 and parameters 
[-3.72742901  1.70522636  0.71941543 -0.11653036  0.5538889  -2.77010862
  3.06858516  2.18960275  1.18551851 -1.06648284  2.14092072  1.14432654
  1.58314769 -1.87354535  0.72965309  2.88578483 -0.54534289 -0.47522254
 -2.02654286  0.72897376  1.60512697  2.83077093 -1.26456488 -0.25135922]. 
Working on 0.4 fold... 
[Fri Apr  5 14:19:42 2024]  Iteration number: 0 with current cost as 0.3224777053385867 and parameters 
[-3.27519046  2.25709375 -2.09148432 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.56933536  1.14432445
  1.74788225 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Fri Apr  5 14:30:51 2024]  Iteration number: 50 with current cost as 0.20486308703930756 and parameters 
[-3.72397172  1.64578053  0.69862092 -0.11653194  0.55388787 -2.7701095
  3.06858426  2.18960166  1.18551946 -1.06648446  2.2095001   1.14432376
  1.59413582 -1.87354633  0.72965025  2.88578522 -0.54534353 -0.47522442
 -2.02654308  0.72897283  1.6051259   2.83077052 -1.26456708 -0.25136165]. 
Working on 0.6 fold... 
[Fri Apr  5 14:32:04 2024]  Iteration number: 0 with current cost as 0.32513488215811104 and parameters 
[-3.27523253  2.26676962 -2.0891852  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57166946  1.14432445
  1.74642117 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Fri Apr  5 14:38:48 2024]  Iteration number: 0 with current cost as 0.30581830611818633 and parameters 
[-3.27475338  2.22493854 -2.10197661 -0.11653103  0.55388707 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.56515227  1.14432445
  1.75486206 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 14:48:08 2024]  Iteration number: 0 with current cost as 0.3187300152283721 and parameters 
[-3.30557073  2.24363227 -2.08772003 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.56181803  1.14432446
  1.77986359 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 3040.233843564987 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 14:58:07 2024]  Iteration number: 0 with current cost as 0.3240662658895387 and parameters 
[-3.3141811   2.23624957 -2.09218479 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.56278673  1.14432446
  1.79362613 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Fri Apr  5 15:08:20 2024]  Iteration number: 50 with current cost as 0.20324097213927872 and parameters 
[-3.72742901  1.70522636  0.71941543 -0.11653036  0.5538889  -2.77010862
  3.06858516  2.18960275  1.18551851 -1.06648284  2.14092072  1.14432654
  1.58314769 -1.87354535  0.72965309  2.88578483 -0.54534289 -0.47522254
 -2.02654286  0.72897376  1.60512697  2.83077093 -1.26456488 -0.25135922]. 
Working on 0.4 fold... 
[Fri Apr  5 15:10:13 2024]  Iteration number: 0 with current cost as 0.3224777053385867 and parameters 
[-3.27519046  2.25709375 -2.09148432 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.56933536  1.14432445
  1.74788225 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Fri Apr  5 15:20:18 2024]  Iteration number: 50 with current cost as 0.20486308703930756 and parameters 
[-3.72397172  1.64578053  0.69862092 -0.11653194  0.55388787 -2.7701095
  3.06858426  2.18960166  1.18551946 -1.06648446  2.2095001   1.14432376
  1.59413582 -1.87354633  0.72965025  2.88578522 -0.54534353 -0.47522442
 -2.02654308  0.72897283  1.6051259   2.83077052 -1.26456708 -0.25136165]. 
Working on 0.6 fold... 
[Fri Apr  5 15:21:31 2024]  Iteration number: 0 with current cost as 0.32513488215811104 and parameters 
[-3.27523253  2.26676962 -2.0891852  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57166946  1.14432445
  1.74642117 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Fri Apr  5 15:28:12 2024]  Iteration number: 0 with current cost as 0.30581830611818633 and parameters 
[-3.27475338  2.22493854 -2.10197661 -0.11653103  0.55388707 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.56515227  1.14432445
  1.75486206 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 15:37:29 2024]  Iteration number: 0 with current cost as 0.3187300152283721 and parameters 
[-3.30557073  2.24363227 -2.08772003 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.56181803  1.14432446
  1.77986359 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2945.696410894394 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 15:47:13 2024]  Iteration number: 0 with current cost as 0.3240662658895387 and parameters 
[-3.3141811   2.23624957 -2.09218479 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.56278673  1.14432446
  1.79362613 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Fri Apr  5 15:57:11 2024]  Iteration number: 50 with current cost as 0.20324097213927872 and parameters 
[-3.72742901  1.70522636  0.71941543 -0.11653036  0.5538889  -2.77010862
  3.06858516  2.18960275  1.18551851 -1.06648284  2.14092072  1.14432654
  1.58314769 -1.87354535  0.72965309  2.88578483 -0.54534289 -0.47522254
 -2.02654286  0.72897376  1.60512697  2.83077093 -1.26456488 -0.25135922]. 
Working on 0.4 fold... 
[Fri Apr  5 15:59:05 2024]  Iteration number: 0 with current cost as 0.3224777053385867 and parameters 
[-3.27519046  2.25709375 -2.09148432 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.56933536  1.14432445
  1.74788225 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Fri Apr  5 16:09:09 2024]  Iteration number: 50 with current cost as 0.20486308703930756 and parameters 
[-3.72397172  1.64578053  0.69862092 -0.11653194  0.55388787 -2.7701095
  3.06858426  2.18960166  1.18551946 -1.06648446  2.2095001   1.14432376
  1.59413582 -1.87354633  0.72965025  2.88578522 -0.54534353 -0.47522442
 -2.02654308  0.72897283  1.6051259   2.83077052 -1.26456708 -0.25136165]. 
Working on 0.6 fold... 
[Fri Apr  5 16:10:20 2024]  Iteration number: 0 with current cost as 0.32513488215811104 and parameters 
[-3.27523253  2.26676962 -2.0891852  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57166946  1.14432445
  1.74642117 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Fri Apr  5 16:16:58 2024]  Iteration number: 0 with current cost as 0.30581830611818633 and parameters 
[-3.27475338  2.22493854 -2.10197661 -0.11653103  0.55388707 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.56515227  1.14432445
  1.75486206 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 16:26:11 2024]  Iteration number: 0 with current cost as 0.3187300152283721 and parameters 
[-3.30557073  2.24363227 -2.08772003 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.56181803  1.14432446
  1.77986359 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2924.5630118846893 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 16:35:56 2024]  Iteration number: 0 with current cost as 0.3240662658895387 and parameters 
[-3.3141811   2.23624957 -2.09218479 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.56278673  1.14432446
  1.79362613 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Fri Apr  5 16:45:53 2024]  Iteration number: 50 with current cost as 0.20324097213927872 and parameters 
[-3.72742901  1.70522636  0.71941543 -0.11653036  0.5538889  -2.77010862
  3.06858516  2.18960275  1.18551851 -1.06648284  2.14092072  1.14432654
  1.58314769 -1.87354535  0.72965309  2.88578483 -0.54534289 -0.47522254
 -2.02654286  0.72897376  1.60512697  2.83077093 -1.26456488 -0.25135922]. 
Working on 0.4 fold... 
[Fri Apr  5 16:47:46 2024]  Iteration number: 0 with current cost as 0.3224777053385867 and parameters 
[-3.27519046  2.25709375 -2.09148432 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.56933536  1.14432445
  1.74788225 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Fri Apr  5 16:57:54 2024]  Iteration number: 50 with current cost as 0.20486308703930756 and parameters 
[-3.72397172  1.64578053  0.69862092 -0.11653194  0.55388787 -2.7701095
  3.06858426  2.18960166  1.18551946 -1.06648446  2.2095001   1.14432376
  1.59413582 -1.87354633  0.72965025  2.88578522 -0.54534353 -0.47522442
 -2.02654308  0.72897283  1.6051259   2.83077052 -1.26456708 -0.25136165]. 
Working on 0.6 fold... 
[Fri Apr  5 16:59:07 2024]  Iteration number: 0 with current cost as 0.32513488215811104 and parameters 
[-3.27523253  2.26676962 -2.0891852  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57166946  1.14432445
  1.74642117 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Fri Apr  5 17:05:54 2024]  Iteration number: 0 with current cost as 0.30581830611818633 and parameters 
[-3.27475338  2.22493854 -2.10197661 -0.11653103  0.55388707 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.56515227  1.14432445
  1.75486206 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 17:15:08 2024]  Iteration number: 0 with current cost as 0.3187300152283721 and parameters 
[-3.30557073  2.24363227 -2.08772003 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.56181803  1.14432446
  1.77986359 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2937.1656074523926 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 17:24:53 2024]  Iteration number: 0 with current cost as 0.3240662658895387 and parameters 
[-3.3141811   2.23624957 -2.09218479 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.56278673  1.14432446
  1.79362613 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Fri Apr  5 17:34:52 2024]  Iteration number: 50 with current cost as 0.20324097213927872 and parameters 
[-3.72742901  1.70522636  0.71941543 -0.11653036  0.5538889  -2.77010862
  3.06858516  2.18960275  1.18551851 -1.06648284  2.14092072  1.14432654
  1.58314769 -1.87354535  0.72965309  2.88578483 -0.54534289 -0.47522254
 -2.02654286  0.72897376  1.60512697  2.83077093 -1.26456488 -0.25135922]. 
Working on 0.4 fold... 
[Fri Apr  5 17:36:45 2024]  Iteration number: 0 with current cost as 0.3224777053385867 and parameters 
[-3.27519046  2.25709375 -2.09148432 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.56933536  1.14432445
  1.74788225 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Fri Apr  5 17:47:02 2024]  Iteration number: 50 with current cost as 0.20486308703930756 and parameters 
[-3.72397172  1.64578053  0.69862092 -0.11653194  0.55388787 -2.7701095
  3.06858426  2.18960166  1.18551946 -1.06648446  2.2095001   1.14432376
  1.59413582 -1.87354633  0.72965025  2.88578522 -0.54534353 -0.47522442
 -2.02654308  0.72897283  1.6051259   2.83077052 -1.26456708 -0.25136165]. 
Working on 0.6 fold... 
[Fri Apr  5 17:48:16 2024]  Iteration number: 0 with current cost as 0.32513488215811104 and parameters 
[-3.27523253  2.26676962 -2.0891852  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57166946  1.14432445
  1.74642117 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Fri Apr  5 17:55:09 2024]  Iteration number: 0 with current cost as 0.30581830611818633 and parameters 
[-3.27475338  2.22493854 -2.10197661 -0.11653103  0.55388707 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.56515227  1.14432445
  1.75486206 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 18:04:23 2024]  Iteration number: 0 with current cost as 0.3187300152283721 and parameters 
[-3.30557073  2.24363227 -2.08772003 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.56181803  1.14432446
  1.77986359 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2951.104041814804 seconds. 
Discarding model... 

Training complete taking 74159.56460523605 total seconds. 
Now scoring model... 
Scoring complete taking 1.0269677639007568 seconds. 
Saved predicted values as M-M-CNOT_Full-Pauli-CRZ_predicted_values.csv
Model scores: {'MSE_train': (0.34351106626307704,), 'R2_train': 0.3159885984432511, 'MAE_train': 0.5075592879720331, 'MSE_test': 0.3761420097415599, 'R2_test': 0.2985083354674811, 'MAE_test': 0.5376298110382727}. 
Saved model results as M-M-CNOT_Full-Pauli-CRZ_results.json. 
