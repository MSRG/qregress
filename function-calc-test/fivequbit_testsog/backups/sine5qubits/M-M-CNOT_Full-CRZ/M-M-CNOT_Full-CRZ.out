/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /scratch/j/jacobsen/gjones/sine5qubits/sine_train.bin... 
Successfully loaded /scratch/j/jacobsen/gjones/sine5qubits/sine_train.bin into X and y data. 
Loading dataset from /scratch/j/jacobsen/gjones/sine5qubits/sine_test.bin... 
Successfully loaded /scratch/j/jacobsen/gjones/sine5qubits/sine_test.bin into X and y data. 
Training model with dataset /scratch/j/jacobsen/gjones/sine5qubits/sine_train.bin 
 at time Thu Apr  4 21:37:32 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 21:44:19 2024]  Iteration number: 0 with current cost as 0.24221701830883216 and parameters 
[ 1.36565611  2.23743464 -2.12427931 -0.11653086  0.55388724 -2.77010881
  3.06858498  2.18960162  1.18551998 -1.06648308  0.6027151   1.14432478
  1.31029915 -1.8735468   0.7296508   2.88578403 -0.54534319 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077091 -1.26456693 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 21:52:58 2024]  Iteration number: 0 with current cost as 0.3708304824455289 and parameters 
[11.25917721  2.23743464 -2.124279   -0.11653039  0.55388835 -2.77010834
  3.06858435  2.18960209  1.18552125 -1.06648245  0.60271574  1.14432508
  1.31030025 -1.87354617  0.7296508   2.88578419 -0.54534272 -0.47522422
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 22:02:28 2024]  Iteration number: 0 with current cost as 0.3769550224216388 and parameters 
[11.00344079  2.237434   -2.124279   -0.11653103  0.55388708 -2.77010897
  3.06858435  2.18960209  1.18552062 -1.06648308  0.6027151   1.14432572
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654304  0.7289737   1.60512664  2.8307698  -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002265]. 
Working on 0.8 fold... 
[Thu Apr  4 22:15:05 2024]  Iteration number: 0 with current cost as 0.2493114851340123 and parameters 
[ 1.31247515  2.23743464 -2.12427947 -0.11653069  0.55388708 -2.77010897
  3.06858498  2.18960179  1.18552032 -1.06648308  0.60271544  1.14432478
  1.31029915 -1.87354647  0.7296508   2.88578419 -0.54534318 -0.47522452
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 22:24:52 2024]  Iteration number: 0 with current cost as 0.6176603482739749 and parameters 
[12.35305744  2.23743464 -2.12427837 -0.11652976  0.55388708 -2.77010897
  3.06858498  2.18960272  1.18552125 -1.06648308  0.60271574  1.14432572
  1.31030025 -1.87354553  0.7296508   2.88578419 -0.54534272 -0.47522422
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456646 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550724 -2.69002202]. 
Training complete taking 3261.4688787460327 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 22:38:43 2024]  Iteration number: 0 with current cost as 0.24221701830883216 and parameters 
[ 1.36565611  2.23743464 -2.12427931 -0.11653086  0.55388724 -2.77010881
  3.06858498  2.18960162  1.18551998 -1.06648308  0.6027151   1.14432478
  1.31029915 -1.8735468   0.7296508   2.88578403 -0.54534319 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077091 -1.26456693 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 22:47:22 2024]  Iteration number: 0 with current cost as 0.3708304824455289 and parameters 
[11.25917721  2.23743464 -2.124279   -0.11653039  0.55388835 -2.77010834
  3.06858435  2.18960209  1.18552125 -1.06648245  0.60271574  1.14432508
  1.31030025 -1.87354617  0.7296508   2.88578419 -0.54534272 -0.47522422
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 22:57:13 2024]  Iteration number: 0 with current cost as 0.3769550224216388 and parameters 
[11.00344079  2.237434   -2.124279   -0.11653103  0.55388708 -2.77010897
  3.06858435  2.18960209  1.18552062 -1.06648308  0.6027151   1.14432572
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654304  0.7289737   1.60512664  2.8307698  -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002265]. 
Working on 0.8 fold... 
[Thu Apr  4 23:10:04 2024]  Iteration number: 0 with current cost as 0.2493114851340123 and parameters 
[ 1.31247515  2.23743464 -2.12427947 -0.11653069  0.55388708 -2.77010897
  3.06858498  2.18960179  1.18552032 -1.06648308  0.60271544  1.14432478
  1.31029915 -1.87354647  0.7296508   2.88578419 -0.54534318 -0.47522452
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 23:19:35 2024]  Iteration number: 0 with current cost as 0.6176603482739749 and parameters 
[12.35305744  2.23743464 -2.12427837 -0.11652976  0.55388708 -2.77010897
  3.06858498  2.18960272  1.18552125 -1.06648308  0.60271574  1.14432572
  1.31030025 -1.87354553  0.7296508   2.88578419 -0.54534272 -0.47522422
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456646 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550724 -2.69002202]. 
Training complete taking 3270.7232563495636 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 23:32:58 2024]  Iteration number: 0 with current cost as 0.24221701830883216 and parameters 
[ 1.36565611  2.23743464 -2.12427931 -0.11653086  0.55388724 -2.77010881
  3.06858498  2.18960162  1.18551998 -1.06648308  0.6027151   1.14432478
  1.31029915 -1.8735468   0.7296508   2.88578403 -0.54534319 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077091 -1.26456693 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 23:41:45 2024]  Iteration number: 0 with current cost as 0.3708304824455289 and parameters 
[11.25917721  2.23743464 -2.124279   -0.11653039  0.55388835 -2.77010834
  3.06858435  2.18960209  1.18552125 -1.06648245  0.60271574  1.14432508
  1.31030025 -1.87354617  0.7296508   2.88578419 -0.54534272 -0.47522422
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 23:51:23 2024]  Iteration number: 0 with current cost as 0.3769550224216388 and parameters 
[11.00344079  2.237434   -2.124279   -0.11653103  0.55388708 -2.77010897
  3.06858435  2.18960209  1.18552062 -1.06648308  0.6027151   1.14432572
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654304  0.7289737   1.60512664  2.8307698  -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002265]. 
Working on 0.8 fold... 
[Fri Apr  5 00:04:19 2024]  Iteration number: 0 with current cost as 0.2493114851340123 and parameters 
[ 1.31247515  2.23743464 -2.12427947 -0.11653069  0.55388708 -2.77010897
  3.06858498  2.18960179  1.18552032 -1.06648308  0.60271544  1.14432478
  1.31029915 -1.87354647  0.7296508   2.88578419 -0.54534318 -0.47522452
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 00:14:15 2024]  Iteration number: 0 with current cost as 0.6176603482739749 and parameters 
[12.35305744  2.23743464 -2.12427837 -0.11652976  0.55388708 -2.77010897
  3.06858498  2.18960272  1.18552125 -1.06648308  0.60271574  1.14432572
  1.31030025 -1.87354553  0.7296508   2.88578419 -0.54534272 -0.47522422
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456646 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550724 -2.69002202]. 
Training complete taking 3296.263593673706 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 00:28:20 2024]  Iteration number: 0 with current cost as 0.24221701830883216 and parameters 
[ 1.36565611  2.23743464 -2.12427931 -0.11653086  0.55388724 -2.77010881
  3.06858498  2.18960162  1.18551998 -1.06648308  0.6027151   1.14432478
  1.31029915 -1.8735468   0.7296508   2.88578403 -0.54534319 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077091 -1.26456693 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 00:37:23 2024]  Iteration number: 0 with current cost as 0.3708304824455289 and parameters 
[11.25917721  2.23743464 -2.124279   -0.11653039  0.55388835 -2.77010834
  3.06858435  2.18960209  1.18552125 -1.06648245  0.60271574  1.14432508
  1.31030025 -1.87354617  0.7296508   2.88578419 -0.54534272 -0.47522422
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 00:47:02 2024]  Iteration number: 0 with current cost as 0.3769550224216388 and parameters 
[11.00344079  2.237434   -2.124279   -0.11653103  0.55388708 -2.77010897
  3.06858435  2.18960209  1.18552062 -1.06648308  0.6027151   1.14432572
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654304  0.7289737   1.60512664  2.8307698  -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002265]. 
Working on 0.8 fold... 
[Fri Apr  5 00:59:47 2024]  Iteration number: 0 with current cost as 0.2493114851340123 and parameters 
[ 1.31247515  2.23743464 -2.12427947 -0.11653069  0.55388708 -2.77010897
  3.06858498  2.18960179  1.18552032 -1.06648308  0.60271544  1.14432478
  1.31029915 -1.87354647  0.7296508   2.88578419 -0.54534318 -0.47522452
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 01:09:33 2024]  Iteration number: 0 with current cost as 0.6176603482739749 and parameters 
[12.35305744  2.23743464 -2.12427837 -0.11652976  0.55388708 -2.77010897
  3.06858498  2.18960272  1.18552125 -1.06648308  0.60271574  1.14432572
  1.31030025 -1.87354553  0.7296508   2.88578419 -0.54534272 -0.47522422
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456646 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550724 -2.69002202]. 
Training complete taking 3312.7428777217865 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 01:23:39 2024]  Iteration number: 0 with current cost as 0.24221701830883216 and parameters 
[ 1.36565611  2.23743464 -2.12427931 -0.11653086  0.55388724 -2.77010881
  3.06858498  2.18960162  1.18551998 -1.06648308  0.6027151   1.14432478
  1.31029915 -1.8735468   0.7296508   2.88578403 -0.54534319 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077091 -1.26456693 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 01:32:35 2024]  Iteration number: 0 with current cost as 0.3708304824455289 and parameters 
[11.25917721  2.23743464 -2.124279   -0.11653039  0.55388835 -2.77010834
  3.06858435  2.18960209  1.18552125 -1.06648245  0.60271574  1.14432508
  1.31030025 -1.87354617  0.7296508   2.88578419 -0.54534272 -0.47522422
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 01:42:21 2024]  Iteration number: 0 with current cost as 0.3769550224216388 and parameters 
[11.00344079  2.237434   -2.124279   -0.11653103  0.55388708 -2.77010897
  3.06858435  2.18960209  1.18552062 -1.06648308  0.6027151   1.14432572
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654304  0.7289737   1.60512664  2.8307698  -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002265]. 
Working on 0.8 fold... 
[Fri Apr  5 01:55:05 2024]  Iteration number: 0 with current cost as 0.2493114851340123 and parameters 
[ 1.31247515  2.23743464 -2.12427947 -0.11653069  0.55388708 -2.77010897
  3.06858498  2.18960179  1.18552032 -1.06648308  0.60271544  1.14432478
  1.31029915 -1.87354647  0.7296508   2.88578419 -0.54534318 -0.47522452
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 02:04:47 2024]  Iteration number: 0 with current cost as 0.6176603482739749 and parameters 
[12.35305744  2.23743464 -2.12427837 -0.11652976  0.55388708 -2.77010897
  3.06858498  2.18960272  1.18552125 -1.06648308  0.60271574  1.14432572
  1.31030025 -1.87354553  0.7296508   2.88578419 -0.54534272 -0.47522422
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456646 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550724 -2.69002202]. 
Training complete taking 3306.8170115947723 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 02:18:19 2024]  Iteration number: 0 with current cost as 0.24221701830883216 and parameters 
[ 1.36565611  2.23743464 -2.12427931 -0.11653086  0.55388724 -2.77010881
  3.06858498  2.18960162  1.18551998 -1.06648308  0.6027151   1.14432478
  1.31029915 -1.8735468   0.7296508   2.88578403 -0.54534319 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077091 -1.26456693 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 02:27:05 2024]  Iteration number: 0 with current cost as 0.3708304824455289 and parameters 
[11.25917721  2.23743464 -2.124279   -0.11653039  0.55388835 -2.77010834
  3.06858435  2.18960209  1.18552125 -1.06648245  0.60271574  1.14432508
  1.31030025 -1.87354617  0.7296508   2.88578419 -0.54534272 -0.47522422
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 02:37:05 2024]  Iteration number: 0 with current cost as 0.3769550224216388 and parameters 
[11.00344079  2.237434   -2.124279   -0.11653103  0.55388708 -2.77010897
  3.06858435  2.18960209  1.18552062 -1.06648308  0.6027151   1.14432572
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654304  0.7289737   1.60512664  2.8307698  -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002265]. 
Working on 0.8 fold... 
[Fri Apr  5 02:49:48 2024]  Iteration number: 0 with current cost as 0.2493114851340123 and parameters 
[ 1.31247515  2.23743464 -2.12427947 -0.11653069  0.55388708 -2.77010897
  3.06858498  2.18960179  1.18552032 -1.06648308  0.60271544  1.14432478
  1.31029915 -1.87354647  0.7296508   2.88578419 -0.54534318 -0.47522452
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 02:59:21 2024]  Iteration number: 0 with current cost as 0.6176603482739749 and parameters 
[12.35305744  2.23743464 -2.12427837 -0.11652976  0.55388708 -2.77010897
  3.06858498  2.18960272  1.18552125 -1.06648308  0.60271574  1.14432572
  1.31030025 -1.87354553  0.7296508   2.88578419 -0.54534272 -0.47522422
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456646 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550724 -2.69002202]. 
Training complete taking 3282.3499920368195 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 03:13:19 2024]  Iteration number: 0 with current cost as 0.24221701830883216 and parameters 
[ 1.36565611  2.23743464 -2.12427931 -0.11653086  0.55388724 -2.77010881
  3.06858498  2.18960162  1.18551998 -1.06648308  0.6027151   1.14432478
  1.31029915 -1.8735468   0.7296508   2.88578403 -0.54534319 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077091 -1.26456693 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 03:22:04 2024]  Iteration number: 0 with current cost as 0.3708304824455289 and parameters 
[11.25917721  2.23743464 -2.124279   -0.11653039  0.55388835 -2.77010834
  3.06858435  2.18960209  1.18552125 -1.06648245  0.60271574  1.14432508
  1.31030025 -1.87354617  0.7296508   2.88578419 -0.54534272 -0.47522422
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 03:31:56 2024]  Iteration number: 0 with current cost as 0.3769550224216388 and parameters 
[11.00344079  2.237434   -2.124279   -0.11653103  0.55388708 -2.77010897
  3.06858435  2.18960209  1.18552062 -1.06648308  0.6027151   1.14432572
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654304  0.7289737   1.60512664  2.8307698  -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002265]. 
Working on 0.8 fold... 
[Fri Apr  5 03:44:31 2024]  Iteration number: 0 with current cost as 0.2493114851340123 and parameters 
[ 1.31247515  2.23743464 -2.12427947 -0.11653069  0.55388708 -2.77010897
  3.06858498  2.18960179  1.18552032 -1.06648308  0.60271544  1.14432478
  1.31029915 -1.87354647  0.7296508   2.88578419 -0.54534318 -0.47522452
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 03:53:57 2024]  Iteration number: 0 with current cost as 0.6176603482739749 and parameters 
[12.35305744  2.23743464 -2.12427837 -0.11652976  0.55388708 -2.77010897
  3.06858498  2.18960272  1.18552125 -1.06648308  0.60271574  1.14432572
  1.31030025 -1.87354553  0.7296508   2.88578419 -0.54534272 -0.47522422
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456646 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550724 -2.69002202]. 
Training complete taking 3255.9484939575195 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 04:07:19 2024]  Iteration number: 0 with current cost as 0.24221701830883216 and parameters 
[ 1.36565611  2.23743464 -2.12427931 -0.11653086  0.55388724 -2.77010881
  3.06858498  2.18960162  1.18551998 -1.06648308  0.6027151   1.14432478
  1.31029915 -1.8735468   0.7296508   2.88578403 -0.54534319 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077091 -1.26456693 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 04:15:54 2024]  Iteration number: 0 with current cost as 0.3708304824455289 and parameters 
[11.25917721  2.23743464 -2.124279   -0.11653039  0.55388835 -2.77010834
  3.06858435  2.18960209  1.18552125 -1.06648245  0.60271574  1.14432508
  1.31030025 -1.87354617  0.7296508   2.88578419 -0.54534272 -0.47522422
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 04:25:30 2024]  Iteration number: 0 with current cost as 0.3769550224216388 and parameters 
[11.00344079  2.237434   -2.124279   -0.11653103  0.55388708 -2.77010897
  3.06858435  2.18960209  1.18552062 -1.06648308  0.6027151   1.14432572
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654304  0.7289737   1.60512664  2.8307698  -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002265]. 
Working on 0.8 fold... 
[Fri Apr  5 04:38:23 2024]  Iteration number: 0 with current cost as 0.2493114851340123 and parameters 
[ 1.31247515  2.23743464 -2.12427947 -0.11653069  0.55388708 -2.77010897
  3.06858498  2.18960179  1.18552032 -1.06648308  0.60271544  1.14432478
  1.31029915 -1.87354647  0.7296508   2.88578419 -0.54534318 -0.47522452
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 04:47:56 2024]  Iteration number: 0 with current cost as 0.6176603482739749 and parameters 
[12.35305744  2.23743464 -2.12427837 -0.11652976  0.55388708 -2.77010897
  3.06858498  2.18960272  1.18552125 -1.06648308  0.60271574  1.14432572
  1.31030025 -1.87354553  0.7296508   2.88578419 -0.54534272 -0.47522422
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456646 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550724 -2.69002202]. 
Training complete taking 3273.977520465851 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 05:02:30 2024]  Iteration number: 0 with current cost as 0.24221701830883216 and parameters 
[ 1.36565611  2.23743464 -2.12427931 -0.11653086  0.55388724 -2.77010881
  3.06858498  2.18960162  1.18551998 -1.06648308  0.6027151   1.14432478
  1.31029915 -1.8735468   0.7296508   2.88578403 -0.54534319 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077091 -1.26456693 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 05:11:45 2024]  Iteration number: 0 with current cost as 0.3708304824455289 and parameters 
[11.25917721  2.23743464 -2.124279   -0.11653039  0.55388835 -2.77010834
  3.06858435  2.18960209  1.18552125 -1.06648245  0.60271574  1.14432508
  1.31030025 -1.87354617  0.7296508   2.88578419 -0.54534272 -0.47522422
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 05:21:12 2024]  Iteration number: 0 with current cost as 0.3769550224216388 and parameters 
[11.00344079  2.237434   -2.124279   -0.11653103  0.55388708 -2.77010897
  3.06858435  2.18960209  1.18552062 -1.06648308  0.6027151   1.14432572
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654304  0.7289737   1.60512664  2.8307698  -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002265]. 
Working on 0.8 fold... 
[Fri Apr  5 05:33:18 2024]  Iteration number: 0 with current cost as 0.2493114851340123 and parameters 
[ 1.31247515  2.23743464 -2.12427947 -0.11653069  0.55388708 -2.77010897
  3.06858498  2.18960179  1.18552032 -1.06648308  0.60271544  1.14432478
  1.31029915 -1.87354647  0.7296508   2.88578419 -0.54534318 -0.47522452
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 05:42:27 2024]  Iteration number: 0 with current cost as 0.6176603482739749 and parameters 
[12.35305744  2.23743464 -2.12427837 -0.11652976  0.55388708 -2.77010897
  3.06858498  2.18960272  1.18552125 -1.06648308  0.60271574  1.14432572
  1.31030025 -1.87354553  0.7296508   2.88578419 -0.54534272 -0.47522422
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456646 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550724 -2.69002202]. 
Training complete taking 3220.4079439640045 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 05:55:36 2024]  Iteration number: 0 with current cost as 0.24221701830883216 and parameters 
[ 1.36565611  2.23743464 -2.12427931 -0.11653086  0.55388724 -2.77010881
  3.06858498  2.18960162  1.18551998 -1.06648308  0.6027151   1.14432478
  1.31029915 -1.8735468   0.7296508   2.88578403 -0.54534319 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077091 -1.26456693 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 06:04:05 2024]  Iteration number: 0 with current cost as 0.3708304824455289 and parameters 
[11.25917721  2.23743464 -2.124279   -0.11653039  0.55388835 -2.77010834
  3.06858435  2.18960209  1.18552125 -1.06648245  0.60271574  1.14432508
  1.31030025 -1.87354617  0.7296508   2.88578419 -0.54534272 -0.47522422
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 06:13:23 2024]  Iteration number: 0 with current cost as 0.3769550224216388 and parameters 
[11.00344079  2.237434   -2.124279   -0.11653103  0.55388708 -2.77010897
  3.06858435  2.18960209  1.18552062 -1.06648308  0.6027151   1.14432572
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654304  0.7289737   1.60512664  2.8307698  -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002265]. 
Working on 0.8 fold... 
[Fri Apr  5 06:25:37 2024]  Iteration number: 0 with current cost as 0.2493114851340123 and parameters 
[ 1.31247515  2.23743464 -2.12427947 -0.11653069  0.55388708 -2.77010897
  3.06858498  2.18960179  1.18552032 -1.06648308  0.60271544  1.14432478
  1.31029915 -1.87354647  0.7296508   2.88578419 -0.54534318 -0.47522452
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 06:34:52 2024]  Iteration number: 0 with current cost as 0.6176603482739749 and parameters 
[12.35305744  2.23743464 -2.12427837 -0.11652976  0.55388708 -2.77010897
  3.06858498  2.18960272  1.18552125 -1.06648308  0.60271574  1.14432572
  1.31030025 -1.87354553  0.7296508   2.88578419 -0.54534272 -0.47522422
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456646 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550724 -2.69002202]. 
Training complete taking 3155.6169617176056 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 06:48:05 2024]  Iteration number: 0 with current cost as 0.24221701830883216 and parameters 
[ 1.36565611  2.23743464 -2.12427931 -0.11653086  0.55388724 -2.77010881
  3.06858498  2.18960162  1.18551998 -1.06648308  0.6027151   1.14432478
  1.31029915 -1.8735468   0.7296508   2.88578403 -0.54534319 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077091 -1.26456693 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 06:56:30 2024]  Iteration number: 0 with current cost as 0.3708304824455289 and parameters 
[11.25917721  2.23743464 -2.124279   -0.11653039  0.55388835 -2.77010834
  3.06858435  2.18960209  1.18552125 -1.06648245  0.60271574  1.14432508
  1.31030025 -1.87354617  0.7296508   2.88578419 -0.54534272 -0.47522422
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 07:06:04 2024]  Iteration number: 0 with current cost as 0.3769550224216388 and parameters 
[11.00344079  2.237434   -2.124279   -0.11653103  0.55388708 -2.77010897
  3.06858435  2.18960209  1.18552062 -1.06648308  0.6027151   1.14432572
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654304  0.7289737   1.60512664  2.8307698  -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002265]. 
Working on 0.8 fold... 
[Fri Apr  5 07:18:30 2024]  Iteration number: 0 with current cost as 0.2493114851340123 and parameters 
[ 1.31247515  2.23743464 -2.12427947 -0.11653069  0.55388708 -2.77010897
  3.06858498  2.18960179  1.18552032 -1.06648308  0.60271544  1.14432478
  1.31029915 -1.87354647  0.7296508   2.88578419 -0.54534318 -0.47522452
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 07:27:49 2024]  Iteration number: 0 with current cost as 0.6176603482739749 and parameters 
[12.35305744  2.23743464 -2.12427837 -0.11652976  0.55388708 -2.77010897
  3.06858498  2.18960272  1.18552125 -1.06648308  0.60271574  1.14432572
  1.31030025 -1.87354553  0.7296508   2.88578419 -0.54534272 -0.47522422
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456646 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550724 -2.69002202]. 
Training complete taking 3186.084138393402 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 07:41:10 2024]  Iteration number: 0 with current cost as 0.24221701830883216 and parameters 
[ 1.36565611  2.23743464 -2.12427931 -0.11653086  0.55388724 -2.77010881
  3.06858498  2.18960162  1.18551998 -1.06648308  0.6027151   1.14432478
  1.31029915 -1.8735468   0.7296508   2.88578403 -0.54534319 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077091 -1.26456693 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 07:49:41 2024]  Iteration number: 0 with current cost as 0.3708304824455289 and parameters 
[11.25917721  2.23743464 -2.124279   -0.11653039  0.55388835 -2.77010834
  3.06858435  2.18960209  1.18552125 -1.06648245  0.60271574  1.14432508
  1.31030025 -1.87354617  0.7296508   2.88578419 -0.54534272 -0.47522422
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 07:59:54 2024]  Iteration number: 0 with current cost as 0.3769550224216388 and parameters 
[11.00344079  2.237434   -2.124279   -0.11653103  0.55388708 -2.77010897
  3.06858435  2.18960209  1.18552062 -1.06648308  0.6027151   1.14432572
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654304  0.7289737   1.60512664  2.8307698  -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002265]. 
Working on 0.8 fold... 
[Fri Apr  5 08:12:30 2024]  Iteration number: 0 with current cost as 0.2493114851340123 and parameters 
[ 1.31247515  2.23743464 -2.12427947 -0.11653069  0.55388708 -2.77010897
  3.06858498  2.18960179  1.18552032 -1.06648308  0.60271544  1.14432478
  1.31029915 -1.87354647  0.7296508   2.88578419 -0.54534318 -0.47522452
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 08:22:21 2024]  Iteration number: 0 with current cost as 0.6176603482739749 and parameters 
[12.35305744  2.23743464 -2.12427837 -0.11652976  0.55388708 -2.77010897
  3.06858498  2.18960272  1.18552125 -1.06648308  0.60271574  1.14432572
  1.31030025 -1.87354553  0.7296508   2.88578419 -0.54534272 -0.47522422
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456646 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550724 -2.69002202]. 
Training complete taking 3286.0828030109406 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 08:36:02 2024]  Iteration number: 0 with current cost as 0.24221701830883216 and parameters 
[ 1.36565611  2.23743464 -2.12427931 -0.11653086  0.55388724 -2.77010881
  3.06858498  2.18960162  1.18551998 -1.06648308  0.6027151   1.14432478
  1.31029915 -1.8735468   0.7296508   2.88578403 -0.54534319 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077091 -1.26456693 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 08:44:57 2024]  Iteration number: 0 with current cost as 0.3708304824455289 and parameters 
[11.25917721  2.23743464 -2.124279   -0.11653039  0.55388835 -2.77010834
  3.06858435  2.18960209  1.18552125 -1.06648245  0.60271574  1.14432508
  1.31030025 -1.87354617  0.7296508   2.88578419 -0.54534272 -0.47522422
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 08:54:06 2024]  Iteration number: 0 with current cost as 0.3769550224216388 and parameters 
[11.00344079  2.237434   -2.124279   -0.11653103  0.55388708 -2.77010897
  3.06858435  2.18960209  1.18552062 -1.06648308  0.6027151   1.14432572
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654304  0.7289737   1.60512664  2.8307698  -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002265]. 
Working on 0.8 fold... 
[Fri Apr  5 09:06:23 2024]  Iteration number: 0 with current cost as 0.2493114851340123 and parameters 
[ 1.31247515  2.23743464 -2.12427947 -0.11653069  0.55388708 -2.77010897
  3.06858498  2.18960179  1.18552032 -1.06648308  0.60271544  1.14432478
  1.31029915 -1.87354647  0.7296508   2.88578419 -0.54534318 -0.47522452
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 09:15:53 2024]  Iteration number: 0 with current cost as 0.6176603482739749 and parameters 
[12.35305744  2.23743464 -2.12427837 -0.11652976  0.55388708 -2.77010897
  3.06858498  2.18960272  1.18552125 -1.06648308  0.60271574  1.14432572
  1.31030025 -1.87354553  0.7296508   2.88578419 -0.54534272 -0.47522422
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456646 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550724 -2.69002202]. 
Training complete taking 3187.549759864807 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 09:29:17 2024]  Iteration number: 0 with current cost as 0.24221701830883216 and parameters 
[ 1.36565611  2.23743464 -2.12427931 -0.11653086  0.55388724 -2.77010881
  3.06858498  2.18960162  1.18551998 -1.06648308  0.6027151   1.14432478
  1.31029915 -1.8735468   0.7296508   2.88578403 -0.54534319 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077091 -1.26456693 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 09:37:49 2024]  Iteration number: 0 with current cost as 0.3708304824455289 and parameters 
[11.25917721  2.23743464 -2.124279   -0.11653039  0.55388835 -2.77010834
  3.06858435  2.18960209  1.18552125 -1.06648245  0.60271574  1.14432508
  1.31030025 -1.87354617  0.7296508   2.88578419 -0.54534272 -0.47522422
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 09:47:31 2024]  Iteration number: 0 with current cost as 0.3769550224216388 and parameters 
[11.00344079  2.237434   -2.124279   -0.11653103  0.55388708 -2.77010897
  3.06858435  2.18960209  1.18552062 -1.06648308  0.6027151   1.14432572
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654304  0.7289737   1.60512664  2.8307698  -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002265]. 
Working on 0.8 fold... 
[Fri Apr  5 10:00:00 2024]  Iteration number: 0 with current cost as 0.2493114851340123 and parameters 
[ 1.31247515  2.23743464 -2.12427947 -0.11653069  0.55388708 -2.77010897
  3.06858498  2.18960179  1.18552032 -1.06648308  0.60271544  1.14432478
  1.31029915 -1.87354647  0.7296508   2.88578419 -0.54534318 -0.47522452
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 10:09:43 2024]  Iteration number: 0 with current cost as 0.6176603482739749 and parameters 
[12.35305744  2.23743464 -2.12427837 -0.11652976  0.55388708 -2.77010897
  3.06858498  2.18960272  1.18552125 -1.06648308  0.60271574  1.14432572
  1.31030025 -1.87354553  0.7296508   2.88578419 -0.54534272 -0.47522422
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456646 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550724 -2.69002202]. 
Training complete taking 3252.653296470642 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 10:23:31 2024]  Iteration number: 0 with current cost as 0.24221701830883216 and parameters 
[ 1.36565611  2.23743464 -2.12427931 -0.11653086  0.55388724 -2.77010881
  3.06858498  2.18960162  1.18551998 -1.06648308  0.6027151   1.14432478
  1.31029915 -1.8735468   0.7296508   2.88578403 -0.54534319 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077091 -1.26456693 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 10:32:23 2024]  Iteration number: 0 with current cost as 0.3708304824455289 and parameters 
[11.25917721  2.23743464 -2.124279   -0.11653039  0.55388835 -2.77010834
  3.06858435  2.18960209  1.18552125 -1.06648245  0.60271574  1.14432508
  1.31030025 -1.87354617  0.7296508   2.88578419 -0.54534272 -0.47522422
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 10:42:09 2024]  Iteration number: 0 with current cost as 0.3769550224216388 and parameters 
[11.00344079  2.237434   -2.124279   -0.11653103  0.55388708 -2.77010897
  3.06858435  2.18960209  1.18552062 -1.06648308  0.6027151   1.14432572
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654304  0.7289737   1.60512664  2.8307698  -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002265]. 
Working on 0.8 fold... 
[Fri Apr  5 10:54:59 2024]  Iteration number: 0 with current cost as 0.2493114851340123 and parameters 
[ 1.31247515  2.23743464 -2.12427947 -0.11653069  0.55388708 -2.77010897
  3.06858498  2.18960179  1.18552032 -1.06648308  0.60271544  1.14432478
  1.31029915 -1.87354647  0.7296508   2.88578419 -0.54534318 -0.47522452
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 11:04:33 2024]  Iteration number: 0 with current cost as 0.6176603482739749 and parameters 
[12.35305744  2.23743464 -2.12427837 -0.11652976  0.55388708 -2.77010897
  3.06858498  2.18960272  1.18552125 -1.06648308  0.60271574  1.14432572
  1.31030025 -1.87354553  0.7296508   2.88578419 -0.54534272 -0.47522422
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456646 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550724 -2.69002202]. 
Training complete taking 3280.0570237636566 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 11:18:17 2024]  Iteration number: 0 with current cost as 0.24221701830883216 and parameters 
[ 1.36565611  2.23743464 -2.12427931 -0.11653086  0.55388724 -2.77010881
  3.06858498  2.18960162  1.18551998 -1.06648308  0.6027151   1.14432478
  1.31029915 -1.8735468   0.7296508   2.88578403 -0.54534319 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077091 -1.26456693 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 11:27:13 2024]  Iteration number: 0 with current cost as 0.3708304824455289 and parameters 
[11.25917721  2.23743464 -2.124279   -0.11653039  0.55388835 -2.77010834
  3.06858435  2.18960209  1.18552125 -1.06648245  0.60271574  1.14432508
  1.31030025 -1.87354617  0.7296508   2.88578419 -0.54534272 -0.47522422
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 11:36:50 2024]  Iteration number: 0 with current cost as 0.3769550224216388 and parameters 
[11.00344079  2.237434   -2.124279   -0.11653103  0.55388708 -2.77010897
  3.06858435  2.18960209  1.18552062 -1.06648308  0.6027151   1.14432572
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654304  0.7289737   1.60512664  2.8307698  -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002265]. 
Working on 0.8 fold... 
[Fri Apr  5 11:49:24 2024]  Iteration number: 0 with current cost as 0.2493114851340123 and parameters 
[ 1.31247515  2.23743464 -2.12427947 -0.11653069  0.55388708 -2.77010897
  3.06858498  2.18960179  1.18552032 -1.06648308  0.60271544  1.14432478
  1.31029915 -1.87354647  0.7296508   2.88578419 -0.54534318 -0.47522452
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 11:59:17 2024]  Iteration number: 0 with current cost as 0.6176603482739749 and parameters 
[12.35305744  2.23743464 -2.12427837 -0.11652976  0.55388708 -2.77010897
  3.06858498  2.18960272  1.18552125 -1.06648308  0.60271574  1.14432572
  1.31030025 -1.87354553  0.7296508   2.88578419 -0.54534272 -0.47522422
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456646 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550724 -2.69002202]. 
Training complete taking 3301.631872177124 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 12:13:22 2024]  Iteration number: 0 with current cost as 0.24221701830883216 and parameters 
[ 1.36565611  2.23743464 -2.12427931 -0.11653086  0.55388724 -2.77010881
  3.06858498  2.18960162  1.18551998 -1.06648308  0.6027151   1.14432478
  1.31029915 -1.8735468   0.7296508   2.88578403 -0.54534319 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077091 -1.26456693 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 12:22:21 2024]  Iteration number: 0 with current cost as 0.3708304824455289 and parameters 
[11.25917721  2.23743464 -2.124279   -0.11653039  0.55388835 -2.77010834
  3.06858435  2.18960209  1.18552125 -1.06648245  0.60271574  1.14432508
  1.31030025 -1.87354617  0.7296508   2.88578419 -0.54534272 -0.47522422
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 12:32:05 2024]  Iteration number: 0 with current cost as 0.3769550224216388 and parameters 
[11.00344079  2.237434   -2.124279   -0.11653103  0.55388708 -2.77010897
  3.06858435  2.18960209  1.18552062 -1.06648308  0.6027151   1.14432572
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654304  0.7289737   1.60512664  2.8307698  -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002265]. 
Working on 0.8 fold... 
[Fri Apr  5 12:44:57 2024]  Iteration number: 0 with current cost as 0.2493114851340123 and parameters 
[ 1.31247515  2.23743464 -2.12427947 -0.11653069  0.55388708 -2.77010897
  3.06858498  2.18960179  1.18552032 -1.06648308  0.60271544  1.14432478
  1.31029915 -1.87354647  0.7296508   2.88578419 -0.54534318 -0.47522452
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 12:54:48 2024]  Iteration number: 0 with current cost as 0.6176603482739749 and parameters 
[12.35305744  2.23743464 -2.12427837 -0.11652976  0.55388708 -2.77010897
  3.06858498  2.18960272  1.18552125 -1.06648308  0.60271574  1.14432572
  1.31030025 -1.87354553  0.7296508   2.88578419 -0.54534272 -0.47522422
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456646 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550724 -2.69002202]. 
Training complete taking 3331.6270277500153 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 13:08:55 2024]  Iteration number: 0 with current cost as 0.24221701830883216 and parameters 
[ 1.36565611  2.23743464 -2.12427931 -0.11653086  0.55388724 -2.77010881
  3.06858498  2.18960162  1.18551998 -1.06648308  0.6027151   1.14432478
  1.31029915 -1.8735468   0.7296508   2.88578403 -0.54534319 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077091 -1.26456693 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 13:17:42 2024]  Iteration number: 0 with current cost as 0.3708304824455289 and parameters 
[11.25917721  2.23743464 -2.124279   -0.11653039  0.55388835 -2.77010834
  3.06858435  2.18960209  1.18552125 -1.06648245  0.60271574  1.14432508
  1.31030025 -1.87354617  0.7296508   2.88578419 -0.54534272 -0.47522422
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 13:27:34 2024]  Iteration number: 0 with current cost as 0.3769550224216388 and parameters 
[11.00344079  2.237434   -2.124279   -0.11653103  0.55388708 -2.77010897
  3.06858435  2.18960209  1.18552062 -1.06648308  0.6027151   1.14432572
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654304  0.7289737   1.60512664  2.8307698  -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002265]. 
Working on 0.8 fold... 
[Fri Apr  5 13:40:04 2024]  Iteration number: 0 with current cost as 0.2493114851340123 and parameters 
[ 1.31247515  2.23743464 -2.12427947 -0.11653069  0.55388708 -2.77010897
  3.06858498  2.18960179  1.18552032 -1.06648308  0.60271544  1.14432478
  1.31029915 -1.87354647  0.7296508   2.88578419 -0.54534318 -0.47522452
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 13:49:14 2024]  Iteration number: 0 with current cost as 0.6176603482739749 and parameters 
[12.35305744  2.23743464 -2.12427837 -0.11652976  0.55388708 -2.77010897
  3.06858498  2.18960272  1.18552125 -1.06648308  0.60271574  1.14432572
  1.31030025 -1.87354553  0.7296508   2.88578419 -0.54534272 -0.47522422
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456646 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550724 -2.69002202]. 
Training complete taking 3260.426755428314 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 14:03:35 2024]  Iteration number: 0 with current cost as 0.24221701830883216 and parameters 
[ 1.36565611  2.23743464 -2.12427931 -0.11653086  0.55388724 -2.77010881
  3.06858498  2.18960162  1.18551998 -1.06648308  0.6027151   1.14432478
  1.31029915 -1.8735468   0.7296508   2.88578403 -0.54534319 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077091 -1.26456693 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 14:12:23 2024]  Iteration number: 0 with current cost as 0.3708304824455289 and parameters 
[11.25917721  2.23743464 -2.124279   -0.11653039  0.55388835 -2.77010834
  3.06858435  2.18960209  1.18552125 -1.06648245  0.60271574  1.14432508
  1.31030025 -1.87354617  0.7296508   2.88578419 -0.54534272 -0.47522422
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 14:22:39 2024]  Iteration number: 0 with current cost as 0.3769550224216388 and parameters 
[11.00344079  2.237434   -2.124279   -0.11653103  0.55388708 -2.77010897
  3.06858435  2.18960209  1.18552062 -1.06648308  0.6027151   1.14432572
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654304  0.7289737   1.60512664  2.8307698  -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002265]. 
Working on 0.8 fold... 
[Fri Apr  5 14:35:19 2024]  Iteration number: 0 with current cost as 0.2493114851340123 and parameters 
[ 1.31247515  2.23743464 -2.12427947 -0.11653069  0.55388708 -2.77010897
  3.06858498  2.18960179  1.18552032 -1.06648308  0.60271544  1.14432478
  1.31029915 -1.87354647  0.7296508   2.88578419 -0.54534318 -0.47522452
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 14:45:06 2024]  Iteration number: 0 with current cost as 0.6176603482739749 and parameters 
[12.35305744  2.23743464 -2.12427837 -0.11652976  0.55388708 -2.77010897
  3.06858498  2.18960272  1.18552125 -1.06648308  0.60271574  1.14432572
  1.31030025 -1.87354553  0.7296508   2.88578419 -0.54534272 -0.47522422
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456646 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550724 -2.69002202]. 
Training complete taking 3379.8992063999176 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 14:59:38 2024]  Iteration number: 0 with current cost as 0.24221701830883216 and parameters 
[ 1.36565611  2.23743464 -2.12427931 -0.11653086  0.55388724 -2.77010881
  3.06858498  2.18960162  1.18551998 -1.06648308  0.6027151   1.14432478
  1.31029915 -1.8735468   0.7296508   2.88578403 -0.54534319 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077091 -1.26456693 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 15:08:51 2024]  Iteration number: 0 with current cost as 0.3708304824455289 and parameters 
[11.25917721  2.23743464 -2.124279   -0.11653039  0.55388835 -2.77010834
  3.06858435  2.18960209  1.18552125 -1.06648245  0.60271574  1.14432508
  1.31030025 -1.87354617  0.7296508   2.88578419 -0.54534272 -0.47522422
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 15:19:01 2024]  Iteration number: 0 with current cost as 0.3769550224216388 and parameters 
[11.00344079  2.237434   -2.124279   -0.11653103  0.55388708 -2.77010897
  3.06858435  2.18960209  1.18552062 -1.06648308  0.6027151   1.14432572
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654304  0.7289737   1.60512664  2.8307698  -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002265]. 
Working on 0.8 fold... 
[Fri Apr  5 15:31:58 2024]  Iteration number: 0 with current cost as 0.2493114851340123 and parameters 
[ 1.31247515  2.23743464 -2.12427947 -0.11653069  0.55388708 -2.77010897
  3.06858498  2.18960179  1.18552032 -1.06648308  0.60271544  1.14432478
  1.31029915 -1.87354647  0.7296508   2.88578419 -0.54534318 -0.47522452
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 15:41:41 2024]  Iteration number: 0 with current cost as 0.6176603482739749 and parameters 
[12.35305744  2.23743464 -2.12427837 -0.11652976  0.55388708 -2.77010897
  3.06858498  2.18960272  1.18552125 -1.06648308  0.60271574  1.14432572
  1.31030025 -1.87354553  0.7296508   2.88578419 -0.54534272 -0.47522422
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456646 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550724 -2.69002202]. 
Training complete taking 3366.5050027370453 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 15:55:40 2024]  Iteration number: 0 with current cost as 0.24221701830883216 and parameters 
[ 1.36565611  2.23743464 -2.12427931 -0.11653086  0.55388724 -2.77010881
  3.06858498  2.18960162  1.18551998 -1.06648308  0.6027151   1.14432478
  1.31029915 -1.8735468   0.7296508   2.88578403 -0.54534319 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077091 -1.26456693 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 16:04:44 2024]  Iteration number: 0 with current cost as 0.3708304824455289 and parameters 
[11.25917721  2.23743464 -2.124279   -0.11653039  0.55388835 -2.77010834
  3.06858435  2.18960209  1.18552125 -1.06648245  0.60271574  1.14432508
  1.31030025 -1.87354617  0.7296508   2.88578419 -0.54534272 -0.47522422
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 16:14:40 2024]  Iteration number: 0 with current cost as 0.3769550224216388 and parameters 
[11.00344079  2.237434   -2.124279   -0.11653103  0.55388708 -2.77010897
  3.06858435  2.18960209  1.18552062 -1.06648308  0.6027151   1.14432572
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654304  0.7289737   1.60512664  2.8307698  -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002265]. 
Working on 0.8 fold... 
[Fri Apr  5 16:27:45 2024]  Iteration number: 0 with current cost as 0.2493114851340123 and parameters 
[ 1.31247515  2.23743464 -2.12427947 -0.11653069  0.55388708 -2.77010897
  3.06858498  2.18960179  1.18552032 -1.06648308  0.60271544  1.14432478
  1.31029915 -1.87354647  0.7296508   2.88578419 -0.54534318 -0.47522452
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 16:37:57 2024]  Iteration number: 0 with current cost as 0.6176603482739749 and parameters 
[12.35305744  2.23743464 -2.12427837 -0.11652976  0.55388708 -2.77010897
  3.06858498  2.18960272  1.18552125 -1.06648308  0.60271574  1.14432572
  1.31030025 -1.87354553  0.7296508   2.88578419 -0.54534272 -0.47522422
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456646 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550724 -2.69002202]. 
Training complete taking 3387.399804353714 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 16:52:01 2024]  Iteration number: 0 with current cost as 0.24221701830883216 and parameters 
[ 1.36565611  2.23743464 -2.12427931 -0.11653086  0.55388724 -2.77010881
  3.06858498  2.18960162  1.18551998 -1.06648308  0.6027151   1.14432478
  1.31029915 -1.8735468   0.7296508   2.88578403 -0.54534319 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077091 -1.26456693 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 17:01:11 2024]  Iteration number: 0 with current cost as 0.3708304824455289 and parameters 
[11.25917721  2.23743464 -2.124279   -0.11653039  0.55388835 -2.77010834
  3.06858435  2.18960209  1.18552125 -1.06648245  0.60271574  1.14432508
  1.31030025 -1.87354617  0.7296508   2.88578419 -0.54534272 -0.47522422
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 17:11:21 2024]  Iteration number: 0 with current cost as 0.3769550224216388 and parameters 
[11.00344079  2.237434   -2.124279   -0.11653103  0.55388708 -2.77010897
  3.06858435  2.18960209  1.18552062 -1.06648308  0.6027151   1.14432572
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654304  0.7289737   1.60512664  2.8307698  -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002265]. 
Working on 0.8 fold... 
[Fri Apr  5 17:24:02 2024]  Iteration number: 0 with current cost as 0.2493114851340123 and parameters 
[ 1.31247515  2.23743464 -2.12427947 -0.11653069  0.55388708 -2.77010897
  3.06858498  2.18960179  1.18552032 -1.06648308  0.60271544  1.14432478
  1.31029915 -1.87354647  0.7296508   2.88578419 -0.54534318 -0.47522452
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 17:34:20 2024]  Iteration number: 0 with current cost as 0.6176603482739749 and parameters 
[12.35305744  2.23743464 -2.12427837 -0.11652976  0.55388708 -2.77010897
  3.06858498  2.18960272  1.18552125 -1.06648308  0.60271574  1.14432572
  1.31030025 -1.87354553  0.7296508   2.88578419 -0.54534272 -0.47522422
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456646 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550724 -2.69002202]. 
Training complete taking 3373.3711547851562 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 17:48:31 2024]  Iteration number: 0 with current cost as 0.24221701830883216 and parameters 
[ 1.36565611  2.23743464 -2.12427931 -0.11653086  0.55388724 -2.77010881
  3.06858498  2.18960162  1.18551998 -1.06648308  0.6027151   1.14432478
  1.31029915 -1.8735468   0.7296508   2.88578403 -0.54534319 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077091 -1.26456693 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 17:57:38 2024]  Iteration number: 0 with current cost as 0.3708304824455289 and parameters 
[11.25917721  2.23743464 -2.124279   -0.11653039  0.55388835 -2.77010834
  3.06858435  2.18960209  1.18552125 -1.06648245  0.60271574  1.14432508
  1.31030025 -1.87354617  0.7296508   2.88578419 -0.54534272 -0.47522422
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 18:07:24 2024]  Iteration number: 0 with current cost as 0.3769550224216388 and parameters 
[11.00344079  2.237434   -2.124279   -0.11653103  0.55388708 -2.77010897
  3.06858435  2.18960209  1.18552062 -1.06648308  0.6027151   1.14432572
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654304  0.7289737   1.60512664  2.8307698  -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002265]. 
Working on 0.8 fold... 
[Fri Apr  5 18:19:45 2024]  Iteration number: 0 with current cost as 0.2493114851340123 and parameters 
[ 1.31247515  2.23743464 -2.12427947 -0.11653069  0.55388708 -2.77010897
  3.06858498  2.18960179  1.18552032 -1.06648308  0.60271544  1.14432478
  1.31029915 -1.87354647  0.7296508   2.88578419 -0.54534318 -0.47522452
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 18:28:44 2024]  Iteration number: 0 with current cost as 0.6176603482739749 and parameters 
[12.35305744  2.23743464 -2.12427837 -0.11652976  0.55388708 -2.77010897
  3.06858498  2.18960272  1.18552125 -1.06648308  0.60271574  1.14432572
  1.31030025 -1.87354553  0.7296508   2.88578419 -0.54534272 -0.47522422
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456646 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550724 -2.69002202]. 
Training complete taking 3226.903023481369 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 18:41:47 2024]  Iteration number: 0 with current cost as 0.24221701830883216 and parameters 
[ 1.36565611  2.23743464 -2.12427931 -0.11653086  0.55388724 -2.77010881
  3.06858498  2.18960162  1.18551998 -1.06648308  0.6027151   1.14432478
  1.31029915 -1.8735468   0.7296508   2.88578403 -0.54534319 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077091 -1.26456693 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 18:50:38 2024]  Iteration number: 0 with current cost as 0.3708304824455289 and parameters 
[11.25917721  2.23743464 -2.124279   -0.11653039  0.55388835 -2.77010834
  3.06858435  2.18960209  1.18552125 -1.06648245  0.60271574  1.14432508
  1.31030025 -1.87354617  0.7296508   2.88578419 -0.54534272 -0.47522422
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 19:00:43 2024]  Iteration number: 0 with current cost as 0.3769550224216388 and parameters 
[11.00344079  2.237434   -2.124279   -0.11653103  0.55388708 -2.77010897
  3.06858435  2.18960209  1.18552062 -1.06648308  0.6027151   1.14432572
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654304  0.7289737   1.60512664  2.8307698  -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002265]. 
Working on 0.8 fold... 
[Fri Apr  5 19:13:36 2024]  Iteration number: 0 with current cost as 0.2493114851340123 and parameters 
[ 1.31247515  2.23743464 -2.12427947 -0.11653069  0.55388708 -2.77010897
  3.06858498  2.18960179  1.18552032 -1.06648308  0.60271544  1.14432478
  1.31029915 -1.87354647  0.7296508   2.88578419 -0.54534318 -0.47522452
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 19:23:39 2024]  Iteration number: 0 with current cost as 0.6176603482739749 and parameters 
[12.35305744  2.23743464 -2.12427837 -0.11652976  0.55388708 -2.77010897
  3.06858498  2.18960272  1.18552125 -1.06648308  0.60271574  1.14432572
  1.31030025 -1.87354553  0.7296508   2.88578419 -0.54534272 -0.47522422
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456646 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550724 -2.69002202]. 
Training complete taking 3337.0486476421356 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 19:37:43 2024]  Iteration number: 0 with current cost as 0.24221701830883216 and parameters 
[ 1.36565611  2.23743464 -2.12427931 -0.11653086  0.55388724 -2.77010881
  3.06858498  2.18960162  1.18551998 -1.06648308  0.6027151   1.14432478
  1.31029915 -1.8735468   0.7296508   2.88578403 -0.54534319 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077091 -1.26456693 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 19:46:26 2024]  Iteration number: 0 with current cost as 0.3708304824455289 and parameters 
[11.25917721  2.23743464 -2.124279   -0.11653039  0.55388835 -2.77010834
  3.06858435  2.18960209  1.18552125 -1.06648245  0.60271574  1.14432508
  1.31030025 -1.87354617  0.7296508   2.88578419 -0.54534272 -0.47522422
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 19:56:26 2024]  Iteration number: 0 with current cost as 0.3769550224216388 and parameters 
[11.00344079  2.237434   -2.124279   -0.11653103  0.55388708 -2.77010897
  3.06858435  2.18960209  1.18552062 -1.06648308  0.6027151   1.14432572
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654304  0.7289737   1.60512664  2.8307698  -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002265]. 
Working on 0.8 fold... 
[Fri Apr  5 20:09:50 2024]  Iteration number: 0 with current cost as 0.2493114851340123 and parameters 
[ 1.31247515  2.23743464 -2.12427947 -0.11653069  0.55388708 -2.77010897
  3.06858498  2.18960179  1.18552032 -1.06648308  0.60271544  1.14432478
  1.31029915 -1.87354647  0.7296508   2.88578419 -0.54534318 -0.47522452
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 20:19:37 2024]  Iteration number: 0 with current cost as 0.6176603482739749 and parameters 
[12.35305744  2.23743464 -2.12427837 -0.11652976  0.55388708 -2.77010897
  3.06858498  2.18960272  1.18552125 -1.06648308  0.60271574  1.14432572
  1.31030025 -1.87354553  0.7296508   2.88578419 -0.54534272 -0.47522422
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456646 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550724 -2.69002202]. 
Training complete taking 3368.786298274994 seconds. 
Discarding model... 

Training complete taking 82162.34298014641 total seconds. 
Now scoring model... 
Scoring complete taking 2.781496286392212 seconds. 
Saved predicted values as M-M-CNOT_Full-CRZ_predicted_values.csv
Model scores: {'MSE_train': (0.4071508973366257,), 'R2_train': 0.18926671282540874, 'MAE_train': 0.5538781925598177, 'MSE_test': 0.41711844645070667, 'R2_test': 0.22208871721356505, 'MAE_test': 0.571355506959325}. 
Saved model results as M-M-CNOT_Full-CRZ_results.json. 
