/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /scratch/j/jacobsen/gjones/sine5qubits/sine_train.bin... 
Successfully loaded /scratch/j/jacobsen/gjones/sine5qubits/sine_train.bin into X and y data. 
Loading dataset from /scratch/j/jacobsen/gjones/sine5qubits/sine_test.bin... 
Successfully loaded /scratch/j/jacobsen/gjones/sine5qubits/sine_test.bin into X and y data. 
Training model with dataset /scratch/j/jacobsen/gjones/sine5qubits/sine_train.bin 
 at time Thu Apr  4 21:37:31 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 21:42:14 2024]  Iteration number: 0 with current cost as 0.07705837457514815 and parameters 
[-4.66635358  2.23743464 -2.12427944 -0.11653103  0.55388708 -2.77010917
  3.06858498  2.18960155  1.18552008 -1.06648308  0.6027152   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578429 -0.54534325 -0.47522466
 -2.0265424   0.7289737   1.60512664  2.83077097 -1.264567   -0.25136095
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 21:49:40 2024]  Iteration number: 0 with current cost as 0.08738631662889126 and parameters 
[-4.56668026  2.23743464 -2.12427955 -0.11653103  0.55388708 -2.77010914
  3.06858498  2.18960154  1.18552016 -1.06648326  0.60271519  1.14432454
  1.31029899 -1.87354672  0.72965089  2.88578428 -0.54534326 -0.47522468
 -2.0265424   0.7289737   1.60512664  2.83077099 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856967 -0.67550779 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 21:59:10 2024]  Iteration number: 0 with current cost as 0.08304524024500662 and parameters 
[-4.56688056  2.23743464 -2.12427947 -0.11653103  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648317  0.60271527  1.14432454
  1.31029907 -1.87354672  0.7296508   2.88578428 -0.54534335 -0.47522477
 -2.0265424   0.7289737   1.60512664  2.83077099 -1.26456701 -0.25136096
 -2.39279209 -2.27309774  3.13337155  2.54856967 -0.67550779 -2.69002193]. 
Working on 0.8 fold... 
[Thu Apr  4 22:10:37 2024]  Iteration number: 0 with current cost as 0.07566735934656398 and parameters 
[-4.6635206   2.23743455 -2.12427947 -0.11653103  0.553887   -2.77010914
  3.0685849   2.18960145  1.18551998 -1.06648325  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897361  1.60512664  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 22:18:38 2024]  Iteration number: 0 with current cost as 0.08118796814429569 and parameters 
[-4.61868058  2.23743464 -2.12427945 -0.11653103  0.55388708 -2.77010907
  3.06858498  2.18960155  1.18552018 -1.06648308  0.6027152   1.14432455
  1.31029918 -1.87354671  0.7296509   2.88578429 -0.54534316 -0.47522466
 -2.0265424   0.7289737   1.60512664  2.83077098 -1.2645671  -0.25136095
 -2.39279218 -2.27309774  3.13337164  2.54856968 -0.67550778 -2.69002202]. 
Training complete taking 2714.9546110630035 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 22:27:54 2024]  Iteration number: 0 with current cost as 0.07705837457514815 and parameters 
[-4.66635358  2.23743464 -2.12427944 -0.11653103  0.55388708 -2.77010917
  3.06858498  2.18960155  1.18552008 -1.06648308  0.6027152   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578429 -0.54534325 -0.47522466
 -2.0265424   0.7289737   1.60512664  2.83077097 -1.264567   -0.25136095
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 22:35:48 2024]  Iteration number: 0 with current cost as 0.08738631662889126 and parameters 
[-4.56668026  2.23743464 -2.12427955 -0.11653103  0.55388708 -2.77010914
  3.06858498  2.18960154  1.18552016 -1.06648326  0.60271519  1.14432454
  1.31029899 -1.87354672  0.72965089  2.88578428 -0.54534326 -0.47522468
 -2.0265424   0.7289737   1.60512664  2.83077099 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856967 -0.67550779 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 22:45:33 2024]  Iteration number: 0 with current cost as 0.08304524024500662 and parameters 
[-4.56688056  2.23743464 -2.12427947 -0.11653103  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648317  0.60271527  1.14432454
  1.31029907 -1.87354672  0.7296508   2.88578428 -0.54534335 -0.47522477
 -2.0265424   0.7289737   1.60512664  2.83077099 -1.26456701 -0.25136096
 -2.39279209 -2.27309774  3.13337155  2.54856967 -0.67550779 -2.69002193]. 
Working on 0.8 fold... 
[Thu Apr  4 22:55:53 2024]  Iteration number: 0 with current cost as 0.07566735934656398 and parameters 
[-4.6635206   2.23743455 -2.12427947 -0.11653103  0.553887   -2.77010914
  3.0685849   2.18960145  1.18551998 -1.06648325  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897361  1.60512664  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 23:03:26 2024]  Iteration number: 0 with current cost as 0.08118796814429569 and parameters 
[-4.61868058  2.23743464 -2.12427945 -0.11653103  0.55388708 -2.77010907
  3.06858498  2.18960155  1.18552018 -1.06648308  0.6027152   1.14432455
  1.31029918 -1.87354671  0.7296509   2.88578429 -0.54534316 -0.47522466
 -2.0265424   0.7289737   1.60512664  2.83077098 -1.2645671  -0.25136095
 -2.39279218 -2.27309774  3.13337164  2.54856968 -0.67550778 -2.69002202]. 
Training complete taking 2659.167953491211 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 23:11:50 2024]  Iteration number: 0 with current cost as 0.07705837457514815 and parameters 
[-4.66635358  2.23743464 -2.12427944 -0.11653103  0.55388708 -2.77010917
  3.06858498  2.18960155  1.18552008 -1.06648308  0.6027152   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578429 -0.54534325 -0.47522466
 -2.0265424   0.7289737   1.60512664  2.83077097 -1.264567   -0.25136095
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 23:19:15 2024]  Iteration number: 0 with current cost as 0.08738631662889126 and parameters 
[-4.56668026  2.23743464 -2.12427955 -0.11653103  0.55388708 -2.77010914
  3.06858498  2.18960154  1.18552016 -1.06648326  0.60271519  1.14432454
  1.31029899 -1.87354672  0.72965089  2.88578428 -0.54534326 -0.47522468
 -2.0265424   0.7289737   1.60512664  2.83077099 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856967 -0.67550779 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 23:28:28 2024]  Iteration number: 0 with current cost as 0.08304524024500662 and parameters 
[-4.56688056  2.23743464 -2.12427947 -0.11653103  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648317  0.60271527  1.14432454
  1.31029907 -1.87354672  0.7296508   2.88578428 -0.54534335 -0.47522477
 -2.0265424   0.7289737   1.60512664  2.83077099 -1.26456701 -0.25136096
 -2.39279209 -2.27309774  3.13337155  2.54856967 -0.67550779 -2.69002193]. 
Working on 0.8 fold... 
[Thu Apr  4 23:39:01 2024]  Iteration number: 0 with current cost as 0.07566735934656398 and parameters 
[-4.6635206   2.23743455 -2.12427947 -0.11653103  0.553887   -2.77010914
  3.0685849   2.18960145  1.18551998 -1.06648325  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897361  1.60512664  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 23:46:43 2024]  Iteration number: 0 with current cost as 0.08118796814429569 and parameters 
[-4.61868058  2.23743464 -2.12427945 -0.11653103  0.55388708 -2.77010907
  3.06858498  2.18960155  1.18552018 -1.06648308  0.6027152   1.14432455
  1.31029918 -1.87354671  0.7296509   2.88578429 -0.54534316 -0.47522466
 -2.0265424   0.7289737   1.60512664  2.83077098 -1.2645671  -0.25136095
 -2.39279218 -2.27309774  3.13337164  2.54856968 -0.67550778 -2.69002202]. 
Training complete taking 2607.9639418125153 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 23:55:19 2024]  Iteration number: 0 with current cost as 0.07705837457514815 and parameters 
[-4.66635358  2.23743464 -2.12427944 -0.11653103  0.55388708 -2.77010917
  3.06858498  2.18960155  1.18552008 -1.06648308  0.6027152   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578429 -0.54534325 -0.47522466
 -2.0265424   0.7289737   1.60512664  2.83077097 -1.264567   -0.25136095
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 00:02:56 2024]  Iteration number: 0 with current cost as 0.08738631662889126 and parameters 
[-4.56668026  2.23743464 -2.12427955 -0.11653103  0.55388708 -2.77010914
  3.06858498  2.18960154  1.18552016 -1.06648326  0.60271519  1.14432454
  1.31029899 -1.87354672  0.72965089  2.88578428 -0.54534326 -0.47522468
 -2.0265424   0.7289737   1.60512664  2.83077099 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856967 -0.67550779 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 00:12:05 2024]  Iteration number: 0 with current cost as 0.08304524024500662 and parameters 
[-4.56688056  2.23743464 -2.12427947 -0.11653103  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648317  0.60271527  1.14432454
  1.31029907 -1.87354672  0.7296508   2.88578428 -0.54534335 -0.47522477
 -2.0265424   0.7289737   1.60512664  2.83077099 -1.26456701 -0.25136096
 -2.39279209 -2.27309774  3.13337155  2.54856967 -0.67550779 -2.69002193]. 
Working on 0.8 fold... 
[Fri Apr  5 00:22:25 2024]  Iteration number: 0 with current cost as 0.07566735934656398 and parameters 
[-4.6635206   2.23743455 -2.12427947 -0.11653103  0.553887   -2.77010914
  3.0685849   2.18960145  1.18551998 -1.06648325  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897361  1.60512664  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 00:30:01 2024]  Iteration number: 0 with current cost as 0.08118796814429569 and parameters 
[-4.61868058  2.23743464 -2.12427945 -0.11653103  0.55388708 -2.77010907
  3.06858498  2.18960155  1.18552018 -1.06648308  0.6027152   1.14432455
  1.31029918 -1.87354671  0.7296509   2.88578429 -0.54534316 -0.47522466
 -2.0265424   0.7289737   1.60512664  2.83077098 -1.2645671  -0.25136095
 -2.39279218 -2.27309774  3.13337164  2.54856968 -0.67550778 -2.69002202]. 
Training complete taking 2593.3984014987946 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 00:38:38 2024]  Iteration number: 0 with current cost as 0.07705837457514815 and parameters 
[-4.66635358  2.23743464 -2.12427944 -0.11653103  0.55388708 -2.77010917
  3.06858498  2.18960155  1.18552008 -1.06648308  0.6027152   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578429 -0.54534325 -0.47522466
 -2.0265424   0.7289737   1.60512664  2.83077097 -1.264567   -0.25136095
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 00:46:01 2024]  Iteration number: 0 with current cost as 0.08738631662889126 and parameters 
[-4.56668026  2.23743464 -2.12427955 -0.11653103  0.55388708 -2.77010914
  3.06858498  2.18960154  1.18552016 -1.06648326  0.60271519  1.14432454
  1.31029899 -1.87354672  0.72965089  2.88578428 -0.54534326 -0.47522468
 -2.0265424   0.7289737   1.60512664  2.83077099 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856967 -0.67550779 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 00:55:16 2024]  Iteration number: 0 with current cost as 0.08304524024500662 and parameters 
[-4.56688056  2.23743464 -2.12427947 -0.11653103  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648317  0.60271527  1.14432454
  1.31029907 -1.87354672  0.7296508   2.88578428 -0.54534335 -0.47522477
 -2.0265424   0.7289737   1.60512664  2.83077099 -1.26456701 -0.25136096
 -2.39279209 -2.27309774  3.13337155  2.54856967 -0.67550779 -2.69002193]. 
Working on 0.8 fold... 
[Fri Apr  5 01:05:29 2024]  Iteration number: 0 with current cost as 0.07566735934656398 and parameters 
[-4.6635206   2.23743455 -2.12427947 -0.11653103  0.553887   -2.77010914
  3.0685849   2.18960145  1.18551998 -1.06648325  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897361  1.60512664  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 01:12:50 2024]  Iteration number: 0 with current cost as 0.08118796814429569 and parameters 
[-4.61868058  2.23743464 -2.12427945 -0.11653103  0.55388708 -2.77010907
  3.06858498  2.18960155  1.18552018 -1.06648308  0.6027152   1.14432455
  1.31029918 -1.87354671  0.7296509   2.88578429 -0.54534316 -0.47522466
 -2.0265424   0.7289737   1.60512664  2.83077098 -1.2645671  -0.25136095
 -2.39279218 -2.27309774  3.13337164  2.54856968 -0.67550778 -2.69002202]. 
Training complete taking 2565.3729400634766 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 01:21:12 2024]  Iteration number: 0 with current cost as 0.07705837457514815 and parameters 
[-4.66635358  2.23743464 -2.12427944 -0.11653103  0.55388708 -2.77010917
  3.06858498  2.18960155  1.18552008 -1.06648308  0.6027152   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578429 -0.54534325 -0.47522466
 -2.0265424   0.7289737   1.60512664  2.83077097 -1.264567   -0.25136095
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 01:28:30 2024]  Iteration number: 0 with current cost as 0.08738631662889126 and parameters 
[-4.56668026  2.23743464 -2.12427955 -0.11653103  0.55388708 -2.77010914
  3.06858498  2.18960154  1.18552016 -1.06648326  0.60271519  1.14432454
  1.31029899 -1.87354672  0.72965089  2.88578428 -0.54534326 -0.47522468
 -2.0265424   0.7289737   1.60512664  2.83077099 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856967 -0.67550779 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 01:37:42 2024]  Iteration number: 0 with current cost as 0.08304524024500662 and parameters 
[-4.56688056  2.23743464 -2.12427947 -0.11653103  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648317  0.60271527  1.14432454
  1.31029907 -1.87354672  0.7296508   2.88578428 -0.54534335 -0.47522477
 -2.0265424   0.7289737   1.60512664  2.83077099 -1.26456701 -0.25136096
 -2.39279209 -2.27309774  3.13337155  2.54856967 -0.67550779 -2.69002193]. 
Working on 0.8 fold... 
[Fri Apr  5 01:48:02 2024]  Iteration number: 0 with current cost as 0.07566735934656398 and parameters 
[-4.6635206   2.23743455 -2.12427947 -0.11653103  0.553887   -2.77010914
  3.0685849   2.18960145  1.18551998 -1.06648325  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897361  1.60512664  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 01:55:36 2024]  Iteration number: 0 with current cost as 0.08118796814429569 and parameters 
[-4.61868058  2.23743464 -2.12427945 -0.11653103  0.55388708 -2.77010907
  3.06858498  2.18960155  1.18552018 -1.06648308  0.6027152   1.14432455
  1.31029918 -1.87354671  0.7296509   2.88578429 -0.54534316 -0.47522466
 -2.0265424   0.7289737   1.60512664  2.83077098 -1.2645671  -0.25136095
 -2.39279218 -2.27309774  3.13337164  2.54856968 -0.67550778 -2.69002202]. 
Training complete taking 2565.864056110382 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 02:04:00 2024]  Iteration number: 0 with current cost as 0.07705837457514815 and parameters 
[-4.66635358  2.23743464 -2.12427944 -0.11653103  0.55388708 -2.77010917
  3.06858498  2.18960155  1.18552008 -1.06648308  0.6027152   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578429 -0.54534325 -0.47522466
 -2.0265424   0.7289737   1.60512664  2.83077097 -1.264567   -0.25136095
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 02:11:28 2024]  Iteration number: 0 with current cost as 0.08738631662889126 and parameters 
[-4.56668026  2.23743464 -2.12427955 -0.11653103  0.55388708 -2.77010914
  3.06858498  2.18960154  1.18552016 -1.06648326  0.60271519  1.14432454
  1.31029899 -1.87354672  0.72965089  2.88578428 -0.54534326 -0.47522468
 -2.0265424   0.7289737   1.60512664  2.83077099 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856967 -0.67550779 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 02:20:36 2024]  Iteration number: 0 with current cost as 0.08304524024500662 and parameters 
[-4.56688056  2.23743464 -2.12427947 -0.11653103  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648317  0.60271527  1.14432454
  1.31029907 -1.87354672  0.7296508   2.88578428 -0.54534335 -0.47522477
 -2.0265424   0.7289737   1.60512664  2.83077099 -1.26456701 -0.25136096
 -2.39279209 -2.27309774  3.13337155  2.54856967 -0.67550779 -2.69002193]. 
Working on 0.8 fold... 
[Fri Apr  5 02:30:37 2024]  Iteration number: 0 with current cost as 0.07566735934656398 and parameters 
[-4.6635206   2.23743455 -2.12427947 -0.11653103  0.553887   -2.77010914
  3.0685849   2.18960145  1.18551998 -1.06648325  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897361  1.60512664  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 02:37:54 2024]  Iteration number: 0 with current cost as 0.08118796814429569 and parameters 
[-4.61868058  2.23743464 -2.12427945 -0.11653103  0.55388708 -2.77010907
  3.06858498  2.18960155  1.18552018 -1.06648308  0.6027152   1.14432455
  1.31029918 -1.87354671  0.7296509   2.88578429 -0.54534316 -0.47522466
 -2.0265424   0.7289737   1.60512664  2.83077098 -1.2645671  -0.25136095
 -2.39279218 -2.27309774  3.13337164  2.54856968 -0.67550778 -2.69002202]. 
Training complete taking 2538.2844939231873 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 02:46:07 2024]  Iteration number: 0 with current cost as 0.07705837457514815 and parameters 
[-4.66635358  2.23743464 -2.12427944 -0.11653103  0.55388708 -2.77010917
  3.06858498  2.18960155  1.18552008 -1.06648308  0.6027152   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578429 -0.54534325 -0.47522466
 -2.0265424   0.7289737   1.60512664  2.83077097 -1.264567   -0.25136095
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 02:53:28 2024]  Iteration number: 0 with current cost as 0.08738631662889126 and parameters 
[-4.56668026  2.23743464 -2.12427955 -0.11653103  0.55388708 -2.77010914
  3.06858498  2.18960154  1.18552016 -1.06648326  0.60271519  1.14432454
  1.31029899 -1.87354672  0.72965089  2.88578428 -0.54534326 -0.47522468
 -2.0265424   0.7289737   1.60512664  2.83077099 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856967 -0.67550779 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 03:02:46 2024]  Iteration number: 0 with current cost as 0.08304524024500662 and parameters 
[-4.56688056  2.23743464 -2.12427947 -0.11653103  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648317  0.60271527  1.14432454
  1.31029907 -1.87354672  0.7296508   2.88578428 -0.54534335 -0.47522477
 -2.0265424   0.7289737   1.60512664  2.83077099 -1.26456701 -0.25136096
 -2.39279209 -2.27309774  3.13337155  2.54856967 -0.67550779 -2.69002193]. 
Working on 0.8 fold... 
[Fri Apr  5 03:13:08 2024]  Iteration number: 0 with current cost as 0.07566735934656398 and parameters 
[-4.6635206   2.23743455 -2.12427947 -0.11653103  0.553887   -2.77010914
  3.0685849   2.18960145  1.18551998 -1.06648325  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897361  1.60512664  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 03:20:30 2024]  Iteration number: 0 with current cost as 0.08118796814429569 and parameters 
[-4.61868058  2.23743464 -2.12427945 -0.11653103  0.55388708 -2.77010907
  3.06858498  2.18960155  1.18552018 -1.06648308  0.6027152   1.14432455
  1.31029918 -1.87354671  0.7296509   2.88578429 -0.54534316 -0.47522466
 -2.0265424   0.7289737   1.60512664  2.83077098 -1.2645671  -0.25136095
 -2.39279218 -2.27309774  3.13337164  2.54856968 -0.67550778 -2.69002202]. 
Training complete taking 2562.9549186229706 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 03:29:08 2024]  Iteration number: 0 with current cost as 0.07705837457514815 and parameters 
[-4.66635358  2.23743464 -2.12427944 -0.11653103  0.55388708 -2.77010917
  3.06858498  2.18960155  1.18552008 -1.06648308  0.6027152   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578429 -0.54534325 -0.47522466
 -2.0265424   0.7289737   1.60512664  2.83077097 -1.264567   -0.25136095
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 03:36:52 2024]  Iteration number: 0 with current cost as 0.08738631662889126 and parameters 
[-4.56668026  2.23743464 -2.12427955 -0.11653103  0.55388708 -2.77010914
  3.06858498  2.18960154  1.18552016 -1.06648326  0.60271519  1.14432454
  1.31029899 -1.87354672  0.72965089  2.88578428 -0.54534326 -0.47522468
 -2.0265424   0.7289737   1.60512664  2.83077099 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856967 -0.67550779 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 03:46:56 2024]  Iteration number: 0 with current cost as 0.08304524024500662 and parameters 
[-4.56688056  2.23743464 -2.12427947 -0.11653103  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648317  0.60271527  1.14432454
  1.31029907 -1.87354672  0.7296508   2.88578428 -0.54534335 -0.47522477
 -2.0265424   0.7289737   1.60512664  2.83077099 -1.26456701 -0.25136096
 -2.39279209 -2.27309774  3.13337155  2.54856967 -0.67550779 -2.69002193]. 
Working on 0.8 fold... 
[Fri Apr  5 03:58:05 2024]  Iteration number: 0 with current cost as 0.07566735934656398 and parameters 
[-4.6635206   2.23743455 -2.12427947 -0.11653103  0.553887   -2.77010914
  3.0685849   2.18960145  1.18551998 -1.06648325  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897361  1.60512664  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 04:06:10 2024]  Iteration number: 0 with current cost as 0.08118796814429569 and parameters 
[-4.61868058  2.23743464 -2.12427945 -0.11653103  0.55388708 -2.77010907
  3.06858498  2.18960155  1.18552018 -1.06648308  0.6027152   1.14432455
  1.31029918 -1.87354671  0.7296509   2.88578429 -0.54534316 -0.47522466
 -2.0265424   0.7289737   1.60512664  2.83077098 -1.2645671  -0.25136095
 -2.39279218 -2.27309774  3.13337164  2.54856968 -0.67550778 -2.69002202]. 
Training complete taking 2751.517381668091 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 04:15:01 2024]  Iteration number: 0 with current cost as 0.07705837457514815 and parameters 
[-4.66635358  2.23743464 -2.12427944 -0.11653103  0.55388708 -2.77010917
  3.06858498  2.18960155  1.18552008 -1.06648308  0.6027152   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578429 -0.54534325 -0.47522466
 -2.0265424   0.7289737   1.60512664  2.83077097 -1.264567   -0.25136095
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 04:23:05 2024]  Iteration number: 0 with current cost as 0.08738631662889126 and parameters 
[-4.56668026  2.23743464 -2.12427955 -0.11653103  0.55388708 -2.77010914
  3.06858498  2.18960154  1.18552016 -1.06648326  0.60271519  1.14432454
  1.31029899 -1.87354672  0.72965089  2.88578428 -0.54534326 -0.47522468
 -2.0265424   0.7289737   1.60512664  2.83077099 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856967 -0.67550779 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 04:33:15 2024]  Iteration number: 0 with current cost as 0.08304524024500662 and parameters 
[-4.56688056  2.23743464 -2.12427947 -0.11653103  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648317  0.60271527  1.14432454
  1.31029907 -1.87354672  0.7296508   2.88578428 -0.54534335 -0.47522477
 -2.0265424   0.7289737   1.60512664  2.83077099 -1.26456701 -0.25136096
 -2.39279209 -2.27309774  3.13337155  2.54856967 -0.67550779 -2.69002193]. 
Working on 0.8 fold... 
[Fri Apr  5 04:44:16 2024]  Iteration number: 0 with current cost as 0.07566735934656398 and parameters 
[-4.6635206   2.23743455 -2.12427947 -0.11653103  0.553887   -2.77010914
  3.0685849   2.18960145  1.18551998 -1.06648325  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897361  1.60512664  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 04:51:56 2024]  Iteration number: 0 with current cost as 0.08118796814429569 and parameters 
[-4.61868058  2.23743464 -2.12427945 -0.11653103  0.55388708 -2.77010907
  3.06858498  2.18960155  1.18552018 -1.06648308  0.6027152   1.14432455
  1.31029918 -1.87354671  0.7296509   2.88578429 -0.54534316 -0.47522466
 -2.0265424   0.7289737   1.60512664  2.83077098 -1.2645671  -0.25136095
 -2.39279218 -2.27309774  3.13337164  2.54856968 -0.67550778 -2.69002202]. 
Training complete taking 2739.5415935516357 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 05:00:51 2024]  Iteration number: 0 with current cost as 0.07705837457514815 and parameters 
[-4.66635358  2.23743464 -2.12427944 -0.11653103  0.55388708 -2.77010917
  3.06858498  2.18960155  1.18552008 -1.06648308  0.6027152   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578429 -0.54534325 -0.47522466
 -2.0265424   0.7289737   1.60512664  2.83077097 -1.264567   -0.25136095
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 05:08:32 2024]  Iteration number: 0 with current cost as 0.08738631662889126 and parameters 
[-4.56668026  2.23743464 -2.12427955 -0.11653103  0.55388708 -2.77010914
  3.06858498  2.18960154  1.18552016 -1.06648326  0.60271519  1.14432454
  1.31029899 -1.87354672  0.72965089  2.88578428 -0.54534326 -0.47522468
 -2.0265424   0.7289737   1.60512664  2.83077099 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856967 -0.67550779 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 05:18:25 2024]  Iteration number: 0 with current cost as 0.08304524024500662 and parameters 
[-4.56688056  2.23743464 -2.12427947 -0.11653103  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648317  0.60271527  1.14432454
  1.31029907 -1.87354672  0.7296508   2.88578428 -0.54534335 -0.47522477
 -2.0265424   0.7289737   1.60512664  2.83077099 -1.26456701 -0.25136096
 -2.39279209 -2.27309774  3.13337155  2.54856967 -0.67550779 -2.69002193]. 
Working on 0.8 fold... 
[Fri Apr  5 05:29:16 2024]  Iteration number: 0 with current cost as 0.07566735934656398 and parameters 
[-4.6635206   2.23743455 -2.12427947 -0.11653103  0.553887   -2.77010914
  3.0685849   2.18960145  1.18551998 -1.06648325  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897361  1.60512664  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 05:37:09 2024]  Iteration number: 0 with current cost as 0.08118796814429569 and parameters 
[-4.61868058  2.23743464 -2.12427945 -0.11653103  0.55388708 -2.77010907
  3.06858498  2.18960155  1.18552018 -1.06648308  0.6027152   1.14432455
  1.31029918 -1.87354671  0.7296509   2.88578429 -0.54534316 -0.47522466
 -2.0265424   0.7289737   1.60512664  2.83077098 -1.2645671  -0.25136095
 -2.39279218 -2.27309774  3.13337164  2.54856968 -0.67550778 -2.69002202]. 
Training complete taking 2712.347405433655 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 05:45:52 2024]  Iteration number: 0 with current cost as 0.07705837457514815 and parameters 
[-4.66635358  2.23743464 -2.12427944 -0.11653103  0.55388708 -2.77010917
  3.06858498  2.18960155  1.18552008 -1.06648308  0.6027152   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578429 -0.54534325 -0.47522466
 -2.0265424   0.7289737   1.60512664  2.83077097 -1.264567   -0.25136095
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 05:53:51 2024]  Iteration number: 0 with current cost as 0.08738631662889126 and parameters 
[-4.56668026  2.23743464 -2.12427955 -0.11653103  0.55388708 -2.77010914
  3.06858498  2.18960154  1.18552016 -1.06648326  0.60271519  1.14432454
  1.31029899 -1.87354672  0.72965089  2.88578428 -0.54534326 -0.47522468
 -2.0265424   0.7289737   1.60512664  2.83077099 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856967 -0.67550779 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 06:03:38 2024]  Iteration number: 0 with current cost as 0.08304524024500662 and parameters 
[-4.56688056  2.23743464 -2.12427947 -0.11653103  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648317  0.60271527  1.14432454
  1.31029907 -1.87354672  0.7296508   2.88578428 -0.54534335 -0.47522477
 -2.0265424   0.7289737   1.60512664  2.83077099 -1.26456701 -0.25136096
 -2.39279209 -2.27309774  3.13337155  2.54856967 -0.67550779 -2.69002193]. 
Working on 0.8 fold... 
[Fri Apr  5 06:14:11 2024]  Iteration number: 0 with current cost as 0.07566735934656398 and parameters 
[-4.6635206   2.23743455 -2.12427947 -0.11653103  0.553887   -2.77010914
  3.0685849   2.18960145  1.18551998 -1.06648325  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897361  1.60512664  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 06:22:07 2024]  Iteration number: 0 with current cost as 0.08118796814429569 and parameters 
[-4.61868058  2.23743464 -2.12427945 -0.11653103  0.55388708 -2.77010907
  3.06858498  2.18960155  1.18552018 -1.06648308  0.6027152   1.14432455
  1.31029918 -1.87354671  0.7296509   2.88578429 -0.54534316 -0.47522466
 -2.0265424   0.7289737   1.60512664  2.83077098 -1.2645671  -0.25136095
 -2.39279218 -2.27309774  3.13337164  2.54856968 -0.67550778 -2.69002202]. 
Training complete taking 2705.8525099754333 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 06:31:04 2024]  Iteration number: 0 with current cost as 0.07705837457514815 and parameters 
[-4.66635358  2.23743464 -2.12427944 -0.11653103  0.55388708 -2.77010917
  3.06858498  2.18960155  1.18552008 -1.06648308  0.6027152   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578429 -0.54534325 -0.47522466
 -2.0265424   0.7289737   1.60512664  2.83077097 -1.264567   -0.25136095
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 06:39:06 2024]  Iteration number: 0 with current cost as 0.08738631662889126 and parameters 
[-4.56668026  2.23743464 -2.12427955 -0.11653103  0.55388708 -2.77010914
  3.06858498  2.18960154  1.18552016 -1.06648326  0.60271519  1.14432454
  1.31029899 -1.87354672  0.72965089  2.88578428 -0.54534326 -0.47522468
 -2.0265424   0.7289737   1.60512664  2.83077099 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856967 -0.67550779 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 06:48:50 2024]  Iteration number: 0 with current cost as 0.08304524024500662 and parameters 
[-4.56688056  2.23743464 -2.12427947 -0.11653103  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648317  0.60271527  1.14432454
  1.31029907 -1.87354672  0.7296508   2.88578428 -0.54534335 -0.47522477
 -2.0265424   0.7289737   1.60512664  2.83077099 -1.26456701 -0.25136096
 -2.39279209 -2.27309774  3.13337155  2.54856967 -0.67550779 -2.69002193]. 
Working on 0.8 fold... 
[Fri Apr  5 06:59:21 2024]  Iteration number: 0 with current cost as 0.07566735934656398 and parameters 
[-4.6635206   2.23743455 -2.12427947 -0.11653103  0.553887   -2.77010914
  3.0685849   2.18960145  1.18551998 -1.06648325  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897361  1.60512664  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 07:07:04 2024]  Iteration number: 0 with current cost as 0.08118796814429569 and parameters 
[-4.61868058  2.23743464 -2.12427945 -0.11653103  0.55388708 -2.77010907
  3.06858498  2.18960155  1.18552018 -1.06648308  0.6027152   1.14432455
  1.31029918 -1.87354671  0.7296509   2.88578429 -0.54534316 -0.47522466
 -2.0265424   0.7289737   1.60512664  2.83077098 -1.2645671  -0.25136095
 -2.39279218 -2.27309774  3.13337164  2.54856968 -0.67550778 -2.69002202]. 
Training complete taking 2687.894056081772 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 07:15:46 2024]  Iteration number: 0 with current cost as 0.07705837457514815 and parameters 
[-4.66635358  2.23743464 -2.12427944 -0.11653103  0.55388708 -2.77010917
  3.06858498  2.18960155  1.18552008 -1.06648308  0.6027152   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578429 -0.54534325 -0.47522466
 -2.0265424   0.7289737   1.60512664  2.83077097 -1.264567   -0.25136095
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 07:23:43 2024]  Iteration number: 0 with current cost as 0.08738631662889126 and parameters 
[-4.56668026  2.23743464 -2.12427955 -0.11653103  0.55388708 -2.77010914
  3.06858498  2.18960154  1.18552016 -1.06648326  0.60271519  1.14432454
  1.31029899 -1.87354672  0.72965089  2.88578428 -0.54534326 -0.47522468
 -2.0265424   0.7289737   1.60512664  2.83077099 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856967 -0.67550779 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 07:33:29 2024]  Iteration number: 0 with current cost as 0.08304524024500662 and parameters 
[-4.56688056  2.23743464 -2.12427947 -0.11653103  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648317  0.60271527  1.14432454
  1.31029907 -1.87354672  0.7296508   2.88578428 -0.54534335 -0.47522477
 -2.0265424   0.7289737   1.60512664  2.83077099 -1.26456701 -0.25136096
 -2.39279209 -2.27309774  3.13337155  2.54856967 -0.67550779 -2.69002193]. 
Working on 0.8 fold... 
[Fri Apr  5 07:44:05 2024]  Iteration number: 0 with current cost as 0.07566735934656398 and parameters 
[-4.6635206   2.23743455 -2.12427947 -0.11653103  0.553887   -2.77010914
  3.0685849   2.18960145  1.18551998 -1.06648325  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897361  1.60512664  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 07:51:49 2024]  Iteration number: 0 with current cost as 0.08118796814429569 and parameters 
[-4.61868058  2.23743464 -2.12427945 -0.11653103  0.55388708 -2.77010907
  3.06858498  2.18960155  1.18552018 -1.06648308  0.6027152   1.14432455
  1.31029918 -1.87354671  0.7296509   2.88578429 -0.54534316 -0.47522466
 -2.0265424   0.7289737   1.60512664  2.83077098 -1.2645671  -0.25136095
 -2.39279218 -2.27309774  3.13337164  2.54856968 -0.67550778 -2.69002202]. 
Training complete taking 2686.672911167145 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 08:00:34 2024]  Iteration number: 0 with current cost as 0.07705837457514815 and parameters 
[-4.66635358  2.23743464 -2.12427944 -0.11653103  0.55388708 -2.77010917
  3.06858498  2.18960155  1.18552008 -1.06648308  0.6027152   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578429 -0.54534325 -0.47522466
 -2.0265424   0.7289737   1.60512664  2.83077097 -1.264567   -0.25136095
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 08:07:54 2024]  Iteration number: 0 with current cost as 0.08738631662889126 and parameters 
[-4.56668026  2.23743464 -2.12427955 -0.11653103  0.55388708 -2.77010914
  3.06858498  2.18960154  1.18552016 -1.06648326  0.60271519  1.14432454
  1.31029899 -1.87354672  0.72965089  2.88578428 -0.54534326 -0.47522468
 -2.0265424   0.7289737   1.60512664  2.83077099 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856967 -0.67550779 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 08:17:03 2024]  Iteration number: 0 with current cost as 0.08304524024500662 and parameters 
[-4.56688056  2.23743464 -2.12427947 -0.11653103  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648317  0.60271527  1.14432454
  1.31029907 -1.87354672  0.7296508   2.88578428 -0.54534335 -0.47522477
 -2.0265424   0.7289737   1.60512664  2.83077099 -1.26456701 -0.25136096
 -2.39279209 -2.27309774  3.13337155  2.54856967 -0.67550779 -2.69002193]. 
Working on 0.8 fold... 
[Fri Apr  5 08:27:06 2024]  Iteration number: 0 with current cost as 0.07566735934656398 and parameters 
[-4.6635206   2.23743455 -2.12427947 -0.11653103  0.553887   -2.77010914
  3.0685849   2.18960145  1.18551998 -1.06648325  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897361  1.60512664  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 08:34:07 2024]  Iteration number: 0 with current cost as 0.08118796814429569 and parameters 
[-4.61868058  2.23743464 -2.12427945 -0.11653103  0.55388708 -2.77010907
  3.06858498  2.18960155  1.18552018 -1.06648308  0.6027152   1.14432455
  1.31029918 -1.87354671  0.7296509   2.88578429 -0.54534316 -0.47522466
 -2.0265424   0.7289737   1.60512664  2.83077098 -1.2645671  -0.25136095
 -2.39279218 -2.27309774  3.13337164  2.54856968 -0.67550778 -2.69002202]. 
Training complete taking 2516.769040107727 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 08:42:02 2024]  Iteration number: 0 with current cost as 0.07705837457514815 and parameters 
[-4.66635358  2.23743464 -2.12427944 -0.11653103  0.55388708 -2.77010917
  3.06858498  2.18960155  1.18552008 -1.06648308  0.6027152   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578429 -0.54534325 -0.47522466
 -2.0265424   0.7289737   1.60512664  2.83077097 -1.264567   -0.25136095
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 08:49:02 2024]  Iteration number: 0 with current cost as 0.08738631662889126 and parameters 
[-4.56668026  2.23743464 -2.12427955 -0.11653103  0.55388708 -2.77010914
  3.06858498  2.18960154  1.18552016 -1.06648326  0.60271519  1.14432454
  1.31029899 -1.87354672  0.72965089  2.88578428 -0.54534326 -0.47522468
 -2.0265424   0.7289737   1.60512664  2.83077099 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856967 -0.67550779 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 08:57:52 2024]  Iteration number: 0 with current cost as 0.08304524024500662 and parameters 
[-4.56688056  2.23743464 -2.12427947 -0.11653103  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648317  0.60271527  1.14432454
  1.31029907 -1.87354672  0.7296508   2.88578428 -0.54534335 -0.47522477
 -2.0265424   0.7289737   1.60512664  2.83077099 -1.26456701 -0.25136096
 -2.39279209 -2.27309774  3.13337155  2.54856967 -0.67550779 -2.69002193]. 
Working on 0.8 fold... 
[Fri Apr  5 09:07:45 2024]  Iteration number: 0 with current cost as 0.07566735934656398 and parameters 
[-4.6635206   2.23743455 -2.12427947 -0.11653103  0.553887   -2.77010914
  3.0685849   2.18960145  1.18551998 -1.06648325  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897361  1.60512664  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 09:14:52 2024]  Iteration number: 0 with current cost as 0.08118796814429569 and parameters 
[-4.61868058  2.23743464 -2.12427945 -0.11653103  0.55388708 -2.77010907
  3.06858498  2.18960155  1.18552018 -1.06648308  0.6027152   1.14432455
  1.31029918 -1.87354671  0.7296509   2.88578429 -0.54534316 -0.47522466
 -2.0265424   0.7289737   1.60512664  2.83077098 -1.2645671  -0.25136095
 -2.39279218 -2.27309774  3.13337164  2.54856968 -0.67550778 -2.69002202]. 
Training complete taking 2453.645942926407 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 09:23:12 2024]  Iteration number: 0 with current cost as 0.07705837457514815 and parameters 
[-4.66635358  2.23743464 -2.12427944 -0.11653103  0.55388708 -2.77010917
  3.06858498  2.18960155  1.18552008 -1.06648308  0.6027152   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578429 -0.54534325 -0.47522466
 -2.0265424   0.7289737   1.60512664  2.83077097 -1.264567   -0.25136095
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 09:30:30 2024]  Iteration number: 0 with current cost as 0.08738631662889126 and parameters 
[-4.56668026  2.23743464 -2.12427955 -0.11653103  0.55388708 -2.77010914
  3.06858498  2.18960154  1.18552016 -1.06648326  0.60271519  1.14432454
  1.31029899 -1.87354672  0.72965089  2.88578428 -0.54534326 -0.47522468
 -2.0265424   0.7289737   1.60512664  2.83077099 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856967 -0.67550779 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 09:39:28 2024]  Iteration number: 0 with current cost as 0.08304524024500662 and parameters 
[-4.56688056  2.23743464 -2.12427947 -0.11653103  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648317  0.60271527  1.14432454
  1.31029907 -1.87354672  0.7296508   2.88578428 -0.54534335 -0.47522477
 -2.0265424   0.7289737   1.60512664  2.83077099 -1.26456701 -0.25136096
 -2.39279209 -2.27309774  3.13337155  2.54856967 -0.67550779 -2.69002193]. 
Working on 0.8 fold... 
[Fri Apr  5 09:49:33 2024]  Iteration number: 0 with current cost as 0.07566735934656398 and parameters 
[-4.6635206   2.23743455 -2.12427947 -0.11653103  0.553887   -2.77010914
  3.0685849   2.18960145  1.18551998 -1.06648325  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897361  1.60512664  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 09:56:51 2024]  Iteration number: 0 with current cost as 0.08118796814429569 and parameters 
[-4.61868058  2.23743464 -2.12427945 -0.11653103  0.55388708 -2.77010907
  3.06858498  2.18960155  1.18552018 -1.06648308  0.6027152   1.14432455
  1.31029918 -1.87354671  0.7296509   2.88578429 -0.54534316 -0.47522466
 -2.0265424   0.7289737   1.60512664  2.83077098 -1.2645671  -0.25136095
 -2.39279218 -2.27309774  3.13337164  2.54856968 -0.67550778 -2.69002202]. 
Training complete taking 2514.692995786667 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 10:04:59 2024]  Iteration number: 0 with current cost as 0.07705837457514815 and parameters 
[-4.66635358  2.23743464 -2.12427944 -0.11653103  0.55388708 -2.77010917
  3.06858498  2.18960155  1.18552008 -1.06648308  0.6027152   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578429 -0.54534325 -0.47522466
 -2.0265424   0.7289737   1.60512664  2.83077097 -1.264567   -0.25136095
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 10:12:09 2024]  Iteration number: 0 with current cost as 0.08738631662889126 and parameters 
[-4.56668026  2.23743464 -2.12427955 -0.11653103  0.55388708 -2.77010914
  3.06858498  2.18960154  1.18552016 -1.06648326  0.60271519  1.14432454
  1.31029899 -1.87354672  0.72965089  2.88578428 -0.54534326 -0.47522468
 -2.0265424   0.7289737   1.60512664  2.83077099 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856967 -0.67550779 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 10:21:02 2024]  Iteration number: 0 with current cost as 0.08304524024500662 and parameters 
[-4.56688056  2.23743464 -2.12427947 -0.11653103  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648317  0.60271527  1.14432454
  1.31029907 -1.87354672  0.7296508   2.88578428 -0.54534335 -0.47522477
 -2.0265424   0.7289737   1.60512664  2.83077099 -1.26456701 -0.25136096
 -2.39279209 -2.27309774  3.13337155  2.54856967 -0.67550779 -2.69002193]. 
Working on 0.8 fold... 
[Fri Apr  5 10:31:06 2024]  Iteration number: 0 with current cost as 0.07566735934656398 and parameters 
[-4.6635206   2.23743455 -2.12427947 -0.11653103  0.553887   -2.77010914
  3.0685849   2.18960145  1.18551998 -1.06648325  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897361  1.60512664  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 10:38:16 2024]  Iteration number: 0 with current cost as 0.08118796814429569 and parameters 
[-4.61868058  2.23743464 -2.12427945 -0.11653103  0.55388708 -2.77010907
  3.06858498  2.18960155  1.18552018 -1.06648308  0.6027152   1.14432455
  1.31029918 -1.87354671  0.7296509   2.88578429 -0.54534316 -0.47522466
 -2.0265424   0.7289737   1.60512664  2.83077098 -1.2645671  -0.25136095
 -2.39279218 -2.27309774  3.13337164  2.54856968 -0.67550778 -2.69002202]. 
Training complete taking 2482.3740000724792 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 10:46:17 2024]  Iteration number: 0 with current cost as 0.07705837457514815 and parameters 
[-4.66635358  2.23743464 -2.12427944 -0.11653103  0.55388708 -2.77010917
  3.06858498  2.18960155  1.18552008 -1.06648308  0.6027152   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578429 -0.54534325 -0.47522466
 -2.0265424   0.7289737   1.60512664  2.83077097 -1.264567   -0.25136095
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 10:53:21 2024]  Iteration number: 0 with current cost as 0.08738631662889126 and parameters 
[-4.56668026  2.23743464 -2.12427955 -0.11653103  0.55388708 -2.77010914
  3.06858498  2.18960154  1.18552016 -1.06648326  0.60271519  1.14432454
  1.31029899 -1.87354672  0.72965089  2.88578428 -0.54534326 -0.47522468
 -2.0265424   0.7289737   1.60512664  2.83077099 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856967 -0.67550779 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 11:02:11 2024]  Iteration number: 0 with current cost as 0.08304524024500662 and parameters 
[-4.56688056  2.23743464 -2.12427947 -0.11653103  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648317  0.60271527  1.14432454
  1.31029907 -1.87354672  0.7296508   2.88578428 -0.54534335 -0.47522477
 -2.0265424   0.7289737   1.60512664  2.83077099 -1.26456701 -0.25136096
 -2.39279209 -2.27309774  3.13337155  2.54856967 -0.67550779 -2.69002193]. 
Working on 0.8 fold... 
[Fri Apr  5 11:12:07 2024]  Iteration number: 0 with current cost as 0.07566735934656398 and parameters 
[-4.6635206   2.23743455 -2.12427947 -0.11653103  0.553887   -2.77010914
  3.0685849   2.18960145  1.18551998 -1.06648325  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897361  1.60512664  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 11:19:20 2024]  Iteration number: 0 with current cost as 0.08118796814429569 and parameters 
[-4.61868058  2.23743464 -2.12427945 -0.11653103  0.55388708 -2.77010907
  3.06858498  2.18960155  1.18552018 -1.06648308  0.6027152   1.14432455
  1.31029918 -1.87354671  0.7296509   2.88578429 -0.54534316 -0.47522466
 -2.0265424   0.7289737   1.60512664  2.83077098 -1.2645671  -0.25136095
 -2.39279218 -2.27309774  3.13337164  2.54856968 -0.67550778 -2.69002202]. 
Training complete taking 2467.2220437526703 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 11:27:34 2024]  Iteration number: 0 with current cost as 0.07705837457514815 and parameters 
[-4.66635358  2.23743464 -2.12427944 -0.11653103  0.55388708 -2.77010917
  3.06858498  2.18960155  1.18552008 -1.06648308  0.6027152   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578429 -0.54534325 -0.47522466
 -2.0265424   0.7289737   1.60512664  2.83077097 -1.264567   -0.25136095
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 11:34:48 2024]  Iteration number: 0 with current cost as 0.08738631662889126 and parameters 
[-4.56668026  2.23743464 -2.12427955 -0.11653103  0.55388708 -2.77010914
  3.06858498  2.18960154  1.18552016 -1.06648326  0.60271519  1.14432454
  1.31029899 -1.87354672  0.72965089  2.88578428 -0.54534326 -0.47522468
 -2.0265424   0.7289737   1.60512664  2.83077099 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856967 -0.67550779 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 11:43:45 2024]  Iteration number: 0 with current cost as 0.08304524024500662 and parameters 
[-4.56688056  2.23743464 -2.12427947 -0.11653103  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648317  0.60271527  1.14432454
  1.31029907 -1.87354672  0.7296508   2.88578428 -0.54534335 -0.47522477
 -2.0265424   0.7289737   1.60512664  2.83077099 -1.26456701 -0.25136096
 -2.39279209 -2.27309774  3.13337155  2.54856967 -0.67550779 -2.69002193]. 
Working on 0.8 fold... 
[Fri Apr  5 11:53:38 2024]  Iteration number: 0 with current cost as 0.07566735934656398 and parameters 
[-4.6635206   2.23743455 -2.12427947 -0.11653103  0.553887   -2.77010914
  3.0685849   2.18960145  1.18551998 -1.06648325  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897361  1.60512664  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 12:00:41 2024]  Iteration number: 0 with current cost as 0.08118796814429569 and parameters 
[-4.61868058  2.23743464 -2.12427945 -0.11653103  0.55388708 -2.77010907
  3.06858498  2.18960155  1.18552018 -1.06648308  0.6027152   1.14432455
  1.31029918 -1.87354671  0.7296509   2.88578429 -0.54534316 -0.47522466
 -2.0265424   0.7289737   1.60512664  2.83077098 -1.2645671  -0.25136095
 -2.39279218 -2.27309774  3.13337164  2.54856968 -0.67550778 -2.69002202]. 
Training complete taking 2479.7529611587524 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 12:08:43 2024]  Iteration number: 0 with current cost as 0.07705837457514815 and parameters 
[-4.66635358  2.23743464 -2.12427944 -0.11653103  0.55388708 -2.77010917
  3.06858498  2.18960155  1.18552008 -1.06648308  0.6027152   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578429 -0.54534325 -0.47522466
 -2.0265424   0.7289737   1.60512664  2.83077097 -1.264567   -0.25136095
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 12:16:00 2024]  Iteration number: 0 with current cost as 0.08738631662889126 and parameters 
[-4.56668026  2.23743464 -2.12427955 -0.11653103  0.55388708 -2.77010914
  3.06858498  2.18960154  1.18552016 -1.06648326  0.60271519  1.14432454
  1.31029899 -1.87354672  0.72965089  2.88578428 -0.54534326 -0.47522468
 -2.0265424   0.7289737   1.60512664  2.83077099 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856967 -0.67550779 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 12:24:57 2024]  Iteration number: 0 with current cost as 0.08304524024500662 and parameters 
[-4.56688056  2.23743464 -2.12427947 -0.11653103  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648317  0.60271527  1.14432454
  1.31029907 -1.87354672  0.7296508   2.88578428 -0.54534335 -0.47522477
 -2.0265424   0.7289737   1.60512664  2.83077099 -1.26456701 -0.25136096
 -2.39279209 -2.27309774  3.13337155  2.54856967 -0.67550779 -2.69002193]. 
Working on 0.8 fold... 
[Fri Apr  5 12:35:00 2024]  Iteration number: 0 with current cost as 0.07566735934656398 and parameters 
[-4.6635206   2.23743455 -2.12427947 -0.11653103  0.553887   -2.77010914
  3.0685849   2.18960145  1.18551998 -1.06648325  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897361  1.60512664  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 12:42:08 2024]  Iteration number: 0 with current cost as 0.08118796814429569 and parameters 
[-4.61868058  2.23743464 -2.12427945 -0.11653103  0.55388708 -2.77010907
  3.06858498  2.18960155  1.18552018 -1.06648308  0.6027152   1.14432455
  1.31029918 -1.87354671  0.7296509   2.88578429 -0.54534316 -0.47522466
 -2.0265424   0.7289737   1.60512664  2.83077098 -1.2645671  -0.25136095
 -2.39279218 -2.27309774  3.13337164  2.54856968 -0.67550778 -2.69002202]. 
Training complete taking 2483.316971540451 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 12:50:05 2024]  Iteration number: 0 with current cost as 0.07705837457514815 and parameters 
[-4.66635358  2.23743464 -2.12427944 -0.11653103  0.55388708 -2.77010917
  3.06858498  2.18960155  1.18552008 -1.06648308  0.6027152   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578429 -0.54534325 -0.47522466
 -2.0265424   0.7289737   1.60512664  2.83077097 -1.264567   -0.25136095
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 12:57:27 2024]  Iteration number: 0 with current cost as 0.08738631662889126 and parameters 
[-4.56668026  2.23743464 -2.12427955 -0.11653103  0.55388708 -2.77010914
  3.06858498  2.18960154  1.18552016 -1.06648326  0.60271519  1.14432454
  1.31029899 -1.87354672  0.72965089  2.88578428 -0.54534326 -0.47522468
 -2.0265424   0.7289737   1.60512664  2.83077099 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856967 -0.67550779 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 13:06:34 2024]  Iteration number: 0 with current cost as 0.08304524024500662 and parameters 
[-4.56688056  2.23743464 -2.12427947 -0.11653103  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648317  0.60271527  1.14432454
  1.31029907 -1.87354672  0.7296508   2.88578428 -0.54534335 -0.47522477
 -2.0265424   0.7289737   1.60512664  2.83077099 -1.26456701 -0.25136096
 -2.39279209 -2.27309774  3.13337155  2.54856967 -0.67550779 -2.69002193]. 
Working on 0.8 fold... 
[Fri Apr  5 13:16:25 2024]  Iteration number: 0 with current cost as 0.07566735934656398 and parameters 
[-4.6635206   2.23743455 -2.12427947 -0.11653103  0.553887   -2.77010914
  3.0685849   2.18960145  1.18551998 -1.06648325  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897361  1.60512664  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 13:23:34 2024]  Iteration number: 0 with current cost as 0.08118796814429569 and parameters 
[-4.61868058  2.23743464 -2.12427945 -0.11653103  0.55388708 -2.77010907
  3.06858498  2.18960155  1.18552018 -1.06648308  0.6027152   1.14432455
  1.31029918 -1.87354671  0.7296509   2.88578429 -0.54534316 -0.47522466
 -2.0265424   0.7289737   1.60512664  2.83077098 -1.2645671  -0.25136095
 -2.39279218 -2.27309774  3.13337164  2.54856968 -0.67550778 -2.69002202]. 
Training complete taking 2483.690937280655 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 13:31:38 2024]  Iteration number: 0 with current cost as 0.07705837457514815 and parameters 
[-4.66635358  2.23743464 -2.12427944 -0.11653103  0.55388708 -2.77010917
  3.06858498  2.18960155  1.18552008 -1.06648308  0.6027152   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578429 -0.54534325 -0.47522466
 -2.0265424   0.7289737   1.60512664  2.83077097 -1.264567   -0.25136095
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 13:38:46 2024]  Iteration number: 0 with current cost as 0.08738631662889126 and parameters 
[-4.56668026  2.23743464 -2.12427955 -0.11653103  0.55388708 -2.77010914
  3.06858498  2.18960154  1.18552016 -1.06648326  0.60271519  1.14432454
  1.31029899 -1.87354672  0.72965089  2.88578428 -0.54534326 -0.47522468
 -2.0265424   0.7289737   1.60512664  2.83077099 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856967 -0.67550779 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 13:47:43 2024]  Iteration number: 0 with current cost as 0.08304524024500662 and parameters 
[-4.56688056  2.23743464 -2.12427947 -0.11653103  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648317  0.60271527  1.14432454
  1.31029907 -1.87354672  0.7296508   2.88578428 -0.54534335 -0.47522477
 -2.0265424   0.7289737   1.60512664  2.83077099 -1.26456701 -0.25136096
 -2.39279209 -2.27309774  3.13337155  2.54856967 -0.67550779 -2.69002193]. 
Working on 0.8 fold... 
[Fri Apr  5 13:57:30 2024]  Iteration number: 0 with current cost as 0.07566735934656398 and parameters 
[-4.6635206   2.23743455 -2.12427947 -0.11653103  0.553887   -2.77010914
  3.0685849   2.18960145  1.18551998 -1.06648325  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897361  1.60512664  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 14:04:31 2024]  Iteration number: 0 with current cost as 0.08118796814429569 and parameters 
[-4.61868058  2.23743464 -2.12427945 -0.11653103  0.55388708 -2.77010907
  3.06858498  2.18960155  1.18552018 -1.06648308  0.6027152   1.14432455
  1.31029918 -1.87354671  0.7296509   2.88578429 -0.54534316 -0.47522466
 -2.0265424   0.7289737   1.60512664  2.83077098 -1.2645671  -0.25136095
 -2.39279218 -2.27309774  3.13337164  2.54856968 -0.67550778 -2.69002202]. 
Training complete taking 2456.3812408447266 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 14:12:39 2024]  Iteration number: 0 with current cost as 0.07705837457514815 and parameters 
[-4.66635358  2.23743464 -2.12427944 -0.11653103  0.55388708 -2.77010917
  3.06858498  2.18960155  1.18552008 -1.06648308  0.6027152   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578429 -0.54534325 -0.47522466
 -2.0265424   0.7289737   1.60512664  2.83077097 -1.264567   -0.25136095
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 14:19:48 2024]  Iteration number: 0 with current cost as 0.08738631662889126 and parameters 
[-4.56668026  2.23743464 -2.12427955 -0.11653103  0.55388708 -2.77010914
  3.06858498  2.18960154  1.18552016 -1.06648326  0.60271519  1.14432454
  1.31029899 -1.87354672  0.72965089  2.88578428 -0.54534326 -0.47522468
 -2.0265424   0.7289737   1.60512664  2.83077099 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856967 -0.67550779 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 14:28:44 2024]  Iteration number: 0 with current cost as 0.08304524024500662 and parameters 
[-4.56688056  2.23743464 -2.12427947 -0.11653103  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648317  0.60271527  1.14432454
  1.31029907 -1.87354672  0.7296508   2.88578428 -0.54534335 -0.47522477
 -2.0265424   0.7289737   1.60512664  2.83077099 -1.26456701 -0.25136096
 -2.39279209 -2.27309774  3.13337155  2.54856967 -0.67550779 -2.69002193]. 
Working on 0.8 fold... 
[Fri Apr  5 14:38:29 2024]  Iteration number: 0 with current cost as 0.07566735934656398 and parameters 
[-4.6635206   2.23743455 -2.12427947 -0.11653103  0.553887   -2.77010914
  3.0685849   2.18960145  1.18551998 -1.06648325  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897361  1.60512664  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 14:45:37 2024]  Iteration number: 0 with current cost as 0.08118796814429569 and parameters 
[-4.61868058  2.23743464 -2.12427945 -0.11653103  0.55388708 -2.77010907
  3.06858498  2.18960155  1.18552018 -1.06648308  0.6027152   1.14432455
  1.31029918 -1.87354671  0.7296509   2.88578429 -0.54534316 -0.47522466
 -2.0265424   0.7289737   1.60512664  2.83077098 -1.2645671  -0.25136095
 -2.39279218 -2.27309774  3.13337164  2.54856968 -0.67550778 -2.69002202]. 
Training complete taking 2470.220894098282 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 14:53:35 2024]  Iteration number: 0 with current cost as 0.07705837457514815 and parameters 
[-4.66635358  2.23743464 -2.12427944 -0.11653103  0.55388708 -2.77010917
  3.06858498  2.18960155  1.18552008 -1.06648308  0.6027152   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578429 -0.54534325 -0.47522466
 -2.0265424   0.7289737   1.60512664  2.83077097 -1.264567   -0.25136095
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 15:00:42 2024]  Iteration number: 0 with current cost as 0.08738631662889126 and parameters 
[-4.56668026  2.23743464 -2.12427955 -0.11653103  0.55388708 -2.77010914
  3.06858498  2.18960154  1.18552016 -1.06648326  0.60271519  1.14432454
  1.31029899 -1.87354672  0.72965089  2.88578428 -0.54534326 -0.47522468
 -2.0265424   0.7289737   1.60512664  2.83077099 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856967 -0.67550779 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 15:09:40 2024]  Iteration number: 0 with current cost as 0.08304524024500662 and parameters 
[-4.56688056  2.23743464 -2.12427947 -0.11653103  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648317  0.60271527  1.14432454
  1.31029907 -1.87354672  0.7296508   2.88578428 -0.54534335 -0.47522477
 -2.0265424   0.7289737   1.60512664  2.83077099 -1.26456701 -0.25136096
 -2.39279209 -2.27309774  3.13337155  2.54856967 -0.67550779 -2.69002193]. 
Working on 0.8 fold... 
[Fri Apr  5 15:19:28 2024]  Iteration number: 0 with current cost as 0.07566735934656398 and parameters 
[-4.6635206   2.23743455 -2.12427947 -0.11653103  0.553887   -2.77010914
  3.0685849   2.18960145  1.18551998 -1.06648325  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897361  1.60512664  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 15:26:33 2024]  Iteration number: 0 with current cost as 0.08118796814429569 and parameters 
[-4.61868058  2.23743464 -2.12427945 -0.11653103  0.55388708 -2.77010907
  3.06858498  2.18960155  1.18552018 -1.06648308  0.6027152   1.14432455
  1.31029918 -1.87354671  0.7296509   2.88578429 -0.54534316 -0.47522466
 -2.0265424   0.7289737   1.60512664  2.83077098 -1.2645671  -0.25136095
 -2.39279218 -2.27309774  3.13337164  2.54856968 -0.67550778 -2.69002202]. 
Training complete taking 2452.837102174759 seconds. 
Discarding model... 

Training complete taking 64352.69193935394 total seconds. 
Now scoring model... 
Scoring complete taking 2.6643526554107666 seconds. 
Saved predicted values as M-A1-CZ_Full-CRZ_predicted_values.csv
Model scores: {'MSE_train': (0.13014806436291862,), 'R2_train': 0.7408445646796133, 'MAE_train': 0.29940538582445003, 'MSE_test': 0.17072722192316062, 'R2_test': 0.6815997150380037, 'MAE_test': 0.3589018713734059}. 
Saved model results as M-A1-CZ_Full-CRZ_results.json. 
