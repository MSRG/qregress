/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /scratch/j/jacobsen/gjones/sine5qubits/sine_train.bin... 
Successfully loaded /scratch/j/jacobsen/gjones/sine5qubits/sine_train.bin into X and y data. 
Loading dataset from /scratch/j/jacobsen/gjones/sine5qubits/sine_test.bin... 
Successfully loaded /scratch/j/jacobsen/gjones/sine5qubits/sine_test.bin into X and y data. 
Training model with dataset /scratch/j/jacobsen/gjones/sine5qubits/sine_train.bin 
 at time Thu Apr  4 21:37:30 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 21:39:04 2024]  Iteration number: 0 with current cost as 0.2983418006131027 and parameters 
[-1.64478195  2.23743459 -2.12427964 -0.11653094  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552003 -1.06648308  0.60271515  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Apr  4 21:41:45 2024]  Iteration number: 0 with current cost as 0.3019561857368379 and parameters 
[-1.69367212  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858493  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 21:44:05 2024]  Iteration number: 0 with current cost as 0.3049447951343181 and parameters 
[-1.55694931  2.23743453 -2.12427964 -0.11653092  0.55388708 -2.77010897
  3.06858493  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 21:46:26 2024]  Iteration number: 0 with current cost as 0.2886119492109513 and parameters 
[-1.55420648  2.23743452 -2.12427958 -0.11653097  0.55388708 -2.77010903
  3.06858493  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 21:48:54 2024]  Iteration number: 0 with current cost as 0.29636229275036075 and parameters 
[-1.57267791  2.23743454 -2.12427964 -0.11653103  0.55388703 -2.77010897
  3.06858488  2.18960145  1.18551998 -1.06648318  0.6027151   1.1443244
  1.31029894 -1.8735468 ]. 
Training complete taking 719.5030014514923 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 21:50:59 2024]  Iteration number: 0 with current cost as 0.2983418006131027 and parameters 
[-1.64478195  2.23743459 -2.12427964 -0.11653094  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552003 -1.06648308  0.60271515  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Apr  4 21:53:38 2024]  Iteration number: 0 with current cost as 0.3019561857368379 and parameters 
[-1.69367212  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858493  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 21:56:01 2024]  Iteration number: 0 with current cost as 0.3049447951343181 and parameters 
[-1.55694931  2.23743453 -2.12427964 -0.11653092  0.55388708 -2.77010897
  3.06858493  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 21:58:19 2024]  Iteration number: 0 with current cost as 0.2886119492109513 and parameters 
[-1.55420648  2.23743452 -2.12427958 -0.11653097  0.55388708 -2.77010903
  3.06858493  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 22:00:36 2024]  Iteration number: 0 with current cost as 0.29636229275036075 and parameters 
[-1.57267791  2.23743454 -2.12427964 -0.11653103  0.55388703 -2.77010897
  3.06858488  2.18960145  1.18551998 -1.06648318  0.6027151   1.1443244
  1.31029894 -1.8735468 ]. 
Training complete taking 700.2573800086975 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 22:02:39 2024]  Iteration number: 0 with current cost as 0.2983418006131027 and parameters 
[-1.64478195  2.23743459 -2.12427964 -0.11653094  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552003 -1.06648308  0.60271515  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Apr  4 22:05:20 2024]  Iteration number: 0 with current cost as 0.3019561857368379 and parameters 
[-1.69367212  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858493  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 22:07:41 2024]  Iteration number: 0 with current cost as 0.3049447951343181 and parameters 
[-1.55694931  2.23743453 -2.12427964 -0.11653092  0.55388708 -2.77010897
  3.06858493  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 22:10:04 2024]  Iteration number: 0 with current cost as 0.2886119492109513 and parameters 
[-1.55420648  2.23743452 -2.12427958 -0.11653097  0.55388708 -2.77010903
  3.06858493  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 22:12:25 2024]  Iteration number: 0 with current cost as 0.29636229275036075 and parameters 
[-1.57267791  2.23743454 -2.12427964 -0.11653103  0.55388703 -2.77010897
  3.06858488  2.18960145  1.18551998 -1.06648318  0.6027151   1.1443244
  1.31029894 -1.8735468 ]. 
Training complete taking 711.9572920799255 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 22:14:31 2024]  Iteration number: 0 with current cost as 0.2983418006131027 and parameters 
[-1.64478195  2.23743459 -2.12427964 -0.11653094  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552003 -1.06648308  0.60271515  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Apr  4 22:17:16 2024]  Iteration number: 0 with current cost as 0.3019561857368379 and parameters 
[-1.69367212  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858493  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 22:19:36 2024]  Iteration number: 0 with current cost as 0.3049447951343181 and parameters 
[-1.55694931  2.23743453 -2.12427964 -0.11653092  0.55388708 -2.77010897
  3.06858493  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 22:21:57 2024]  Iteration number: 0 with current cost as 0.2886119492109513 and parameters 
[-1.55420648  2.23743452 -2.12427958 -0.11653097  0.55388708 -2.77010903
  3.06858493  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 22:24:17 2024]  Iteration number: 0 with current cost as 0.29636229275036075 and parameters 
[-1.57267791  2.23743454 -2.12427964 -0.11653103  0.55388703 -2.77010897
  3.06858488  2.18960145  1.18551998 -1.06648318  0.6027151   1.1443244
  1.31029894 -1.8735468 ]. 
Training complete taking 712.9236822128296 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 22:26:25 2024]  Iteration number: 0 with current cost as 0.2983418006131027 and parameters 
[-1.64478195  2.23743459 -2.12427964 -0.11653094  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552003 -1.06648308  0.60271515  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Apr  4 22:29:05 2024]  Iteration number: 0 with current cost as 0.3019561857368379 and parameters 
[-1.69367212  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858493  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 22:31:28 2024]  Iteration number: 0 with current cost as 0.3049447951343181 and parameters 
[-1.55694931  2.23743453 -2.12427964 -0.11653092  0.55388708 -2.77010897
  3.06858493  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 22:33:54 2024]  Iteration number: 0 with current cost as 0.2886119492109513 and parameters 
[-1.55420648  2.23743452 -2.12427958 -0.11653097  0.55388708 -2.77010903
  3.06858493  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 22:36:17 2024]  Iteration number: 0 with current cost as 0.29636229275036075 and parameters 
[-1.57267791  2.23743454 -2.12427964 -0.11653103  0.55388703 -2.77010897
  3.06858488  2.18960145  1.18551998 -1.06648318  0.6027151   1.1443244
  1.31029894 -1.8735468 ]. 
Training complete taking 718.0869445800781 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 22:38:25 2024]  Iteration number: 0 with current cost as 0.2983418006131027 and parameters 
[-1.64478195  2.23743459 -2.12427964 -0.11653094  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552003 -1.06648308  0.60271515  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Apr  4 22:41:06 2024]  Iteration number: 0 with current cost as 0.3019561857368379 and parameters 
[-1.69367212  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858493  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 22:43:29 2024]  Iteration number: 0 with current cost as 0.3049447951343181 and parameters 
[-1.55694931  2.23743453 -2.12427964 -0.11653092  0.55388708 -2.77010897
  3.06858493  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 22:45:50 2024]  Iteration number: 0 with current cost as 0.2886119492109513 and parameters 
[-1.55420648  2.23743452 -2.12427958 -0.11653097  0.55388708 -2.77010903
  3.06858493  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 22:48:16 2024]  Iteration number: 0 with current cost as 0.29636229275036075 and parameters 
[-1.57267791  2.23743454 -2.12427964 -0.11653103  0.55388703 -2.77010897
  3.06858488  2.18960145  1.18551998 -1.06648318  0.6027151   1.1443244
  1.31029894 -1.8735468 ]. 
Training complete taking 717.5944075584412 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 22:50:23 2024]  Iteration number: 0 with current cost as 0.2983418006131027 and parameters 
[-1.64478195  2.23743459 -2.12427964 -0.11653094  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552003 -1.06648308  0.60271515  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Apr  4 22:53:06 2024]  Iteration number: 0 with current cost as 0.3019561857368379 and parameters 
[-1.69367212  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858493  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 22:55:38 2024]  Iteration number: 0 with current cost as 0.3049447951343181 and parameters 
[-1.55694931  2.23743453 -2.12427964 -0.11653092  0.55388708 -2.77010897
  3.06858493  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 22:58:00 2024]  Iteration number: 0 with current cost as 0.2886119492109513 and parameters 
[-1.55420648  2.23743452 -2.12427958 -0.11653097  0.55388708 -2.77010903
  3.06858493  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 23:00:22 2024]  Iteration number: 0 with current cost as 0.29636229275036075 and parameters 
[-1.57267791  2.23743454 -2.12427964 -0.11653103  0.55388703 -2.77010897
  3.06858488  2.18960145  1.18551998 -1.06648318  0.6027151   1.1443244
  1.31029894 -1.8735468 ]. 
Training complete taking 730.2848892211914 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 23:02:36 2024]  Iteration number: 0 with current cost as 0.2983418006131027 and parameters 
[-1.64478195  2.23743459 -2.12427964 -0.11653094  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552003 -1.06648308  0.60271515  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Apr  4 23:05:19 2024]  Iteration number: 0 with current cost as 0.3019561857368379 and parameters 
[-1.69367212  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858493  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 23:07:41 2024]  Iteration number: 0 with current cost as 0.3049447951343181 and parameters 
[-1.55694931  2.23743453 -2.12427964 -0.11653092  0.55388708 -2.77010897
  3.06858493  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 23:10:00 2024]  Iteration number: 0 with current cost as 0.2886119492109513 and parameters 
[-1.55420648  2.23743452 -2.12427958 -0.11653097  0.55388708 -2.77010903
  3.06858493  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 23:12:26 2024]  Iteration number: 0 with current cost as 0.29636229275036075 and parameters 
[-1.57267791  2.23743454 -2.12427964 -0.11653103  0.55388703 -2.77010897
  3.06858488  2.18960145  1.18551998 -1.06648318  0.6027151   1.1443244
  1.31029894 -1.8735468 ]. 
Training complete taking 721.8910324573517 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 23:14:31 2024]  Iteration number: 0 with current cost as 0.2983418006131027 and parameters 
[-1.64478195  2.23743459 -2.12427964 -0.11653094  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552003 -1.06648308  0.60271515  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Apr  4 23:17:18 2024]  Iteration number: 0 with current cost as 0.3019561857368379 and parameters 
[-1.69367212  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858493  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 23:19:47 2024]  Iteration number: 0 with current cost as 0.3049447951343181 and parameters 
[-1.55694931  2.23743453 -2.12427964 -0.11653092  0.55388708 -2.77010897
  3.06858493  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 23:22:15 2024]  Iteration number: 0 with current cost as 0.2886119492109513 and parameters 
[-1.55420648  2.23743452 -2.12427958 -0.11653097  0.55388708 -2.77010903
  3.06858493  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 23:24:38 2024]  Iteration number: 0 with current cost as 0.29636229275036075 and parameters 
[-1.57267791  2.23743454 -2.12427964 -0.11653103  0.55388703 -2.77010897
  3.06858488  2.18960145  1.18551998 -1.06648318  0.6027151   1.1443244
  1.31029894 -1.8735468 ]. 
Training complete taking 730.406881570816 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 23:26:41 2024]  Iteration number: 0 with current cost as 0.2983418006131027 and parameters 
[-1.64478195  2.23743459 -2.12427964 -0.11653094  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552003 -1.06648308  0.60271515  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Apr  4 23:29:19 2024]  Iteration number: 0 with current cost as 0.3019561857368379 and parameters 
[-1.69367212  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858493  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 23:31:45 2024]  Iteration number: 0 with current cost as 0.3049447951343181 and parameters 
[-1.55694931  2.23743453 -2.12427964 -0.11653092  0.55388708 -2.77010897
  3.06858493  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 23:34:12 2024]  Iteration number: 0 with current cost as 0.2886119492109513 and parameters 
[-1.55420648  2.23743452 -2.12427958 -0.11653097  0.55388708 -2.77010903
  3.06858493  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 23:36:38 2024]  Iteration number: 0 with current cost as 0.29636229275036075 and parameters 
[-1.57267791  2.23743454 -2.12427964 -0.11653103  0.55388703 -2.77010897
  3.06858488  2.18960145  1.18551998 -1.06648318  0.6027151   1.1443244
  1.31029894 -1.8735468 ]. 
Training complete taking 723.4649744033813 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 23:38:52 2024]  Iteration number: 0 with current cost as 0.2983418006131027 and parameters 
[-1.64478195  2.23743459 -2.12427964 -0.11653094  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552003 -1.06648308  0.60271515  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Apr  4 23:41:41 2024]  Iteration number: 0 with current cost as 0.3019561857368379 and parameters 
[-1.69367212  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858493  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 23:44:03 2024]  Iteration number: 0 with current cost as 0.3049447951343181 and parameters 
[-1.55694931  2.23743453 -2.12427964 -0.11653092  0.55388708 -2.77010897
  3.06858493  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 23:46:32 2024]  Iteration number: 0 with current cost as 0.2886119492109513 and parameters 
[-1.55420648  2.23743452 -2.12427958 -0.11653097  0.55388708 -2.77010903
  3.06858493  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 23:49:04 2024]  Iteration number: 0 with current cost as 0.29636229275036075 and parameters 
[-1.57267791  2.23743454 -2.12427964 -0.11653103  0.55388703 -2.77010897
  3.06858488  2.18960145  1.18551998 -1.06648318  0.6027151   1.1443244
  1.31029894 -1.8735468 ]. 
Training complete taking 744.7979018688202 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 23:51:14 2024]  Iteration number: 0 with current cost as 0.2983418006131027 and parameters 
[-1.64478195  2.23743459 -2.12427964 -0.11653094  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552003 -1.06648308  0.60271515  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Apr  4 23:53:56 2024]  Iteration number: 0 with current cost as 0.3019561857368379 and parameters 
[-1.69367212  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858493  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 23:56:23 2024]  Iteration number: 0 with current cost as 0.3049447951343181 and parameters 
[-1.55694931  2.23743453 -2.12427964 -0.11653092  0.55388708 -2.77010897
  3.06858493  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 23:58:50 2024]  Iteration number: 0 with current cost as 0.2886119492109513 and parameters 
[-1.55420648  2.23743452 -2.12427958 -0.11653097  0.55388708 -2.77010903
  3.06858493  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Fri Apr  5 00:01:16 2024]  Iteration number: 0 with current cost as 0.29636229275036075 and parameters 
[-1.57267791  2.23743454 -2.12427964 -0.11653103  0.55388703 -2.77010897
  3.06858488  2.18960145  1.18551998 -1.06648318  0.6027151   1.1443244
  1.31029894 -1.8735468 ]. 
Training complete taking 731.5473272800446 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 00:03:28 2024]  Iteration number: 0 with current cost as 0.2983418006131027 and parameters 
[-1.64478195  2.23743459 -2.12427964 -0.11653094  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552003 -1.06648308  0.60271515  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Fri Apr  5 00:06:12 2024]  Iteration number: 0 with current cost as 0.3019561857368379 and parameters 
[-1.69367212  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858493  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Fri Apr  5 00:08:36 2024]  Iteration number: 0 with current cost as 0.3049447951343181 and parameters 
[-1.55694931  2.23743453 -2.12427964 -0.11653092  0.55388708 -2.77010897
  3.06858493  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Fri Apr  5 00:11:14 2024]  Iteration number: 0 with current cost as 0.2886119492109513 and parameters 
[-1.55420648  2.23743452 -2.12427958 -0.11653097  0.55388708 -2.77010903
  3.06858493  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Fri Apr  5 00:13:45 2024]  Iteration number: 0 with current cost as 0.29636229275036075 and parameters 
[-1.57267791  2.23743454 -2.12427964 -0.11653103  0.55388703 -2.77010897
  3.06858488  2.18960145  1.18551998 -1.06648318  0.6027151   1.1443244
  1.31029894 -1.8735468 ]. 
Training complete taking 749.4937329292297 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 00:15:54 2024]  Iteration number: 0 with current cost as 0.2983418006131027 and parameters 
[-1.64478195  2.23743459 -2.12427964 -0.11653094  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552003 -1.06648308  0.60271515  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Fri Apr  5 00:18:40 2024]  Iteration number: 0 with current cost as 0.3019561857368379 and parameters 
[-1.69367212  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858493  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Fri Apr  5 00:21:06 2024]  Iteration number: 0 with current cost as 0.3049447951343181 and parameters 
[-1.55694931  2.23743453 -2.12427964 -0.11653092  0.55388708 -2.77010897
  3.06858493  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Fri Apr  5 00:23:37 2024]  Iteration number: 0 with current cost as 0.2886119492109513 and parameters 
[-1.55420648  2.23743452 -2.12427958 -0.11653097  0.55388708 -2.77010903
  3.06858493  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Fri Apr  5 00:26:05 2024]  Iteration number: 0 with current cost as 0.29636229275036075 and parameters 
[-1.57267791  2.23743454 -2.12427964 -0.11653103  0.55388703 -2.77010897
  3.06858488  2.18960145  1.18551998 -1.06648318  0.6027151   1.1443244
  1.31029894 -1.8735468 ]. 
Training complete taking 740.8858866691589 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 00:28:25 2024]  Iteration number: 0 with current cost as 0.2983418006131027 and parameters 
[-1.64478195  2.23743459 -2.12427964 -0.11653094  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552003 -1.06648308  0.60271515  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Fri Apr  5 00:31:19 2024]  Iteration number: 0 with current cost as 0.3019561857368379 and parameters 
[-1.69367212  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858493  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Fri Apr  5 00:33:50 2024]  Iteration number: 0 with current cost as 0.3049447951343181 and parameters 
[-1.55694931  2.23743453 -2.12427964 -0.11653092  0.55388708 -2.77010897
  3.06858493  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Fri Apr  5 00:36:16 2024]  Iteration number: 0 with current cost as 0.2886119492109513 and parameters 
[-1.55420648  2.23743452 -2.12427958 -0.11653097  0.55388708 -2.77010903
  3.06858493  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Fri Apr  5 00:38:42 2024]  Iteration number: 0 with current cost as 0.29636229275036075 and parameters 
[-1.57267791  2.23743454 -2.12427964 -0.11653103  0.55388703 -2.77010897
  3.06858488  2.18960145  1.18551998 -1.06648318  0.6027151   1.1443244
  1.31029894 -1.8735468 ]. 
Training complete taking 753.6959903240204 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 00:40:49 2024]  Iteration number: 0 with current cost as 0.2983418006131027 and parameters 
[-1.64478195  2.23743459 -2.12427964 -0.11653094  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552003 -1.06648308  0.60271515  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Fri Apr  5 00:43:37 2024]  Iteration number: 0 with current cost as 0.3019561857368379 and parameters 
[-1.69367212  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858493  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Fri Apr  5 00:46:09 2024]  Iteration number: 0 with current cost as 0.3049447951343181 and parameters 
[-1.55694931  2.23743453 -2.12427964 -0.11653092  0.55388708 -2.77010897
  3.06858493  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Fri Apr  5 00:48:32 2024]  Iteration number: 0 with current cost as 0.2886119492109513 and parameters 
[-1.55420648  2.23743452 -2.12427958 -0.11653097  0.55388708 -2.77010903
  3.06858493  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Fri Apr  5 00:50:59 2024]  Iteration number: 0 with current cost as 0.29636229275036075 and parameters 
[-1.57267791  2.23743454 -2.12427964 -0.11653103  0.55388703 -2.77010897
  3.06858488  2.18960145  1.18551998 -1.06648318  0.6027151   1.1443244
  1.31029894 -1.8735468 ]. 
Training complete taking 740.8080637454987 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 00:53:10 2024]  Iteration number: 0 with current cost as 0.2983418006131027 and parameters 
[-1.64478195  2.23743459 -2.12427964 -0.11653094  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552003 -1.06648308  0.60271515  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Fri Apr  5 00:55:54 2024]  Iteration number: 0 with current cost as 0.3019561857368379 and parameters 
[-1.69367212  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858493  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Fri Apr  5 00:58:23 2024]  Iteration number: 0 with current cost as 0.3049447951343181 and parameters 
[-1.55694931  2.23743453 -2.12427964 -0.11653092  0.55388708 -2.77010897
  3.06858493  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Fri Apr  5 01:00:54 2024]  Iteration number: 0 with current cost as 0.2886119492109513 and parameters 
[-1.55420648  2.23743452 -2.12427958 -0.11653097  0.55388708 -2.77010903
  3.06858493  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Fri Apr  5 01:03:26 2024]  Iteration number: 0 with current cost as 0.29636229275036075 and parameters 
[-1.57267791  2.23743454 -2.12427964 -0.11653103  0.55388703 -2.77010897
  3.06858488  2.18960145  1.18551998 -1.06648318  0.6027151   1.1443244
  1.31029894 -1.8735468 ]. 
Training complete taking 744.6033205986023 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 01:05:37 2024]  Iteration number: 0 with current cost as 0.2983418006131027 and parameters 
[-1.64478195  2.23743459 -2.12427964 -0.11653094  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552003 -1.06648308  0.60271515  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Fri Apr  5 01:08:19 2024]  Iteration number: 0 with current cost as 0.3019561857368379 and parameters 
[-1.69367212  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858493  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Fri Apr  5 01:10:47 2024]  Iteration number: 0 with current cost as 0.3049447951343181 and parameters 
[-1.55694931  2.23743453 -2.12427964 -0.11653092  0.55388708 -2.77010897
  3.06858493  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Fri Apr  5 01:13:08 2024]  Iteration number: 0 with current cost as 0.2886119492109513 and parameters 
[-1.55420648  2.23743452 -2.12427958 -0.11653097  0.55388708 -2.77010903
  3.06858493  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Fri Apr  5 01:15:36 2024]  Iteration number: 0 with current cost as 0.29636229275036075 and parameters 
[-1.57267791  2.23743454 -2.12427964 -0.11653103  0.55388703 -2.77010897
  3.06858488  2.18960145  1.18551998 -1.06648318  0.6027151   1.1443244
  1.31029894 -1.8735468 ]. 
Training complete taking 734.3041138648987 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 01:17:54 2024]  Iteration number: 0 with current cost as 0.2983418006131027 and parameters 
[-1.64478195  2.23743459 -2.12427964 -0.11653094  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552003 -1.06648308  0.60271515  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Fri Apr  5 01:20:53 2024]  Iteration number: 0 with current cost as 0.3019561857368379 and parameters 
[-1.69367212  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858493  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Fri Apr  5 01:23:17 2024]  Iteration number: 0 with current cost as 0.3049447951343181 and parameters 
[-1.55694931  2.23743453 -2.12427964 -0.11653092  0.55388708 -2.77010897
  3.06858493  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Fri Apr  5 01:25:50 2024]  Iteration number: 0 with current cost as 0.2886119492109513 and parameters 
[-1.55420648  2.23743452 -2.12427958 -0.11653097  0.55388708 -2.77010903
  3.06858493  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Fri Apr  5 01:28:20 2024]  Iteration number: 0 with current cost as 0.29636229275036075 and parameters 
[-1.57267791  2.23743454 -2.12427964 -0.11653103  0.55388703 -2.77010897
  3.06858488  2.18960145  1.18551998 -1.06648318  0.6027151   1.1443244
  1.31029894 -1.8735468 ]. 
Training complete taking 759.8820555210114 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 01:30:38 2024]  Iteration number: 0 with current cost as 0.2983418006131027 and parameters 
[-1.64478195  2.23743459 -2.12427964 -0.11653094  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552003 -1.06648308  0.60271515  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Fri Apr  5 01:33:34 2024]  Iteration number: 0 with current cost as 0.3019561857368379 and parameters 
[-1.69367212  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858493  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Fri Apr  5 01:36:06 2024]  Iteration number: 0 with current cost as 0.3049447951343181 and parameters 
[-1.55694931  2.23743453 -2.12427964 -0.11653092  0.55388708 -2.77010897
  3.06858493  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Fri Apr  5 01:38:45 2024]  Iteration number: 0 with current cost as 0.2886119492109513 and parameters 
[-1.55420648  2.23743452 -2.12427958 -0.11653097  0.55388708 -2.77010903
  3.06858493  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Fri Apr  5 01:41:14 2024]  Iteration number: 0 with current cost as 0.29636229275036075 and parameters 
[-1.57267791  2.23743454 -2.12427964 -0.11653103  0.55388703 -2.77010897
  3.06858488  2.18960145  1.18551998 -1.06648318  0.6027151   1.1443244
  1.31029894 -1.8735468 ]. 
Training complete taking 777.5924594402313 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 01:43:31 2024]  Iteration number: 0 with current cost as 0.2983418006131027 and parameters 
[-1.64478195  2.23743459 -2.12427964 -0.11653094  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552003 -1.06648308  0.60271515  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Fri Apr  5 01:46:17 2024]  Iteration number: 0 with current cost as 0.3019561857368379 and parameters 
[-1.69367212  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858493  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Fri Apr  5 01:48:38 2024]  Iteration number: 0 with current cost as 0.3049447951343181 and parameters 
[-1.55694931  2.23743453 -2.12427964 -0.11653092  0.55388708 -2.77010897
  3.06858493  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Fri Apr  5 01:51:14 2024]  Iteration number: 0 with current cost as 0.2886119492109513 and parameters 
[-1.55420648  2.23743452 -2.12427958 -0.11653097  0.55388708 -2.77010903
  3.06858493  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Fri Apr  5 01:53:39 2024]  Iteration number: 0 with current cost as 0.29636229275036075 and parameters 
[-1.57267791  2.23743454 -2.12427964 -0.11653103  0.55388703 -2.77010897
  3.06858488  2.18960145  1.18551998 -1.06648318  0.6027151   1.1443244
  1.31029894 -1.8735468 ]. 
Training complete taking 744.9837648868561 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 01:55:53 2024]  Iteration number: 0 with current cost as 0.2983418006131027 and parameters 
[-1.64478195  2.23743459 -2.12427964 -0.11653094  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552003 -1.06648308  0.60271515  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Fri Apr  5 01:58:52 2024]  Iteration number: 0 with current cost as 0.3019561857368379 and parameters 
[-1.69367212  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858493  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Fri Apr  5 02:01:25 2024]  Iteration number: 0 with current cost as 0.3049447951343181 and parameters 
[-1.55694931  2.23743453 -2.12427964 -0.11653092  0.55388708 -2.77010897
  3.06858493  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Fri Apr  5 02:03:54 2024]  Iteration number: 0 with current cost as 0.2886119492109513 and parameters 
[-1.55420648  2.23743452 -2.12427958 -0.11653097  0.55388708 -2.77010903
  3.06858493  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Fri Apr  5 02:06:40 2024]  Iteration number: 0 with current cost as 0.29636229275036075 and parameters 
[-1.57267791  2.23743454 -2.12427964 -0.11653103  0.55388703 -2.77010897
  3.06858488  2.18960145  1.18551998 -1.06648318  0.6027151   1.1443244
  1.31029894 -1.8735468 ]. 
Training complete taking 776.0890264511108 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 02:08:45 2024]  Iteration number: 0 with current cost as 0.2983418006131027 and parameters 
[-1.64478195  2.23743459 -2.12427964 -0.11653094  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552003 -1.06648308  0.60271515  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Fri Apr  5 02:11:31 2024]  Iteration number: 0 with current cost as 0.3019561857368379 and parameters 
[-1.69367212  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858493  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Fri Apr  5 02:14:01 2024]  Iteration number: 0 with current cost as 0.3049447951343181 and parameters 
[-1.55694931  2.23743453 -2.12427964 -0.11653092  0.55388708 -2.77010897
  3.06858493  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Fri Apr  5 02:16:32 2024]  Iteration number: 0 with current cost as 0.2886119492109513 and parameters 
[-1.55420648  2.23743452 -2.12427958 -0.11653097  0.55388708 -2.77010903
  3.06858493  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Fri Apr  5 02:19:02 2024]  Iteration number: 0 with current cost as 0.29636229275036075 and parameters 
[-1.57267791  2.23743454 -2.12427964 -0.11653103  0.55388703 -2.77010897
  3.06858488  2.18960145  1.18551998 -1.06648318  0.6027151   1.1443244
  1.31029894 -1.8735468 ]. 
Training complete taking 748.4519062042236 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 02:21:20 2024]  Iteration number: 0 with current cost as 0.2983418006131027 and parameters 
[-1.64478195  2.23743459 -2.12427964 -0.11653094  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552003 -1.06648308  0.60271515  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Fri Apr  5 02:24:10 2024]  Iteration number: 0 with current cost as 0.3019561857368379 and parameters 
[-1.69367212  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858493  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Fri Apr  5 02:26:40 2024]  Iteration number: 0 with current cost as 0.3049447951343181 and parameters 
[-1.55694931  2.23743453 -2.12427964 -0.11653092  0.55388708 -2.77010897
  3.06858493  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Fri Apr  5 02:29:23 2024]  Iteration number: 0 with current cost as 0.2886119492109513 and parameters 
[-1.55420648  2.23743452 -2.12427958 -0.11653097  0.55388708 -2.77010903
  3.06858493  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Fri Apr  5 02:31:51 2024]  Iteration number: 0 with current cost as 0.29636229275036075 and parameters 
[-1.57267791  2.23743454 -2.12427964 -0.11653103  0.55388703 -2.77010897
  3.06858488  2.18960145  1.18551998 -1.06648318  0.6027151   1.1443244
  1.31029894 -1.8735468 ]. 
Training complete taking 764.9379470348358 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 02:34:00 2024]  Iteration number: 0 with current cost as 0.2983418006131027 and parameters 
[-1.64478195  2.23743459 -2.12427964 -0.11653094  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552003 -1.06648308  0.60271515  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Fri Apr  5 02:36:51 2024]  Iteration number: 0 with current cost as 0.3019561857368379 and parameters 
[-1.69367212  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858493  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Fri Apr  5 02:39:23 2024]  Iteration number: 0 with current cost as 0.3049447951343181 and parameters 
[-1.55694931  2.23743453 -2.12427964 -0.11653092  0.55388708 -2.77010897
  3.06858493  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Fri Apr  5 02:42:06 2024]  Iteration number: 0 with current cost as 0.2886119492109513 and parameters 
[-1.55420648  2.23743452 -2.12427958 -0.11653097  0.55388708 -2.77010903
  3.06858493  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Fri Apr  5 02:44:34 2024]  Iteration number: 0 with current cost as 0.29636229275036075 and parameters 
[-1.57267791  2.23743454 -2.12427964 -0.11653103  0.55388703 -2.77010897
  3.06858488  2.18960145  1.18551998 -1.06648318  0.6027151   1.1443244
  1.31029894 -1.8735468 ]. 
Training complete taking 764.3253829479218 seconds. 
Discarding model... 

Training complete taking 18462.770403385162 total seconds. 
Now scoring model... 
Scoring complete taking 2.167386054992676 seconds. 
Saved predicted values as A1-A1-CZ_Efficient-CRZ_predicted_values.csv
Model scores: {'MSE_train': (0.5018599584699153,), 'R2_train': 0.0006786759081582883, 'MAE_train': 0.6185497528459297, 'MSE_test': 0.5733852507947689, 'R2_test': -0.06934339579559179, 'MAE_test': 0.6788977848439731}. 
Saved model results as A1-A1-CZ_Efficient-CRZ_results.json. 
