/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /scratch/j/jacobsen/gjones/sine5qubits/sine_train.bin... 
Successfully loaded /scratch/j/jacobsen/gjones/sine5qubits/sine_train.bin into X and y data. 
Loading dataset from /scratch/j/jacobsen/gjones/sine5qubits/sine_test.bin... 
Successfully loaded /scratch/j/jacobsen/gjones/sine5qubits/sine_test.bin into X and y data. 
Training model with dataset /scratch/j/jacobsen/gjones/sine5qubits/sine_train.bin 
 at time Thu Apr  4 21:37:45 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 21:39:07 2024]  Iteration number: 0 with current cost as 0.6016901345575991 and parameters 
[-0.18336971  2.23743479 -2.12427932 -0.11653087  0.55388724 -2.77010913
  3.06858514  2.18960145  1.18552014 -1.06648324  0.6027151   1.14432461
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Apr  4 21:42:41 2024]  Iteration number: 0 with current cost as 0.3672680875820209 and parameters 
[-1.3683026   2.23743464 -2.12427938 -0.11653103  0.55388721 -2.77010897
  3.06858511  2.18960158  1.18552011 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 21:45:29 2024]  Iteration number: 0 with current cost as 0.3708577281807032 and parameters 
[-1.4505243   2.23743464 -2.12427953 -0.11653081  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552009 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 21:48:40 2024]  Iteration number: 0 with current cost as 0.3404778403236841 and parameters 
[-1.43259807  2.23743464 -2.12427953 -0.11653103  0.55388708 -2.77010919
  3.06858498  2.18960145  1.18552009 -1.0664833   0.6027151   1.14432456
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 21:51:52 2024]  Iteration number: 0 with current cost as 0.3547029854076195 and parameters 
[-1.42087431  2.23743464 -2.12427946 -0.11653085  0.55388717 -2.77010897
  3.06858516  2.18960163  1.18552016 -1.06648308  0.60271528  1.14432463
  1.31029907 -1.8735468 ]. 
Training complete taking 914.7690711021423 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 21:54:29 2024]  Iteration number: 0 with current cost as 0.6016901345575991 and parameters 
[-0.18336971  2.23743479 -2.12427932 -0.11653087  0.55388724 -2.77010913
  3.06858514  2.18960145  1.18552014 -1.06648324  0.6027151   1.14432461
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Apr  4 21:57:58 2024]  Iteration number: 0 with current cost as 0.3672680875820209 and parameters 
[-1.3683026   2.23743464 -2.12427938 -0.11653103  0.55388721 -2.77010897
  3.06858511  2.18960158  1.18552011 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 22:00:47 2024]  Iteration number: 0 with current cost as 0.3708577281807032 and parameters 
[-1.4505243   2.23743464 -2.12427953 -0.11653081  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552009 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 22:04:03 2024]  Iteration number: 0 with current cost as 0.3404778403236841 and parameters 
[-1.43259807  2.23743464 -2.12427953 -0.11653103  0.55388708 -2.77010919
  3.06858498  2.18960145  1.18552009 -1.0664833   0.6027151   1.14432456
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 22:07:10 2024]  Iteration number: 0 with current cost as 0.3547029854076195 and parameters 
[-1.42087431  2.23743464 -2.12427946 -0.11653085  0.55388717 -2.77010897
  3.06858516  2.18960163  1.18552016 -1.06648308  0.60271528  1.14432463
  1.31029907 -1.8735468 ]. 
Training complete taking 910.6747488975525 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 22:09:34 2024]  Iteration number: 0 with current cost as 0.6016901345575991 and parameters 
[-0.18336971  2.23743479 -2.12427932 -0.11653087  0.55388724 -2.77010913
  3.06858514  2.18960145  1.18552014 -1.06648324  0.6027151   1.14432461
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Apr  4 22:13:07 2024]  Iteration number: 0 with current cost as 0.3672680875820209 and parameters 
[-1.3683026   2.23743464 -2.12427938 -0.11653103  0.55388721 -2.77010897
  3.06858511  2.18960158  1.18552011 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 22:15:54 2024]  Iteration number: 0 with current cost as 0.3708577281807032 and parameters 
[-1.4505243   2.23743464 -2.12427953 -0.11653081  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552009 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 22:19:00 2024]  Iteration number: 0 with current cost as 0.3404778403236841 and parameters 
[-1.43259807  2.23743464 -2.12427953 -0.11653103  0.55388708 -2.77010919
  3.06858498  2.18960145  1.18552009 -1.0664833   0.6027151   1.14432456
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 22:22:01 2024]  Iteration number: 0 with current cost as 0.3547029854076195 and parameters 
[-1.42087431  2.23743464 -2.12427946 -0.11653085  0.55388717 -2.77010897
  3.06858516  2.18960163  1.18552016 -1.06648308  0.60271528  1.14432463
  1.31029907 -1.8735468 ]. 
Training complete taking 891.5427012443542 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 22:24:22 2024]  Iteration number: 0 with current cost as 0.6016901345575991 and parameters 
[-0.18336971  2.23743479 -2.12427932 -0.11653087  0.55388724 -2.77010913
  3.06858514  2.18960145  1.18552014 -1.06648324  0.6027151   1.14432461
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Apr  4 22:27:49 2024]  Iteration number: 0 with current cost as 0.3672680875820209 and parameters 
[-1.3683026   2.23743464 -2.12427938 -0.11653103  0.55388721 -2.77010897
  3.06858511  2.18960158  1.18552011 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 22:30:33 2024]  Iteration number: 0 with current cost as 0.3708577281807032 and parameters 
[-1.4505243   2.23743464 -2.12427953 -0.11653081  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552009 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 22:33:32 2024]  Iteration number: 0 with current cost as 0.3404778403236841 and parameters 
[-1.43259807  2.23743464 -2.12427953 -0.11653103  0.55388708 -2.77010919
  3.06858498  2.18960145  1.18552009 -1.0664833   0.6027151   1.14432456
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 22:36:34 2024]  Iteration number: 0 with current cost as 0.3547029854076195 and parameters 
[-1.42087431  2.23743464 -2.12427946 -0.11653085  0.55388717 -2.77010897
  3.06858516  2.18960163  1.18552016 -1.06648308  0.60271528  1.14432463
  1.31029907 -1.8735468 ]. 
Training complete taking 872.1023616790771 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 22:38:54 2024]  Iteration number: 0 with current cost as 0.6016901345575991 and parameters 
[-0.18336971  2.23743479 -2.12427932 -0.11653087  0.55388724 -2.77010913
  3.06858514  2.18960145  1.18552014 -1.06648324  0.6027151   1.14432461
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Apr  4 22:42:20 2024]  Iteration number: 0 with current cost as 0.3672680875820209 and parameters 
[-1.3683026   2.23743464 -2.12427938 -0.11653103  0.55388721 -2.77010897
  3.06858511  2.18960158  1.18552011 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 22:45:01 2024]  Iteration number: 0 with current cost as 0.3708577281807032 and parameters 
[-1.4505243   2.23743464 -2.12427953 -0.11653081  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552009 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 22:48:01 2024]  Iteration number: 0 with current cost as 0.3404778403236841 and parameters 
[-1.43259807  2.23743464 -2.12427953 -0.11653103  0.55388708 -2.77010919
  3.06858498  2.18960145  1.18552009 -1.0664833   0.6027151   1.14432456
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 22:51:07 2024]  Iteration number: 0 with current cost as 0.3547029854076195 and parameters 
[-1.42087431  2.23743464 -2.12427946 -0.11653085  0.55388717 -2.77010897
  3.06858516  2.18960163  1.18552016 -1.06648308  0.60271528  1.14432463
  1.31029907 -1.8735468 ]. 
Training complete taking 874.2957122325897 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 22:53:30 2024]  Iteration number: 0 with current cost as 0.6016901345575991 and parameters 
[-0.18336971  2.23743479 -2.12427932 -0.11653087  0.55388724 -2.77010913
  3.06858514  2.18960145  1.18552014 -1.06648324  0.6027151   1.14432461
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Apr  4 22:56:56 2024]  Iteration number: 0 with current cost as 0.3672680875820209 and parameters 
[-1.3683026   2.23743464 -2.12427938 -0.11653103  0.55388721 -2.77010897
  3.06858511  2.18960158  1.18552011 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 22:59:40 2024]  Iteration number: 0 with current cost as 0.3708577281807032 and parameters 
[-1.4505243   2.23743464 -2.12427953 -0.11653081  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552009 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 23:02:48 2024]  Iteration number: 0 with current cost as 0.3404778403236841 and parameters 
[-1.43259807  2.23743464 -2.12427953 -0.11653103  0.55388708 -2.77010919
  3.06858498  2.18960145  1.18552009 -1.0664833   0.6027151   1.14432456
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 23:05:52 2024]  Iteration number: 0 with current cost as 0.3547029854076195 and parameters 
[-1.42087431  2.23743464 -2.12427946 -0.11653085  0.55388717 -2.77010897
  3.06858516  2.18960163  1.18552016 -1.06648308  0.60271528  1.14432463
  1.31029907 -1.8735468 ]. 
Training complete taking 885.7041714191437 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 23:08:17 2024]  Iteration number: 0 with current cost as 0.6016901345575991 and parameters 
[-0.18336971  2.23743479 -2.12427932 -0.11653087  0.55388724 -2.77010913
  3.06858514  2.18960145  1.18552014 -1.06648324  0.6027151   1.14432461
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Apr  4 23:11:39 2024]  Iteration number: 0 with current cost as 0.3672680875820209 and parameters 
[-1.3683026   2.23743464 -2.12427938 -0.11653103  0.55388721 -2.77010897
  3.06858511  2.18960158  1.18552011 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 23:14:20 2024]  Iteration number: 0 with current cost as 0.3708577281807032 and parameters 
[-1.4505243   2.23743464 -2.12427953 -0.11653081  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552009 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 23:17:21 2024]  Iteration number: 0 with current cost as 0.3404778403236841 and parameters 
[-1.43259807  2.23743464 -2.12427953 -0.11653103  0.55388708 -2.77010919
  3.06858498  2.18960145  1.18552009 -1.0664833   0.6027151   1.14432456
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 23:20:26 2024]  Iteration number: 0 with current cost as 0.3547029854076195 and parameters 
[-1.42087431  2.23743464 -2.12427946 -0.11653085  0.55388717 -2.77010897
  3.06858516  2.18960163  1.18552016 -1.06648308  0.60271528  1.14432463
  1.31029907 -1.8735468 ]. 
Training complete taking 873.2814164161682 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 23:22:52 2024]  Iteration number: 0 with current cost as 0.6016901345575991 and parameters 
[-0.18336971  2.23743479 -2.12427932 -0.11653087  0.55388724 -2.77010913
  3.06858514  2.18960145  1.18552014 -1.06648324  0.6027151   1.14432461
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Apr  4 23:26:14 2024]  Iteration number: 0 with current cost as 0.3672680875820209 and parameters 
[-1.3683026   2.23743464 -2.12427938 -0.11653103  0.55388721 -2.77010897
  3.06858511  2.18960158  1.18552011 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 23:28:55 2024]  Iteration number: 0 with current cost as 0.3708577281807032 and parameters 
[-1.4505243   2.23743464 -2.12427953 -0.11653081  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552009 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 23:32:03 2024]  Iteration number: 0 with current cost as 0.3404778403236841 and parameters 
[-1.43259807  2.23743464 -2.12427953 -0.11653103  0.55388708 -2.77010919
  3.06858498  2.18960145  1.18552009 -1.0664833   0.6027151   1.14432456
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 23:35:10 2024]  Iteration number: 0 with current cost as 0.3547029854076195 and parameters 
[-1.42087431  2.23743464 -2.12427946 -0.11653085  0.55388717 -2.77010897
  3.06858516  2.18960163  1.18552016 -1.06648308  0.60271528  1.14432463
  1.31029907 -1.8735468 ]. 
Training complete taking 884.1867542266846 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 23:37:34 2024]  Iteration number: 0 with current cost as 0.6016901345575991 and parameters 
[-0.18336971  2.23743479 -2.12427932 -0.11653087  0.55388724 -2.77010913
  3.06858514  2.18960145  1.18552014 -1.06648324  0.6027151   1.14432461
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Apr  4 23:41:04 2024]  Iteration number: 0 with current cost as 0.3672680875820209 and parameters 
[-1.3683026   2.23743464 -2.12427938 -0.11653103  0.55388721 -2.77010897
  3.06858511  2.18960158  1.18552011 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 23:43:54 2024]  Iteration number: 0 with current cost as 0.3708577281807032 and parameters 
[-1.4505243   2.23743464 -2.12427953 -0.11653081  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552009 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 23:47:00 2024]  Iteration number: 0 with current cost as 0.3404778403236841 and parameters 
[-1.43259807  2.23743464 -2.12427953 -0.11653103  0.55388708 -2.77010919
  3.06858498  2.18960145  1.18552009 -1.0664833   0.6027151   1.14432456
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 23:50:11 2024]  Iteration number: 0 with current cost as 0.3547029854076195 and parameters 
[-1.42087431  2.23743464 -2.12427946 -0.11653085  0.55388717 -2.77010897
  3.06858516  2.18960163  1.18552016 -1.06648308  0.60271528  1.14432463
  1.31029907 -1.8735468 ]. 
Training complete taking 904.5040442943573 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 23:52:40 2024]  Iteration number: 0 with current cost as 0.6016901345575991 and parameters 
[-0.18336971  2.23743479 -2.12427932 -0.11653087  0.55388724 -2.77010913
  3.06858514  2.18960145  1.18552014 -1.06648324  0.6027151   1.14432461
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Apr  4 23:56:12 2024]  Iteration number: 0 with current cost as 0.3672680875820209 and parameters 
[-1.3683026   2.23743464 -2.12427938 -0.11653103  0.55388721 -2.77010897
  3.06858511  2.18960158  1.18552011 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 23:58:57 2024]  Iteration number: 0 with current cost as 0.3708577281807032 and parameters 
[-1.4505243   2.23743464 -2.12427953 -0.11653081  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552009 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Fri Apr  5 00:02:03 2024]  Iteration number: 0 with current cost as 0.3404778403236841 and parameters 
[-1.43259807  2.23743464 -2.12427953 -0.11653103  0.55388708 -2.77010919
  3.06858498  2.18960145  1.18552009 -1.0664833   0.6027151   1.14432456
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Fri Apr  5 00:05:14 2024]  Iteration number: 0 with current cost as 0.3547029854076195 and parameters 
[-1.42087431  2.23743464 -2.12427946 -0.11653085  0.55388717 -2.77010897
  3.06858516  2.18960163  1.18552016 -1.06648308  0.60271528  1.14432463
  1.31029907 -1.8735468 ]. 
Training complete taking 902.6034474372864 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 00:07:43 2024]  Iteration number: 0 with current cost as 0.6016901345575991 and parameters 
[-0.18336971  2.23743479 -2.12427932 -0.11653087  0.55388724 -2.77010913
  3.06858514  2.18960145  1.18552014 -1.06648324  0.6027151   1.14432461
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Fri Apr  5 00:11:18 2024]  Iteration number: 0 with current cost as 0.3672680875820209 and parameters 
[-1.3683026   2.23743464 -2.12427938 -0.11653103  0.55388721 -2.77010897
  3.06858511  2.18960158  1.18552011 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Fri Apr  5 00:14:17 2024]  Iteration number: 0 with current cost as 0.3708577281807032 and parameters 
[-1.4505243   2.23743464 -2.12427953 -0.11653081  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552009 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Fri Apr  5 00:17:30 2024]  Iteration number: 0 with current cost as 0.3404778403236841 and parameters 
[-1.43259807  2.23743464 -2.12427953 -0.11653103  0.55388708 -2.77010919
  3.06858498  2.18960145  1.18552009 -1.0664833   0.6027151   1.14432456
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Fri Apr  5 00:20:39 2024]  Iteration number: 0 with current cost as 0.3547029854076195 and parameters 
[-1.42087431  2.23743464 -2.12427946 -0.11653085  0.55388717 -2.77010897
  3.06858516  2.18960163  1.18552016 -1.06648308  0.60271528  1.14432463
  1.31029907 -1.8735468 ]. 
Training complete taking 923.3042964935303 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 00:23:08 2024]  Iteration number: 0 with current cost as 0.6016901345575991 and parameters 
[-0.18336971  2.23743479 -2.12427932 -0.11653087  0.55388724 -2.77010913
  3.06858514  2.18960145  1.18552014 -1.06648324  0.6027151   1.14432461
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Fri Apr  5 00:26:40 2024]  Iteration number: 0 with current cost as 0.3672680875820209 and parameters 
[-1.3683026   2.23743464 -2.12427938 -0.11653103  0.55388721 -2.77010897
  3.06858511  2.18960158  1.18552011 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Fri Apr  5 00:29:27 2024]  Iteration number: 0 with current cost as 0.3708577281807032 and parameters 
[-1.4505243   2.23743464 -2.12427953 -0.11653081  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552009 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Fri Apr  5 00:32:37 2024]  Iteration number: 0 with current cost as 0.3404778403236841 and parameters 
[-1.43259807  2.23743464 -2.12427953 -0.11653103  0.55388708 -2.77010919
  3.06858498  2.18960145  1.18552009 -1.0664833   0.6027151   1.14432456
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Fri Apr  5 00:35:48 2024]  Iteration number: 0 with current cost as 0.3547029854076195 and parameters 
[-1.42087431  2.23743464 -2.12427946 -0.11653085  0.55388717 -2.77010897
  3.06858516  2.18960163  1.18552016 -1.06648308  0.60271528  1.14432463
  1.31029907 -1.8735468 ]. 
Training complete taking 911.4775826931 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 00:38:21 2024]  Iteration number: 0 with current cost as 0.6016901345575991 and parameters 
[-0.18336971  2.23743479 -2.12427932 -0.11653087  0.55388724 -2.77010913
  3.06858514  2.18960145  1.18552014 -1.06648324  0.6027151   1.14432461
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Fri Apr  5 00:41:53 2024]  Iteration number: 0 with current cost as 0.3672680875820209 and parameters 
[-1.3683026   2.23743464 -2.12427938 -0.11653103  0.55388721 -2.77010897
  3.06858511  2.18960158  1.18552011 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Fri Apr  5 00:44:44 2024]  Iteration number: 0 with current cost as 0.3708577281807032 and parameters 
[-1.4505243   2.23743464 -2.12427953 -0.11653081  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552009 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Fri Apr  5 00:47:50 2024]  Iteration number: 0 with current cost as 0.3404778403236841 and parameters 
[-1.43259807  2.23743464 -2.12427953 -0.11653103  0.55388708 -2.77010919
  3.06858498  2.18960145  1.18552009 -1.0664833   0.6027151   1.14432456
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Fri Apr  5 00:51:05 2024]  Iteration number: 0 with current cost as 0.3547029854076195 and parameters 
[-1.42087431  2.23743464 -2.12427946 -0.11653085  0.55388717 -2.77010897
  3.06858516  2.18960163  1.18552016 -1.06648308  0.60271528  1.14432463
  1.31029907 -1.8735468 ]. 
Training complete taking 915.2639963626862 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 00:53:36 2024]  Iteration number: 0 with current cost as 0.6016901345575991 and parameters 
[-0.18336971  2.23743479 -2.12427932 -0.11653087  0.55388724 -2.77010913
  3.06858514  2.18960145  1.18552014 -1.06648324  0.6027151   1.14432461
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Fri Apr  5 00:57:18 2024]  Iteration number: 0 with current cost as 0.3672680875820209 and parameters 
[-1.3683026   2.23743464 -2.12427938 -0.11653103  0.55388721 -2.77010897
  3.06858511  2.18960158  1.18552011 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Fri Apr  5 01:00:11 2024]  Iteration number: 0 with current cost as 0.3708577281807032 and parameters 
[-1.4505243   2.23743464 -2.12427953 -0.11653081  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552009 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Fri Apr  5 01:03:19 2024]  Iteration number: 0 with current cost as 0.3404778403236841 and parameters 
[-1.43259807  2.23743464 -2.12427953 -0.11653103  0.55388708 -2.77010919
  3.06858498  2.18960145  1.18552009 -1.0664833   0.6027151   1.14432456
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Fri Apr  5 01:06:30 2024]  Iteration number: 0 with current cost as 0.3547029854076195 and parameters 
[-1.42087431  2.23743464 -2.12427946 -0.11653085  0.55388717 -2.77010897
  3.06858516  2.18960163  1.18552016 -1.06648308  0.60271528  1.14432463
  1.31029907 -1.8735468 ]. 
Training complete taking 924.7699625492096 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 01:08:55 2024]  Iteration number: 0 with current cost as 0.6016901345575991 and parameters 
[-0.18336971  2.23743479 -2.12427932 -0.11653087  0.55388724 -2.77010913
  3.06858514  2.18960145  1.18552014 -1.06648324  0.6027151   1.14432461
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Fri Apr  5 01:12:26 2024]  Iteration number: 0 with current cost as 0.3672680875820209 and parameters 
[-1.3683026   2.23743464 -2.12427938 -0.11653103  0.55388721 -2.77010897
  3.06858511  2.18960158  1.18552011 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Fri Apr  5 01:15:14 2024]  Iteration number: 0 with current cost as 0.3708577281807032 and parameters 
[-1.4505243   2.23743464 -2.12427953 -0.11653081  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552009 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Fri Apr  5 01:18:23 2024]  Iteration number: 0 with current cost as 0.3404778403236841 and parameters 
[-1.43259807  2.23743464 -2.12427953 -0.11653103  0.55388708 -2.77010919
  3.06858498  2.18960145  1.18552009 -1.0664833   0.6027151   1.14432456
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Fri Apr  5 01:21:28 2024]  Iteration number: 0 with current cost as 0.3547029854076195 and parameters 
[-1.42087431  2.23743464 -2.12427946 -0.11653085  0.55388717 -2.77010897
  3.06858516  2.18960163  1.18552016 -1.06648308  0.60271528  1.14432463
  1.31029907 -1.8735468 ]. 
Training complete taking 895.4362146854401 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 01:23:55 2024]  Iteration number: 0 with current cost as 0.6016901345575991 and parameters 
[-0.18336971  2.23743479 -2.12427932 -0.11653087  0.55388724 -2.77010913
  3.06858514  2.18960145  1.18552014 -1.06648324  0.6027151   1.14432461
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Fri Apr  5 01:27:25 2024]  Iteration number: 0 with current cost as 0.3672680875820209 and parameters 
[-1.3683026   2.23743464 -2.12427938 -0.11653103  0.55388721 -2.77010897
  3.06858511  2.18960158  1.18552011 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Fri Apr  5 01:30:13 2024]  Iteration number: 0 with current cost as 0.3708577281807032 and parameters 
[-1.4505243   2.23743464 -2.12427953 -0.11653081  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552009 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Fri Apr  5 01:33:25 2024]  Iteration number: 0 with current cost as 0.3404778403236841 and parameters 
[-1.43259807  2.23743464 -2.12427953 -0.11653103  0.55388708 -2.77010919
  3.06858498  2.18960145  1.18552009 -1.0664833   0.6027151   1.14432456
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Fri Apr  5 01:36:35 2024]  Iteration number: 0 with current cost as 0.3547029854076195 and parameters 
[-1.42087431  2.23743464 -2.12427946 -0.11653085  0.55388717 -2.77010897
  3.06858516  2.18960163  1.18552016 -1.06648308  0.60271528  1.14432463
  1.31029907 -1.8735468 ]. 
Training complete taking 908.2373247146606 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 01:39:00 2024]  Iteration number: 0 with current cost as 0.6016901345575991 and parameters 
[-0.18336971  2.23743479 -2.12427932 -0.11653087  0.55388724 -2.77010913
  3.06858514  2.18960145  1.18552014 -1.06648324  0.6027151   1.14432461
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Fri Apr  5 01:42:32 2024]  Iteration number: 0 with current cost as 0.3672680875820209 and parameters 
[-1.3683026   2.23743464 -2.12427938 -0.11653103  0.55388721 -2.77010897
  3.06858511  2.18960158  1.18552011 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Fri Apr  5 01:45:21 2024]  Iteration number: 0 with current cost as 0.3708577281807032 and parameters 
[-1.4505243   2.23743464 -2.12427953 -0.11653081  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552009 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Fri Apr  5 01:48:31 2024]  Iteration number: 0 with current cost as 0.3404778403236841 and parameters 
[-1.43259807  2.23743464 -2.12427953 -0.11653103  0.55388708 -2.77010919
  3.06858498  2.18960145  1.18552009 -1.0664833   0.6027151   1.14432456
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Fri Apr  5 01:51:39 2024]  Iteration number: 0 with current cost as 0.3547029854076195 and parameters 
[-1.42087431  2.23743464 -2.12427946 -0.11653085  0.55388717 -2.77010897
  3.06858516  2.18960163  1.18552016 -1.06648308  0.60271528  1.14432463
  1.31029907 -1.8735468 ]. 
Training complete taking 904.7494165897369 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 01:54:05 2024]  Iteration number: 0 with current cost as 0.6016901345575991 and parameters 
[-0.18336971  2.23743479 -2.12427932 -0.11653087  0.55388724 -2.77010913
  3.06858514  2.18960145  1.18552014 -1.06648324  0.6027151   1.14432461
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Fri Apr  5 01:57:35 2024]  Iteration number: 0 with current cost as 0.3672680875820209 and parameters 
[-1.3683026   2.23743464 -2.12427938 -0.11653103  0.55388721 -2.77010897
  3.06858511  2.18960158  1.18552011 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Fri Apr  5 02:00:22 2024]  Iteration number: 0 with current cost as 0.3708577281807032 and parameters 
[-1.4505243   2.23743464 -2.12427953 -0.11653081  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552009 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Fri Apr  5 02:03:29 2024]  Iteration number: 0 with current cost as 0.3404778403236841 and parameters 
[-1.43259807  2.23743464 -2.12427953 -0.11653103  0.55388708 -2.77010919
  3.06858498  2.18960145  1.18552009 -1.0664833   0.6027151   1.14432456
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Fri Apr  5 02:06:39 2024]  Iteration number: 0 with current cost as 0.3547029854076195 and parameters 
[-1.42087431  2.23743464 -2.12427946 -0.11653085  0.55388717 -2.77010897
  3.06858516  2.18960163  1.18552016 -1.06648308  0.60271528  1.14432463
  1.31029907 -1.8735468 ]. 
Training complete taking 900.3371829986572 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 02:09:07 2024]  Iteration number: 0 with current cost as 0.6016901345575991 and parameters 
[-0.18336971  2.23743479 -2.12427932 -0.11653087  0.55388724 -2.77010913
  3.06858514  2.18960145  1.18552014 -1.06648324  0.6027151   1.14432461
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Fri Apr  5 02:12:35 2024]  Iteration number: 0 with current cost as 0.3672680875820209 and parameters 
[-1.3683026   2.23743464 -2.12427938 -0.11653103  0.55388721 -2.77010897
  3.06858511  2.18960158  1.18552011 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Fri Apr  5 02:15:24 2024]  Iteration number: 0 with current cost as 0.3708577281807032 and parameters 
[-1.4505243   2.23743464 -2.12427953 -0.11653081  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552009 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Fri Apr  5 02:18:35 2024]  Iteration number: 0 with current cost as 0.3404778403236841 and parameters 
[-1.43259807  2.23743464 -2.12427953 -0.11653103  0.55388708 -2.77010919
  3.06858498  2.18960145  1.18552009 -1.0664833   0.6027151   1.14432456
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Fri Apr  5 02:21:46 2024]  Iteration number: 0 with current cost as 0.3547029854076195 and parameters 
[-1.42087431  2.23743464 -2.12427946 -0.11653085  0.55388717 -2.77010897
  3.06858516  2.18960163  1.18552016 -1.06648308  0.60271528  1.14432463
  1.31029907 -1.8735468 ]. 
Training complete taking 907.5161244869232 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 02:24:11 2024]  Iteration number: 0 with current cost as 0.6016901345575991 and parameters 
[-0.18336971  2.23743479 -2.12427932 -0.11653087  0.55388724 -2.77010913
  3.06858514  2.18960145  1.18552014 -1.06648324  0.6027151   1.14432461
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Fri Apr  5 02:27:44 2024]  Iteration number: 0 with current cost as 0.3672680875820209 and parameters 
[-1.3683026   2.23743464 -2.12427938 -0.11653103  0.55388721 -2.77010897
  3.06858511  2.18960158  1.18552011 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Fri Apr  5 02:30:35 2024]  Iteration number: 0 with current cost as 0.3708577281807032 and parameters 
[-1.4505243   2.23743464 -2.12427953 -0.11653081  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552009 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Fri Apr  5 02:33:42 2024]  Iteration number: 0 with current cost as 0.3404778403236841 and parameters 
[-1.43259807  2.23743464 -2.12427953 -0.11653103  0.55388708 -2.77010919
  3.06858498  2.18960145  1.18552009 -1.0664833   0.6027151   1.14432456
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Fri Apr  5 02:36:52 2024]  Iteration number: 0 with current cost as 0.3547029854076195 and parameters 
[-1.42087431  2.23743464 -2.12427946 -0.11653085  0.55388717 -2.77010897
  3.06858516  2.18960163  1.18552016 -1.06648308  0.60271528  1.14432463
  1.31029907 -1.8735468 ]. 
Training complete taking 905.700845003128 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 02:39:20 2024]  Iteration number: 0 with current cost as 0.6016901345575991 and parameters 
[-0.18336971  2.23743479 -2.12427932 -0.11653087  0.55388724 -2.77010913
  3.06858514  2.18960145  1.18552014 -1.06648324  0.6027151   1.14432461
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Fri Apr  5 02:42:49 2024]  Iteration number: 0 with current cost as 0.3672680875820209 and parameters 
[-1.3683026   2.23743464 -2.12427938 -0.11653103  0.55388721 -2.77010897
  3.06858511  2.18960158  1.18552011 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Fri Apr  5 02:45:37 2024]  Iteration number: 0 with current cost as 0.3708577281807032 and parameters 
[-1.4505243   2.23743464 -2.12427953 -0.11653081  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552009 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Fri Apr  5 02:48:49 2024]  Iteration number: 0 with current cost as 0.3404778403236841 and parameters 
[-1.43259807  2.23743464 -2.12427953 -0.11653103  0.55388708 -2.77010919
  3.06858498  2.18960145  1.18552009 -1.0664833   0.6027151   1.14432456
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Fri Apr  5 02:52:02 2024]  Iteration number: 0 with current cost as 0.3547029854076195 and parameters 
[-1.42087431  2.23743464 -2.12427946 -0.11653085  0.55388717 -2.77010897
  3.06858516  2.18960163  1.18552016 -1.06648308  0.60271528  1.14432463
  1.31029907 -1.8735468 ]. 
Training complete taking 913.3876299858093 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 02:54:37 2024]  Iteration number: 0 with current cost as 0.6016901345575991 and parameters 
[-0.18336971  2.23743479 -2.12427932 -0.11653087  0.55388724 -2.77010913
  3.06858514  2.18960145  1.18552014 -1.06648324  0.6027151   1.14432461
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Fri Apr  5 02:58:03 2024]  Iteration number: 0 with current cost as 0.3672680875820209 and parameters 
[-1.3683026   2.23743464 -2.12427938 -0.11653103  0.55388721 -2.77010897
  3.06858511  2.18960158  1.18552011 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Fri Apr  5 03:00:55 2024]  Iteration number: 0 with current cost as 0.3708577281807032 and parameters 
[-1.4505243   2.23743464 -2.12427953 -0.11653081  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552009 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Fri Apr  5 03:04:08 2024]  Iteration number: 0 with current cost as 0.3404778403236841 and parameters 
[-1.43259807  2.23743464 -2.12427953 -0.11653103  0.55388708 -2.77010919
  3.06858498  2.18960145  1.18552009 -1.0664833   0.6027151   1.14432456
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Fri Apr  5 03:07:18 2024]  Iteration number: 0 with current cost as 0.3547029854076195 and parameters 
[-1.42087431  2.23743464 -2.12427946 -0.11653085  0.55388717 -2.77010897
  3.06858516  2.18960163  1.18552016 -1.06648308  0.60271528  1.14432463
  1.31029907 -1.8735468 ]. 
Training complete taking 912.5710220336914 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 03:09:46 2024]  Iteration number: 0 with current cost as 0.6016901345575991 and parameters 
[-0.18336971  2.23743479 -2.12427932 -0.11653087  0.55388724 -2.77010913
  3.06858514  2.18960145  1.18552014 -1.06648324  0.6027151   1.14432461
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Fri Apr  5 03:13:17 2024]  Iteration number: 0 with current cost as 0.3672680875820209 and parameters 
[-1.3683026   2.23743464 -2.12427938 -0.11653103  0.55388721 -2.77010897
  3.06858511  2.18960158  1.18552011 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Fri Apr  5 03:16:04 2024]  Iteration number: 0 with current cost as 0.3708577281807032 and parameters 
[-1.4505243   2.23743464 -2.12427953 -0.11653081  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552009 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Fri Apr  5 03:19:16 2024]  Iteration number: 0 with current cost as 0.3404778403236841 and parameters 
[-1.43259807  2.23743464 -2.12427953 -0.11653103  0.55388708 -2.77010919
  3.06858498  2.18960145  1.18552009 -1.0664833   0.6027151   1.14432456
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Fri Apr  5 03:22:29 2024]  Iteration number: 0 with current cost as 0.3547029854076195 and parameters 
[-1.42087431  2.23743464 -2.12427946 -0.11653085  0.55388717 -2.77010897
  3.06858516  2.18960163  1.18552016 -1.06648308  0.60271528  1.14432463
  1.31029907 -1.8735468 ]. 
Training complete taking 911.326886177063 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 03:24:56 2024]  Iteration number: 0 with current cost as 0.6016901345575991 and parameters 
[-0.18336971  2.23743479 -2.12427932 -0.11653087  0.55388724 -2.77010913
  3.06858514  2.18960145  1.18552014 -1.06648324  0.6027151   1.14432461
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Fri Apr  5 03:28:29 2024]  Iteration number: 0 with current cost as 0.3672680875820209 and parameters 
[-1.3683026   2.23743464 -2.12427938 -0.11653103  0.55388721 -2.77010897
  3.06858511  2.18960158  1.18552011 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Fri Apr  5 03:31:22 2024]  Iteration number: 0 with current cost as 0.3708577281807032 and parameters 
[-1.4505243   2.23743464 -2.12427953 -0.11653081  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552009 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Fri Apr  5 03:34:27 2024]  Iteration number: 0 with current cost as 0.3404778403236841 and parameters 
[-1.43259807  2.23743464 -2.12427953 -0.11653103  0.55388708 -2.77010919
  3.06858498  2.18960145  1.18552009 -1.0664833   0.6027151   1.14432456
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Fri Apr  5 03:37:36 2024]  Iteration number: 0 with current cost as 0.3547029854076195 and parameters 
[-1.42087431  2.23743464 -2.12427946 -0.11653085  0.55388717 -2.77010897
  3.06858516  2.18960163  1.18552016 -1.06648308  0.60271528  1.14432463
  1.31029907 -1.8735468 ]. 
Training complete taking 907.2211129665375 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 03:40:03 2024]  Iteration number: 0 with current cost as 0.6016901345575991 and parameters 
[-0.18336971  2.23743479 -2.12427932 -0.11653087  0.55388724 -2.77010913
  3.06858514  2.18960145  1.18552014 -1.06648324  0.6027151   1.14432461
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Fri Apr  5 03:43:34 2024]  Iteration number: 0 with current cost as 0.3672680875820209 and parameters 
[-1.3683026   2.23743464 -2.12427938 -0.11653103  0.55388721 -2.77010897
  3.06858511  2.18960158  1.18552011 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Fri Apr  5 03:46:21 2024]  Iteration number: 0 with current cost as 0.3708577281807032 and parameters 
[-1.4505243   2.23743464 -2.12427953 -0.11653081  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552009 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Fri Apr  5 03:49:31 2024]  Iteration number: 0 with current cost as 0.3404778403236841 and parameters 
[-1.43259807  2.23743464 -2.12427953 -0.11653103  0.55388708 -2.77010919
  3.06858498  2.18960145  1.18552009 -1.0664833   0.6027151   1.14432456
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Fri Apr  5 03:52:39 2024]  Iteration number: 0 with current cost as 0.3547029854076195 and parameters 
[-1.42087431  2.23743464 -2.12427946 -0.11653085  0.55388717 -2.77010897
  3.06858516  2.18960163  1.18552016 -1.06648308  0.60271528  1.14432463
  1.31029907 -1.8735468 ]. 
Training complete taking 900.8292372226715 seconds. 
Discarding model... 

Training complete taking 22555.794303417206 total seconds. 
Now scoring model... 
Scoring complete taking 2.2568609714508057 seconds. 
Saved predicted values as M-A1-CNOT_Efficient-CRZ_predicted_values.csv
Model scores: {'MSE_train': (0.6017804311781931,), 'R2_train': -0.19828650831405503, 'MAE_train': 0.7025604329990941, 'MSE_test': 0.5763091985131001, 'R2_test': -0.07479645580701533, 'MAE_test': 0.7124948134439035}. 
Saved model results as M-A1-CNOT_Efficient-CRZ_results.json. 
