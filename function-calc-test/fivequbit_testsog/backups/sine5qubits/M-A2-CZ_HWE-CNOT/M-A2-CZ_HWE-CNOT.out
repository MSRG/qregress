/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /scratch/j/jacobsen/gjones/sine5qubits/sine_train.bin... 
Successfully loaded /scratch/j/jacobsen/gjones/sine5qubits/sine_train.bin into X and y data. 
Loading dataset from /scratch/j/jacobsen/gjones/sine5qubits/sine_test.bin... 
Successfully loaded /scratch/j/jacobsen/gjones/sine5qubits/sine_test.bin into X and y data. 
Training model with dataset /scratch/j/jacobsen/gjones/sine5qubits/sine_train.bin 
 at time Thu Apr  4 21:45:39 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 21:45:54 2024]  Iteration number: 0 with current cost as 0.31735821327508795 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.04137946  0.57384051 -2.65006874
  3.107895    2.25295558  1.0816713  -1.02240864  0.68962124  1.2249977
  1.4653264  -1.76610863  0.59430406]. 
[Thu Apr  4 21:52:15 2024]  Iteration number: 50 with current cost as 0.07393396623399207 and parameters 
[-2.90318321  2.23743469 -2.12427942  0.42859649 -2.09113177 -2.72404399
  3.51044454  2.63329473  0.85104023 -1.40347608 -1.60475656  0.49425432
  2.72122509 -2.48316658 -0.15431325]. 
Working on 0.4 fold... 
[Thu Apr  4 21:52:35 2024]  Iteration number: 0 with current cost as 0.32062112905749274 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.03259352  0.57305269 -2.64098894
  3.09985099  2.2612858   1.09079474 -1.02475134  0.69672181  1.22437397
  1.47113797 -1.76276607  0.58677797]. 
[Thu Apr  4 21:59:05 2024]  Iteration number: 50 with current cost as 0.07640087212405042 and parameters 
[-2.90318338  2.23743544 -2.1242791  -0.31031493 -1.20677679 -2.99691187
  3.11878624  2.54061653  1.47162514 -1.1447239  -0.31115288  0.71959608
  2.69624179 -2.48531193 -0.71162952]. 
Working on 0.6 fold... 
[Thu Apr  4 22:00:40 2024]  Iteration number: 0 with current cost as 0.31477495916359444 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.03218794  0.57179817 -2.64250367
  3.09713766  2.26479293  1.09347508 -1.02686268  0.70115279  1.22319642
  1.47176474 -1.76517093  0.57606648]. 
[Thu Apr  4 22:06:57 2024]  Iteration number: 50 with current cost as 0.073471871001489 and parameters 
[-2.90318328  2.23743427 -2.12427991  0.16102157 -1.48858133 -2.73706209
  3.45155038  2.61632521  0.82274913 -1.50265899 -1.40348067  0.4825953
  2.25231371 -2.96619073 -0.55058175]. 
Working on 0.8 fold... 
[Thu Apr  4 22:08:00 2024]  Iteration number: 0 with current cost as 0.3159729752008216 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.03610715  0.57897498 -2.6357183
  3.11321921  2.26208668  1.06729244 -1.02394466  0.70048354  1.22652933
  1.47455747 -1.7599202   0.58549684]. 
Working on 1.0 fold... 
[Thu Apr  4 22:13:51 2024]  Iteration number: 0 with current cost as 0.318838186304086 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.04277717  0.57130767 -2.65573315
  3.10671538  2.24813852  1.0864482  -1.02080161  0.68129037  1.22434376
  1.45928898 -1.76890305  0.60455002]. 
Training complete taking 1937.644790172577 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 22:18:10 2024]  Iteration number: 0 with current cost as 0.31735821327508795 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.04137946  0.57384051 -2.65006874
  3.107895    2.25295558  1.0816713  -1.02240864  0.68962124  1.2249977
  1.4653264  -1.76610863  0.59430406]. 
[Thu Apr  4 22:24:29 2024]  Iteration number: 50 with current cost as 0.07393396623399207 and parameters 
[-2.90318321  2.23743469 -2.12427942  0.42859649 -2.09113177 -2.72404399
  3.51044454  2.63329473  0.85104023 -1.40347608 -1.60475656  0.49425432
  2.72122509 -2.48316658 -0.15431325]. 
Working on 0.4 fold... 
[Thu Apr  4 22:24:50 2024]  Iteration number: 0 with current cost as 0.32062112905749274 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.03259352  0.57305269 -2.64098894
  3.09985099  2.2612858   1.09079474 -1.02475134  0.69672181  1.22437397
  1.47113797 -1.76276607  0.58677797]. 
[Thu Apr  4 22:31:15 2024]  Iteration number: 50 with current cost as 0.07640087212405042 and parameters 
[-2.90318338  2.23743544 -2.1242791  -0.31031493 -1.20677679 -2.99691187
  3.11878624  2.54061653  1.47162514 -1.1447239  -0.31115288  0.71959608
  2.69624179 -2.48531193 -0.71162952]. 
Working on 0.6 fold... 
[Thu Apr  4 22:32:47 2024]  Iteration number: 0 with current cost as 0.31477495916359444 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.03218794  0.57179817 -2.64250367
  3.09713766  2.26479293  1.09347508 -1.02686268  0.70115279  1.22319642
  1.47176474 -1.76517093  0.57606648]. 
[Thu Apr  4 22:39:04 2024]  Iteration number: 50 with current cost as 0.073471871001489 and parameters 
[-2.90318328  2.23743427 -2.12427991  0.16102157 -1.48858133 -2.73706209
  3.45155038  2.61632521  0.82274913 -1.50265899 -1.40348067  0.4825953
  2.25231371 -2.96619073 -0.55058175]. 
Working on 0.8 fold... 
[Thu Apr  4 22:40:06 2024]  Iteration number: 0 with current cost as 0.3159729752008216 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.03610715  0.57897498 -2.6357183
  3.11321921  2.26208668  1.06729244 -1.02394466  0.70048354  1.22652933
  1.47455747 -1.7599202   0.58549684]. 
Working on 1.0 fold... 
[Thu Apr  4 22:45:58 2024]  Iteration number: 0 with current cost as 0.318838186304086 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.04277717  0.57130767 -2.65573315
  3.10671538  2.24813852  1.0864482  -1.02080161  0.68129037  1.22434376
  1.45928898 -1.76890305  0.60455002]. 
Training complete taking 1925.4261310100555 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 22:50:16 2024]  Iteration number: 0 with current cost as 0.31735821327508795 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.04137946  0.57384051 -2.65006874
  3.107895    2.25295558  1.0816713  -1.02240864  0.68962124  1.2249977
  1.4653264  -1.76610863  0.59430406]. 
[Thu Apr  4 22:56:35 2024]  Iteration number: 50 with current cost as 0.07393396623399207 and parameters 
[-2.90318321  2.23743469 -2.12427942  0.42859649 -2.09113177 -2.72404399
  3.51044454  2.63329473  0.85104023 -1.40347608 -1.60475656  0.49425432
  2.72122509 -2.48316658 -0.15431325]. 
Working on 0.4 fold... 
[Thu Apr  4 22:56:57 2024]  Iteration number: 0 with current cost as 0.32062112905749274 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.03259352  0.57305269 -2.64098894
  3.09985099  2.2612858   1.09079474 -1.02475134  0.69672181  1.22437397
  1.47113797 -1.76276607  0.58677797]. 
[Thu Apr  4 23:03:23 2024]  Iteration number: 50 with current cost as 0.07640087212405042 and parameters 
[-2.90318338  2.23743544 -2.1242791  -0.31031493 -1.20677679 -2.99691187
  3.11878624  2.54061653  1.47162514 -1.1447239  -0.31115288  0.71959608
  2.69624179 -2.48531193 -0.71162952]. 
Working on 0.6 fold... 
[Thu Apr  4 23:04:54 2024]  Iteration number: 0 with current cost as 0.31477495916359444 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.03218794  0.57179817 -2.64250367
  3.09713766  2.26479293  1.09347508 -1.02686268  0.70115279  1.22319642
  1.47176474 -1.76517093  0.57606648]. 
[Thu Apr  4 23:11:11 2024]  Iteration number: 50 with current cost as 0.073471871001489 and parameters 
[-2.90318328  2.23743427 -2.12427991  0.16102157 -1.48858133 -2.73706209
  3.45155038  2.61632521  0.82274913 -1.50265899 -1.40348067  0.4825953
  2.25231371 -2.96619073 -0.55058175]. 
Working on 0.8 fold... 
[Thu Apr  4 23:12:14 2024]  Iteration number: 0 with current cost as 0.3159729752008216 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.03610715  0.57897498 -2.6357183
  3.11321921  2.26208668  1.06729244 -1.02394466  0.70048354  1.22652933
  1.47455747 -1.7599202   0.58549684]. 
Working on 1.0 fold... 
[Thu Apr  4 23:18:11 2024]  Iteration number: 0 with current cost as 0.318838186304086 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.04277717  0.57130767 -2.65573315
  3.10671538  2.24813852  1.0864482  -1.02080161  0.68129037  1.22434376
  1.45928898 -1.76890305  0.60455002]. 
Training complete taking 1953.6993870735168 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 23:22:51 2024]  Iteration number: 0 with current cost as 0.31735821327508795 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.04137946  0.57384051 -2.65006874
  3.107895    2.25295558  1.0816713  -1.02240864  0.68962124  1.2249977
  1.4653264  -1.76610863  0.59430406]. 
[Thu Apr  4 23:29:27 2024]  Iteration number: 50 with current cost as 0.07393396623399207 and parameters 
[-2.90318321  2.23743469 -2.12427942  0.42859649 -2.09113177 -2.72404399
  3.51044454  2.63329473  0.85104023 -1.40347608 -1.60475656  0.49425432
  2.72122509 -2.48316658 -0.15431325]. 
Working on 0.4 fold... 
[Thu Apr  4 23:29:47 2024]  Iteration number: 0 with current cost as 0.32062112905749274 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.03259352  0.57305269 -2.64098894
  3.09985099  2.2612858   1.09079474 -1.02475134  0.69672181  1.22437397
  1.47113797 -1.76276607  0.58677797]. 
[Thu Apr  4 23:36:09 2024]  Iteration number: 50 with current cost as 0.07640087212405042 and parameters 
[-2.90318338  2.23743544 -2.1242791  -0.31031493 -1.20677679 -2.99691187
  3.11878624  2.54061653  1.47162514 -1.1447239  -0.31115288  0.71959608
  2.69624179 -2.48531193 -0.71162952]. 
Working on 0.6 fold... 
[Thu Apr  4 23:37:55 2024]  Iteration number: 0 with current cost as 0.31477495916359444 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.03218794  0.57179817 -2.64250367
  3.09713766  2.26479293  1.09347508 -1.02686268  0.70115279  1.22319642
  1.47176474 -1.76517093  0.57606648]. 
[Thu Apr  4 23:44:12 2024]  Iteration number: 50 with current cost as 0.073471871001489 and parameters 
[-2.90318328  2.23743427 -2.12427991  0.16102157 -1.48858133 -2.73706209
  3.45155038  2.61632521  0.82274913 -1.50265899 -1.40348067  0.4825953
  2.25231371 -2.96619073 -0.55058175]. 
Working on 0.8 fold... 
[Thu Apr  4 23:45:15 2024]  Iteration number: 0 with current cost as 0.3159729752008216 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.03610715  0.57897498 -2.6357183
  3.11321921  2.26208668  1.06729244 -1.02394466  0.70048354  1.22652933
  1.47455747 -1.7599202   0.58549684]. 
Working on 1.0 fold... 
[Thu Apr  4 23:51:34 2024]  Iteration number: 0 with current cost as 0.318838186304086 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.04277717  0.57130767 -2.65573315
  3.10671538  2.24813852  1.0864482  -1.02080161  0.68129037  1.22434376
  1.45928898 -1.76890305  0.60455002]. 
Training complete taking 1993.129703760147 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 23:56:04 2024]  Iteration number: 0 with current cost as 0.31735821327508795 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.04137946  0.57384051 -2.65006874
  3.107895    2.25295558  1.0816713  -1.02240864  0.68962124  1.2249977
  1.4653264  -1.76610863  0.59430406]. 
[Fri Apr  5 00:02:34 2024]  Iteration number: 50 with current cost as 0.07393396623399207 and parameters 
[-2.90318321  2.23743469 -2.12427942  0.42859649 -2.09113177 -2.72404399
  3.51044454  2.63329473  0.85104023 -1.40347608 -1.60475656  0.49425432
  2.72122509 -2.48316658 -0.15431325]. 
Working on 0.4 fold... 
[Fri Apr  5 00:02:55 2024]  Iteration number: 0 with current cost as 0.32062112905749274 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.03259352  0.57305269 -2.64098894
  3.09985099  2.2612858   1.09079474 -1.02475134  0.69672181  1.22437397
  1.47113797 -1.76276607  0.58677797]. 
[Fri Apr  5 00:09:25 2024]  Iteration number: 50 with current cost as 0.07640087212405042 and parameters 
[-2.90318338  2.23743544 -2.1242791  -0.31031493 -1.20677679 -2.99691187
  3.11878624  2.54061653  1.47162514 -1.1447239  -0.31115288  0.71959608
  2.69624179 -2.48531193 -0.71162952]. 
Working on 0.6 fold... 
[Fri Apr  5 00:10:55 2024]  Iteration number: 0 with current cost as 0.31477495916359444 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.03218794  0.57179817 -2.64250367
  3.09713766  2.26479293  1.09347508 -1.02686268  0.70115279  1.22319642
  1.47176474 -1.76517093  0.57606648]. 
[Fri Apr  5 00:17:13 2024]  Iteration number: 50 with current cost as 0.073471871001489 and parameters 
[-2.90318328  2.23743427 -2.12427991  0.16102157 -1.48858133 -2.73706209
  3.45155038  2.61632521  0.82274913 -1.50265899 -1.40348067  0.4825953
  2.25231371 -2.96619073 -0.55058175]. 
Working on 0.8 fold... 
[Fri Apr  5 00:18:16 2024]  Iteration number: 0 with current cost as 0.3159729752008216 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.03610715  0.57897498 -2.6357183
  3.11321921  2.26208668  1.06729244 -1.02394466  0.70048354  1.22652933
  1.47455747 -1.7599202   0.58549684]. 
Working on 1.0 fold... 
[Fri Apr  5 00:24:37 2024]  Iteration number: 0 with current cost as 0.318838186304086 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.04277717  0.57130767 -2.65573315
  3.10671538  2.24813852  1.0864482  -1.02080161  0.68129037  1.22434376
  1.45928898 -1.76890305  0.60455002]. 
Training complete taking 1979.2164647579193 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 00:29:03 2024]  Iteration number: 0 with current cost as 0.31735821327508795 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.04137946  0.57384051 -2.65006874
  3.107895    2.25295558  1.0816713  -1.02240864  0.68962124  1.2249977
  1.4653264  -1.76610863  0.59430406]. 
[Fri Apr  5 00:35:22 2024]  Iteration number: 50 with current cost as 0.07393396623399207 and parameters 
[-2.90318321  2.23743469 -2.12427942  0.42859649 -2.09113177 -2.72404399
  3.51044454  2.63329473  0.85104023 -1.40347608 -1.60475656  0.49425432
  2.72122509 -2.48316658 -0.15431325]. 
Working on 0.4 fold... 
[Fri Apr  5 00:35:44 2024]  Iteration number: 0 with current cost as 0.32062112905749274 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.03259352  0.57305269 -2.64098894
  3.09985099  2.2612858   1.09079474 -1.02475134  0.69672181  1.22437397
  1.47113797 -1.76276607  0.58677797]. 
[Fri Apr  5 00:42:26 2024]  Iteration number: 50 with current cost as 0.07640087212405042 and parameters 
[-2.90318338  2.23743544 -2.1242791  -0.31031493 -1.20677679 -2.99691187
  3.11878624  2.54061653  1.47162514 -1.1447239  -0.31115288  0.71959608
  2.69624179 -2.48531193 -0.71162952]. 
Working on 0.6 fold... 
[Fri Apr  5 00:44:03 2024]  Iteration number: 0 with current cost as 0.31477495916359444 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.03218794  0.57179817 -2.64250367
  3.09713766  2.26479293  1.09347508 -1.02686268  0.70115279  1.22319642
  1.47176474 -1.76517093  0.57606648]. 
[Fri Apr  5 00:50:30 2024]  Iteration number: 50 with current cost as 0.073471871001489 and parameters 
[-2.90318328  2.23743427 -2.12427991  0.16102157 -1.48858133 -2.73706209
  3.45155038  2.61632521  0.82274913 -1.50265899 -1.40348067  0.4825953
  2.25231371 -2.96619073 -0.55058175]. 
Working on 0.8 fold... 
[Fri Apr  5 00:51:44 2024]  Iteration number: 0 with current cost as 0.3159729752008216 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.03610715  0.57897498 -2.6357183
  3.11321921  2.26208668  1.06729244 -1.02394466  0.70048354  1.22652933
  1.47455747 -1.7599202   0.58549684]. 
Working on 1.0 fold... 
[Fri Apr  5 00:57:53 2024]  Iteration number: 0 with current cost as 0.318838186304086 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.04277717  0.57130767 -2.65573315
  3.10671538  2.24813852  1.0864482  -1.02080161  0.68129037  1.22434376
  1.45928898 -1.76890305  0.60455002]. 
Training complete taking 1989.876160144806 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 01:02:11 2024]  Iteration number: 0 with current cost as 0.31735821327508795 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.04137946  0.57384051 -2.65006874
  3.107895    2.25295558  1.0816713  -1.02240864  0.68962124  1.2249977
  1.4653264  -1.76610863  0.59430406]. 
[Fri Apr  5 01:08:45 2024]  Iteration number: 50 with current cost as 0.07393396623399207 and parameters 
[-2.90318321  2.23743469 -2.12427942  0.42859649 -2.09113177 -2.72404399
  3.51044454  2.63329473  0.85104023 -1.40347608 -1.60475656  0.49425432
  2.72122509 -2.48316658 -0.15431325]. 
Working on 0.4 fold... 
[Fri Apr  5 01:09:06 2024]  Iteration number: 0 with current cost as 0.32062112905749274 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.03259352  0.57305269 -2.64098894
  3.09985099  2.2612858   1.09079474 -1.02475134  0.69672181  1.22437397
  1.47113797 -1.76276607  0.58677797]. 
[Fri Apr  5 01:15:45 2024]  Iteration number: 50 with current cost as 0.07640087212405042 and parameters 
[-2.90318338  2.23743544 -2.1242791  -0.31031493 -1.20677679 -2.99691187
  3.11878624  2.54061653  1.47162514 -1.1447239  -0.31115288  0.71959608
  2.69624179 -2.48531193 -0.71162952]. 
Working on 0.6 fold... 
[Fri Apr  5 01:17:26 2024]  Iteration number: 0 with current cost as 0.31477495916359444 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.03218794  0.57179817 -2.64250367
  3.09713766  2.26479293  1.09347508 -1.02686268  0.70115279  1.22319642
  1.47176474 -1.76517093  0.57606648]. 
[Fri Apr  5 01:24:08 2024]  Iteration number: 50 with current cost as 0.073471871001489 and parameters 
[-2.90318328  2.23743427 -2.12427991  0.16102157 -1.48858133 -2.73706209
  3.45155038  2.61632521  0.82274913 -1.50265899 -1.40348067  0.4825953
  2.25231371 -2.96619073 -0.55058175]. 
Working on 0.8 fold... 
[Fri Apr  5 01:25:10 2024]  Iteration number: 0 with current cost as 0.3159729752008216 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.03610715  0.57897498 -2.6357183
  3.11321921  2.26208668  1.06729244 -1.02394466  0.70048354  1.22652933
  1.47455747 -1.7599202   0.58549684]. 
Working on 1.0 fold... 
[Fri Apr  5 01:31:11 2024]  Iteration number: 0 with current cost as 0.318838186304086 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.04277717  0.57130767 -2.65573315
  3.10671538  2.24813852  1.0864482  -1.02080161  0.68129037  1.22434376
  1.45928898 -1.76890305  0.60455002]. 
Training complete taking 2003.5017561912537 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 01:35:38 2024]  Iteration number: 0 with current cost as 0.31735821327508795 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.04137946  0.57384051 -2.65006874
  3.107895    2.25295558  1.0816713  -1.02240864  0.68962124  1.2249977
  1.4653264  -1.76610863  0.59430406]. 
[Fri Apr  5 01:42:05 2024]  Iteration number: 50 with current cost as 0.07393396623399207 and parameters 
[-2.90318321  2.23743469 -2.12427942  0.42859649 -2.09113177 -2.72404399
  3.51044454  2.63329473  0.85104023 -1.40347608 -1.60475656  0.49425432
  2.72122509 -2.48316658 -0.15431325]. 
Working on 0.4 fold... 
[Fri Apr  5 01:42:25 2024]  Iteration number: 0 with current cost as 0.32062112905749274 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.03259352  0.57305269 -2.64098894
  3.09985099  2.2612858   1.09079474 -1.02475134  0.69672181  1.22437397
  1.47113797 -1.76276607  0.58677797]. 
[Fri Apr  5 01:49:06 2024]  Iteration number: 50 with current cost as 0.07640087212405042 and parameters 
[-2.90318338  2.23743544 -2.1242791  -0.31031493 -1.20677679 -2.99691187
  3.11878624  2.54061653  1.47162514 -1.1447239  -0.31115288  0.71959608
  2.69624179 -2.48531193 -0.71162952]. 
Working on 0.6 fold... 
[Fri Apr  5 01:50:37 2024]  Iteration number: 0 with current cost as 0.31477495916359444 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.03218794  0.57179817 -2.64250367
  3.09713766  2.26479293  1.09347508 -1.02686268  0.70115279  1.22319642
  1.47176474 -1.76517093  0.57606648]. 
[Fri Apr  5 01:57:01 2024]  Iteration number: 50 with current cost as 0.073471871001489 and parameters 
[-2.90318328  2.23743427 -2.12427991  0.16102157 -1.48858133 -2.73706209
  3.45155038  2.61632521  0.82274913 -1.50265899 -1.40348067  0.4825953
  2.25231371 -2.96619073 -0.55058175]. 
Working on 0.8 fold... 
[Fri Apr  5 01:58:04 2024]  Iteration number: 0 with current cost as 0.3159729752008216 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.03610715  0.57897498 -2.6357183
  3.11321921  2.26208668  1.06729244 -1.02394466  0.70048354  1.22652933
  1.47455747 -1.7599202   0.58549684]. 
Working on 1.0 fold... 
[Fri Apr  5 02:03:53 2024]  Iteration number: 0 with current cost as 0.318838186304086 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.04277717  0.57130767 -2.65573315
  3.10671538  2.24813852  1.0864482  -1.02080161  0.68129037  1.22434376
  1.45928898 -1.76890305  0.60455002]. 
Training complete taking 1955.7092638015747 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 02:08:12 2024]  Iteration number: 0 with current cost as 0.31735821327508795 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.04137946  0.57384051 -2.65006874
  3.107895    2.25295558  1.0816713  -1.02240864  0.68962124  1.2249977
  1.4653264  -1.76610863  0.59430406]. 
[Fri Apr  5 02:14:32 2024]  Iteration number: 50 with current cost as 0.07393396623399207 and parameters 
[-2.90318321  2.23743469 -2.12427942  0.42859649 -2.09113177 -2.72404399
  3.51044454  2.63329473  0.85104023 -1.40347608 -1.60475656  0.49425432
  2.72122509 -2.48316658 -0.15431325]. 
Working on 0.4 fold... 
[Fri Apr  5 02:14:52 2024]  Iteration number: 0 with current cost as 0.32062112905749274 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.03259352  0.57305269 -2.64098894
  3.09985099  2.2612858   1.09079474 -1.02475134  0.69672181  1.22437397
  1.47113797 -1.76276607  0.58677797]. 
[Fri Apr  5 02:21:14 2024]  Iteration number: 50 with current cost as 0.07640087212405042 and parameters 
[-2.90318338  2.23743544 -2.1242791  -0.31031493 -1.20677679 -2.99691187
  3.11878624  2.54061653  1.47162514 -1.1447239  -0.31115288  0.71959608
  2.69624179 -2.48531193 -0.71162952]. 
Working on 0.6 fold... 
[Fri Apr  5 02:22:45 2024]  Iteration number: 0 with current cost as 0.31477495916359444 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.03218794  0.57179817 -2.64250367
  3.09713766  2.26479293  1.09347508 -1.02686268  0.70115279  1.22319642
  1.47176474 -1.76517093  0.57606648]. 
[Fri Apr  5 02:29:05 2024]  Iteration number: 50 with current cost as 0.073471871001489 and parameters 
[-2.90318328  2.23743427 -2.12427991  0.16102157 -1.48858133 -2.73706209
  3.45155038  2.61632521  0.82274913 -1.50265899 -1.40348067  0.4825953
  2.25231371 -2.96619073 -0.55058175]. 
Working on 0.8 fold... 
[Fri Apr  5 02:30:07 2024]  Iteration number: 0 with current cost as 0.3159729752008216 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.03610715  0.57897498 -2.6357183
  3.11321921  2.26208668  1.06729244 -1.02394466  0.70048354  1.22652933
  1.47455747 -1.7599202   0.58549684]. 
Working on 1.0 fold... 
[Fri Apr  5 02:36:02 2024]  Iteration number: 0 with current cost as 0.318838186304086 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.04277717  0.57130767 -2.65573315
  3.10671538  2.24813852  1.0864482  -1.02080161  0.68129037  1.22434376
  1.45928898 -1.76890305  0.60455002]. 
Training complete taking 1929.14288854599 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 02:40:20 2024]  Iteration number: 0 with current cost as 0.31735821327508795 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.04137946  0.57384051 -2.65006874
  3.107895    2.25295558  1.0816713  -1.02240864  0.68962124  1.2249977
  1.4653264  -1.76610863  0.59430406]. 
[Fri Apr  5 02:46:46 2024]  Iteration number: 50 with current cost as 0.07393396623399207 and parameters 
[-2.90318321  2.23743469 -2.12427942  0.42859649 -2.09113177 -2.72404399
  3.51044454  2.63329473  0.85104023 -1.40347608 -1.60475656  0.49425432
  2.72122509 -2.48316658 -0.15431325]. 
Working on 0.4 fold... 
[Fri Apr  5 02:47:07 2024]  Iteration number: 0 with current cost as 0.32062112905749274 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.03259352  0.57305269 -2.64098894
  3.09985099  2.2612858   1.09079474 -1.02475134  0.69672181  1.22437397
  1.47113797 -1.76276607  0.58677797]. 
[Fri Apr  5 02:53:47 2024]  Iteration number: 50 with current cost as 0.07640087212405042 and parameters 
[-2.90318338  2.23743544 -2.1242791  -0.31031493 -1.20677679 -2.99691187
  3.11878624  2.54061653  1.47162514 -1.1447239  -0.31115288  0.71959608
  2.69624179 -2.48531193 -0.71162952]. 
Working on 0.6 fold... 
[Fri Apr  5 02:55:17 2024]  Iteration number: 0 with current cost as 0.31477495916359444 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.03218794  0.57179817 -2.64250367
  3.09713766  2.26479293  1.09347508 -1.02686268  0.70115279  1.22319642
  1.47176474 -1.76517093  0.57606648]. 
[Fri Apr  5 03:01:36 2024]  Iteration number: 50 with current cost as 0.073471871001489 and parameters 
[-2.90318328  2.23743427 -2.12427991  0.16102157 -1.48858133 -2.73706209
  3.45155038  2.61632521  0.82274913 -1.50265899 -1.40348067  0.4825953
  2.25231371 -2.96619073 -0.55058175]. 
Working on 0.8 fold... 
[Fri Apr  5 03:02:38 2024]  Iteration number: 0 with current cost as 0.3159729752008216 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.03610715  0.57897498 -2.6357183
  3.11321921  2.26208668  1.06729244 -1.02394466  0.70048354  1.22652933
  1.47455747 -1.7599202   0.58549684]. 
Working on 1.0 fold... 
[Fri Apr  5 03:08:25 2024]  Iteration number: 0 with current cost as 0.318838186304086 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.04277717  0.57130767 -2.65573315
  3.10671538  2.24813852  1.0864482  -1.02080161  0.68129037  1.22434376
  1.45928898 -1.76890305  0.60455002]. 
Training complete taking 1941.896734237671 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 03:12:43 2024]  Iteration number: 0 with current cost as 0.31735821327508795 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.04137946  0.57384051 -2.65006874
  3.107895    2.25295558  1.0816713  -1.02240864  0.68962124  1.2249977
  1.4653264  -1.76610863  0.59430406]. 
[Fri Apr  5 03:19:00 2024]  Iteration number: 50 with current cost as 0.07393396623399207 and parameters 
[-2.90318321  2.23743469 -2.12427942  0.42859649 -2.09113177 -2.72404399
  3.51044454  2.63329473  0.85104023 -1.40347608 -1.60475656  0.49425432
  2.72122509 -2.48316658 -0.15431325]. 
Working on 0.4 fold... 
[Fri Apr  5 03:19:22 2024]  Iteration number: 0 with current cost as 0.32062112905749274 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.03259352  0.57305269 -2.64098894
  3.09985099  2.2612858   1.09079474 -1.02475134  0.69672181  1.22437397
  1.47113797 -1.76276607  0.58677797]. 
[Fri Apr  5 03:25:46 2024]  Iteration number: 50 with current cost as 0.07640087212405042 and parameters 
[-2.90318338  2.23743544 -2.1242791  -0.31031493 -1.20677679 -2.99691187
  3.11878624  2.54061653  1.47162514 -1.1447239  -0.31115288  0.71959608
  2.69624179 -2.48531193 -0.71162952]. 
Working on 0.6 fold... 
[Fri Apr  5 03:27:17 2024]  Iteration number: 0 with current cost as 0.31477495916359444 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.03218794  0.57179817 -2.64250367
  3.09713766  2.26479293  1.09347508 -1.02686268  0.70115279  1.22319642
  1.47176474 -1.76517093  0.57606648]. 
[Fri Apr  5 03:33:33 2024]  Iteration number: 50 with current cost as 0.073471871001489 and parameters 
[-2.90318328  2.23743427 -2.12427991  0.16102157 -1.48858133 -2.73706209
  3.45155038  2.61632521  0.82274913 -1.50265899 -1.40348067  0.4825953
  2.25231371 -2.96619073 -0.55058175]. 
Working on 0.8 fold... 
[Fri Apr  5 03:34:37 2024]  Iteration number: 0 with current cost as 0.3159729752008216 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.03610715  0.57897498 -2.6357183
  3.11321921  2.26208668  1.06729244 -1.02394466  0.70048354  1.22652933
  1.47455747 -1.7599202   0.58549684]. 
Working on 1.0 fold... 
[Fri Apr  5 03:40:23 2024]  Iteration number: 0 with current cost as 0.318838186304086 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.04277717  0.57130767 -2.65573315
  3.10671538  2.24813852  1.0864482  -1.02080161  0.68129037  1.22434376
  1.45928898 -1.76890305  0.60455002]. 
Training complete taking 1917.0421741008759 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 03:44:40 2024]  Iteration number: 0 with current cost as 0.31735821327508795 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.04137946  0.57384051 -2.65006874
  3.107895    2.25295558  1.0816713  -1.02240864  0.68962124  1.2249977
  1.4653264  -1.76610863  0.59430406]. 
[Fri Apr  5 03:50:55 2024]  Iteration number: 50 with current cost as 0.07393396623399207 and parameters 
[-2.90318321  2.23743469 -2.12427942  0.42859649 -2.09113177 -2.72404399
  3.51044454  2.63329473  0.85104023 -1.40347608 -1.60475656  0.49425432
  2.72122509 -2.48316658 -0.15431325]. 
Working on 0.4 fold... 
[Fri Apr  5 03:51:16 2024]  Iteration number: 0 with current cost as 0.32062112905749274 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.03259352  0.57305269 -2.64098894
  3.09985099  2.2612858   1.09079474 -1.02475134  0.69672181  1.22437397
  1.47113797 -1.76276607  0.58677797]. 
[Fri Apr  5 03:57:40 2024]  Iteration number: 50 with current cost as 0.07640087212405042 and parameters 
[-2.90318338  2.23743544 -2.1242791  -0.31031493 -1.20677679 -2.99691187
  3.11878624  2.54061653  1.47162514 -1.1447239  -0.31115288  0.71959608
  2.69624179 -2.48531193 -0.71162952]. 
Working on 0.6 fold... 
[Fri Apr  5 03:59:11 2024]  Iteration number: 0 with current cost as 0.31477495916359444 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.03218794  0.57179817 -2.64250367
  3.09713766  2.26479293  1.09347508 -1.02686268  0.70115279  1.22319642
  1.47176474 -1.76517093  0.57606648]. 
[Fri Apr  5 04:05:27 2024]  Iteration number: 50 with current cost as 0.073471871001489 and parameters 
[-2.90318328  2.23743427 -2.12427991  0.16102157 -1.48858133 -2.73706209
  3.45155038  2.61632521  0.82274913 -1.50265899 -1.40348067  0.4825953
  2.25231371 -2.96619073 -0.55058175]. 
Working on 0.8 fold... 
[Fri Apr  5 04:06:29 2024]  Iteration number: 0 with current cost as 0.3159729752008216 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.03610715  0.57897498 -2.6357183
  3.11321921  2.26208668  1.06729244 -1.02394466  0.70048354  1.22652933
  1.47455747 -1.7599202   0.58549684]. 
Working on 1.0 fold... 
[Fri Apr  5 04:12:17 2024]  Iteration number: 0 with current cost as 0.318838186304086 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.04277717  0.57130767 -2.65573315
  3.10671538  2.24813852  1.0864482  -1.02080161  0.68129037  1.22434376
  1.45928898 -1.76890305  0.60455002]. 
Training complete taking 1915.3343253135681 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 04:16:34 2024]  Iteration number: 0 with current cost as 0.31735821327508795 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.04137946  0.57384051 -2.65006874
  3.107895    2.25295558  1.0816713  -1.02240864  0.68962124  1.2249977
  1.4653264  -1.76610863  0.59430406]. 
[Fri Apr  5 04:22:49 2024]  Iteration number: 50 with current cost as 0.07393396623399207 and parameters 
[-2.90318321  2.23743469 -2.12427942  0.42859649 -2.09113177 -2.72404399
  3.51044454  2.63329473  0.85104023 -1.40347608 -1.60475656  0.49425432
  2.72122509 -2.48316658 -0.15431325]. 
Working on 0.4 fold... 
[Fri Apr  5 04:23:10 2024]  Iteration number: 0 with current cost as 0.32062112905749274 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.03259352  0.57305269 -2.64098894
  3.09985099  2.2612858   1.09079474 -1.02475134  0.69672181  1.22437397
  1.47113797 -1.76276607  0.58677797]. 
[Fri Apr  5 04:29:32 2024]  Iteration number: 50 with current cost as 0.07640087212405042 and parameters 
[-2.90318338  2.23743544 -2.1242791  -0.31031493 -1.20677679 -2.99691187
  3.11878624  2.54061653  1.47162514 -1.1447239  -0.31115288  0.71959608
  2.69624179 -2.48531193 -0.71162952]. 
Working on 0.6 fold... 
[Fri Apr  5 04:31:03 2024]  Iteration number: 0 with current cost as 0.31477495916359444 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.03218794  0.57179817 -2.64250367
  3.09713766  2.26479293  1.09347508 -1.02686268  0.70115279  1.22319642
  1.47176474 -1.76517093  0.57606648]. 
[Fri Apr  5 04:37:20 2024]  Iteration number: 50 with current cost as 0.073471871001489 and parameters 
[-2.90318328  2.23743427 -2.12427991  0.16102157 -1.48858133 -2.73706209
  3.45155038  2.61632521  0.82274913 -1.50265899 -1.40348067  0.4825953
  2.25231371 -2.96619073 -0.55058175]. 
Working on 0.8 fold... 
[Fri Apr  5 04:38:23 2024]  Iteration number: 0 with current cost as 0.3159729752008216 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.03610715  0.57897498 -2.6357183
  3.11321921  2.26208668  1.06729244 -1.02394466  0.70048354  1.22652933
  1.47455747 -1.7599202   0.58549684]. 
Working on 1.0 fold... 
[Fri Apr  5 04:44:12 2024]  Iteration number: 0 with current cost as 0.318838186304086 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.04277717  0.57130767 -2.65573315
  3.10671538  2.24813852  1.0864482  -1.02080161  0.68129037  1.22434376
  1.45928898 -1.76890305  0.60455002]. 
Training complete taking 1915.3938417434692 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 04:48:29 2024]  Iteration number: 0 with current cost as 0.31735821327508795 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.04137946  0.57384051 -2.65006874
  3.107895    2.25295558  1.0816713  -1.02240864  0.68962124  1.2249977
  1.4653264  -1.76610863  0.59430406]. 
[Fri Apr  5 04:54:46 2024]  Iteration number: 50 with current cost as 0.07393396623399207 and parameters 
[-2.90318321  2.23743469 -2.12427942  0.42859649 -2.09113177 -2.72404399
  3.51044454  2.63329473  0.85104023 -1.40347608 -1.60475656  0.49425432
  2.72122509 -2.48316658 -0.15431325]. 
Working on 0.4 fold... 
[Fri Apr  5 04:55:06 2024]  Iteration number: 0 with current cost as 0.32062112905749274 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.03259352  0.57305269 -2.64098894
  3.09985099  2.2612858   1.09079474 -1.02475134  0.69672181  1.22437397
  1.47113797 -1.76276607  0.58677797]. 
[Fri Apr  5 05:01:27 2024]  Iteration number: 50 with current cost as 0.07640087212405042 and parameters 
[-2.90318338  2.23743544 -2.1242791  -0.31031493 -1.20677679 -2.99691187
  3.11878624  2.54061653  1.47162514 -1.1447239  -0.31115288  0.71959608
  2.69624179 -2.48531193 -0.71162952]. 
Working on 0.6 fold... 
[Fri Apr  5 05:02:58 2024]  Iteration number: 0 with current cost as 0.31477495916359444 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.03218794  0.57179817 -2.64250367
  3.09713766  2.26479293  1.09347508 -1.02686268  0.70115279  1.22319642
  1.47176474 -1.76517093  0.57606648]. 
[Fri Apr  5 05:09:16 2024]  Iteration number: 50 with current cost as 0.073471871001489 and parameters 
[-2.90318328  2.23743427 -2.12427991  0.16102157 -1.48858133 -2.73706209
  3.45155038  2.61632521  0.82274913 -1.50265899 -1.40348067  0.4825953
  2.25231371 -2.96619073 -0.55058175]. 
Working on 0.8 fold... 
[Fri Apr  5 05:10:20 2024]  Iteration number: 0 with current cost as 0.3159729752008216 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.03610715  0.57897498 -2.6357183
  3.11321921  2.26208668  1.06729244 -1.02394466  0.70048354  1.22652933
  1.47455747 -1.7599202   0.58549684]. 
Working on 1.0 fold... 
[Fri Apr  5 05:16:13 2024]  Iteration number: 0 with current cost as 0.318838186304086 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.04277717  0.57130767 -2.65573315
  3.10671538  2.24813852  1.0864482  -1.02080161  0.68129037  1.22434376
  1.45928898 -1.76890305  0.60455002]. 
Training complete taking 1922.099023103714 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 05:20:32 2024]  Iteration number: 0 with current cost as 0.31735821327508795 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.04137946  0.57384051 -2.65006874
  3.107895    2.25295558  1.0816713  -1.02240864  0.68962124  1.2249977
  1.4653264  -1.76610863  0.59430406]. 
[Fri Apr  5 05:26:54 2024]  Iteration number: 50 with current cost as 0.07393396623399207 and parameters 
[-2.90318321  2.23743469 -2.12427942  0.42859649 -2.09113177 -2.72404399
  3.51044454  2.63329473  0.85104023 -1.40347608 -1.60475656  0.49425432
  2.72122509 -2.48316658 -0.15431325]. 
Working on 0.4 fold... 
[Fri Apr  5 05:27:16 2024]  Iteration number: 0 with current cost as 0.32062112905749274 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.03259352  0.57305269 -2.64098894
  3.09985099  2.2612858   1.09079474 -1.02475134  0.69672181  1.22437397
  1.47113797 -1.76276607  0.58677797]. 
[Fri Apr  5 05:33:38 2024]  Iteration number: 50 with current cost as 0.07640087212405042 and parameters 
[-2.90318338  2.23743544 -2.1242791  -0.31031493 -1.20677679 -2.99691187
  3.11878624  2.54061653  1.47162514 -1.1447239  -0.31115288  0.71959608
  2.69624179 -2.48531193 -0.71162952]. 
Working on 0.6 fold... 
[Fri Apr  5 05:35:08 2024]  Iteration number: 0 with current cost as 0.31477495916359444 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.03218794  0.57179817 -2.64250367
  3.09713766  2.26479293  1.09347508 -1.02686268  0.70115279  1.22319642
  1.47176474 -1.76517093  0.57606648]. 
[Fri Apr  5 05:41:22 2024]  Iteration number: 50 with current cost as 0.073471871001489 and parameters 
[-2.90318328  2.23743427 -2.12427991  0.16102157 -1.48858133 -2.73706209
  3.45155038  2.61632521  0.82274913 -1.50265899 -1.40348067  0.4825953
  2.25231371 -2.96619073 -0.55058175]. 
Working on 0.8 fold... 
[Fri Apr  5 05:42:24 2024]  Iteration number: 0 with current cost as 0.3159729752008216 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.03610715  0.57897498 -2.6357183
  3.11321921  2.26208668  1.06729244 -1.02394466  0.70048354  1.22652933
  1.47455747 -1.7599202   0.58549684]. 
Working on 1.0 fold... 
[Fri Apr  5 05:48:18 2024]  Iteration number: 0 with current cost as 0.318838186304086 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.04277717  0.57130767 -2.65573315
  3.10671538  2.24813852  1.0864482  -1.02080161  0.68129037  1.22434376
  1.45928898 -1.76890305  0.60455002]. 
Training complete taking 1931.8510808944702 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 05:52:45 2024]  Iteration number: 0 with current cost as 0.31735821327508795 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.04137946  0.57384051 -2.65006874
  3.107895    2.25295558  1.0816713  -1.02240864  0.68962124  1.2249977
  1.4653264  -1.76610863  0.59430406]. 
[Fri Apr  5 05:59:09 2024]  Iteration number: 50 with current cost as 0.07393396623399207 and parameters 
[-2.90318321  2.23743469 -2.12427942  0.42859649 -2.09113177 -2.72404399
  3.51044454  2.63329473  0.85104023 -1.40347608 -1.60475656  0.49425432
  2.72122509 -2.48316658 -0.15431325]. 
Working on 0.4 fold... 
[Fri Apr  5 05:59:31 2024]  Iteration number: 0 with current cost as 0.32062112905749274 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.03259352  0.57305269 -2.64098894
  3.09985099  2.2612858   1.09079474 -1.02475134  0.69672181  1.22437397
  1.47113797 -1.76276607  0.58677797]. 
[Fri Apr  5 06:05:58 2024]  Iteration number: 50 with current cost as 0.07640087212405042 and parameters 
[-2.90318338  2.23743544 -2.1242791  -0.31031493 -1.20677679 -2.99691187
  3.11878624  2.54061653  1.47162514 -1.1447239  -0.31115288  0.71959608
  2.69624179 -2.48531193 -0.71162952]. 
Working on 0.6 fold... 
[Fri Apr  5 06:07:30 2024]  Iteration number: 0 with current cost as 0.31477495916359444 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.03218794  0.57179817 -2.64250367
  3.09713766  2.26479293  1.09347508 -1.02686268  0.70115279  1.22319642
  1.47176474 -1.76517093  0.57606648]. 
[Fri Apr  5 06:13:46 2024]  Iteration number: 50 with current cost as 0.073471871001489 and parameters 
[-2.90318328  2.23743427 -2.12427991  0.16102157 -1.48858133 -2.73706209
  3.45155038  2.61632521  0.82274913 -1.50265899 -1.40348067  0.4825953
  2.25231371 -2.96619073 -0.55058175]. 
Working on 0.8 fold... 
[Fri Apr  5 06:14:50 2024]  Iteration number: 0 with current cost as 0.3159729752008216 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.03610715  0.57897498 -2.6357183
  3.11321921  2.26208668  1.06729244 -1.02394466  0.70048354  1.22652933
  1.47455747 -1.7599202   0.58549684]. 
Working on 1.0 fold... 
[Fri Apr  5 06:20:35 2024]  Iteration number: 0 with current cost as 0.318838186304086 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.04277717  0.57130767 -2.65573315
  3.10671538  2.24813852  1.0864482  -1.02080161  0.68129037  1.22434376
  1.45928898 -1.76890305  0.60455002]. 
Training complete taking 1931.592899799347 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 06:24:56 2024]  Iteration number: 0 with current cost as 0.31735821327508795 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.04137946  0.57384051 -2.65006874
  3.107895    2.25295558  1.0816713  -1.02240864  0.68962124  1.2249977
  1.4653264  -1.76610863  0.59430406]. 
[Fri Apr  5 06:31:15 2024]  Iteration number: 50 with current cost as 0.07393396623399207 and parameters 
[-2.90318321  2.23743469 -2.12427942  0.42859649 -2.09113177 -2.72404399
  3.51044454  2.63329473  0.85104023 -1.40347608 -1.60475656  0.49425432
  2.72122509 -2.48316658 -0.15431325]. 
Working on 0.4 fold... 
[Fri Apr  5 06:31:41 2024]  Iteration number: 0 with current cost as 0.32062112905749274 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.03259352  0.57305269 -2.64098894
  3.09985099  2.2612858   1.09079474 -1.02475134  0.69672181  1.22437397
  1.47113797 -1.76276607  0.58677797]. 
[Fri Apr  5 06:38:06 2024]  Iteration number: 50 with current cost as 0.07640087212405042 and parameters 
[-2.90318338  2.23743544 -2.1242791  -0.31031493 -1.20677679 -2.99691187
  3.11878624  2.54061653  1.47162514 -1.1447239  -0.31115288  0.71959608
  2.69624179 -2.48531193 -0.71162952]. 
Working on 0.6 fold... 
[Fri Apr  5 06:39:36 2024]  Iteration number: 0 with current cost as 0.31477495916359444 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.03218794  0.57179817 -2.64250367
  3.09713766  2.26479293  1.09347508 -1.02686268  0.70115279  1.22319642
  1.47176474 -1.76517093  0.57606648]. 
[Fri Apr  5 06:46:27 2024]  Iteration number: 50 with current cost as 0.073471871001489 and parameters 
[-2.90318328  2.23743427 -2.12427991  0.16102157 -1.48858133 -2.73706209
  3.45155038  2.61632521  0.82274913 -1.50265899 -1.40348067  0.4825953
  2.25231371 -2.96619073 -0.55058175]. 
Working on 0.8 fold... 
[Fri Apr  5 06:47:30 2024]  Iteration number: 0 with current cost as 0.3159729752008216 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.03610715  0.57897498 -2.6357183
  3.11321921  2.26208668  1.06729244 -1.02394466  0.70048354  1.22652933
  1.47455747 -1.7599202   0.58549684]. 
Working on 1.0 fold... 
[Fri Apr  5 06:53:19 2024]  Iteration number: 0 with current cost as 0.318838186304086 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.04277717  0.57130767 -2.65573315
  3.10671538  2.24813852  1.0864482  -1.02080161  0.68129037  1.22434376
  1.45928898 -1.76890305  0.60455002]. 
Training complete taking 1959.7890405654907 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 06:57:35 2024]  Iteration number: 0 with current cost as 0.31735821327508795 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.04137946  0.57384051 -2.65006874
  3.107895    2.25295558  1.0816713  -1.02240864  0.68962124  1.2249977
  1.4653264  -1.76610863  0.59430406]. 
[Fri Apr  5 07:03:52 2024]  Iteration number: 50 with current cost as 0.07393396623399207 and parameters 
[-2.90318321  2.23743469 -2.12427942  0.42859649 -2.09113177 -2.72404399
  3.51044454  2.63329473  0.85104023 -1.40347608 -1.60475656  0.49425432
  2.72122509 -2.48316658 -0.15431325]. 
Working on 0.4 fold... 
[Fri Apr  5 07:04:12 2024]  Iteration number: 0 with current cost as 0.32062112905749274 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.03259352  0.57305269 -2.64098894
  3.09985099  2.2612858   1.09079474 -1.02475134  0.69672181  1.22437397
  1.47113797 -1.76276607  0.58677797]. 
[Fri Apr  5 07:10:35 2024]  Iteration number: 50 with current cost as 0.07640087212405042 and parameters 
[-2.90318338  2.23743544 -2.1242791  -0.31031493 -1.20677679 -2.99691187
  3.11878624  2.54061653  1.47162514 -1.1447239  -0.31115288  0.71959608
  2.69624179 -2.48531193 -0.71162952]. 
Working on 0.6 fold... 
[Fri Apr  5 07:12:06 2024]  Iteration number: 0 with current cost as 0.31477495916359444 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.03218794  0.57179817 -2.64250367
  3.09713766  2.26479293  1.09347508 -1.02686268  0.70115279  1.22319642
  1.47176474 -1.76517093  0.57606648]. 
[Fri Apr  5 07:18:23 2024]  Iteration number: 50 with current cost as 0.073471871001489 and parameters 
[-2.90318328  2.23743427 -2.12427991  0.16102157 -1.48858133 -2.73706209
  3.45155038  2.61632521  0.82274913 -1.50265899 -1.40348067  0.4825953
  2.25231371 -2.96619073 -0.55058175]. 
Working on 0.8 fold... 
[Fri Apr  5 07:19:26 2024]  Iteration number: 0 with current cost as 0.3159729752008216 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.03610715  0.57897498 -2.6357183
  3.11321921  2.26208668  1.06729244 -1.02394466  0.70048354  1.22652933
  1.47455747 -1.7599202   0.58549684]. 
Working on 1.0 fold... 
[Fri Apr  5 07:25:23 2024]  Iteration number: 0 with current cost as 0.318838186304086 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.04277717  0.57130767 -2.65573315
  3.10671538  2.24813852  1.0864482  -1.02080161  0.68129037  1.22434376
  1.45928898 -1.76890305  0.60455002]. 
Training complete taking 1926.2300698757172 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 07:29:41 2024]  Iteration number: 0 with current cost as 0.31735821327508795 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.04137946  0.57384051 -2.65006874
  3.107895    2.25295558  1.0816713  -1.02240864  0.68962124  1.2249977
  1.4653264  -1.76610863  0.59430406]. 
[Fri Apr  5 07:35:55 2024]  Iteration number: 50 with current cost as 0.07393396623399207 and parameters 
[-2.90318321  2.23743469 -2.12427942  0.42859649 -2.09113177 -2.72404399
  3.51044454  2.63329473  0.85104023 -1.40347608 -1.60475656  0.49425432
  2.72122509 -2.48316658 -0.15431325]. 
Working on 0.4 fold... 
[Fri Apr  5 07:36:15 2024]  Iteration number: 0 with current cost as 0.32062112905749274 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.03259352  0.57305269 -2.64098894
  3.09985099  2.2612858   1.09079474 -1.02475134  0.69672181  1.22437397
  1.47113797 -1.76276607  0.58677797]. 
[Fri Apr  5 07:42:43 2024]  Iteration number: 50 with current cost as 0.07640087212405042 and parameters 
[-2.90318338  2.23743544 -2.1242791  -0.31031493 -1.20677679 -2.99691187
  3.11878624  2.54061653  1.47162514 -1.1447239  -0.31115288  0.71959608
  2.69624179 -2.48531193 -0.71162952]. 
Working on 0.6 fold... 
[Fri Apr  5 07:44:14 2024]  Iteration number: 0 with current cost as 0.31477495916359444 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.03218794  0.57179817 -2.64250367
  3.09713766  2.26479293  1.09347508 -1.02686268  0.70115279  1.22319642
  1.47176474 -1.76517093  0.57606648]. 
[Fri Apr  5 07:51:13 2024]  Iteration number: 50 with current cost as 0.073471871001489 and parameters 
[-2.90318328  2.23743427 -2.12427991  0.16102157 -1.48858133 -2.73706209
  3.45155038  2.61632521  0.82274913 -1.50265899 -1.40348067  0.4825953
  2.25231371 -2.96619073 -0.55058175]. 
Working on 0.8 fold... 
[Fri Apr  5 07:52:16 2024]  Iteration number: 0 with current cost as 0.3159729752008216 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.03610715  0.57897498 -2.6357183
  3.11321921  2.26208668  1.06729244 -1.02394466  0.70048354  1.22652933
  1.47455747 -1.7599202   0.58549684]. 
Working on 1.0 fold... 
[Fri Apr  5 07:58:23 2024]  Iteration number: 0 with current cost as 0.318838186304086 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.04277717  0.57130767 -2.65573315
  3.10671538  2.24813852  1.0864482  -1.02080161  0.68129037  1.22434376
  1.45928898 -1.76890305  0.60455002]. 
Training complete taking 2005.912810087204 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 08:03:07 2024]  Iteration number: 0 with current cost as 0.31735821327508795 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.04137946  0.57384051 -2.65006874
  3.107895    2.25295558  1.0816713  -1.02240864  0.68962124  1.2249977
  1.4653264  -1.76610863  0.59430406]. 
[Fri Apr  5 08:09:24 2024]  Iteration number: 50 with current cost as 0.07393396623399207 and parameters 
[-2.90318321  2.23743469 -2.12427942  0.42859649 -2.09113177 -2.72404399
  3.51044454  2.63329473  0.85104023 -1.40347608 -1.60475656  0.49425432
  2.72122509 -2.48316658 -0.15431325]. 
Working on 0.4 fold... 
[Fri Apr  5 08:09:44 2024]  Iteration number: 0 with current cost as 0.32062112905749274 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.03259352  0.57305269 -2.64098894
  3.09985099  2.2612858   1.09079474 -1.02475134  0.69672181  1.22437397
  1.47113797 -1.76276607  0.58677797]. 
[Fri Apr  5 08:16:14 2024]  Iteration number: 50 with current cost as 0.07640087212405042 and parameters 
[-2.90318338  2.23743544 -2.1242791  -0.31031493 -1.20677679 -2.99691187
  3.11878624  2.54061653  1.47162514 -1.1447239  -0.31115288  0.71959608
  2.69624179 -2.48531193 -0.71162952]. 
Working on 0.6 fold... 
[Fri Apr  5 08:17:44 2024]  Iteration number: 0 with current cost as 0.31477495916359444 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.03218794  0.57179817 -2.64250367
  3.09713766  2.26479293  1.09347508 -1.02686268  0.70115279  1.22319642
  1.47176474 -1.76517093  0.57606648]. 
[Fri Apr  5 08:24:12 2024]  Iteration number: 50 with current cost as 0.073471871001489 and parameters 
[-2.90318328  2.23743427 -2.12427991  0.16102157 -1.48858133 -2.73706209
  3.45155038  2.61632521  0.82274913 -1.50265899 -1.40348067  0.4825953
  2.25231371 -2.96619073 -0.55058175]. 
Working on 0.8 fold... 
[Fri Apr  5 08:25:15 2024]  Iteration number: 0 with current cost as 0.3159729752008216 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.03610715  0.57897498 -2.6357183
  3.11321921  2.26208668  1.06729244 -1.02394466  0.70048354  1.22652933
  1.47455747 -1.7599202   0.58549684]. 
Working on 1.0 fold... 
[Fri Apr  5 08:31:04 2024]  Iteration number: 0 with current cost as 0.318838186304086 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.04277717  0.57130767 -2.65573315
  3.10671538  2.24813852  1.0864482  -1.02080161  0.68129037  1.22434376
  1.45928898 -1.76890305  0.60455002]. 
Training complete taking 1935.3442096710205 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 08:35:23 2024]  Iteration number: 0 with current cost as 0.31735821327508795 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.04137946  0.57384051 -2.65006874
  3.107895    2.25295558  1.0816713  -1.02240864  0.68962124  1.2249977
  1.4653264  -1.76610863  0.59430406]. 
[Fri Apr  5 08:42:00 2024]  Iteration number: 50 with current cost as 0.07393396623399207 and parameters 
[-2.90318321  2.23743469 -2.12427942  0.42859649 -2.09113177 -2.72404399
  3.51044454  2.63329473  0.85104023 -1.40347608 -1.60475656  0.49425432
  2.72122509 -2.48316658 -0.15431325]. 
Working on 0.4 fold... 
[Fri Apr  5 08:42:21 2024]  Iteration number: 0 with current cost as 0.32062112905749274 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.03259352  0.57305269 -2.64098894
  3.09985099  2.2612858   1.09079474 -1.02475134  0.69672181  1.22437397
  1.47113797 -1.76276607  0.58677797]. 
[Fri Apr  5 08:48:46 2024]  Iteration number: 50 with current cost as 0.07640087212405042 and parameters 
[-2.90318338  2.23743544 -2.1242791  -0.31031493 -1.20677679 -2.99691187
  3.11878624  2.54061653  1.47162514 -1.1447239  -0.31115288  0.71959608
  2.69624179 -2.48531193 -0.71162952]. 
Working on 0.6 fold... 
[Fri Apr  5 08:50:16 2024]  Iteration number: 0 with current cost as 0.31477495916359444 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.03218794  0.57179817 -2.64250367
  3.09713766  2.26479293  1.09347508 -1.02686268  0.70115279  1.22319642
  1.47176474 -1.76517093  0.57606648]. 
[Fri Apr  5 08:56:31 2024]  Iteration number: 50 with current cost as 0.073471871001489 and parameters 
[-2.90318328  2.23743427 -2.12427991  0.16102157 -1.48858133 -2.73706209
  3.45155038  2.61632521  0.82274913 -1.50265899 -1.40348067  0.4825953
  2.25231371 -2.96619073 -0.55058175]. 
Working on 0.8 fold... 
[Fri Apr  5 08:57:33 2024]  Iteration number: 0 with current cost as 0.3159729752008216 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.03610715  0.57897498 -2.6357183
  3.11321921  2.26208668  1.06729244 -1.02394466  0.70048354  1.22652933
  1.47455747 -1.7599202   0.58549684]. 
Working on 1.0 fold... 
[Fri Apr  5 09:03:36 2024]  Iteration number: 0 with current cost as 0.318838186304086 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.04277717  0.57130767 -2.65573315
  3.10671538  2.24813852  1.0864482  -1.02080161  0.68129037  1.22434376
  1.45928898 -1.76890305  0.60455002]. 
Training complete taking 1952.0886361598969 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 09:07:55 2024]  Iteration number: 0 with current cost as 0.31735821327508795 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.04137946  0.57384051 -2.65006874
  3.107895    2.25295558  1.0816713  -1.02240864  0.68962124  1.2249977
  1.4653264  -1.76610863  0.59430406]. 
[Fri Apr  5 09:15:07 2024]  Iteration number: 50 with current cost as 0.07393396623399207 and parameters 
[-2.90318321  2.23743469 -2.12427942  0.42859649 -2.09113177 -2.72404399
  3.51044454  2.63329473  0.85104023 -1.40347608 -1.60475656  0.49425432
  2.72122509 -2.48316658 -0.15431325]. 
Working on 0.4 fold... 
[Fri Apr  5 09:15:29 2024]  Iteration number: 0 with current cost as 0.32062112905749274 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.03259352  0.57305269 -2.64098894
  3.09985099  2.2612858   1.09079474 -1.02475134  0.69672181  1.22437397
  1.47113797 -1.76276607  0.58677797]. 
[Fri Apr  5 09:21:55 2024]  Iteration number: 50 with current cost as 0.07640087212405042 and parameters 
[-2.90318338  2.23743544 -2.1242791  -0.31031493 -1.20677679 -2.99691187
  3.11878624  2.54061653  1.47162514 -1.1447239  -0.31115288  0.71959608
  2.69624179 -2.48531193 -0.71162952]. 
Working on 0.6 fold... 
[Fri Apr  5 09:23:28 2024]  Iteration number: 0 with current cost as 0.31477495916359444 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.03218794  0.57179817 -2.64250367
  3.09713766  2.26479293  1.09347508 -1.02686268  0.70115279  1.22319642
  1.47176474 -1.76517093  0.57606648]. 
[Fri Apr  5 09:30:03 2024]  Iteration number: 50 with current cost as 0.073471871001489 and parameters 
[-2.90318328  2.23743427 -2.12427991  0.16102157 -1.48858133 -2.73706209
  3.45155038  2.61632521  0.82274913 -1.50265899 -1.40348067  0.4825953
  2.25231371 -2.96619073 -0.55058175]. 
Working on 0.8 fold... 
[Fri Apr  5 09:31:15 2024]  Iteration number: 0 with current cost as 0.3159729752008216 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.03610715  0.57897498 -2.6357183
  3.11321921  2.26208668  1.06729244 -1.02394466  0.70048354  1.22652933
  1.47455747 -1.7599202   0.58549684]. 
Working on 1.0 fold... 
[Fri Apr  5 09:37:02 2024]  Iteration number: 0 with current cost as 0.318838186304086 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.04277717  0.57130767 -2.65573315
  3.10671538  2.24813852  1.0864482  -1.02080161  0.68129037  1.22434376
  1.45928898 -1.76890305  0.60455002]. 
Training complete taking 2004.8914246559143 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 09:41:19 2024]  Iteration number: 0 with current cost as 0.31735821327508795 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.04137946  0.57384051 -2.65006874
  3.107895    2.25295558  1.0816713  -1.02240864  0.68962124  1.2249977
  1.4653264  -1.76610863  0.59430406]. 
[Fri Apr  5 09:47:37 2024]  Iteration number: 50 with current cost as 0.07393396623399207 and parameters 
[-2.90318321  2.23743469 -2.12427942  0.42859649 -2.09113177 -2.72404399
  3.51044454  2.63329473  0.85104023 -1.40347608 -1.60475656  0.49425432
  2.72122509 -2.48316658 -0.15431325]. 
Working on 0.4 fold... 
[Fri Apr  5 09:47:59 2024]  Iteration number: 0 with current cost as 0.32062112905749274 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.03259352  0.57305269 -2.64098894
  3.09985099  2.2612858   1.09079474 -1.02475134  0.69672181  1.22437397
  1.47113797 -1.76276607  0.58677797]. 
[Fri Apr  5 09:54:22 2024]  Iteration number: 50 with current cost as 0.07640087212405042 and parameters 
[-2.90318338  2.23743544 -2.1242791  -0.31031493 -1.20677679 -2.99691187
  3.11878624  2.54061653  1.47162514 -1.1447239  -0.31115288  0.71959608
  2.69624179 -2.48531193 -0.71162952]. 
Working on 0.6 fold... 
[Fri Apr  5 09:55:53 2024]  Iteration number: 0 with current cost as 0.31477495916359444 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.03218794  0.57179817 -2.64250367
  3.09713766  2.26479293  1.09347508 -1.02686268  0.70115279  1.22319642
  1.47176474 -1.76517093  0.57606648]. 
[Fri Apr  5 10:02:10 2024]  Iteration number: 50 with current cost as 0.073471871001489 and parameters 
[-2.90318328  2.23743427 -2.12427991  0.16102157 -1.48858133 -2.73706209
  3.45155038  2.61632521  0.82274913 -1.50265899 -1.40348067  0.4825953
  2.25231371 -2.96619073 -0.55058175]. 
Working on 0.8 fold... 
[Fri Apr  5 10:03:13 2024]  Iteration number: 0 with current cost as 0.3159729752008216 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.03610715  0.57897498 -2.6357183
  3.11321921  2.26208668  1.06729244 -1.02394466  0.70048354  1.22652933
  1.47455747 -1.7599202   0.58549684]. 
Working on 1.0 fold... 
[Fri Apr  5 10:09:01 2024]  Iteration number: 0 with current cost as 0.318838186304086 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.04277717  0.57130767 -2.65573315
  3.10671538  2.24813852  1.0864482  -1.02080161  0.68129037  1.22434376
  1.45928898 -1.76890305  0.60455002]. 
Training complete taking 1917.8049166202545 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 10:13:17 2024]  Iteration number: 0 with current cost as 0.31735821327508795 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.04137946  0.57384051 -2.65006874
  3.107895    2.25295558  1.0816713  -1.02240864  0.68962124  1.2249977
  1.4653264  -1.76610863  0.59430406]. 
[Fri Apr  5 10:19:36 2024]  Iteration number: 50 with current cost as 0.07393396623399207 and parameters 
[-2.90318321  2.23743469 -2.12427942  0.42859649 -2.09113177 -2.72404399
  3.51044454  2.63329473  0.85104023 -1.40347608 -1.60475656  0.49425432
  2.72122509 -2.48316658 -0.15431325]. 
Working on 0.4 fold... 
[Fri Apr  5 10:19:56 2024]  Iteration number: 0 with current cost as 0.32062112905749274 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.03259352  0.57305269 -2.64098894
  3.09985099  2.2612858   1.09079474 -1.02475134  0.69672181  1.22437397
  1.47113797 -1.76276607  0.58677797]. 
[Fri Apr  5 10:26:41 2024]  Iteration number: 50 with current cost as 0.07640087212405042 and parameters 
[-2.90318338  2.23743544 -2.1242791  -0.31031493 -1.20677679 -2.99691187
  3.11878624  2.54061653  1.47162514 -1.1447239  -0.31115288  0.71959608
  2.69624179 -2.48531193 -0.71162952]. 
Working on 0.6 fold... 
[Fri Apr  5 10:28:12 2024]  Iteration number: 0 with current cost as 0.31477495916359444 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.03218794  0.57179817 -2.64250367
  3.09713766  2.26479293  1.09347508 -1.02686268  0.70115279  1.22319642
  1.47176474 -1.76517093  0.57606648]. 
[Fri Apr  5 10:34:34 2024]  Iteration number: 50 with current cost as 0.073471871001489 and parameters 
[-2.90318328  2.23743427 -2.12427991  0.16102157 -1.48858133 -2.73706209
  3.45155038  2.61632521  0.82274913 -1.50265899 -1.40348067  0.4825953
  2.25231371 -2.96619073 -0.55058175]. 
Working on 0.8 fold... 
[Fri Apr  5 10:35:36 2024]  Iteration number: 0 with current cost as 0.3159729752008216 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.03610715  0.57897498 -2.6357183
  3.11321921  2.26208668  1.06729244 -1.02394466  0.70048354  1.22652933
  1.47455747 -1.7599202   0.58549684]. 
Working on 1.0 fold... 
[Fri Apr  5 10:41:25 2024]  Iteration number: 0 with current cost as 0.318838186304086 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.04277717  0.57130767 -2.65573315
  3.10671538  2.24813852  1.0864482  -1.02080161  0.68129037  1.22434376
  1.45928898 -1.76890305  0.60455002]. 
Training complete taking 1944.6314272880554 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 10:45:43 2024]  Iteration number: 0 with current cost as 0.31735821327508795 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.04137946  0.57384051 -2.65006874
  3.107895    2.25295558  1.0816713  -1.02240864  0.68962124  1.2249977
  1.4653264  -1.76610863  0.59430406]. 
[Fri Apr  5 10:51:56 2024]  Iteration number: 50 with current cost as 0.07393396623399207 and parameters 
[-2.90318321  2.23743469 -2.12427942  0.42859649 -2.09113177 -2.72404399
  3.51044454  2.63329473  0.85104023 -1.40347608 -1.60475656  0.49425432
  2.72122509 -2.48316658 -0.15431325]. 
Working on 0.4 fold... 
[Fri Apr  5 10:52:17 2024]  Iteration number: 0 with current cost as 0.32062112905749274 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.03259352  0.57305269 -2.64098894
  3.09985099  2.2612858   1.09079474 -1.02475134  0.69672181  1.22437397
  1.47113797 -1.76276607  0.58677797]. 
[Fri Apr  5 10:58:41 2024]  Iteration number: 50 with current cost as 0.07640087212405042 and parameters 
[-2.90318338  2.23743544 -2.1242791  -0.31031493 -1.20677679 -2.99691187
  3.11878624  2.54061653  1.47162514 -1.1447239  -0.31115288  0.71959608
  2.69624179 -2.48531193 -0.71162952]. 
Working on 0.6 fold... 
[Fri Apr  5 11:00:12 2024]  Iteration number: 0 with current cost as 0.31477495916359444 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.03218794  0.57179817 -2.64250367
  3.09713766  2.26479293  1.09347508 -1.02686268  0.70115279  1.22319642
  1.47176474 -1.76517093  0.57606648]. 
[Fri Apr  5 11:06:29 2024]  Iteration number: 50 with current cost as 0.073471871001489 and parameters 
[-2.90318328  2.23743427 -2.12427991  0.16102157 -1.48858133 -2.73706209
  3.45155038  2.61632521  0.82274913 -1.50265899 -1.40348067  0.4825953
  2.25231371 -2.96619073 -0.55058175]. 
Working on 0.8 fold... 
[Fri Apr  5 11:07:33 2024]  Iteration number: 0 with current cost as 0.3159729752008216 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.03610715  0.57897498 -2.6357183
  3.11321921  2.26208668  1.06729244 -1.02394466  0.70048354  1.22652933
  1.47455747 -1.7599202   0.58549684]. 
Working on 1.0 fold... 
[Fri Apr  5 11:13:24 2024]  Iteration number: 0 with current cost as 0.318838186304086 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.04277717  0.57130767 -2.65573315
  3.10671538  2.24813852  1.0864482  -1.02080161  0.68129037  1.22434376
  1.45928898 -1.76890305  0.60455002]. 
Training complete taking 1919.627976179123 seconds. 
Discarding model... 

Training complete taking 48708.879026174545 total seconds. 
Now scoring model... 
Scoring complete taking 1.2720565795898438 seconds. 
Saved predicted values as M-A2-CZ_HWE-CNOT_predicted_values.csv
Model scores: {'MSE_train': (0.1807100436813308,), 'R2_train': 0.6401637606656208, 'MAE_train': 0.35790302568491544, 'MSE_test': 0.2164267290632902, 'R2_test': 0.5963717359721408, 'MAE_test': 0.39657086365510535}. 
Saved model results as M-A2-CZ_HWE-CNOT_results.json. 
