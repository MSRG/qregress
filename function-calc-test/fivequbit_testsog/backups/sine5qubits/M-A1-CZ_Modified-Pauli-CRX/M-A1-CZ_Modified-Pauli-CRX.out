/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /scratch/j/jacobsen/gjones/sine5qubits/sine_train.bin... 
Successfully loaded /scratch/j/jacobsen/gjones/sine5qubits/sine_train.bin into X and y data. 
Loading dataset from /scratch/j/jacobsen/gjones/sine5qubits/sine_test.bin... 
Successfully loaded /scratch/j/jacobsen/gjones/sine5qubits/sine_test.bin into X and y data. 
Training model with dataset /scratch/j/jacobsen/gjones/sine5qubits/sine_train.bin 
 at time Thu Apr  4 21:41:17 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 21:41:40 2024]  Iteration number: 0 with current cost as 0.1861154604244234 and parameters 
[-3.2540628   2.26526942 -2.21885988 -0.11653102  0.55388709 -2.77010896
  3.06858499  2.18960146  1.18552    -1.06648308  1.16402267  1.14432446
  1.310299   -1.87354679  0.7296508   2.8857842  -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 21:47:29 2024]  Iteration number: 0 with current cost as 0.5439789310219487 and parameters 
[-4.44414248  2.39560629 -2.52516599 -0.11653103  0.55388708 -2.77010894
  3.06858498  2.18960145  1.18552002 -1.06648308  2.99085313  1.14432449
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 21:53:20 2024]  Iteration number: 0 with current cost as 0.5815993526674532 and parameters 
[-4.50590276  2.38564356 -2.53983696 -0.11653099  0.55388712 -2.77010897
  3.06858502  2.18960149  1.18552006 -1.06648305  3.10487965  1.14432445
  1.31029902 -1.87354676  0.7296508   2.88578423 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 21:58:34 2024]  Iteration number: 0 with current cost as 0.20093354531128016 and parameters 
[-3.2327515   2.28160015 -2.20744846 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18552    -1.06648308  1.11586495  1.14432446
  1.31029899 -1.87354679  0.7296508   2.8857842  -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 22:04:01 2024]  Iteration number: 0 with current cost as 0.19221184681342818 and parameters 
[-3.24777131  2.26124205 -2.22152344 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  1.1524219   1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1666.7243275642395 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 22:09:27 2024]  Iteration number: 0 with current cost as 0.1861154604244234 and parameters 
[-3.2540628   2.26526942 -2.21885988 -0.11653102  0.55388709 -2.77010896
  3.06858499  2.18960146  1.18552    -1.06648308  1.16402267  1.14432446
  1.310299   -1.87354679  0.7296508   2.8857842  -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 22:14:42 2024]  Iteration number: 0 with current cost as 0.5439789310219487 and parameters 
[-4.44414248  2.39560629 -2.52516599 -0.11653103  0.55388708 -2.77010894
  3.06858498  2.18960145  1.18552002 -1.06648308  2.99085313  1.14432449
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 22:20:36 2024]  Iteration number: 0 with current cost as 0.5815993526674532 and parameters 
[-4.50590276  2.38564356 -2.53983696 -0.11653099  0.55388712 -2.77010897
  3.06858502  2.18960149  1.18552006 -1.06648305  3.10487965  1.14432445
  1.31029902 -1.87354676  0.7296508   2.88578423 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 22:25:58 2024]  Iteration number: 0 with current cost as 0.20093354531128016 and parameters 
[-3.2327515   2.28160015 -2.20744846 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18552    -1.06648308  1.11586495  1.14432446
  1.31029899 -1.87354679  0.7296508   2.8857842  -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 22:31:28 2024]  Iteration number: 0 with current cost as 0.19221184681342818 and parameters 
[-3.24777131  2.26124205 -2.22152344 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  1.1524219   1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1648.148873090744 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 22:36:56 2024]  Iteration number: 0 with current cost as 0.1861154604244234 and parameters 
[-3.2540628   2.26526942 -2.21885988 -0.11653102  0.55388709 -2.77010896
  3.06858499  2.18960146  1.18552    -1.06648308  1.16402267  1.14432446
  1.310299   -1.87354679  0.7296508   2.8857842  -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 22:42:11 2024]  Iteration number: 0 with current cost as 0.5439789310219487 and parameters 
[-4.44414248  2.39560629 -2.52516599 -0.11653103  0.55388708 -2.77010894
  3.06858498  2.18960145  1.18552002 -1.06648308  2.99085313  1.14432449
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 22:48:01 2024]  Iteration number: 0 with current cost as 0.5815993526674532 and parameters 
[-4.50590276  2.38564356 -2.53983696 -0.11653099  0.55388712 -2.77010897
  3.06858502  2.18960149  1.18552006 -1.06648305  3.10487965  1.14432445
  1.31029902 -1.87354676  0.7296508   2.88578423 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 22:53:15 2024]  Iteration number: 0 with current cost as 0.20093354531128016 and parameters 
[-3.2327515   2.28160015 -2.20744846 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18552    -1.06648308  1.11586495  1.14432446
  1.31029899 -1.87354679  0.7296508   2.8857842  -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 22:58:40 2024]  Iteration number: 0 with current cost as 0.19221184681342818 and parameters 
[-3.24777131  2.26124205 -2.22152344 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  1.1524219   1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1640.8006114959717 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 23:04:15 2024]  Iteration number: 0 with current cost as 0.1861154604244234 and parameters 
[-3.2540628   2.26526942 -2.21885988 -0.11653102  0.55388709 -2.77010896
  3.06858499  2.18960146  1.18552    -1.06648308  1.16402267  1.14432446
  1.310299   -1.87354679  0.7296508   2.8857842  -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 23:09:32 2024]  Iteration number: 0 with current cost as 0.5439789310219487 and parameters 
[-4.44414248  2.39560629 -2.52516599 -0.11653103  0.55388708 -2.77010894
  3.06858498  2.18960145  1.18552002 -1.06648308  2.99085313  1.14432449
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 23:15:24 2024]  Iteration number: 0 with current cost as 0.5815993526674532 and parameters 
[-4.50590276  2.38564356 -2.53983696 -0.11653099  0.55388712 -2.77010897
  3.06858502  2.18960149  1.18552006 -1.06648305  3.10487965  1.14432445
  1.31029902 -1.87354676  0.7296508   2.88578423 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 23:20:52 2024]  Iteration number: 0 with current cost as 0.20093354531128016 and parameters 
[-3.2327515   2.28160015 -2.20744846 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18552    -1.06648308  1.11586495  1.14432446
  1.31029899 -1.87354679  0.7296508   2.8857842  -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 23:26:22 2024]  Iteration number: 0 with current cost as 0.19221184681342818 and parameters 
[-3.24777131  2.26124205 -2.22152344 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  1.1524219   1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1668.0349068641663 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 23:32:05 2024]  Iteration number: 0 with current cost as 0.1861154604244234 and parameters 
[-3.2540628   2.26526942 -2.21885988 -0.11653102  0.55388709 -2.77010896
  3.06858499  2.18960146  1.18552    -1.06648308  1.16402267  1.14432446
  1.310299   -1.87354679  0.7296508   2.8857842  -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 23:37:22 2024]  Iteration number: 0 with current cost as 0.5439789310219487 and parameters 
[-4.44414248  2.39560629 -2.52516599 -0.11653103  0.55388708 -2.77010894
  3.06858498  2.18960145  1.18552002 -1.06648308  2.99085313  1.14432449
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 23:43:19 2024]  Iteration number: 0 with current cost as 0.5815993526674532 and parameters 
[-4.50590276  2.38564356 -2.53983696 -0.11653099  0.55388712 -2.77010897
  3.06858502  2.18960149  1.18552006 -1.06648305  3.10487965  1.14432445
  1.31029902 -1.87354676  0.7296508   2.88578423 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 23:48:39 2024]  Iteration number: 0 with current cost as 0.20093354531128016 and parameters 
[-3.2327515   2.28160015 -2.20744846 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18552    -1.06648308  1.11586495  1.14432446
  1.31029899 -1.87354679  0.7296508   2.8857842  -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 23:54:09 2024]  Iteration number: 0 with current cost as 0.19221184681342818 and parameters 
[-3.24777131  2.26124205 -2.22152344 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  1.1524219   1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1651.6779437065125 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 23:59:37 2024]  Iteration number: 0 with current cost as 0.1861154604244234 and parameters 
[-3.2540628   2.26526942 -2.21885988 -0.11653102  0.55388709 -2.77010896
  3.06858499  2.18960146  1.18552    -1.06648308  1.16402267  1.14432446
  1.310299   -1.87354679  0.7296508   2.8857842  -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Fri Apr  5 00:04:48 2024]  Iteration number: 0 with current cost as 0.5439789310219487 and parameters 
[-4.44414248  2.39560629 -2.52516599 -0.11653103  0.55388708 -2.77010894
  3.06858498  2.18960145  1.18552002 -1.06648308  2.99085313  1.14432449
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Fri Apr  5 00:10:36 2024]  Iteration number: 0 with current cost as 0.5815993526674532 and parameters 
[-4.50590276  2.38564356 -2.53983696 -0.11653099  0.55388712 -2.77010897
  3.06858502  2.18960149  1.18552006 -1.06648305  3.10487965  1.14432445
  1.31029902 -1.87354676  0.7296508   2.88578423 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Fri Apr  5 00:16:01 2024]  Iteration number: 0 with current cost as 0.20093354531128016 and parameters 
[-3.2327515   2.28160015 -2.20744846 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18552    -1.06648308  1.11586495  1.14432446
  1.31029899 -1.87354679  0.7296508   2.8857842  -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Fri Apr  5 00:21:33 2024]  Iteration number: 0 with current cost as 0.19221184681342818 and parameters 
[-3.24777131  2.26124205 -2.22152344 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  1.1524219   1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1641.9341416358948 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 00:26:58 2024]  Iteration number: 0 with current cost as 0.1861154604244234 and parameters 
[-3.2540628   2.26526942 -2.21885988 -0.11653102  0.55388709 -2.77010896
  3.06858499  2.18960146  1.18552    -1.06648308  1.16402267  1.14432446
  1.310299   -1.87354679  0.7296508   2.8857842  -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Fri Apr  5 00:32:09 2024]  Iteration number: 0 with current cost as 0.5439789310219487 and parameters 
[-4.44414248  2.39560629 -2.52516599 -0.11653103  0.55388708 -2.77010894
  3.06858498  2.18960145  1.18552002 -1.06648308  2.99085313  1.14432449
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Fri Apr  5 00:38:00 2024]  Iteration number: 0 with current cost as 0.5815993526674532 and parameters 
[-4.50590276  2.38564356 -2.53983696 -0.11653099  0.55388712 -2.77010897
  3.06858502  2.18960149  1.18552006 -1.06648305  3.10487965  1.14432445
  1.31029902 -1.87354676  0.7296508   2.88578423 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Fri Apr  5 00:43:12 2024]  Iteration number: 0 with current cost as 0.20093354531128016 and parameters 
[-3.2327515   2.28160015 -2.20744846 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18552    -1.06648308  1.11586495  1.14432446
  1.31029899 -1.87354679  0.7296508   2.8857842  -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Fri Apr  5 00:48:37 2024]  Iteration number: 0 with current cost as 0.19221184681342818 and parameters 
[-3.24777131  2.26124205 -2.22152344 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  1.1524219   1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1627.4088869094849 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 00:54:05 2024]  Iteration number: 0 with current cost as 0.1861154604244234 and parameters 
[-3.2540628   2.26526942 -2.21885988 -0.11653102  0.55388709 -2.77010896
  3.06858499  2.18960146  1.18552    -1.06648308  1.16402267  1.14432446
  1.310299   -1.87354679  0.7296508   2.8857842  -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Fri Apr  5 00:59:20 2024]  Iteration number: 0 with current cost as 0.5439789310219487 and parameters 
[-4.44414248  2.39560629 -2.52516599 -0.11653103  0.55388708 -2.77010894
  3.06858498  2.18960145  1.18552002 -1.06648308  2.99085313  1.14432449
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Fri Apr  5 01:05:15 2024]  Iteration number: 0 with current cost as 0.5815993526674532 and parameters 
[-4.50590276  2.38564356 -2.53983696 -0.11653099  0.55388712 -2.77010897
  3.06858502  2.18960149  1.18552006 -1.06648305  3.10487965  1.14432445
  1.31029902 -1.87354676  0.7296508   2.88578423 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Fri Apr  5 01:10:27 2024]  Iteration number: 0 with current cost as 0.20093354531128016 and parameters 
[-3.2327515   2.28160015 -2.20744846 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18552    -1.06648308  1.11586495  1.14432446
  1.31029899 -1.87354679  0.7296508   2.8857842  -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Fri Apr  5 01:15:51 2024]  Iteration number: 0 with current cost as 0.19221184681342818 and parameters 
[-3.24777131  2.26124205 -2.22152344 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  1.1524219   1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1630.5066058635712 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 01:21:16 2024]  Iteration number: 0 with current cost as 0.1861154604244234 and parameters 
[-3.2540628   2.26526942 -2.21885988 -0.11653102  0.55388709 -2.77010896
  3.06858499  2.18960146  1.18552    -1.06648308  1.16402267  1.14432446
  1.310299   -1.87354679  0.7296508   2.8857842  -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Fri Apr  5 01:26:30 2024]  Iteration number: 0 with current cost as 0.5439789310219487 and parameters 
[-4.44414248  2.39560629 -2.52516599 -0.11653103  0.55388708 -2.77010894
  3.06858498  2.18960145  1.18552002 -1.06648308  2.99085313  1.14432449
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Fri Apr  5 01:32:20 2024]  Iteration number: 0 with current cost as 0.5815993526674532 and parameters 
[-4.50590276  2.38564356 -2.53983696 -0.11653099  0.55388712 -2.77010897
  3.06858502  2.18960149  1.18552006 -1.06648305  3.10487965  1.14432445
  1.31029902 -1.87354676  0.7296508   2.88578423 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Fri Apr  5 01:37:35 2024]  Iteration number: 0 with current cost as 0.20093354531128016 and parameters 
[-3.2327515   2.28160015 -2.20744846 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18552    -1.06648308  1.11586495  1.14432446
  1.31029899 -1.87354679  0.7296508   2.8857842  -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Fri Apr  5 01:42:57 2024]  Iteration number: 0 with current cost as 0.19221184681342818 and parameters 
[-3.24777131  2.26124205 -2.22152344 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  1.1524219   1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1630.4691812992096 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 01:48:25 2024]  Iteration number: 0 with current cost as 0.1861154604244234 and parameters 
[-3.2540628   2.26526942 -2.21885988 -0.11653102  0.55388709 -2.77010896
  3.06858499  2.18960146  1.18552    -1.06648308  1.16402267  1.14432446
  1.310299   -1.87354679  0.7296508   2.8857842  -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Fri Apr  5 01:53:39 2024]  Iteration number: 0 with current cost as 0.5439789310219487 and parameters 
[-4.44414248  2.39560629 -2.52516599 -0.11653103  0.55388708 -2.77010894
  3.06858498  2.18960145  1.18552002 -1.06648308  2.99085313  1.14432449
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Fri Apr  5 01:59:27 2024]  Iteration number: 0 with current cost as 0.5815993526674532 and parameters 
[-4.50590276  2.38564356 -2.53983696 -0.11653099  0.55388712 -2.77010897
  3.06858502  2.18960149  1.18552006 -1.06648305  3.10487965  1.14432445
  1.31029902 -1.87354676  0.7296508   2.88578423 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Fri Apr  5 02:04:47 2024]  Iteration number: 0 with current cost as 0.20093354531128016 and parameters 
[-3.2327515   2.28160015 -2.20744846 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18552    -1.06648308  1.11586495  1.14432446
  1.31029899 -1.87354679  0.7296508   2.8857842  -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Fri Apr  5 02:10:11 2024]  Iteration number: 0 with current cost as 0.19221184681342818 and parameters 
[-3.24777131  2.26124205 -2.22152344 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  1.1524219   1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1627.9071493148804 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 02:15:34 2024]  Iteration number: 0 with current cost as 0.1861154604244234 and parameters 
[-3.2540628   2.26526942 -2.21885988 -0.11653102  0.55388709 -2.77010896
  3.06858499  2.18960146  1.18552    -1.06648308  1.16402267  1.14432446
  1.310299   -1.87354679  0.7296508   2.8857842  -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Fri Apr  5 02:20:52 2024]  Iteration number: 0 with current cost as 0.5439789310219487 and parameters 
[-4.44414248  2.39560629 -2.52516599 -0.11653103  0.55388708 -2.77010894
  3.06858498  2.18960145  1.18552002 -1.06648308  2.99085313  1.14432449
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Fri Apr  5 02:26:47 2024]  Iteration number: 0 with current cost as 0.5815993526674532 and parameters 
[-4.50590276  2.38564356 -2.53983696 -0.11653099  0.55388712 -2.77010897
  3.06858502  2.18960149  1.18552006 -1.06648305  3.10487965  1.14432445
  1.31029902 -1.87354676  0.7296508   2.88578423 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Fri Apr  5 02:32:10 2024]  Iteration number: 0 with current cost as 0.20093354531128016 and parameters 
[-3.2327515   2.28160015 -2.20744846 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18552    -1.06648308  1.11586495  1.14432446
  1.31029899 -1.87354679  0.7296508   2.8857842  -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Fri Apr  5 02:37:37 2024]  Iteration number: 0 with current cost as 0.19221184681342818 and parameters 
[-3.24777131  2.26124205 -2.22152344 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  1.1524219   1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1648.7719957828522 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 02:43:03 2024]  Iteration number: 0 with current cost as 0.1861154604244234 and parameters 
[-3.2540628   2.26526942 -2.21885988 -0.11653102  0.55388709 -2.77010896
  3.06858499  2.18960146  1.18552    -1.06648308  1.16402267  1.14432446
  1.310299   -1.87354679  0.7296508   2.8857842  -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Fri Apr  5 02:48:17 2024]  Iteration number: 0 with current cost as 0.5439789310219487 and parameters 
[-4.44414248  2.39560629 -2.52516599 -0.11653103  0.55388708 -2.77010894
  3.06858498  2.18960145  1.18552002 -1.06648308  2.99085313  1.14432449
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Fri Apr  5 02:54:07 2024]  Iteration number: 0 with current cost as 0.5815993526674532 and parameters 
[-4.50590276  2.38564356 -2.53983696 -0.11653099  0.55388712 -2.77010897
  3.06858502  2.18960149  1.18552006 -1.06648305  3.10487965  1.14432445
  1.31029902 -1.87354676  0.7296508   2.88578423 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Fri Apr  5 02:59:19 2024]  Iteration number: 0 with current cost as 0.20093354531128016 and parameters 
[-3.2327515   2.28160015 -2.20744846 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18552    -1.06648308  1.11586495  1.14432446
  1.31029899 -1.87354679  0.7296508   2.8857842  -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Fri Apr  5 03:04:44 2024]  Iteration number: 0 with current cost as 0.19221184681342818 and parameters 
[-3.24777131  2.26124205 -2.22152344 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  1.1524219   1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1626.0482556819916 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 03:10:09 2024]  Iteration number: 0 with current cost as 0.1861154604244234 and parameters 
[-3.2540628   2.26526942 -2.21885988 -0.11653102  0.55388709 -2.77010896
  3.06858499  2.18960146  1.18552    -1.06648308  1.16402267  1.14432446
  1.310299   -1.87354679  0.7296508   2.8857842  -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Fri Apr  5 03:15:21 2024]  Iteration number: 0 with current cost as 0.5439789310219487 and parameters 
[-4.44414248  2.39560629 -2.52516599 -0.11653103  0.55388708 -2.77010894
  3.06858498  2.18960145  1.18552002 -1.06648308  2.99085313  1.14432449
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Fri Apr  5 03:21:08 2024]  Iteration number: 0 with current cost as 0.5815993526674532 and parameters 
[-4.50590276  2.38564356 -2.53983696 -0.11653099  0.55388712 -2.77010897
  3.06858502  2.18960149  1.18552006 -1.06648305  3.10487965  1.14432445
  1.31029902 -1.87354676  0.7296508   2.88578423 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Fri Apr  5 03:26:19 2024]  Iteration number: 0 with current cost as 0.20093354531128016 and parameters 
[-3.2327515   2.28160015 -2.20744846 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18552    -1.06648308  1.11586495  1.14432446
  1.31029899 -1.87354679  0.7296508   2.8857842  -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Fri Apr  5 03:31:44 2024]  Iteration number: 0 with current cost as 0.19221184681342818 and parameters 
[-3.24777131  2.26124205 -2.22152344 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  1.1524219   1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1620.0464358329773 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 03:37:09 2024]  Iteration number: 0 with current cost as 0.1861154604244234 and parameters 
[-3.2540628   2.26526942 -2.21885988 -0.11653102  0.55388709 -2.77010896
  3.06858499  2.18960146  1.18552    -1.06648308  1.16402267  1.14432446
  1.310299   -1.87354679  0.7296508   2.8857842  -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Fri Apr  5 03:42:21 2024]  Iteration number: 0 with current cost as 0.5439789310219487 and parameters 
[-4.44414248  2.39560629 -2.52516599 -0.11653103  0.55388708 -2.77010894
  3.06858498  2.18960145  1.18552002 -1.06648308  2.99085313  1.14432449
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Fri Apr  5 03:48:09 2024]  Iteration number: 0 with current cost as 0.5815993526674532 and parameters 
[-4.50590276  2.38564356 -2.53983696 -0.11653099  0.55388712 -2.77010897
  3.06858502  2.18960149  1.18552006 -1.06648305  3.10487965  1.14432445
  1.31029902 -1.87354676  0.7296508   2.88578423 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Fri Apr  5 03:53:20 2024]  Iteration number: 0 with current cost as 0.20093354531128016 and parameters 
[-3.2327515   2.28160015 -2.20744846 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18552    -1.06648308  1.11586495  1.14432446
  1.31029899 -1.87354679  0.7296508   2.8857842  -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Fri Apr  5 03:58:56 2024]  Iteration number: 0 with current cost as 0.19221184681342818 and parameters 
[-3.24777131  2.26124205 -2.22152344 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  1.1524219   1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1634.0666630268097 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 04:04:23 2024]  Iteration number: 0 with current cost as 0.1861154604244234 and parameters 
[-3.2540628   2.26526942 -2.21885988 -0.11653102  0.55388709 -2.77010896
  3.06858499  2.18960146  1.18552    -1.06648308  1.16402267  1.14432446
  1.310299   -1.87354679  0.7296508   2.8857842  -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Fri Apr  5 04:09:57 2024]  Iteration number: 0 with current cost as 0.5439789310219487 and parameters 
[-4.44414248  2.39560629 -2.52516599 -0.11653103  0.55388708 -2.77010894
  3.06858498  2.18960145  1.18552002 -1.06648308  2.99085313  1.14432449
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Fri Apr  5 04:15:46 2024]  Iteration number: 0 with current cost as 0.5815993526674532 and parameters 
[-4.50590276  2.38564356 -2.53983696 -0.11653099  0.55388712 -2.77010897
  3.06858502  2.18960149  1.18552006 -1.06648305  3.10487965  1.14432445
  1.31029902 -1.87354676  0.7296508   2.88578423 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Fri Apr  5 04:20:57 2024]  Iteration number: 0 with current cost as 0.20093354531128016 and parameters 
[-3.2327515   2.28160015 -2.20744846 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18552    -1.06648308  1.11586495  1.14432446
  1.31029899 -1.87354679  0.7296508   2.8857842  -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Fri Apr  5 04:26:20 2024]  Iteration number: 0 with current cost as 0.19221184681342818 and parameters 
[-3.24777131  2.26124205 -2.22152344 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  1.1524219   1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1641.7905523777008 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 04:31:45 2024]  Iteration number: 0 with current cost as 0.1861154604244234 and parameters 
[-3.2540628   2.26526942 -2.21885988 -0.11653102  0.55388709 -2.77010896
  3.06858499  2.18960146  1.18552    -1.06648308  1.16402267  1.14432446
  1.310299   -1.87354679  0.7296508   2.8857842  -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Fri Apr  5 04:36:58 2024]  Iteration number: 0 with current cost as 0.5439789310219487 and parameters 
[-4.44414248  2.39560629 -2.52516599 -0.11653103  0.55388708 -2.77010894
  3.06858498  2.18960145  1.18552002 -1.06648308  2.99085313  1.14432449
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Fri Apr  5 04:42:46 2024]  Iteration number: 0 with current cost as 0.5815993526674532 and parameters 
[-4.50590276  2.38564356 -2.53983696 -0.11653099  0.55388712 -2.77010897
  3.06858502  2.18960149  1.18552006 -1.06648305  3.10487965  1.14432445
  1.31029902 -1.87354676  0.7296508   2.88578423 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Fri Apr  5 04:47:59 2024]  Iteration number: 0 with current cost as 0.20093354531128016 and parameters 
[-3.2327515   2.28160015 -2.20744846 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18552    -1.06648308  1.11586495  1.14432446
  1.31029899 -1.87354679  0.7296508   2.8857842  -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Fri Apr  5 04:53:23 2024]  Iteration number: 0 with current cost as 0.19221184681342818 and parameters 
[-3.24777131  2.26124205 -2.22152344 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  1.1524219   1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1620.9106838703156 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 04:58:46 2024]  Iteration number: 0 with current cost as 0.1861154604244234 and parameters 
[-3.2540628   2.26526942 -2.21885988 -0.11653102  0.55388709 -2.77010896
  3.06858499  2.18960146  1.18552    -1.06648308  1.16402267  1.14432446
  1.310299   -1.87354679  0.7296508   2.8857842  -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Fri Apr  5 05:04:00 2024]  Iteration number: 0 with current cost as 0.5439789310219487 and parameters 
[-4.44414248  2.39560629 -2.52516599 -0.11653103  0.55388708 -2.77010894
  3.06858498  2.18960145  1.18552002 -1.06648308  2.99085313  1.14432449
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Fri Apr  5 05:09:49 2024]  Iteration number: 0 with current cost as 0.5815993526674532 and parameters 
[-4.50590276  2.38564356 -2.53983696 -0.11653099  0.55388712 -2.77010897
  3.06858502  2.18960149  1.18552006 -1.06648305  3.10487965  1.14432445
  1.31029902 -1.87354676  0.7296508   2.88578423 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Fri Apr  5 05:15:06 2024]  Iteration number: 0 with current cost as 0.20093354531128016 and parameters 
[-3.2327515   2.28160015 -2.20744846 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18552    -1.06648308  1.11586495  1.14432446
  1.31029899 -1.87354679  0.7296508   2.8857842  -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Fri Apr  5 05:20:55 2024]  Iteration number: 0 with current cost as 0.19221184681342818 and parameters 
[-3.24777131  2.26124205 -2.22152344 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  1.1524219   1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1660.6346492767334 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 05:26:26 2024]  Iteration number: 0 with current cost as 0.1861154604244234 and parameters 
[-3.2540628   2.26526942 -2.21885988 -0.11653102  0.55388709 -2.77010896
  3.06858499  2.18960146  1.18552    -1.06648308  1.16402267  1.14432446
  1.310299   -1.87354679  0.7296508   2.8857842  -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Fri Apr  5 05:31:37 2024]  Iteration number: 0 with current cost as 0.5439789310219487 and parameters 
[-4.44414248  2.39560629 -2.52516599 -0.11653103  0.55388708 -2.77010894
  3.06858498  2.18960145  1.18552002 -1.06648308  2.99085313  1.14432449
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Fri Apr  5 05:37:34 2024]  Iteration number: 0 with current cost as 0.5815993526674532 and parameters 
[-4.50590276  2.38564356 -2.53983696 -0.11653099  0.55388712 -2.77010897
  3.06858502  2.18960149  1.18552006 -1.06648305  3.10487965  1.14432445
  1.31029902 -1.87354676  0.7296508   2.88578423 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Fri Apr  5 05:42:48 2024]  Iteration number: 0 with current cost as 0.20093354531128016 and parameters 
[-3.2327515   2.28160015 -2.20744846 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18552    -1.06648308  1.11586495  1.14432446
  1.31029899 -1.87354679  0.7296508   2.8857842  -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Fri Apr  5 05:48:12 2024]  Iteration number: 0 with current cost as 0.19221184681342818 and parameters 
[-3.24777131  2.26124205 -2.22152344 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  1.1524219   1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1635.3460583686829 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 05:53:40 2024]  Iteration number: 0 with current cost as 0.1861154604244234 and parameters 
[-3.2540628   2.26526942 -2.21885988 -0.11653102  0.55388709 -2.77010896
  3.06858499  2.18960146  1.18552    -1.06648308  1.16402267  1.14432446
  1.310299   -1.87354679  0.7296508   2.8857842  -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Fri Apr  5 05:59:02 2024]  Iteration number: 0 with current cost as 0.5439789310219487 and parameters 
[-4.44414248  2.39560629 -2.52516599 -0.11653103  0.55388708 -2.77010894
  3.06858498  2.18960145  1.18552002 -1.06648308  2.99085313  1.14432449
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Fri Apr  5 06:05:04 2024]  Iteration number: 0 with current cost as 0.5815993526674532 and parameters 
[-4.50590276  2.38564356 -2.53983696 -0.11653099  0.55388712 -2.77010897
  3.06858502  2.18960149  1.18552006 -1.06648305  3.10487965  1.14432445
  1.31029902 -1.87354676  0.7296508   2.88578423 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Fri Apr  5 06:10:27 2024]  Iteration number: 0 with current cost as 0.20093354531128016 and parameters 
[-3.2327515   2.28160015 -2.20744846 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18552    -1.06648308  1.11586495  1.14432446
  1.31029899 -1.87354679  0.7296508   2.8857842  -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Fri Apr  5 06:16:32 2024]  Iteration number: 0 with current cost as 0.19221184681342818 and parameters 
[-3.24777131  2.26124205 -2.22152344 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  1.1524219   1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1704.071311712265 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 06:22:06 2024]  Iteration number: 0 with current cost as 0.1861154604244234 and parameters 
[-3.2540628   2.26526942 -2.21885988 -0.11653102  0.55388709 -2.77010896
  3.06858499  2.18960146  1.18552    -1.06648308  1.16402267  1.14432446
  1.310299   -1.87354679  0.7296508   2.8857842  -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Fri Apr  5 06:27:28 2024]  Iteration number: 0 with current cost as 0.5439789310219487 and parameters 
[-4.44414248  2.39560629 -2.52516599 -0.11653103  0.55388708 -2.77010894
  3.06858498  2.18960145  1.18552002 -1.06648308  2.99085313  1.14432449
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Fri Apr  5 06:33:25 2024]  Iteration number: 0 with current cost as 0.5815993526674532 and parameters 
[-4.50590276  2.38564356 -2.53983696 -0.11653099  0.55388712 -2.77010897
  3.06858502  2.18960149  1.18552006 -1.06648305  3.10487965  1.14432445
  1.31029902 -1.87354676  0.7296508   2.88578423 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Fri Apr  5 06:38:39 2024]  Iteration number: 0 with current cost as 0.20093354531128016 and parameters 
[-3.2327515   2.28160015 -2.20744846 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18552    -1.06648308  1.11586495  1.14432446
  1.31029899 -1.87354679  0.7296508   2.8857842  -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Fri Apr  5 06:44:07 2024]  Iteration number: 0 with current cost as 0.19221184681342818 and parameters 
[-3.24777131  2.26124205 -2.22152344 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  1.1524219   1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1646.9517800807953 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 06:49:33 2024]  Iteration number: 0 with current cost as 0.1861154604244234 and parameters 
[-3.2540628   2.26526942 -2.21885988 -0.11653102  0.55388709 -2.77010896
  3.06858499  2.18960146  1.18552    -1.06648308  1.16402267  1.14432446
  1.310299   -1.87354679  0.7296508   2.8857842  -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Fri Apr  5 06:54:48 2024]  Iteration number: 0 with current cost as 0.5439789310219487 and parameters 
[-4.44414248  2.39560629 -2.52516599 -0.11653103  0.55388708 -2.77010894
  3.06858498  2.18960145  1.18552002 -1.06648308  2.99085313  1.14432449
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Fri Apr  5 07:00:37 2024]  Iteration number: 0 with current cost as 0.5815993526674532 and parameters 
[-4.50590276  2.38564356 -2.53983696 -0.11653099  0.55388712 -2.77010897
  3.06858502  2.18960149  1.18552006 -1.06648305  3.10487965  1.14432445
  1.31029902 -1.87354676  0.7296508   2.88578423 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Fri Apr  5 07:05:55 2024]  Iteration number: 0 with current cost as 0.20093354531128016 and parameters 
[-3.2327515   2.28160015 -2.20744846 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18552    -1.06648308  1.11586495  1.14432446
  1.31029899 -1.87354679  0.7296508   2.8857842  -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Fri Apr  5 07:11:27 2024]  Iteration number: 0 with current cost as 0.19221184681342818 and parameters 
[-3.24777131  2.26124205 -2.22152344 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  1.1524219   1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1663.6148200035095 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 07:17:15 2024]  Iteration number: 0 with current cost as 0.1861154604244234 and parameters 
[-3.2540628   2.26526942 -2.21885988 -0.11653102  0.55388709 -2.77010896
  3.06858499  2.18960146  1.18552    -1.06648308  1.16402267  1.14432446
  1.310299   -1.87354679  0.7296508   2.8857842  -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Fri Apr  5 07:22:44 2024]  Iteration number: 0 with current cost as 0.5439789310219487 and parameters 
[-4.44414248  2.39560629 -2.52516599 -0.11653103  0.55388708 -2.77010894
  3.06858498  2.18960145  1.18552002 -1.06648308  2.99085313  1.14432449
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Fri Apr  5 07:28:36 2024]  Iteration number: 0 with current cost as 0.5815993526674532 and parameters 
[-4.50590276  2.38564356 -2.53983696 -0.11653099  0.55388712 -2.77010897
  3.06858502  2.18960149  1.18552006 -1.06648305  3.10487965  1.14432445
  1.31029902 -1.87354676  0.7296508   2.88578423 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Fri Apr  5 07:33:59 2024]  Iteration number: 0 with current cost as 0.20093354531128016 and parameters 
[-3.2327515   2.28160015 -2.20744846 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18552    -1.06648308  1.11586495  1.14432446
  1.31029899 -1.87354679  0.7296508   2.8857842  -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Fri Apr  5 07:39:22 2024]  Iteration number: 0 with current cost as 0.19221184681342818 and parameters 
[-3.24777131  2.26124205 -2.22152344 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  1.1524219   1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1650.1693065166473 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 07:44:45 2024]  Iteration number: 0 with current cost as 0.1861154604244234 and parameters 
[-3.2540628   2.26526942 -2.21885988 -0.11653102  0.55388709 -2.77010896
  3.06858499  2.18960146  1.18552    -1.06648308  1.16402267  1.14432446
  1.310299   -1.87354679  0.7296508   2.8857842  -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Fri Apr  5 07:49:59 2024]  Iteration number: 0 with current cost as 0.5439789310219487 and parameters 
[-4.44414248  2.39560629 -2.52516599 -0.11653103  0.55388708 -2.77010894
  3.06858498  2.18960145  1.18552002 -1.06648308  2.99085313  1.14432449
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Fri Apr  5 07:55:57 2024]  Iteration number: 0 with current cost as 0.5815993526674532 and parameters 
[-4.50590276  2.38564356 -2.53983696 -0.11653099  0.55388712 -2.77010897
  3.06858502  2.18960149  1.18552006 -1.06648305  3.10487965  1.14432445
  1.31029902 -1.87354676  0.7296508   2.88578423 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Fri Apr  5 08:01:17 2024]  Iteration number: 0 with current cost as 0.20093354531128016 and parameters 
[-3.2327515   2.28160015 -2.20744846 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18552    -1.06648308  1.11586495  1.14432446
  1.31029899 -1.87354679  0.7296508   2.8857842  -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Fri Apr  5 08:06:43 2024]  Iteration number: 0 with current cost as 0.19221184681342818 and parameters 
[-3.24777131  2.26124205 -2.22152344 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  1.1524219   1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1643.4658930301666 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 08:12:10 2024]  Iteration number: 0 with current cost as 0.1861154604244234 and parameters 
[-3.2540628   2.26526942 -2.21885988 -0.11653102  0.55388709 -2.77010896
  3.06858499  2.18960146  1.18552    -1.06648308  1.16402267  1.14432446
  1.310299   -1.87354679  0.7296508   2.8857842  -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Fri Apr  5 08:17:26 2024]  Iteration number: 0 with current cost as 0.5439789310219487 and parameters 
[-4.44414248  2.39560629 -2.52516599 -0.11653103  0.55388708 -2.77010894
  3.06858498  2.18960145  1.18552002 -1.06648308  2.99085313  1.14432449
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Fri Apr  5 08:23:15 2024]  Iteration number: 0 with current cost as 0.5815993526674532 and parameters 
[-4.50590276  2.38564356 -2.53983696 -0.11653099  0.55388712 -2.77010897
  3.06858502  2.18960149  1.18552006 -1.06648305  3.10487965  1.14432445
  1.31029902 -1.87354676  0.7296508   2.88578423 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Fri Apr  5 08:28:31 2024]  Iteration number: 0 with current cost as 0.20093354531128016 and parameters 
[-3.2327515   2.28160015 -2.20744846 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18552    -1.06648308  1.11586495  1.14432446
  1.31029899 -1.87354679  0.7296508   2.8857842  -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Fri Apr  5 08:34:31 2024]  Iteration number: 0 with current cost as 0.19221184681342818 and parameters 
[-3.24777131  2.26124205 -2.22152344 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  1.1524219   1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1677.5495104789734 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 08:40:08 2024]  Iteration number: 0 with current cost as 0.1861154604244234 and parameters 
[-3.2540628   2.26526942 -2.21885988 -0.11653102  0.55388709 -2.77010896
  3.06858499  2.18960146  1.18552    -1.06648308  1.16402267  1.14432446
  1.310299   -1.87354679  0.7296508   2.8857842  -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Fri Apr  5 08:45:21 2024]  Iteration number: 0 with current cost as 0.5439789310219487 and parameters 
[-4.44414248  2.39560629 -2.52516599 -0.11653103  0.55388708 -2.77010894
  3.06858498  2.18960145  1.18552002 -1.06648308  2.99085313  1.14432449
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Fri Apr  5 08:51:07 2024]  Iteration number: 0 with current cost as 0.5815993526674532 and parameters 
[-4.50590276  2.38564356 -2.53983696 -0.11653099  0.55388712 -2.77010897
  3.06858502  2.18960149  1.18552006 -1.06648305  3.10487965  1.14432445
  1.31029902 -1.87354676  0.7296508   2.88578423 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Fri Apr  5 08:56:20 2024]  Iteration number: 0 with current cost as 0.20093354531128016 and parameters 
[-3.2327515   2.28160015 -2.20744846 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18552    -1.06648308  1.11586495  1.14432446
  1.31029899 -1.87354679  0.7296508   2.8857842  -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Fri Apr  5 09:01:46 2024]  Iteration number: 0 with current cost as 0.19221184681342818 and parameters 
[-3.24777131  2.26124205 -2.22152344 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  1.1524219   1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1627.323302745819 seconds. 
Discarding model... 

Training complete taking 41134.37559223175 total seconds. 
Now scoring model... 
Scoring complete taking 1.1988084316253662 seconds. 
Saved predicted values as M-A1-CZ_Modified-Pauli-CRX_predicted_values.csv
Model scores: {'MSE_train': (0.06977475894156553,), 'R2_train': 0.8610620287255808, 'MAE_train': 0.21697918220644938, 'MSE_test': 0.0973193363595574, 'R2_test': 0.8185028486954378, 'MAE_test': 0.2707079049680107}. 
Saved model results as M-A1-CZ_Modified-Pauli-CRX_results.json. 
