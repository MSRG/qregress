/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /scratch/j/jacobsen/gjones/sine5qubits/sine_train.bin... 
Successfully loaded /scratch/j/jacobsen/gjones/sine5qubits/sine_train.bin into X and y data. 
Loading dataset from /scratch/j/jacobsen/gjones/sine5qubits/sine_test.bin... 
Successfully loaded /scratch/j/jacobsen/gjones/sine5qubits/sine_test.bin into X and y data. 
Training model with dataset /scratch/j/jacobsen/gjones/sine5qubits/sine_train.bin 
 at time Thu Apr  4 21:41:17 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 21:47:07 2024]  Iteration number: 0 with current cost as 0.09346452407210135 and parameters 
[-4.66993293  2.2374349  -2.12427924 -0.11653089  0.55388721 -2.77010897
  3.06858525  2.18960172  1.18552025 -1.06648308  0.60271563  1.14432445
  1.31029912 -1.87354654  0.72965094  2.88578433 -0.54534309 -0.47522459
 -2.0265424   0.72897383  1.60512664  2.83077107 -1.26456696 -0.25136091
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 21:54:49 2024]  Iteration number: 0 with current cost as 0.10149399287826942 and parameters 
[-4.64449707  2.23743464 -2.12427964 -0.11653115  0.55388683 -2.77010922
  3.06858486  2.18960145  1.18551998 -1.06648333  0.6027151   1.14432421
  1.31029874 -1.8735468   0.72965068  2.88578407 -0.54534347 -0.47522473
 -2.02654253  0.72897357  1.60512651  2.83077083 -1.26456722 -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856946 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 22:02:28 2024]  Iteration number: 0 with current cost as 0.09775604804380797 and parameters 
[-4.65004243  2.23743464 -2.12427937 -0.11653103  0.55388695 -2.77010924
  3.06858498  2.18960132  1.18551998 -1.06648335  0.6027151   1.14432432
  1.31029899 -1.8735468   0.72965067  2.88578406 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077081 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 22:10:19 2024]  Iteration number: 0 with current cost as 0.09389774027944237 and parameters 
[-4.70877406  2.23743464 -2.1242794  -0.11653103  0.55388708 -2.77010921
  3.06858498  2.18960145  1.1855201  -1.0664832   0.60271522  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 22:18:55 2024]  Iteration number: 0 with current cost as 0.0941878163243493 and parameters 
[-4.69140476  2.23743464 -2.12427951 -0.11653103  0.55388708 -2.77010922
  3.06858486  2.18960145  1.18551986 -1.06648333  0.60271522  1.14432445
  1.31029886 -1.8735468   0.72965068  2.88578419 -0.54534347 -0.47522485
 -2.02654253  0.7289737   1.60512664  2.83077083 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2428.522649526596 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 22:27:29 2024]  Iteration number: 0 with current cost as 0.09346452407210135 and parameters 
[-4.66993293  2.2374349  -2.12427924 -0.11653089  0.55388721 -2.77010897
  3.06858525  2.18960172  1.18552025 -1.06648308  0.60271563  1.14432445
  1.31029912 -1.87354654  0.72965094  2.88578433 -0.54534309 -0.47522459
 -2.0265424   0.72897383  1.60512664  2.83077107 -1.26456696 -0.25136091
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 22:35:27 2024]  Iteration number: 0 with current cost as 0.10149399287826942 and parameters 
[-4.64449707  2.23743464 -2.12427964 -0.11653115  0.55388683 -2.77010922
  3.06858486  2.18960145  1.18551998 -1.06648333  0.6027151   1.14432421
  1.31029874 -1.8735468   0.72965068  2.88578407 -0.54534347 -0.47522473
 -2.02654253  0.72897357  1.60512651  2.83077083 -1.26456722 -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856946 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 22:43:13 2024]  Iteration number: 0 with current cost as 0.09775604804380797 and parameters 
[-4.65004243  2.23743464 -2.12427937 -0.11653103  0.55388695 -2.77010924
  3.06858498  2.18960132  1.18551998 -1.06648335  0.6027151   1.14432432
  1.31029899 -1.8735468   0.72965067  2.88578406 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077081 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 22:51:07 2024]  Iteration number: 0 with current cost as 0.09389774027944237 and parameters 
[-4.70877406  2.23743464 -2.1242794  -0.11653103  0.55388708 -2.77010921
  3.06858498  2.18960145  1.1855201  -1.0664832   0.60271522  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 23:00:02 2024]  Iteration number: 0 with current cost as 0.0941878163243493 and parameters 
[-4.69140476  2.23743464 -2.12427951 -0.11653103  0.55388708 -2.77010922
  3.06858486  2.18960145  1.18551986 -1.06648333  0.60271522  1.14432445
  1.31029886 -1.8735468   0.72965068  2.88578419 -0.54534347 -0.47522485
 -2.02654253  0.7289737   1.60512664  2.83077083 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2476.158958673477 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 23:08:46 2024]  Iteration number: 0 with current cost as 0.09346452407210135 and parameters 
[-4.66993293  2.2374349  -2.12427924 -0.11653089  0.55388721 -2.77010897
  3.06858525  2.18960172  1.18552025 -1.06648308  0.60271563  1.14432445
  1.31029912 -1.87354654  0.72965094  2.88578433 -0.54534309 -0.47522459
 -2.0265424   0.72897383  1.60512664  2.83077107 -1.26456696 -0.25136091
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 23:16:36 2024]  Iteration number: 0 with current cost as 0.10149399287826942 and parameters 
[-4.64449707  2.23743464 -2.12427964 -0.11653115  0.55388683 -2.77010922
  3.06858486  2.18960145  1.18551998 -1.06648333  0.6027151   1.14432421
  1.31029874 -1.8735468   0.72965068  2.88578407 -0.54534347 -0.47522473
 -2.02654253  0.72897357  1.60512651  2.83077083 -1.26456722 -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856946 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 23:24:39 2024]  Iteration number: 0 with current cost as 0.09775604804380797 and parameters 
[-4.65004243  2.23743464 -2.12427937 -0.11653103  0.55388695 -2.77010924
  3.06858498  2.18960132  1.18551998 -1.06648335  0.6027151   1.14432432
  1.31029899 -1.8735468   0.72965067  2.88578406 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077081 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 23:32:35 2024]  Iteration number: 0 with current cost as 0.09389774027944237 and parameters 
[-4.70877406  2.23743464 -2.1242794  -0.11653103  0.55388708 -2.77010921
  3.06858498  2.18960145  1.1855201  -1.0664832   0.60271522  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 23:41:34 2024]  Iteration number: 0 with current cost as 0.0941878163243493 and parameters 
[-4.69140476  2.23743464 -2.12427951 -0.11653103  0.55388708 -2.77010922
  3.06858486  2.18960145  1.18551986 -1.06648333  0.60271522  1.14432445
  1.31029886 -1.8735468   0.72965068  2.88578419 -0.54534347 -0.47522485
 -2.02654253  0.7289737   1.60512664  2.83077083 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2495.5857598781586 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 23:50:35 2024]  Iteration number: 0 with current cost as 0.09346452407210135 and parameters 
[-4.66993293  2.2374349  -2.12427924 -0.11653089  0.55388721 -2.77010897
  3.06858525  2.18960172  1.18552025 -1.06648308  0.60271563  1.14432445
  1.31029912 -1.87354654  0.72965094  2.88578433 -0.54534309 -0.47522459
 -2.0265424   0.72897383  1.60512664  2.83077107 -1.26456696 -0.25136091
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 23:58:26 2024]  Iteration number: 0 with current cost as 0.10149399287826942 and parameters 
[-4.64449707  2.23743464 -2.12427964 -0.11653115  0.55388683 -2.77010922
  3.06858486  2.18960145  1.18551998 -1.06648333  0.6027151   1.14432421
  1.31029874 -1.8735468   0.72965068  2.88578407 -0.54534347 -0.47522473
 -2.02654253  0.72897357  1.60512651  2.83077083 -1.26456722 -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856946 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 00:06:36 2024]  Iteration number: 0 with current cost as 0.09775604804380797 and parameters 
[-4.65004243  2.23743464 -2.12427937 -0.11653103  0.55388695 -2.77010924
  3.06858498  2.18960132  1.18551998 -1.06648335  0.6027151   1.14432432
  1.31029899 -1.8735468   0.72965067  2.88578406 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077081 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 00:14:38 2024]  Iteration number: 0 with current cost as 0.09389774027944237 and parameters 
[-4.70877406  2.23743464 -2.1242794  -0.11653103  0.55388708 -2.77010921
  3.06858498  2.18960145  1.1855201  -1.0664832   0.60271522  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 00:23:32 2024]  Iteration number: 0 with current cost as 0.0941878163243493 and parameters 
[-4.69140476  2.23743464 -2.12427951 -0.11653103  0.55388708 -2.77010922
  3.06858486  2.18960145  1.18551986 -1.06648333  0.60271522  1.14432445
  1.31029886 -1.8735468   0.72965068  2.88578419 -0.54534347 -0.47522485
 -2.02654253  0.7289737   1.60512664  2.83077083 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2519.7452352046967 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 00:32:40 2024]  Iteration number: 0 with current cost as 0.09346452407210135 and parameters 
[-4.66993293  2.2374349  -2.12427924 -0.11653089  0.55388721 -2.77010897
  3.06858525  2.18960172  1.18552025 -1.06648308  0.60271563  1.14432445
  1.31029912 -1.87354654  0.72965094  2.88578433 -0.54534309 -0.47522459
 -2.0265424   0.72897383  1.60512664  2.83077107 -1.26456696 -0.25136091
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 00:40:24 2024]  Iteration number: 0 with current cost as 0.10149399287826942 and parameters 
[-4.64449707  2.23743464 -2.12427964 -0.11653115  0.55388683 -2.77010922
  3.06858486  2.18960145  1.18551998 -1.06648333  0.6027151   1.14432421
  1.31029874 -1.8735468   0.72965068  2.88578407 -0.54534347 -0.47522473
 -2.02654253  0.72897357  1.60512651  2.83077083 -1.26456722 -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856946 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 00:48:15 2024]  Iteration number: 0 with current cost as 0.09775604804380797 and parameters 
[-4.65004243  2.23743464 -2.12427937 -0.11653103  0.55388695 -2.77010924
  3.06858498  2.18960132  1.18551998 -1.06648335  0.6027151   1.14432432
  1.31029899 -1.8735468   0.72965067  2.88578406 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077081 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 00:56:07 2024]  Iteration number: 0 with current cost as 0.09389774027944237 and parameters 
[-4.70877406  2.23743464 -2.1242794  -0.11653103  0.55388708 -2.77010921
  3.06858498  2.18960145  1.1855201  -1.0664832   0.60271522  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 01:05:09 2024]  Iteration number: 0 with current cost as 0.0941878163243493 and parameters 
[-4.69140476  2.23743464 -2.12427951 -0.11653103  0.55388708 -2.77010922
  3.06858486  2.18960145  1.18551986 -1.06648333  0.60271522  1.14432445
  1.31029886 -1.8735468   0.72965068  2.88578419 -0.54534347 -0.47522485
 -2.02654253  0.7289737   1.60512664  2.83077083 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2481.8979408740997 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 01:14:04 2024]  Iteration number: 0 with current cost as 0.09346452407210135 and parameters 
[-4.66993293  2.2374349  -2.12427924 -0.11653089  0.55388721 -2.77010897
  3.06858525  2.18960172  1.18552025 -1.06648308  0.60271563  1.14432445
  1.31029912 -1.87354654  0.72965094  2.88578433 -0.54534309 -0.47522459
 -2.0265424   0.72897383  1.60512664  2.83077107 -1.26456696 -0.25136091
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 01:22:08 2024]  Iteration number: 0 with current cost as 0.10149399287826942 and parameters 
[-4.64449707  2.23743464 -2.12427964 -0.11653115  0.55388683 -2.77010922
  3.06858486  2.18960145  1.18551998 -1.06648333  0.6027151   1.14432421
  1.31029874 -1.8735468   0.72965068  2.88578407 -0.54534347 -0.47522473
 -2.02654253  0.72897357  1.60512651  2.83077083 -1.26456722 -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856946 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 01:30:16 2024]  Iteration number: 0 with current cost as 0.09775604804380797 and parameters 
[-4.65004243  2.23743464 -2.12427937 -0.11653103  0.55388695 -2.77010924
  3.06858498  2.18960132  1.18551998 -1.06648335  0.6027151   1.14432432
  1.31029899 -1.8735468   0.72965067  2.88578406 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077081 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 01:38:41 2024]  Iteration number: 0 with current cost as 0.09389774027944237 and parameters 
[-4.70877406  2.23743464 -2.1242794  -0.11653103  0.55388708 -2.77010921
  3.06858498  2.18960145  1.1855201  -1.0664832   0.60271522  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 01:47:44 2024]  Iteration number: 0 with current cost as 0.0941878163243493 and parameters 
[-4.69140476  2.23743464 -2.12427951 -0.11653103  0.55388708 -2.77010922
  3.06858486  2.18960145  1.18551986 -1.06648333  0.60271522  1.14432445
  1.31029886 -1.8735468   0.72965068  2.88578419 -0.54534347 -0.47522485
 -2.02654253  0.7289737   1.60512664  2.83077083 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2577.1279134750366 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 01:57:06 2024]  Iteration number: 0 with current cost as 0.09346452407210135 and parameters 
[-4.66993293  2.2374349  -2.12427924 -0.11653089  0.55388721 -2.77010897
  3.06858525  2.18960172  1.18552025 -1.06648308  0.60271563  1.14432445
  1.31029912 -1.87354654  0.72965094  2.88578433 -0.54534309 -0.47522459
 -2.0265424   0.72897383  1.60512664  2.83077107 -1.26456696 -0.25136091
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 02:05:17 2024]  Iteration number: 0 with current cost as 0.10149399287826942 and parameters 
[-4.64449707  2.23743464 -2.12427964 -0.11653115  0.55388683 -2.77010922
  3.06858486  2.18960145  1.18551998 -1.06648333  0.6027151   1.14432421
  1.31029874 -1.8735468   0.72965068  2.88578407 -0.54534347 -0.47522473
 -2.02654253  0.72897357  1.60512651  2.83077083 -1.26456722 -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856946 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 02:13:21 2024]  Iteration number: 0 with current cost as 0.09775604804380797 and parameters 
[-4.65004243  2.23743464 -2.12427937 -0.11653103  0.55388695 -2.77010924
  3.06858498  2.18960132  1.18551998 -1.06648335  0.6027151   1.14432432
  1.31029899 -1.8735468   0.72965067  2.88578406 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077081 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 02:21:28 2024]  Iteration number: 0 with current cost as 0.09389774027944237 and parameters 
[-4.70877406  2.23743464 -2.1242794  -0.11653103  0.55388708 -2.77010921
  3.06858498  2.18960145  1.1855201  -1.0664832   0.60271522  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 02:30:27 2024]  Iteration number: 0 with current cost as 0.0941878163243493 and parameters 
[-4.69140476  2.23743464 -2.12427951 -0.11653103  0.55388708 -2.77010922
  3.06858486  2.18960145  1.18551986 -1.06648333  0.60271522  1.14432445
  1.31029886 -1.8735468   0.72965068  2.88578419 -0.54534347 -0.47522485
 -2.02654253  0.7289737   1.60512664  2.83077083 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2547.2491085529327 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 02:39:14 2024]  Iteration number: 0 with current cost as 0.09346452407210135 and parameters 
[-4.66993293  2.2374349  -2.12427924 -0.11653089  0.55388721 -2.77010897
  3.06858525  2.18960172  1.18552025 -1.06648308  0.60271563  1.14432445
  1.31029912 -1.87354654  0.72965094  2.88578433 -0.54534309 -0.47522459
 -2.0265424   0.72897383  1.60512664  2.83077107 -1.26456696 -0.25136091
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 02:47:14 2024]  Iteration number: 0 with current cost as 0.10149399287826942 and parameters 
[-4.64449707  2.23743464 -2.12427964 -0.11653115  0.55388683 -2.77010922
  3.06858486  2.18960145  1.18551998 -1.06648333  0.6027151   1.14432421
  1.31029874 -1.8735468   0.72965068  2.88578407 -0.54534347 -0.47522473
 -2.02654253  0.72897357  1.60512651  2.83077083 -1.26456722 -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856946 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 02:55:04 2024]  Iteration number: 0 with current cost as 0.09775604804380797 and parameters 
[-4.65004243  2.23743464 -2.12427937 -0.11653103  0.55388695 -2.77010924
  3.06858498  2.18960132  1.18551998 -1.06648335  0.6027151   1.14432432
  1.31029899 -1.8735468   0.72965067  2.88578406 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077081 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 03:02:53 2024]  Iteration number: 0 with current cost as 0.09389774027944237 and parameters 
[-4.70877406  2.23743464 -2.1242794  -0.11653103  0.55388708 -2.77010921
  3.06858498  2.18960145  1.1855201  -1.0664832   0.60271522  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 03:11:44 2024]  Iteration number: 0 with current cost as 0.0941878163243493 and parameters 
[-4.69140476  2.23743464 -2.12427951 -0.11653103  0.55388708 -2.77010922
  3.06858486  2.18960145  1.18551986 -1.06648333  0.60271522  1.14432445
  1.31029886 -1.8735468   0.72965068  2.88578419 -0.54534347 -0.47522485
 -2.02654253  0.7289737   1.60512664  2.83077083 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2476.5650470256805 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 03:20:30 2024]  Iteration number: 0 with current cost as 0.09346452407210135 and parameters 
[-4.66993293  2.2374349  -2.12427924 -0.11653089  0.55388721 -2.77010897
  3.06858525  2.18960172  1.18552025 -1.06648308  0.60271563  1.14432445
  1.31029912 -1.87354654  0.72965094  2.88578433 -0.54534309 -0.47522459
 -2.0265424   0.72897383  1.60512664  2.83077107 -1.26456696 -0.25136091
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 03:28:12 2024]  Iteration number: 0 with current cost as 0.10149399287826942 and parameters 
[-4.64449707  2.23743464 -2.12427964 -0.11653115  0.55388683 -2.77010922
  3.06858486  2.18960145  1.18551998 -1.06648333  0.6027151   1.14432421
  1.31029874 -1.8735468   0.72965068  2.88578407 -0.54534347 -0.47522473
 -2.02654253  0.72897357  1.60512651  2.83077083 -1.26456722 -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856946 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 03:36:02 2024]  Iteration number: 0 with current cost as 0.09775604804380797 and parameters 
[-4.65004243  2.23743464 -2.12427937 -0.11653103  0.55388695 -2.77010924
  3.06858498  2.18960132  1.18551998 -1.06648335  0.6027151   1.14432432
  1.31029899 -1.8735468   0.72965067  2.88578406 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077081 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 03:43:55 2024]  Iteration number: 0 with current cost as 0.09389774027944237 and parameters 
[-4.70877406  2.23743464 -2.1242794  -0.11653103  0.55388708 -2.77010921
  3.06858498  2.18960145  1.1855201  -1.0664832   0.60271522  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 03:52:50 2024]  Iteration number: 0 with current cost as 0.0941878163243493 and parameters 
[-4.69140476  2.23743464 -2.12427951 -0.11653103  0.55388708 -2.77010922
  3.06858486  2.18960145  1.18551986 -1.06648333  0.60271522  1.14432445
  1.31029886 -1.8735468   0.72965068  2.88578419 -0.54534347 -0.47522485
 -2.02654253  0.7289737   1.60512664  2.83077083 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2464.162900686264 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 04:01:52 2024]  Iteration number: 0 with current cost as 0.09346452407210135 and parameters 
[-4.66993293  2.2374349  -2.12427924 -0.11653089  0.55388721 -2.77010897
  3.06858525  2.18960172  1.18552025 -1.06648308  0.60271563  1.14432445
  1.31029912 -1.87354654  0.72965094  2.88578433 -0.54534309 -0.47522459
 -2.0265424   0.72897383  1.60512664  2.83077107 -1.26456696 -0.25136091
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 04:10:01 2024]  Iteration number: 0 with current cost as 0.10149399287826942 and parameters 
[-4.64449707  2.23743464 -2.12427964 -0.11653115  0.55388683 -2.77010922
  3.06858486  2.18960145  1.18551998 -1.06648333  0.6027151   1.14432421
  1.31029874 -1.8735468   0.72965068  2.88578407 -0.54534347 -0.47522473
 -2.02654253  0.72897357  1.60512651  2.83077083 -1.26456722 -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856946 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 04:18:10 2024]  Iteration number: 0 with current cost as 0.09775604804380797 and parameters 
[-4.65004243  2.23743464 -2.12427937 -0.11653103  0.55388695 -2.77010924
  3.06858498  2.18960132  1.18551998 -1.06648335  0.6027151   1.14432432
  1.31029899 -1.8735468   0.72965067  2.88578406 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077081 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 04:26:06 2024]  Iteration number: 0 with current cost as 0.09389774027944237 and parameters 
[-4.70877406  2.23743464 -2.1242794  -0.11653103  0.55388708 -2.77010921
  3.06858498  2.18960145  1.1855201  -1.0664832   0.60271522  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 04:35:12 2024]  Iteration number: 0 with current cost as 0.0941878163243493 and parameters 
[-4.69140476  2.23743464 -2.12427951 -0.11653103  0.55388708 -2.77010922
  3.06858486  2.18960145  1.18551986 -1.06648333  0.60271522  1.14432445
  1.31029886 -1.8735468   0.72965068  2.88578419 -0.54534347 -0.47522485
 -2.02654253  0.7289737   1.60512664  2.83077083 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2544.328362226486 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 04:44:21 2024]  Iteration number: 0 with current cost as 0.09346452407210135 and parameters 
[-4.66993293  2.2374349  -2.12427924 -0.11653089  0.55388721 -2.77010897
  3.06858525  2.18960172  1.18552025 -1.06648308  0.60271563  1.14432445
  1.31029912 -1.87354654  0.72965094  2.88578433 -0.54534309 -0.47522459
 -2.0265424   0.72897383  1.60512664  2.83077107 -1.26456696 -0.25136091
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 04:52:29 2024]  Iteration number: 0 with current cost as 0.10149399287826942 and parameters 
[-4.64449707  2.23743464 -2.12427964 -0.11653115  0.55388683 -2.77010922
  3.06858486  2.18960145  1.18551998 -1.06648333  0.6027151   1.14432421
  1.31029874 -1.8735468   0.72965068  2.88578407 -0.54534347 -0.47522473
 -2.02654253  0.72897357  1.60512651  2.83077083 -1.26456722 -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856946 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 05:00:29 2024]  Iteration number: 0 with current cost as 0.09775604804380797 and parameters 
[-4.65004243  2.23743464 -2.12427937 -0.11653103  0.55388695 -2.77010924
  3.06858498  2.18960132  1.18551998 -1.06648335  0.6027151   1.14432432
  1.31029899 -1.8735468   0.72965067  2.88578406 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077081 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 05:08:27 2024]  Iteration number: 0 with current cost as 0.09389774027944237 and parameters 
[-4.70877406  2.23743464 -2.1242794  -0.11653103  0.55388708 -2.77010921
  3.06858498  2.18960145  1.1855201  -1.0664832   0.60271522  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 05:17:28 2024]  Iteration number: 0 with current cost as 0.0941878163243493 and parameters 
[-4.69140476  2.23743464 -2.12427951 -0.11653103  0.55388708 -2.77010922
  3.06858486  2.18960145  1.18551986 -1.06648333  0.60271522  1.14432445
  1.31029886 -1.8735468   0.72965068  2.88578419 -0.54534347 -0.47522485
 -2.02654253  0.7289737   1.60512664  2.83077083 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2535.182577610016 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 05:26:22 2024]  Iteration number: 0 with current cost as 0.09346452407210135 and parameters 
[-4.66993293  2.2374349  -2.12427924 -0.11653089  0.55388721 -2.77010897
  3.06858525  2.18960172  1.18552025 -1.06648308  0.60271563  1.14432445
  1.31029912 -1.87354654  0.72965094  2.88578433 -0.54534309 -0.47522459
 -2.0265424   0.72897383  1.60512664  2.83077107 -1.26456696 -0.25136091
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 05:34:11 2024]  Iteration number: 0 with current cost as 0.10149399287826942 and parameters 
[-4.64449707  2.23743464 -2.12427964 -0.11653115  0.55388683 -2.77010922
  3.06858486  2.18960145  1.18551998 -1.06648333  0.6027151   1.14432421
  1.31029874 -1.8735468   0.72965068  2.88578407 -0.54534347 -0.47522473
 -2.02654253  0.72897357  1.60512651  2.83077083 -1.26456722 -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856946 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 05:41:58 2024]  Iteration number: 0 with current cost as 0.09775604804380797 and parameters 
[-4.65004243  2.23743464 -2.12427937 -0.11653103  0.55388695 -2.77010924
  3.06858498  2.18960132  1.18551998 -1.06648335  0.6027151   1.14432432
  1.31029899 -1.8735468   0.72965067  2.88578406 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077081 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 05:49:56 2024]  Iteration number: 0 with current cost as 0.09389774027944237 and parameters 
[-4.70877406  2.23743464 -2.1242794  -0.11653103  0.55388708 -2.77010921
  3.06858498  2.18960145  1.1855201  -1.0664832   0.60271522  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 05:58:32 2024]  Iteration number: 0 with current cost as 0.0941878163243493 and parameters 
[-4.69140476  2.23743464 -2.12427951 -0.11653103  0.55388708 -2.77010922
  3.06858486  2.18960145  1.18551986 -1.06648333  0.60271522  1.14432445
  1.31029886 -1.8735468   0.72965068  2.88578419 -0.54534347 -0.47522485
 -2.02654253  0.7289737   1.60512664  2.83077083 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2458.572071313858 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 06:07:08 2024]  Iteration number: 0 with current cost as 0.09346452407210135 and parameters 
[-4.66993293  2.2374349  -2.12427924 -0.11653089  0.55388721 -2.77010897
  3.06858525  2.18960172  1.18552025 -1.06648308  0.60271563  1.14432445
  1.31029912 -1.87354654  0.72965094  2.88578433 -0.54534309 -0.47522459
 -2.0265424   0.72897383  1.60512664  2.83077107 -1.26456696 -0.25136091
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 06:14:50 2024]  Iteration number: 0 with current cost as 0.10149399287826942 and parameters 
[-4.64449707  2.23743464 -2.12427964 -0.11653115  0.55388683 -2.77010922
  3.06858486  2.18960145  1.18551998 -1.06648333  0.6027151   1.14432421
  1.31029874 -1.8735468   0.72965068  2.88578407 -0.54534347 -0.47522473
 -2.02654253  0.72897357  1.60512651  2.83077083 -1.26456722 -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856946 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 06:22:30 2024]  Iteration number: 0 with current cost as 0.09775604804380797 and parameters 
[-4.65004243  2.23743464 -2.12427937 -0.11653103  0.55388695 -2.77010924
  3.06858498  2.18960132  1.18551998 -1.06648335  0.6027151   1.14432432
  1.31029899 -1.8735468   0.72965067  2.88578406 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077081 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 06:30:34 2024]  Iteration number: 0 with current cost as 0.09389774027944237 and parameters 
[-4.70877406  2.23743464 -2.1242794  -0.11653103  0.55388708 -2.77010921
  3.06858498  2.18960145  1.1855201  -1.0664832   0.60271522  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 06:39:25 2024]  Iteration number: 0 with current cost as 0.0941878163243493 and parameters 
[-4.69140476  2.23743464 -2.12427951 -0.11653103  0.55388708 -2.77010922
  3.06858486  2.18960145  1.18551986 -1.06648333  0.60271522  1.14432445
  1.31029886 -1.8735468   0.72965068  2.88578419 -0.54534347 -0.47522485
 -2.02654253  0.7289737   1.60512664  2.83077083 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2458.428890943527 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 06:48:23 2024]  Iteration number: 0 with current cost as 0.09346452407210135 and parameters 
[-4.66993293  2.2374349  -2.12427924 -0.11653089  0.55388721 -2.77010897
  3.06858525  2.18960172  1.18552025 -1.06648308  0.60271563  1.14432445
  1.31029912 -1.87354654  0.72965094  2.88578433 -0.54534309 -0.47522459
 -2.0265424   0.72897383  1.60512664  2.83077107 -1.26456696 -0.25136091
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 06:56:20 2024]  Iteration number: 0 with current cost as 0.10149399287826942 and parameters 
[-4.64449707  2.23743464 -2.12427964 -0.11653115  0.55388683 -2.77010922
  3.06858486  2.18960145  1.18551998 -1.06648333  0.6027151   1.14432421
  1.31029874 -1.8735468   0.72965068  2.88578407 -0.54534347 -0.47522473
 -2.02654253  0.72897357  1.60512651  2.83077083 -1.26456722 -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856946 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 07:04:06 2024]  Iteration number: 0 with current cost as 0.09775604804380797 and parameters 
[-4.65004243  2.23743464 -2.12427937 -0.11653103  0.55388695 -2.77010924
  3.06858498  2.18960132  1.18551998 -1.06648335  0.6027151   1.14432432
  1.31029899 -1.8735468   0.72965067  2.88578406 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077081 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 07:11:49 2024]  Iteration number: 0 with current cost as 0.09389774027944237 and parameters 
[-4.70877406  2.23743464 -2.1242794  -0.11653103  0.55388708 -2.77010921
  3.06858498  2.18960145  1.1855201  -1.0664832   0.60271522  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 07:20:37 2024]  Iteration number: 0 with current cost as 0.0941878163243493 and parameters 
[-4.69140476  2.23743464 -2.12427951 -0.11653103  0.55388708 -2.77010922
  3.06858486  2.18960145  1.18551986 -1.06648333  0.60271522  1.14432445
  1.31029886 -1.8735468   0.72965068  2.88578419 -0.54534347 -0.47522485
 -2.02654253  0.7289737   1.60512664  2.83077083 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2466.9830095767975 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 07:29:19 2024]  Iteration number: 0 with current cost as 0.09346452407210135 and parameters 
[-4.66993293  2.2374349  -2.12427924 -0.11653089  0.55388721 -2.77010897
  3.06858525  2.18960172  1.18552025 -1.06648308  0.60271563  1.14432445
  1.31029912 -1.87354654  0.72965094  2.88578433 -0.54534309 -0.47522459
 -2.0265424   0.72897383  1.60512664  2.83077107 -1.26456696 -0.25136091
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 07:37:02 2024]  Iteration number: 0 with current cost as 0.10149399287826942 and parameters 
[-4.64449707  2.23743464 -2.12427964 -0.11653115  0.55388683 -2.77010922
  3.06858486  2.18960145  1.18551998 -1.06648333  0.6027151   1.14432421
  1.31029874 -1.8735468   0.72965068  2.88578407 -0.54534347 -0.47522473
 -2.02654253  0.72897357  1.60512651  2.83077083 -1.26456722 -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856946 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 07:44:57 2024]  Iteration number: 0 with current cost as 0.09775604804380797 and parameters 
[-4.65004243  2.23743464 -2.12427937 -0.11653103  0.55388695 -2.77010924
  3.06858498  2.18960132  1.18551998 -1.06648335  0.6027151   1.14432432
  1.31029899 -1.8735468   0.72965067  2.88578406 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077081 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 07:53:01 2024]  Iteration number: 0 with current cost as 0.09389774027944237 and parameters 
[-4.70877406  2.23743464 -2.1242794  -0.11653103  0.55388708 -2.77010921
  3.06858498  2.18960145  1.1855201  -1.0664832   0.60271522  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 08:01:52 2024]  Iteration number: 0 with current cost as 0.0941878163243493 and parameters 
[-4.69140476  2.23743464 -2.12427951 -0.11653103  0.55388708 -2.77010922
  3.06858486  2.18960145  1.18551986 -1.06648333  0.60271522  1.14432445
  1.31029886 -1.8735468   0.72965068  2.88578419 -0.54534347 -0.47522485
 -2.02654253  0.7289737   1.60512664  2.83077083 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2491.137922525406 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 08:10:45 2024]  Iteration number: 0 with current cost as 0.09346452407210135 and parameters 
[-4.66993293  2.2374349  -2.12427924 -0.11653089  0.55388721 -2.77010897
  3.06858525  2.18960172  1.18552025 -1.06648308  0.60271563  1.14432445
  1.31029912 -1.87354654  0.72965094  2.88578433 -0.54534309 -0.47522459
 -2.0265424   0.72897383  1.60512664  2.83077107 -1.26456696 -0.25136091
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 08:18:38 2024]  Iteration number: 0 with current cost as 0.10149399287826942 and parameters 
[-4.64449707  2.23743464 -2.12427964 -0.11653115  0.55388683 -2.77010922
  3.06858486  2.18960145  1.18551998 -1.06648333  0.6027151   1.14432421
  1.31029874 -1.8735468   0.72965068  2.88578407 -0.54534347 -0.47522473
 -2.02654253  0.72897357  1.60512651  2.83077083 -1.26456722 -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856946 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 08:26:18 2024]  Iteration number: 0 with current cost as 0.09775604804380797 and parameters 
[-4.65004243  2.23743464 -2.12427937 -0.11653103  0.55388695 -2.77010924
  3.06858498  2.18960132  1.18551998 -1.06648335  0.6027151   1.14432432
  1.31029899 -1.8735468   0.72965067  2.88578406 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077081 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 08:34:11 2024]  Iteration number: 0 with current cost as 0.09389774027944237 and parameters 
[-4.70877406  2.23743464 -2.1242794  -0.11653103  0.55388708 -2.77010921
  3.06858498  2.18960145  1.1855201  -1.0664832   0.60271522  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 08:43:02 2024]  Iteration number: 0 with current cost as 0.0941878163243493 and parameters 
[-4.69140476  2.23743464 -2.12427951 -0.11653103  0.55388708 -2.77010922
  3.06858486  2.18960145  1.18551986 -1.06648333  0.60271522  1.14432445
  1.31029886 -1.8735468   0.72965068  2.88578419 -0.54534347 -0.47522485
 -2.02654253  0.7289737   1.60512664  2.83077083 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2455.675952911377 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 08:51:51 2024]  Iteration number: 0 with current cost as 0.09346452407210135 and parameters 
[-4.66993293  2.2374349  -2.12427924 -0.11653089  0.55388721 -2.77010897
  3.06858525  2.18960172  1.18552025 -1.06648308  0.60271563  1.14432445
  1.31029912 -1.87354654  0.72965094  2.88578433 -0.54534309 -0.47522459
 -2.0265424   0.72897383  1.60512664  2.83077107 -1.26456696 -0.25136091
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 08:59:56 2024]  Iteration number: 0 with current cost as 0.10149399287826942 and parameters 
[-4.64449707  2.23743464 -2.12427964 -0.11653115  0.55388683 -2.77010922
  3.06858486  2.18960145  1.18551998 -1.06648333  0.6027151   1.14432421
  1.31029874 -1.8735468   0.72965068  2.88578407 -0.54534347 -0.47522473
 -2.02654253  0.72897357  1.60512651  2.83077083 -1.26456722 -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856946 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 09:08:26 2024]  Iteration number: 0 with current cost as 0.09775604804380797 and parameters 
[-4.65004243  2.23743464 -2.12427937 -0.11653103  0.55388695 -2.77010924
  3.06858498  2.18960132  1.18551998 -1.06648335  0.6027151   1.14432432
  1.31029899 -1.8735468   0.72965067  2.88578406 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077081 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 09:17:03 2024]  Iteration number: 0 with current cost as 0.09389774027944237 and parameters 
[-4.70877406  2.23743464 -2.1242794  -0.11653103  0.55388708 -2.77010921
  3.06858498  2.18960145  1.1855201  -1.0664832   0.60271522  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 09:26:48 2024]  Iteration number: 0 with current cost as 0.0941878163243493 and parameters 
[-4.69140476  2.23743464 -2.12427951 -0.11653103  0.55388708 -2.77010922
  3.06858486  2.18960145  1.18551986 -1.06648333  0.60271522  1.14432445
  1.31029886 -1.8735468   0.72965068  2.88578419 -0.54534347 -0.47522485
 -2.02654253  0.7289737   1.60512664  2.83077083 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2654.9387154579163 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 09:36:20 2024]  Iteration number: 0 with current cost as 0.09346452407210135 and parameters 
[-4.66993293  2.2374349  -2.12427924 -0.11653089  0.55388721 -2.77010897
  3.06858525  2.18960172  1.18552025 -1.06648308  0.60271563  1.14432445
  1.31029912 -1.87354654  0.72965094  2.88578433 -0.54534309 -0.47522459
 -2.0265424   0.72897383  1.60512664  2.83077107 -1.26456696 -0.25136091
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 09:44:37 2024]  Iteration number: 0 with current cost as 0.10149399287826942 and parameters 
[-4.64449707  2.23743464 -2.12427964 -0.11653115  0.55388683 -2.77010922
  3.06858486  2.18960145  1.18551998 -1.06648333  0.6027151   1.14432421
  1.31029874 -1.8735468   0.72965068  2.88578407 -0.54534347 -0.47522473
 -2.02654253  0.72897357  1.60512651  2.83077083 -1.26456722 -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856946 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 09:53:21 2024]  Iteration number: 0 with current cost as 0.09775604804380797 and parameters 
[-4.65004243  2.23743464 -2.12427937 -0.11653103  0.55388695 -2.77010924
  3.06858498  2.18960132  1.18551998 -1.06648335  0.6027151   1.14432432
  1.31029899 -1.8735468   0.72965067  2.88578406 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077081 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 10:01:55 2024]  Iteration number: 0 with current cost as 0.09389774027944237 and parameters 
[-4.70877406  2.23743464 -2.1242794  -0.11653103  0.55388708 -2.77010921
  3.06858498  2.18960145  1.1855201  -1.0664832   0.60271522  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 10:11:31 2024]  Iteration number: 0 with current cost as 0.0941878163243493 and parameters 
[-4.69140476  2.23743464 -2.12427951 -0.11653103  0.55388708 -2.77010922
  3.06858486  2.18960145  1.18551986 -1.06648333  0.60271522  1.14432445
  1.31029886 -1.8735468   0.72965068  2.88578419 -0.54534347 -0.47522485
 -2.02654253  0.7289737   1.60512664  2.83077083 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2667.5344319343567 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 10:21:02 2024]  Iteration number: 0 with current cost as 0.09346452407210135 and parameters 
[-4.66993293  2.2374349  -2.12427924 -0.11653089  0.55388721 -2.77010897
  3.06858525  2.18960172  1.18552025 -1.06648308  0.60271563  1.14432445
  1.31029912 -1.87354654  0.72965094  2.88578433 -0.54534309 -0.47522459
 -2.0265424   0.72897383  1.60512664  2.83077107 -1.26456696 -0.25136091
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 10:29:19 2024]  Iteration number: 0 with current cost as 0.10149399287826942 and parameters 
[-4.64449707  2.23743464 -2.12427964 -0.11653115  0.55388683 -2.77010922
  3.06858486  2.18960145  1.18551998 -1.06648333  0.6027151   1.14432421
  1.31029874 -1.8735468   0.72965068  2.88578407 -0.54534347 -0.47522473
 -2.02654253  0.72897357  1.60512651  2.83077083 -1.26456722 -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856946 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 10:36:50 2024]  Iteration number: 0 with current cost as 0.09775604804380797 and parameters 
[-4.65004243  2.23743464 -2.12427937 -0.11653103  0.55388695 -2.77010924
  3.06858498  2.18960132  1.18551998 -1.06648335  0.6027151   1.14432432
  1.31029899 -1.8735468   0.72965067  2.88578406 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077081 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 10:44:23 2024]  Iteration number: 0 with current cost as 0.09389774027944237 and parameters 
[-4.70877406  2.23743464 -2.1242794  -0.11653103  0.55388708 -2.77010921
  3.06858498  2.18960145  1.1855201  -1.0664832   0.60271522  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 10:52:57 2024]  Iteration number: 0 with current cost as 0.0941878163243493 and parameters 
[-4.69140476  2.23743464 -2.12427951 -0.11653103  0.55388708 -2.77010922
  3.06858486  2.18960145  1.18551986 -1.06648333  0.60271522  1.14432445
  1.31029886 -1.8735468   0.72965068  2.88578419 -0.54534347 -0.47522485
 -2.02654253  0.7289737   1.60512664  2.83077083 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2468.1188995838165 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 11:01:23 2024]  Iteration number: 0 with current cost as 0.09346452407210135 and parameters 
[-4.66993293  2.2374349  -2.12427924 -0.11653089  0.55388721 -2.77010897
  3.06858525  2.18960172  1.18552025 -1.06648308  0.60271563  1.14432445
  1.31029912 -1.87354654  0.72965094  2.88578433 -0.54534309 -0.47522459
 -2.0265424   0.72897383  1.60512664  2.83077107 -1.26456696 -0.25136091
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 11:08:54 2024]  Iteration number: 0 with current cost as 0.10149399287826942 and parameters 
[-4.64449707  2.23743464 -2.12427964 -0.11653115  0.55388683 -2.77010922
  3.06858486  2.18960145  1.18551998 -1.06648333  0.6027151   1.14432421
  1.31029874 -1.8735468   0.72965068  2.88578407 -0.54534347 -0.47522473
 -2.02654253  0.72897357  1.60512651  2.83077083 -1.26456722 -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856946 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 11:16:27 2024]  Iteration number: 0 with current cost as 0.09775604804380797 and parameters 
[-4.65004243  2.23743464 -2.12427937 -0.11653103  0.55388695 -2.77010924
  3.06858498  2.18960132  1.18551998 -1.06648335  0.6027151   1.14432432
  1.31029899 -1.8735468   0.72965067  2.88578406 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077081 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 11:23:58 2024]  Iteration number: 0 with current cost as 0.09389774027944237 and parameters 
[-4.70877406  2.23743464 -2.1242794  -0.11653103  0.55388708 -2.77010921
  3.06858498  2.18960145  1.1855201  -1.0664832   0.60271522  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 11:32:30 2024]  Iteration number: 0 with current cost as 0.0941878163243493 and parameters 
[-4.69140476  2.23743464 -2.12427951 -0.11653103  0.55388708 -2.77010922
  3.06858486  2.18960145  1.18551986 -1.06648333  0.60271522  1.14432445
  1.31029886 -1.8735468   0.72965068  2.88578419 -0.54534347 -0.47522485
 -2.02654253  0.7289737   1.60512664  2.83077083 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2373.741845846176 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 11:40:55 2024]  Iteration number: 0 with current cost as 0.09346452407210135 and parameters 
[-4.66993293  2.2374349  -2.12427924 -0.11653089  0.55388721 -2.77010897
  3.06858525  2.18960172  1.18552025 -1.06648308  0.60271563  1.14432445
  1.31029912 -1.87354654  0.72965094  2.88578433 -0.54534309 -0.47522459
 -2.0265424   0.72897383  1.60512664  2.83077107 -1.26456696 -0.25136091
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 11:48:29 2024]  Iteration number: 0 with current cost as 0.10149399287826942 and parameters 
[-4.64449707  2.23743464 -2.12427964 -0.11653115  0.55388683 -2.77010922
  3.06858486  2.18960145  1.18551998 -1.06648333  0.6027151   1.14432421
  1.31029874 -1.8735468   0.72965068  2.88578407 -0.54534347 -0.47522473
 -2.02654253  0.72897357  1.60512651  2.83077083 -1.26456722 -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856946 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 11:55:57 2024]  Iteration number: 0 with current cost as 0.09775604804380797 and parameters 
[-4.65004243  2.23743464 -2.12427937 -0.11653103  0.55388695 -2.77010924
  3.06858498  2.18960132  1.18551998 -1.06648335  0.6027151   1.14432432
  1.31029899 -1.8735468   0.72965067  2.88578406 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077081 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 12:03:27 2024]  Iteration number: 0 with current cost as 0.09389774027944237 and parameters 
[-4.70877406  2.23743464 -2.1242794  -0.11653103  0.55388708 -2.77010921
  3.06858498  2.18960145  1.1855201  -1.0664832   0.60271522  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 12:12:20 2024]  Iteration number: 0 with current cost as 0.0941878163243493 and parameters 
[-4.69140476  2.23743464 -2.12427951 -0.11653103  0.55388708 -2.77010922
  3.06858486  2.18960145  1.18551986 -1.06648333  0.60271522  1.14432445
  1.31029886 -1.8735468   0.72965068  2.88578419 -0.54534347 -0.47522485
 -2.02654253  0.7289737   1.60512664  2.83077083 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2410.000043153763 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 12:21:26 2024]  Iteration number: 0 with current cost as 0.09346452407210135 and parameters 
[-4.66993293  2.2374349  -2.12427924 -0.11653089  0.55388721 -2.77010897
  3.06858525  2.18960172  1.18552025 -1.06648308  0.60271563  1.14432445
  1.31029912 -1.87354654  0.72965094  2.88578433 -0.54534309 -0.47522459
 -2.0265424   0.72897383  1.60512664  2.83077107 -1.26456696 -0.25136091
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 12:29:21 2024]  Iteration number: 0 with current cost as 0.10149399287826942 and parameters 
[-4.64449707  2.23743464 -2.12427964 -0.11653115  0.55388683 -2.77010922
  3.06858486  2.18960145  1.18551998 -1.06648333  0.6027151   1.14432421
  1.31029874 -1.8735468   0.72965068  2.88578407 -0.54534347 -0.47522473
 -2.02654253  0.72897357  1.60512651  2.83077083 -1.26456722 -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856946 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 12:37:01 2024]  Iteration number: 0 with current cost as 0.09775604804380797 and parameters 
[-4.65004243  2.23743464 -2.12427937 -0.11653103  0.55388695 -2.77010924
  3.06858498  2.18960132  1.18551998 -1.06648335  0.6027151   1.14432432
  1.31029899 -1.8735468   0.72965067  2.88578406 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077081 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 12:44:57 2024]  Iteration number: 0 with current cost as 0.09389774027944237 and parameters 
[-4.70877406  2.23743464 -2.1242794  -0.11653103  0.55388708 -2.77010921
  3.06858498  2.18960145  1.1855201  -1.0664832   0.60271522  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 12:53:21 2024]  Iteration number: 0 with current cost as 0.0941878163243493 and parameters 
[-4.69140476  2.23743464 -2.12427951 -0.11653103  0.55388708 -2.77010922
  3.06858486  2.18960145  1.18551986 -1.06648333  0.60271522  1.14432445
  1.31029886 -1.8735468   0.72965068  2.88578419 -0.54534347 -0.47522485
 -2.02654253  0.7289737   1.60512664  2.83077083 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2452.3900294303894 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 13:02:10 2024]  Iteration number: 0 with current cost as 0.09346452407210135 and parameters 
[-4.66993293  2.2374349  -2.12427924 -0.11653089  0.55388721 -2.77010897
  3.06858525  2.18960172  1.18552025 -1.06648308  0.60271563  1.14432445
  1.31029912 -1.87354654  0.72965094  2.88578433 -0.54534309 -0.47522459
 -2.0265424   0.72897383  1.60512664  2.83077107 -1.26456696 -0.25136091
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 13:10:07 2024]  Iteration number: 0 with current cost as 0.10149399287826942 and parameters 
[-4.64449707  2.23743464 -2.12427964 -0.11653115  0.55388683 -2.77010922
  3.06858486  2.18960145  1.18551998 -1.06648333  0.6027151   1.14432421
  1.31029874 -1.8735468   0.72965068  2.88578407 -0.54534347 -0.47522473
 -2.02654253  0.72897357  1.60512651  2.83077083 -1.26456722 -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856946 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 13:17:41 2024]  Iteration number: 0 with current cost as 0.09775604804380797 and parameters 
[-4.65004243  2.23743464 -2.12427937 -0.11653103  0.55388695 -2.77010924
  3.06858498  2.18960132  1.18551998 -1.06648335  0.6027151   1.14432432
  1.31029899 -1.8735468   0.72965067  2.88578406 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077081 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 13:25:23 2024]  Iteration number: 0 with current cost as 0.09389774027944237 and parameters 
[-4.70877406  2.23743464 -2.1242794  -0.11653103  0.55388708 -2.77010921
  3.06858498  2.18960145  1.1855201  -1.0664832   0.60271522  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 13:34:19 2024]  Iteration number: 0 with current cost as 0.0941878163243493 and parameters 
[-4.69140476  2.23743464 -2.12427951 -0.11653103  0.55388708 -2.77010922
  3.06858486  2.18960145  1.18551986 -1.06648333  0.60271522  1.14432445
  1.31029886 -1.8735468   0.72965068  2.88578419 -0.54534347 -0.47522485
 -2.02654253  0.7289737   1.60512664  2.83077083 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2454.4355516433716 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 13:43:14 2024]  Iteration number: 0 with current cost as 0.09346452407210135 and parameters 
[-4.66993293  2.2374349  -2.12427924 -0.11653089  0.55388721 -2.77010897
  3.06858525  2.18960172  1.18552025 -1.06648308  0.60271563  1.14432445
  1.31029912 -1.87354654  0.72965094  2.88578433 -0.54534309 -0.47522459
 -2.0265424   0.72897383  1.60512664  2.83077107 -1.26456696 -0.25136091
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 13:50:56 2024]  Iteration number: 0 with current cost as 0.10149399287826942 and parameters 
[-4.64449707  2.23743464 -2.12427964 -0.11653115  0.55388683 -2.77010922
  3.06858486  2.18960145  1.18551998 -1.06648333  0.6027151   1.14432421
  1.31029874 -1.8735468   0.72965068  2.88578407 -0.54534347 -0.47522473
 -2.02654253  0.72897357  1.60512651  2.83077083 -1.26456722 -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856946 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 13:58:56 2024]  Iteration number: 0 with current cost as 0.09775604804380797 and parameters 
[-4.65004243  2.23743464 -2.12427937 -0.11653103  0.55388695 -2.77010924
  3.06858498  2.18960132  1.18551998 -1.06648335  0.6027151   1.14432432
  1.31029899 -1.8735468   0.72965067  2.88578406 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077081 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 14:06:52 2024]  Iteration number: 0 with current cost as 0.09389774027944237 and parameters 
[-4.70877406  2.23743464 -2.1242794  -0.11653103  0.55388708 -2.77010921
  3.06858498  2.18960145  1.1855201  -1.0664832   0.60271522  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 14:15:39 2024]  Iteration number: 0 with current cost as 0.0941878163243493 and parameters 
[-4.69140476  2.23743464 -2.12427951 -0.11653103  0.55388708 -2.77010922
  3.06858486  2.18960145  1.18551986 -1.06648333  0.60271522  1.14432445
  1.31029886 -1.8735468   0.72965068  2.88578419 -0.54534347 -0.47522485
 -2.02654253  0.7289737   1.60512664  2.83077083 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2473.4974608421326 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 14:24:31 2024]  Iteration number: 0 with current cost as 0.09346452407210135 and parameters 
[-4.66993293  2.2374349  -2.12427924 -0.11653089  0.55388721 -2.77010897
  3.06858525  2.18960172  1.18552025 -1.06648308  0.60271563  1.14432445
  1.31029912 -1.87354654  0.72965094  2.88578433 -0.54534309 -0.47522459
 -2.0265424   0.72897383  1.60512664  2.83077107 -1.26456696 -0.25136091
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 14:32:14 2024]  Iteration number: 0 with current cost as 0.10149399287826942 and parameters 
[-4.64449707  2.23743464 -2.12427964 -0.11653115  0.55388683 -2.77010922
  3.06858486  2.18960145  1.18551998 -1.06648333  0.6027151   1.14432421
  1.31029874 -1.8735468   0.72965068  2.88578407 -0.54534347 -0.47522473
 -2.02654253  0.72897357  1.60512651  2.83077083 -1.26456722 -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856946 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 14:39:55 2024]  Iteration number: 0 with current cost as 0.09775604804380797 and parameters 
[-4.65004243  2.23743464 -2.12427937 -0.11653103  0.55388695 -2.77010924
  3.06858498  2.18960132  1.18551998 -1.06648335  0.6027151   1.14432432
  1.31029899 -1.8735468   0.72965067  2.88578406 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077081 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 14:47:51 2024]  Iteration number: 0 with current cost as 0.09389774027944237 and parameters 
[-4.70877406  2.23743464 -2.1242794  -0.11653103  0.55388708 -2.77010921
  3.06858498  2.18960145  1.1855201  -1.0664832   0.60271522  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 14:56:43 2024]  Iteration number: 0 with current cost as 0.0941878163243493 and parameters 
[-4.69140476  2.23743464 -2.12427951 -0.11653103  0.55388708 -2.77010922
  3.06858486  2.18960145  1.18551986 -1.06648333  0.60271522  1.14432445
  1.31029886 -1.8735468   0.72965068  2.88578419 -0.54534347 -0.47522485
 -2.02654253  0.7289737   1.60512664  2.83077083 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2472.1749567985535 seconds. 
Discarding model... 

Training complete taking 62304.156970500946 total seconds. 
Now scoring model... 
Scoring complete taking 3.2437970638275146 seconds. 
Saved predicted values as M-A2-CZ_Full-CRX_predicted_values.csv
Model scores: {'MSE_train': (0.16198587233038672,), 'R2_train': 0.6774479938289837, 'MAE_train': 0.3436058689325775, 'MSE_test': 0.17765403957985074, 'R2_test': 0.6686814428906214, 'MAE_test': 0.37889609969158167}. 
Saved model results as M-A2-CZ_Full-CRX_results.json. 
