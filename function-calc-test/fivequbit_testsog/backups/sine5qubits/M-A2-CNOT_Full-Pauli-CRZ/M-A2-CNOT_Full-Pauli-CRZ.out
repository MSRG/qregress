/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /scratch/j/jacobsen/gjones/sine5qubits/sine_train.bin... 
Successfully loaded /scratch/j/jacobsen/gjones/sine5qubits/sine_train.bin into X and y data. 
Loading dataset from /scratch/j/jacobsen/gjones/sine5qubits/sine_test.bin... 
Successfully loaded /scratch/j/jacobsen/gjones/sine5qubits/sine_test.bin into X and y data. 
Training model with dataset /scratch/j/jacobsen/gjones/sine5qubits/sine_train.bin 
 at time Thu Apr  4 21:37:45 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 21:38:08 2024]  Iteration number: 0 with current cost as 0.34261418806780203 and parameters 
[-3.27124379  2.32906518 -2.11979319 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648308  0.61934035  1.14432445
  1.73196717 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 21:46:24 2024]  Iteration number: 0 with current cost as 0.34827429435743423 and parameters 
[-3.2067587   2.34766468 -2.1184411  -0.11653103  0.55388707 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.62522085  1.14432445
  1.65521803 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 21:53:41 2024]  Iteration number: 0 with current cost as 0.35138092191832193 and parameters 
[-3.22557825  2.36128101 -2.11780714 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.62738133  1.14432445
  1.67549642 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 22:00:41 2024]  Iteration number: 0 with current cost as 0.33077656297476343 and parameters 
[-3.21690164  2.31542056 -2.1263762  -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.62458158  1.14432446
  1.67380471 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 22:08:50 2024]  Iteration number: 0 with current cost as 0.33553618347795183 and parameters 
[-3.25782008  2.34082069 -2.11765865 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.61781376  1.14432445
  1.71275789 -1.87354681  0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.7289737   1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Training complete taking 2344.7626004219055 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 22:17:15 2024]  Iteration number: 0 with current cost as 0.34261418806780203 and parameters 
[-3.27124379  2.32906518 -2.11979319 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648308  0.61934035  1.14432445
  1.73196717 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 22:25:23 2024]  Iteration number: 0 with current cost as 0.34827429435743423 and parameters 
[-3.2067587   2.34766468 -2.1184411  -0.11653103  0.55388707 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.62522085  1.14432445
  1.65521803 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 22:32:36 2024]  Iteration number: 0 with current cost as 0.35138092191832193 and parameters 
[-3.22557825  2.36128101 -2.11780714 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.62738133  1.14432445
  1.67549642 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 22:39:31 2024]  Iteration number: 0 with current cost as 0.33077656297476343 and parameters 
[-3.21690164  2.31542056 -2.1263762  -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.62458158  1.14432446
  1.67380471 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 22:47:53 2024]  Iteration number: 0 with current cost as 0.33553618347795183 and parameters 
[-3.25782008  2.34082069 -2.11765865 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.61781376  1.14432445
  1.71275789 -1.87354681  0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.7289737   1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Training complete taking 2338.606313943863 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 22:56:09 2024]  Iteration number: 0 with current cost as 0.34261418806780203 and parameters 
[-3.27124379  2.32906518 -2.11979319 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648308  0.61934035  1.14432445
  1.73196717 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 23:04:20 2024]  Iteration number: 0 with current cost as 0.34827429435743423 and parameters 
[-3.2067587   2.34766468 -2.1184411  -0.11653103  0.55388707 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.62522085  1.14432445
  1.65521803 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 23:11:48 2024]  Iteration number: 0 with current cost as 0.35138092191832193 and parameters 
[-3.22557825  2.36128101 -2.11780714 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.62738133  1.14432445
  1.67549642 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 23:18:45 2024]  Iteration number: 0 with current cost as 0.33077656297476343 and parameters 
[-3.21690164  2.31542056 -2.1263762  -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.62458158  1.14432446
  1.67380471 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 23:26:48 2024]  Iteration number: 0 with current cost as 0.33553618347795183 and parameters 
[-3.25782008  2.34082069 -2.11765865 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.61781376  1.14432445
  1.71275789 -1.87354681  0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.7289737   1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Training complete taking 2336.2888791561127 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 23:35:04 2024]  Iteration number: 0 with current cost as 0.34261418806780203 and parameters 
[-3.27124379  2.32906518 -2.11979319 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648308  0.61934035  1.14432445
  1.73196717 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 23:43:13 2024]  Iteration number: 0 with current cost as 0.34827429435743423 and parameters 
[-3.2067587   2.34766468 -2.1184411  -0.11653103  0.55388707 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.62522085  1.14432445
  1.65521803 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 23:50:27 2024]  Iteration number: 0 with current cost as 0.35138092191832193 and parameters 
[-3.22557825  2.36128101 -2.11780714 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.62738133  1.14432445
  1.67549642 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 23:57:26 2024]  Iteration number: 0 with current cost as 0.33077656297476343 and parameters 
[-3.21690164  2.31542056 -2.1263762  -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.62458158  1.14432446
  1.67380471 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 00:05:30 2024]  Iteration number: 0 with current cost as 0.33553618347795183 and parameters 
[-3.25782008  2.34082069 -2.11765865 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.61781376  1.14432445
  1.71275789 -1.87354681  0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.7289737   1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Training complete taking 2319.8738746643066 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 00:13:46 2024]  Iteration number: 0 with current cost as 0.34261418806780203 and parameters 
[-3.27124379  2.32906518 -2.11979319 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648308  0.61934035  1.14432445
  1.73196717 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Fri Apr  5 00:21:56 2024]  Iteration number: 0 with current cost as 0.34827429435743423 and parameters 
[-3.2067587   2.34766468 -2.1184411  -0.11653103  0.55388707 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.62522085  1.14432445
  1.65521803 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Fri Apr  5 00:29:09 2024]  Iteration number: 0 with current cost as 0.35138092191832193 and parameters 
[-3.22557825  2.36128101 -2.11780714 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.62738133  1.14432445
  1.67549642 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Fri Apr  5 00:36:05 2024]  Iteration number: 0 with current cost as 0.33077656297476343 and parameters 
[-3.21690164  2.31542056 -2.1263762  -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.62458158  1.14432446
  1.67380471 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 00:44:10 2024]  Iteration number: 0 with current cost as 0.33553618347795183 and parameters 
[-3.25782008  2.34082069 -2.11765865 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.61781376  1.14432445
  1.71275789 -1.87354681  0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.7289737   1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Training complete taking 2319.258575439453 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 00:52:26 2024]  Iteration number: 0 with current cost as 0.34261418806780203 and parameters 
[-3.27124379  2.32906518 -2.11979319 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648308  0.61934035  1.14432445
  1.73196717 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Fri Apr  5 01:00:34 2024]  Iteration number: 0 with current cost as 0.34827429435743423 and parameters 
[-3.2067587   2.34766468 -2.1184411  -0.11653103  0.55388707 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.62522085  1.14432445
  1.65521803 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Fri Apr  5 01:08:18 2024]  Iteration number: 0 with current cost as 0.35138092191832193 and parameters 
[-3.22557825  2.36128101 -2.11780714 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.62738133  1.14432445
  1.67549642 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Fri Apr  5 01:15:40 2024]  Iteration number: 0 with current cost as 0.33077656297476343 and parameters 
[-3.21690164  2.31542056 -2.1263762  -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.62458158  1.14432446
  1.67380471 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 01:24:14 2024]  Iteration number: 0 with current cost as 0.33553618347795183 and parameters 
[-3.25782008  2.34082069 -2.11765865 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.61781376  1.14432445
  1.71275789 -1.87354681  0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.7289737   1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Training complete taking 2422.661812067032 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 01:32:46 2024]  Iteration number: 0 with current cost as 0.34261418806780203 and parameters 
[-3.27124379  2.32906518 -2.11979319 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648308  0.61934035  1.14432445
  1.73196717 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Fri Apr  5 01:41:01 2024]  Iteration number: 0 with current cost as 0.34827429435743423 and parameters 
[-3.2067587   2.34766468 -2.1184411  -0.11653103  0.55388707 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.62522085  1.14432445
  1.65521803 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Fri Apr  5 01:48:18 2024]  Iteration number: 0 with current cost as 0.35138092191832193 and parameters 
[-3.22557825  2.36128101 -2.11780714 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.62738133  1.14432445
  1.67549642 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Fri Apr  5 01:55:15 2024]  Iteration number: 0 with current cost as 0.33077656297476343 and parameters 
[-3.21690164  2.31542056 -2.1263762  -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.62458158  1.14432446
  1.67380471 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 02:03:23 2024]  Iteration number: 0 with current cost as 0.33553618347795183 and parameters 
[-3.25782008  2.34082069 -2.11765865 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.61781376  1.14432445
  1.71275789 -1.87354681  0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.7289737   1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Training complete taking 2340.805932998657 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 02:11:47 2024]  Iteration number: 0 with current cost as 0.34261418806780203 and parameters 
[-3.27124379  2.32906518 -2.11979319 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648308  0.61934035  1.14432445
  1.73196717 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Fri Apr  5 02:20:00 2024]  Iteration number: 0 with current cost as 0.34827429435743423 and parameters 
[-3.2067587   2.34766468 -2.1184411  -0.11653103  0.55388707 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.62522085  1.14432445
  1.65521803 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Fri Apr  5 02:27:28 2024]  Iteration number: 0 with current cost as 0.35138092191832193 and parameters 
[-3.22557825  2.36128101 -2.11780714 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.62738133  1.14432445
  1.67549642 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Fri Apr  5 02:34:29 2024]  Iteration number: 0 with current cost as 0.33077656297476343 and parameters 
[-3.21690164  2.31542056 -2.1263762  -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.62458158  1.14432446
  1.67380471 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 02:42:55 2024]  Iteration number: 0 with current cost as 0.33553618347795183 and parameters 
[-3.25782008  2.34082069 -2.11765865 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.61781376  1.14432445
  1.71275789 -1.87354681  0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.7289737   1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Training complete taking 2405.345742702484 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 02:51:53 2024]  Iteration number: 0 with current cost as 0.34261418806780203 and parameters 
[-3.27124379  2.32906518 -2.11979319 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648308  0.61934035  1.14432445
  1.73196717 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Fri Apr  5 03:00:00 2024]  Iteration number: 0 with current cost as 0.34827429435743423 and parameters 
[-3.2067587   2.34766468 -2.1184411  -0.11653103  0.55388707 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.62522085  1.14432445
  1.65521803 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Fri Apr  5 03:07:17 2024]  Iteration number: 0 with current cost as 0.35138092191832193 and parameters 
[-3.22557825  2.36128101 -2.11780714 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.62738133  1.14432445
  1.67549642 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Fri Apr  5 03:14:14 2024]  Iteration number: 0 with current cost as 0.33077656297476343 and parameters 
[-3.21690164  2.31542056 -2.1263762  -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.62458158  1.14432446
  1.67380471 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 03:22:25 2024]  Iteration number: 0 with current cost as 0.33553618347795183 and parameters 
[-3.25782008  2.34082069 -2.11765865 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.61781376  1.14432445
  1.71275789 -1.87354681  0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.7289737   1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Training complete taking 2331.180830717087 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 03:30:46 2024]  Iteration number: 0 with current cost as 0.34261418806780203 and parameters 
[-3.27124379  2.32906518 -2.11979319 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648308  0.61934035  1.14432445
  1.73196717 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Fri Apr  5 03:38:54 2024]  Iteration number: 0 with current cost as 0.34827429435743423 and parameters 
[-3.2067587   2.34766468 -2.1184411  -0.11653103  0.55388707 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.62522085  1.14432445
  1.65521803 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Fri Apr  5 03:46:11 2024]  Iteration number: 0 with current cost as 0.35138092191832193 and parameters 
[-3.22557825  2.36128101 -2.11780714 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.62738133  1.14432445
  1.67549642 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Fri Apr  5 03:53:04 2024]  Iteration number: 0 with current cost as 0.33077656297476343 and parameters 
[-3.21690164  2.31542056 -2.1263762  -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.62458158  1.14432446
  1.67380471 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 04:01:11 2024]  Iteration number: 0 with current cost as 0.33553618347795183 and parameters 
[-3.25782008  2.34082069 -2.11765865 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.61781376  1.14432445
  1.71275789 -1.87354681  0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.7289737   1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Training complete taking 2323.2564857006073 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 04:09:27 2024]  Iteration number: 0 with current cost as 0.34261418806780203 and parameters 
[-3.27124379  2.32906518 -2.11979319 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648308  0.61934035  1.14432445
  1.73196717 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Fri Apr  5 04:17:37 2024]  Iteration number: 0 with current cost as 0.34827429435743423 and parameters 
[-3.2067587   2.34766468 -2.1184411  -0.11653103  0.55388707 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.62522085  1.14432445
  1.65521803 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Fri Apr  5 04:24:51 2024]  Iteration number: 0 with current cost as 0.35138092191832193 and parameters 
[-3.22557825  2.36128101 -2.11780714 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.62738133  1.14432445
  1.67549642 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Fri Apr  5 04:31:45 2024]  Iteration number: 0 with current cost as 0.33077656297476343 and parameters 
[-3.21690164  2.31542056 -2.1263762  -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.62458158  1.14432446
  1.67380471 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 04:40:03 2024]  Iteration number: 0 with current cost as 0.33553618347795183 and parameters 
[-3.25782008  2.34082069 -2.11765865 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.61781376  1.14432445
  1.71275789 -1.87354681  0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.7289737   1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Training complete taking 2341.0311591625214 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 04:48:28 2024]  Iteration number: 0 with current cost as 0.34261418806780203 and parameters 
[-3.27124379  2.32906518 -2.11979319 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648308  0.61934035  1.14432445
  1.73196717 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Fri Apr  5 04:56:36 2024]  Iteration number: 0 with current cost as 0.34827429435743423 and parameters 
[-3.2067587   2.34766468 -2.1184411  -0.11653103  0.55388707 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.62522085  1.14432445
  1.65521803 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Fri Apr  5 05:03:52 2024]  Iteration number: 0 with current cost as 0.35138092191832193 and parameters 
[-3.22557825  2.36128101 -2.11780714 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.62738133  1.14432445
  1.67549642 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Fri Apr  5 05:10:46 2024]  Iteration number: 0 with current cost as 0.33077656297476343 and parameters 
[-3.21690164  2.31542056 -2.1263762  -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.62458158  1.14432446
  1.67380471 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 05:18:54 2024]  Iteration number: 0 with current cost as 0.33553618347795183 and parameters 
[-3.25782008  2.34082069 -2.11765865 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.61781376  1.14432445
  1.71275789 -1.87354681  0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.7289737   1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Training complete taking 2324.746500968933 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 05:27:14 2024]  Iteration number: 0 with current cost as 0.34261418806780203 and parameters 
[-3.27124379  2.32906518 -2.11979319 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648308  0.61934035  1.14432445
  1.73196717 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Fri Apr  5 05:35:19 2024]  Iteration number: 0 with current cost as 0.34827429435743423 and parameters 
[-3.2067587   2.34766468 -2.1184411  -0.11653103  0.55388707 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.62522085  1.14432445
  1.65521803 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Fri Apr  5 05:42:36 2024]  Iteration number: 0 with current cost as 0.35138092191832193 and parameters 
[-3.22557825  2.36128101 -2.11780714 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.62738133  1.14432445
  1.67549642 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Fri Apr  5 05:49:29 2024]  Iteration number: 0 with current cost as 0.33077656297476343 and parameters 
[-3.21690164  2.31542056 -2.1263762  -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.62458158  1.14432446
  1.67380471 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 05:57:39 2024]  Iteration number: 0 with current cost as 0.33553618347795183 and parameters 
[-3.25782008  2.34082069 -2.11765865 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.61781376  1.14432445
  1.71275789 -1.87354681  0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.7289737   1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Training complete taking 2324.1580197811127 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 06:05:57 2024]  Iteration number: 0 with current cost as 0.34261418806780203 and parameters 
[-3.27124379  2.32906518 -2.11979319 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648308  0.61934035  1.14432445
  1.73196717 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Fri Apr  5 06:14:13 2024]  Iteration number: 0 with current cost as 0.34827429435743423 and parameters 
[-3.2067587   2.34766468 -2.1184411  -0.11653103  0.55388707 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.62522085  1.14432445
  1.65521803 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Fri Apr  5 06:21:28 2024]  Iteration number: 0 with current cost as 0.35138092191832193 and parameters 
[-3.22557825  2.36128101 -2.11780714 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.62738133  1.14432445
  1.67549642 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Fri Apr  5 06:28:57 2024]  Iteration number: 0 with current cost as 0.33077656297476343 and parameters 
[-3.21690164  2.31542056 -2.1263762  -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.62458158  1.14432446
  1.67380471 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 06:37:06 2024]  Iteration number: 0 with current cost as 0.33553618347795183 and parameters 
[-3.25782008  2.34082069 -2.11765865 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.61781376  1.14432445
  1.71275789 -1.87354681  0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.7289737   1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Training complete taking 2382.5294151306152 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 06:45:41 2024]  Iteration number: 0 with current cost as 0.34261418806780203 and parameters 
[-3.27124379  2.32906518 -2.11979319 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648308  0.61934035  1.14432445
  1.73196717 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Fri Apr  5 06:53:50 2024]  Iteration number: 0 with current cost as 0.34827429435743423 and parameters 
[-3.2067587   2.34766468 -2.1184411  -0.11653103  0.55388707 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.62522085  1.14432445
  1.65521803 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Fri Apr  5 07:01:04 2024]  Iteration number: 0 with current cost as 0.35138092191832193 and parameters 
[-3.22557825  2.36128101 -2.11780714 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.62738133  1.14432445
  1.67549642 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Fri Apr  5 07:07:57 2024]  Iteration number: 0 with current cost as 0.33077656297476343 and parameters 
[-3.21690164  2.31542056 -2.1263762  -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.62458158  1.14432446
  1.67380471 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 07:16:03 2024]  Iteration number: 0 with current cost as 0.33553618347795183 and parameters 
[-3.25782008  2.34082069 -2.11765865 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.61781376  1.14432445
  1.71275789 -1.87354681  0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.7289737   1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Training complete taking 2332.7855656147003 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 07:24:32 2024]  Iteration number: 0 with current cost as 0.34261418806780203 and parameters 
[-3.27124379  2.32906518 -2.11979319 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648308  0.61934035  1.14432445
  1.73196717 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Fri Apr  5 07:32:53 2024]  Iteration number: 0 with current cost as 0.34827429435743423 and parameters 
[-3.2067587   2.34766468 -2.1184411  -0.11653103  0.55388707 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.62522085  1.14432445
  1.65521803 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Fri Apr  5 07:40:11 2024]  Iteration number: 0 with current cost as 0.35138092191832193 and parameters 
[-3.22557825  2.36128101 -2.11780714 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.62738133  1.14432445
  1.67549642 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Fri Apr  5 07:47:04 2024]  Iteration number: 0 with current cost as 0.33077656297476343 and parameters 
[-3.21690164  2.31542056 -2.1263762  -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.62458158  1.14432446
  1.67380471 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 07:55:23 2024]  Iteration number: 0 with current cost as 0.33553618347795183 and parameters 
[-3.25782008  2.34082069 -2.11765865 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.61781376  1.14432445
  1.71275789 -1.87354681  0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.7289737   1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Training complete taking 2355.7626054286957 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 08:03:50 2024]  Iteration number: 0 with current cost as 0.34261418806780203 and parameters 
[-3.27124379  2.32906518 -2.11979319 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648308  0.61934035  1.14432445
  1.73196717 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Fri Apr  5 08:11:59 2024]  Iteration number: 0 with current cost as 0.34827429435743423 and parameters 
[-3.2067587   2.34766468 -2.1184411  -0.11653103  0.55388707 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.62522085  1.14432445
  1.65521803 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Fri Apr  5 08:19:16 2024]  Iteration number: 0 with current cost as 0.35138092191832193 and parameters 
[-3.22557825  2.36128101 -2.11780714 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.62738133  1.14432445
  1.67549642 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Fri Apr  5 08:26:10 2024]  Iteration number: 0 with current cost as 0.33077656297476343 and parameters 
[-3.21690164  2.31542056 -2.1263762  -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.62458158  1.14432446
  1.67380471 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 08:34:18 2024]  Iteration number: 0 with current cost as 0.33553618347795183 and parameters 
[-3.25782008  2.34082069 -2.11765865 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.61781376  1.14432445
  1.71275789 -1.87354681  0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.7289737   1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Training complete taking 2322.2610585689545 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 08:42:32 2024]  Iteration number: 0 with current cost as 0.34261418806780203 and parameters 
[-3.27124379  2.32906518 -2.11979319 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648308  0.61934035  1.14432445
  1.73196717 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Fri Apr  5 08:50:38 2024]  Iteration number: 0 with current cost as 0.34827429435743423 and parameters 
[-3.2067587   2.34766468 -2.1184411  -0.11653103  0.55388707 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.62522085  1.14432445
  1.65521803 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Fri Apr  5 08:57:55 2024]  Iteration number: 0 with current cost as 0.35138092191832193 and parameters 
[-3.22557825  2.36128101 -2.11780714 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.62738133  1.14432445
  1.67549642 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Fri Apr  5 09:04:51 2024]  Iteration number: 0 with current cost as 0.33077656297476343 and parameters 
[-3.21690164  2.31542056 -2.1263762  -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.62458158  1.14432446
  1.67380471 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 09:12:59 2024]  Iteration number: 0 with current cost as 0.33553618347795183 and parameters 
[-3.25782008  2.34082069 -2.11765865 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.61781376  1.14432445
  1.71275789 -1.87354681  0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.7289737   1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Training complete taking 2324.145507335663 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 09:21:14 2024]  Iteration number: 0 with current cost as 0.34261418806780203 and parameters 
[-3.27124379  2.32906518 -2.11979319 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648308  0.61934035  1.14432445
  1.73196717 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Fri Apr  5 09:29:21 2024]  Iteration number: 0 with current cost as 0.34827429435743423 and parameters 
[-3.2067587   2.34766468 -2.1184411  -0.11653103  0.55388707 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.62522085  1.14432445
  1.65521803 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Fri Apr  5 09:36:36 2024]  Iteration number: 0 with current cost as 0.35138092191832193 and parameters 
[-3.22557825  2.36128101 -2.11780714 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.62738133  1.14432445
  1.67549642 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Fri Apr  5 09:43:33 2024]  Iteration number: 0 with current cost as 0.33077656297476343 and parameters 
[-3.21690164  2.31542056 -2.1263762  -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.62458158  1.14432446
  1.67380471 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 09:51:40 2024]  Iteration number: 0 with current cost as 0.33553618347795183 and parameters 
[-3.25782008  2.34082069 -2.11765865 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.61781376  1.14432445
  1.71275789 -1.87354681  0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.7289737   1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Training complete taking 2320.6954379081726 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 09:59:57 2024]  Iteration number: 0 with current cost as 0.34261418806780203 and parameters 
[-3.27124379  2.32906518 -2.11979319 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648308  0.61934035  1.14432445
  1.73196717 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Fri Apr  5 10:08:05 2024]  Iteration number: 0 with current cost as 0.34827429435743423 and parameters 
[-3.2067587   2.34766468 -2.1184411  -0.11653103  0.55388707 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.62522085  1.14432445
  1.65521803 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Fri Apr  5 10:15:27 2024]  Iteration number: 0 with current cost as 0.35138092191832193 and parameters 
[-3.22557825  2.36128101 -2.11780714 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.62738133  1.14432445
  1.67549642 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Fri Apr  5 10:22:28 2024]  Iteration number: 0 with current cost as 0.33077656297476343 and parameters 
[-3.21690164  2.31542056 -2.1263762  -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.62458158  1.14432446
  1.67380471 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 10:30:37 2024]  Iteration number: 0 with current cost as 0.33553618347795183 and parameters 
[-3.25782008  2.34082069 -2.11765865 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.61781376  1.14432445
  1.71275789 -1.87354681  0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.7289737   1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Training complete taking 2338.937646627426 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 10:38:56 2024]  Iteration number: 0 with current cost as 0.34261418806780203 and parameters 
[-3.27124379  2.32906518 -2.11979319 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648308  0.61934035  1.14432445
  1.73196717 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Fri Apr  5 10:47:46 2024]  Iteration number: 0 with current cost as 0.34827429435743423 and parameters 
[-3.2067587   2.34766468 -2.1184411  -0.11653103  0.55388707 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.62522085  1.14432445
  1.65521803 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Fri Apr  5 10:55:08 2024]  Iteration number: 0 with current cost as 0.35138092191832193 and parameters 
[-3.22557825  2.36128101 -2.11780714 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.62738133  1.14432445
  1.67549642 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Fri Apr  5 11:02:10 2024]  Iteration number: 0 with current cost as 0.33077656297476343 and parameters 
[-3.21690164  2.31542056 -2.1263762  -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.62458158  1.14432446
  1.67380471 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 11:10:18 2024]  Iteration number: 0 with current cost as 0.33553618347795183 and parameters 
[-3.25782008  2.34082069 -2.11765865 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.61781376  1.14432445
  1.71275789 -1.87354681  0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.7289737   1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Training complete taking 2383.125400543213 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 11:18:39 2024]  Iteration number: 0 with current cost as 0.34261418806780203 and parameters 
[-3.27124379  2.32906518 -2.11979319 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648308  0.61934035  1.14432445
  1.73196717 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Fri Apr  5 11:26:45 2024]  Iteration number: 0 with current cost as 0.34827429435743423 and parameters 
[-3.2067587   2.34766468 -2.1184411  -0.11653103  0.55388707 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.62522085  1.14432445
  1.65521803 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Fri Apr  5 11:34:02 2024]  Iteration number: 0 with current cost as 0.35138092191832193 and parameters 
[-3.22557825  2.36128101 -2.11780714 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.62738133  1.14432445
  1.67549642 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Fri Apr  5 11:40:54 2024]  Iteration number: 0 with current cost as 0.33077656297476343 and parameters 
[-3.21690164  2.31542056 -2.1263762  -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.62458158  1.14432446
  1.67380471 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 11:49:01 2024]  Iteration number: 0 with current cost as 0.33553618347795183 and parameters 
[-3.25782008  2.34082069 -2.11765865 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.61781376  1.14432445
  1.71275789 -1.87354681  0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.7289737   1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Training complete taking 2335.8292179107666 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 11:57:35 2024]  Iteration number: 0 with current cost as 0.34261418806780203 and parameters 
[-3.27124379  2.32906518 -2.11979319 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648308  0.61934035  1.14432445
  1.73196717 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Fri Apr  5 12:05:43 2024]  Iteration number: 0 with current cost as 0.34827429435743423 and parameters 
[-3.2067587   2.34766468 -2.1184411  -0.11653103  0.55388707 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.62522085  1.14432445
  1.65521803 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Fri Apr  5 12:13:00 2024]  Iteration number: 0 with current cost as 0.35138092191832193 and parameters 
[-3.22557825  2.36128101 -2.11780714 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.62738133  1.14432445
  1.67549642 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Fri Apr  5 12:19:52 2024]  Iteration number: 0 with current cost as 0.33077656297476343 and parameters 
[-3.21690164  2.31542056 -2.1263762  -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.62458158  1.14432446
  1.67380471 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 12:27:57 2024]  Iteration number: 0 with current cost as 0.33553618347795183 and parameters 
[-3.25782008  2.34082069 -2.11765865 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.61781376  1.14432445
  1.71275789 -1.87354681  0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.7289737   1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Training complete taking 2318.8862657546997 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 12:36:13 2024]  Iteration number: 0 with current cost as 0.34261418806780203 and parameters 
[-3.27124379  2.32906518 -2.11979319 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648308  0.61934035  1.14432445
  1.73196717 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Fri Apr  5 12:44:20 2024]  Iteration number: 0 with current cost as 0.34827429435743423 and parameters 
[-3.2067587   2.34766468 -2.1184411  -0.11653103  0.55388707 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.62522085  1.14432445
  1.65521803 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Fri Apr  5 12:51:37 2024]  Iteration number: 0 with current cost as 0.35138092191832193 and parameters 
[-3.22557825  2.36128101 -2.11780714 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.62738133  1.14432445
  1.67549642 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Fri Apr  5 12:58:29 2024]  Iteration number: 0 with current cost as 0.33077656297476343 and parameters 
[-3.21690164  2.31542056 -2.1263762  -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.62458158  1.14432446
  1.67380471 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 13:06:39 2024]  Iteration number: 0 with current cost as 0.33553618347795183 and parameters 
[-3.25782008  2.34082069 -2.11765865 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.61781376  1.14432445
  1.71275789 -1.87354681  0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.7289737   1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Training complete taking 2322.716307401657 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 13:14:54 2024]  Iteration number: 0 with current cost as 0.34261418806780203 and parameters 
[-3.27124379  2.32906518 -2.11979319 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648308  0.61934035  1.14432445
  1.73196717 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Fri Apr  5 13:23:00 2024]  Iteration number: 0 with current cost as 0.34827429435743423 and parameters 
[-3.2067587   2.34766468 -2.1184411  -0.11653103  0.55388707 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.62522085  1.14432445
  1.65521803 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Fri Apr  5 13:30:16 2024]  Iteration number: 0 with current cost as 0.35138092191832193 and parameters 
[-3.22557825  2.36128101 -2.11780714 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.62738133  1.14432445
  1.67549642 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Fri Apr  5 13:37:12 2024]  Iteration number: 0 with current cost as 0.33077656297476343 and parameters 
[-3.21690164  2.31542056 -2.1263762  -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.62458158  1.14432446
  1.67380471 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 13:45:23 2024]  Iteration number: 0 with current cost as 0.33553618347795183 and parameters 
[-3.25782008  2.34082069 -2.11765865 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.61781376  1.14432445
  1.71275789 -1.87354681  0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.7289737   1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Training complete taking 2326.0692961215973 seconds. 
Discarding model... 

Training complete taking 58535.72243356705 total seconds. 
Now scoring model... 
Scoring complete taking 1.1183791160583496 seconds. 
Saved predicted values as M-A2-CNOT_Full-Pauli-CRZ_predicted_values.csv
Model scores: {'MSE_train': (0.29013306262877725,), 'R2_train': 0.4222767698124863, 'MAE_train': 0.45581594392389607, 'MSE_test': 0.35363241514823546, 'R2_test': 0.34048794043123554, 'MAE_test': 0.5184028140600493}. 
Saved model results as M-A2-CNOT_Full-Pauli-CRZ_results.json. 
