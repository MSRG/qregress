/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /scratch/j/jacobsen/gjones/sine5qubits/sine_train.bin... 
Successfully loaded /scratch/j/jacobsen/gjones/sine5qubits/sine_train.bin into X and y data. 
Loading dataset from /scratch/j/jacobsen/gjones/sine5qubits/sine_test.bin... 
Successfully loaded /scratch/j/jacobsen/gjones/sine5qubits/sine_test.bin into X and y data. 
Training model with dataset /scratch/j/jacobsen/gjones/sine5qubits/sine_train.bin 
 at time Thu Apr  4 21:00:33 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 21:05:00 2024]  Iteration number: 0 with current cost as 0.3085586392976748 and parameters 
[-4.44578292  2.23743464 -2.12427913 -0.11653103  0.55388708 -2.77010948
  3.06858473  2.18960145  1.18551998 -1.06648359  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965055  2.88578394 -0.5453436  -0.47522485
 -2.02654266  0.72897344  1.60512639  2.83077057 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337105  2.54856958 -0.67550812 -2.69002227]. 
Working on 0.4 fold... 
[Thu Apr  4 21:10:43 2024]  Iteration number: 0 with current cost as 0.3205518678191448 and parameters 
[-4.32247437  2.23743481 -2.12427912 -0.11653085  0.55388725 -2.7701088
  3.06858516  2.18960162  1.18552016 -1.06648291  0.60271545  1.14432462
  1.31029916 -1.87354663  0.7296508   2.88578437 -0.54534352 -0.47522451
 -2.0265424   0.72897387  1.60512664  2.8307709  -1.26456692 -0.25136087
 -2.39279201 -2.27309757  3.13337155  2.54856976 -0.67550787 -2.69002184]. 
Working on 0.6 fold... 
[Thu Apr  4 21:17:38 2024]  Iteration number: 0 with current cost as 0.34250054921095563 and parameters 
[-4.33926417  2.23743464 -2.12427909 -0.11653048  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648336  0.60271565  1.14432445
  1.31029899 -1.8735468   0.72965025  2.88578419 -0.54534335 -0.47522485
 -2.02654268  0.7289737   1.60512664  2.83077052 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 21:23:56 2024]  Iteration number: 0 with current cost as 0.2764559454460585 and parameters 
[-4.45008135  2.23743475 -2.12427941 -0.11653103  0.55388697 -2.77010909
  3.06858498  2.18960145  1.18551998 -1.06648331  0.60271522  1.14432445
  1.31029887 -1.87354691  0.72965058  2.88578408 -0.54534346 -0.47522485
 -2.02654263  0.72897358  1.60512652  2.83077084 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 21:30:12 2024]  Iteration number: 0 with current cost as 0.34884890222912857 and parameters 
[-7.58077526  2.23743464 -2.12427901 -0.1165304   0.5538877  -2.77010959
  3.06858436  2.18960207  1.18552061 -1.06648371  0.60271635  1.14432445
  1.31029899 -1.87354618  0.72965018  2.88578419 -0.54534397 -0.47522423
 -2.02654303  0.72897307  1.60512664  2.83077045 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1892.872269153595 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 21:36:29 2024]  Iteration number: 0 with current cost as 0.3085586392976748 and parameters 
[-4.44578292  2.23743464 -2.12427913 -0.11653103  0.55388708 -2.77010948
  3.06858473  2.18960145  1.18551998 -1.06648359  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965055  2.88578394 -0.5453436  -0.47522485
 -2.02654266  0.72897344  1.60512639  2.83077057 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337105  2.54856958 -0.67550812 -2.69002227]. 
Working on 0.4 fold... 
[Thu Apr  4 21:42:07 2024]  Iteration number: 0 with current cost as 0.3205518678191448 and parameters 
[-4.32247437  2.23743481 -2.12427912 -0.11653085  0.55388725 -2.7701088
  3.06858516  2.18960162  1.18552016 -1.06648291  0.60271545  1.14432462
  1.31029916 -1.87354663  0.7296508   2.88578437 -0.54534352 -0.47522451
 -2.0265424   0.72897387  1.60512664  2.8307709  -1.26456692 -0.25136087
 -2.39279201 -2.27309757  3.13337155  2.54856976 -0.67550787 -2.69002184]. 
Working on 0.6 fold... 
[Thu Apr  4 21:49:03 2024]  Iteration number: 0 with current cost as 0.34250054921095563 and parameters 
[-4.33926417  2.23743464 -2.12427909 -0.11653048  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648336  0.60271565  1.14432445
  1.31029899 -1.8735468   0.72965025  2.88578419 -0.54534335 -0.47522485
 -2.02654268  0.7289737   1.60512664  2.83077052 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 21:55:22 2024]  Iteration number: 0 with current cost as 0.2764559454460585 and parameters 
[-4.45008135  2.23743475 -2.12427941 -0.11653103  0.55388697 -2.77010909
  3.06858498  2.18960145  1.18551998 -1.06648331  0.60271522  1.14432445
  1.31029887 -1.87354691  0.72965058  2.88578408 -0.54534346 -0.47522485
 -2.02654263  0.72897358  1.60512652  2.83077084 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 22:01:40 2024]  Iteration number: 0 with current cost as 0.34884890222912857 and parameters 
[-7.58077526  2.23743464 -2.12427901 -0.1165304   0.5538877  -2.77010959
  3.06858436  2.18960207  1.18552061 -1.06648371  0.60271635  1.14432445
  1.31029899 -1.87354618  0.72965018  2.88578419 -0.54534397 -0.47522423
 -2.02654303  0.72897307  1.60512664  2.83077045 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1888.8812153339386 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 22:08:00 2024]  Iteration number: 0 with current cost as 0.3085586392976748 and parameters 
[-4.44578292  2.23743464 -2.12427913 -0.11653103  0.55388708 -2.77010948
  3.06858473  2.18960145  1.18551998 -1.06648359  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965055  2.88578394 -0.5453436  -0.47522485
 -2.02654266  0.72897344  1.60512639  2.83077057 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337105  2.54856958 -0.67550812 -2.69002227]. 
Working on 0.4 fold... 
[Thu Apr  4 22:13:40 2024]  Iteration number: 0 with current cost as 0.3205518678191448 and parameters 
[-4.32247437  2.23743481 -2.12427912 -0.11653085  0.55388725 -2.7701088
  3.06858516  2.18960162  1.18552016 -1.06648291  0.60271545  1.14432462
  1.31029916 -1.87354663  0.7296508   2.88578437 -0.54534352 -0.47522451
 -2.0265424   0.72897387  1.60512664  2.8307709  -1.26456692 -0.25136087
 -2.39279201 -2.27309757  3.13337155  2.54856976 -0.67550787 -2.69002184]. 
Working on 0.6 fold... 
[Thu Apr  4 22:20:36 2024]  Iteration number: 0 with current cost as 0.34250054921095563 and parameters 
[-4.33926417  2.23743464 -2.12427909 -0.11653048  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648336  0.60271565  1.14432445
  1.31029899 -1.8735468   0.72965025  2.88578419 -0.54534335 -0.47522485
 -2.02654268  0.7289737   1.60512664  2.83077052 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 22:26:54 2024]  Iteration number: 0 with current cost as 0.2764559454460585 and parameters 
[-4.45008135  2.23743475 -2.12427941 -0.11653103  0.55388697 -2.77010909
  3.06858498  2.18960145  1.18551998 -1.06648331  0.60271522  1.14432445
  1.31029887 -1.87354691  0.72965058  2.88578408 -0.54534346 -0.47522485
 -2.02654263  0.72897358  1.60512652  2.83077084 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 22:33:06 2024]  Iteration number: 0 with current cost as 0.34884890222912857 and parameters 
[-7.58077526  2.23743464 -2.12427901 -0.1165304   0.5538877  -2.77010959
  3.06858436  2.18960207  1.18552061 -1.06648371  0.60271635  1.14432445
  1.31029899 -1.87354618  0.72965018  2.88578419 -0.54534397 -0.47522423
 -2.02654303  0.72897307  1.60512664  2.83077045 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1884.1849873065948 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 22:39:15 2024]  Iteration number: 0 with current cost as 0.3085586392976748 and parameters 
[-4.44578292  2.23743464 -2.12427913 -0.11653103  0.55388708 -2.77010948
  3.06858473  2.18960145  1.18551998 -1.06648359  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965055  2.88578394 -0.5453436  -0.47522485
 -2.02654266  0.72897344  1.60512639  2.83077057 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337105  2.54856958 -0.67550812 -2.69002227]. 
Working on 0.4 fold... 
[Thu Apr  4 22:44:53 2024]  Iteration number: 0 with current cost as 0.3205518678191448 and parameters 
[-4.32247437  2.23743481 -2.12427912 -0.11653085  0.55388725 -2.7701088
  3.06858516  2.18960162  1.18552016 -1.06648291  0.60271545  1.14432462
  1.31029916 -1.87354663  0.7296508   2.88578437 -0.54534352 -0.47522451
 -2.0265424   0.72897387  1.60512664  2.8307709  -1.26456692 -0.25136087
 -2.39279201 -2.27309757  3.13337155  2.54856976 -0.67550787 -2.69002184]. 
Working on 0.6 fold... 
[Thu Apr  4 22:51:40 2024]  Iteration number: 0 with current cost as 0.34250054921095563 and parameters 
[-4.33926417  2.23743464 -2.12427909 -0.11653048  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648336  0.60271565  1.14432445
  1.31029899 -1.8735468   0.72965025  2.88578419 -0.54534335 -0.47522485
 -2.02654268  0.7289737   1.60512664  2.83077052 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 22:57:49 2024]  Iteration number: 0 with current cost as 0.2764559454460585 and parameters 
[-4.45008135  2.23743475 -2.12427941 -0.11653103  0.55388697 -2.77010909
  3.06858498  2.18960145  1.18551998 -1.06648331  0.60271522  1.14432445
  1.31029887 -1.87354691  0.72965058  2.88578408 -0.54534346 -0.47522485
 -2.02654263  0.72897358  1.60512652  2.83077084 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 23:03:59 2024]  Iteration number: 0 with current cost as 0.34884890222912857 and parameters 
[-7.58077526  2.23743464 -2.12427901 -0.1165304   0.5538877  -2.77010959
  3.06858436  2.18960207  1.18552061 -1.06648371  0.60271635  1.14432445
  1.31029899 -1.87354618  0.72965018  2.88578419 -0.54534397 -0.47522423
 -2.02654303  0.72897307  1.60512664  2.83077045 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1850.5987401008606 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 23:10:09 2024]  Iteration number: 0 with current cost as 0.3085586392976748 and parameters 
[-4.44578292  2.23743464 -2.12427913 -0.11653103  0.55388708 -2.77010948
  3.06858473  2.18960145  1.18551998 -1.06648359  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965055  2.88578394 -0.5453436  -0.47522485
 -2.02654266  0.72897344  1.60512639  2.83077057 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337105  2.54856958 -0.67550812 -2.69002227]. 
Working on 0.4 fold... 
[Thu Apr  4 23:15:45 2024]  Iteration number: 0 with current cost as 0.3205518678191448 and parameters 
[-4.32247437  2.23743481 -2.12427912 -0.11653085  0.55388725 -2.7701088
  3.06858516  2.18960162  1.18552016 -1.06648291  0.60271545  1.14432462
  1.31029916 -1.87354663  0.7296508   2.88578437 -0.54534352 -0.47522451
 -2.0265424   0.72897387  1.60512664  2.8307709  -1.26456692 -0.25136087
 -2.39279201 -2.27309757  3.13337155  2.54856976 -0.67550787 -2.69002184]. 
Working on 0.6 fold... 
[Thu Apr  4 23:22:35 2024]  Iteration number: 0 with current cost as 0.34250054921095563 and parameters 
[-4.33926417  2.23743464 -2.12427909 -0.11653048  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648336  0.60271565  1.14432445
  1.31029899 -1.8735468   0.72965025  2.88578419 -0.54534335 -0.47522485
 -2.02654268  0.7289737   1.60512664  2.83077052 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 23:28:44 2024]  Iteration number: 0 with current cost as 0.2764559454460585 and parameters 
[-4.45008135  2.23743475 -2.12427941 -0.11653103  0.55388697 -2.77010909
  3.06858498  2.18960145  1.18551998 -1.06648331  0.60271522  1.14432445
  1.31029887 -1.87354691  0.72965058  2.88578408 -0.54534346 -0.47522485
 -2.02654263  0.72897358  1.60512652  2.83077084 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 23:34:56 2024]  Iteration number: 0 with current cost as 0.34884890222912857 and parameters 
[-7.58077526  2.23743464 -2.12427901 -0.1165304   0.5538877  -2.77010959
  3.06858436  2.18960207  1.18552061 -1.06648371  0.60271635  1.14432445
  1.31029899 -1.87354618  0.72965018  2.88578419 -0.54534397 -0.47522423
 -2.02654303  0.72897307  1.60512664  2.83077045 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1858.144966840744 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 23:41:07 2024]  Iteration number: 0 with current cost as 0.3085586392976748 and parameters 
[-4.44578292  2.23743464 -2.12427913 -0.11653103  0.55388708 -2.77010948
  3.06858473  2.18960145  1.18551998 -1.06648359  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965055  2.88578394 -0.5453436  -0.47522485
 -2.02654266  0.72897344  1.60512639  2.83077057 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337105  2.54856958 -0.67550812 -2.69002227]. 
Working on 0.4 fold... 
[Thu Apr  4 23:46:40 2024]  Iteration number: 0 with current cost as 0.3205518678191448 and parameters 
[-4.32247437  2.23743481 -2.12427912 -0.11653085  0.55388725 -2.7701088
  3.06858516  2.18960162  1.18552016 -1.06648291  0.60271545  1.14432462
  1.31029916 -1.87354663  0.7296508   2.88578437 -0.54534352 -0.47522451
 -2.0265424   0.72897387  1.60512664  2.8307709  -1.26456692 -0.25136087
 -2.39279201 -2.27309757  3.13337155  2.54856976 -0.67550787 -2.69002184]. 
Working on 0.6 fold... 
[Thu Apr  4 23:53:27 2024]  Iteration number: 0 with current cost as 0.34250054921095563 and parameters 
[-4.33926417  2.23743464 -2.12427909 -0.11653048  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648336  0.60271565  1.14432445
  1.31029899 -1.8735468   0.72965025  2.88578419 -0.54534335 -0.47522485
 -2.02654268  0.7289737   1.60512664  2.83077052 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 23:59:40 2024]  Iteration number: 0 with current cost as 0.2764559454460585 and parameters 
[-4.45008135  2.23743475 -2.12427941 -0.11653103  0.55388697 -2.77010909
  3.06858498  2.18960145  1.18551998 -1.06648331  0.60271522  1.14432445
  1.31029887 -1.87354691  0.72965058  2.88578408 -0.54534346 -0.47522485
 -2.02654263  0.72897358  1.60512652  2.83077084 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 00:05:50 2024]  Iteration number: 0 with current cost as 0.34884890222912857 and parameters 
[-7.58077526  2.23743464 -2.12427901 -0.1165304   0.5538877  -2.77010959
  3.06858436  2.18960207  1.18552061 -1.06648371  0.60271635  1.14432445
  1.31029899 -1.87354618  0.72965018  2.88578419 -0.54534397 -0.47522423
 -2.02654303  0.72897307  1.60512664  2.83077045 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1856.431198835373 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 00:12:03 2024]  Iteration number: 0 with current cost as 0.3085586392976748 and parameters 
[-4.44578292  2.23743464 -2.12427913 -0.11653103  0.55388708 -2.77010948
  3.06858473  2.18960145  1.18551998 -1.06648359  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965055  2.88578394 -0.5453436  -0.47522485
 -2.02654266  0.72897344  1.60512639  2.83077057 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337105  2.54856958 -0.67550812 -2.69002227]. 
Working on 0.4 fold... 
[Fri Apr  5 00:17:34 2024]  Iteration number: 0 with current cost as 0.3205518678191448 and parameters 
[-4.32247437  2.23743481 -2.12427912 -0.11653085  0.55388725 -2.7701088
  3.06858516  2.18960162  1.18552016 -1.06648291  0.60271545  1.14432462
  1.31029916 -1.87354663  0.7296508   2.88578437 -0.54534352 -0.47522451
 -2.0265424   0.72897387  1.60512664  2.8307709  -1.26456692 -0.25136087
 -2.39279201 -2.27309757  3.13337155  2.54856976 -0.67550787 -2.69002184]. 
Working on 0.6 fold... 
[Fri Apr  5 00:24:25 2024]  Iteration number: 0 with current cost as 0.34250054921095563 and parameters 
[-4.33926417  2.23743464 -2.12427909 -0.11653048  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648336  0.60271565  1.14432445
  1.31029899 -1.8735468   0.72965025  2.88578419 -0.54534335 -0.47522485
 -2.02654268  0.7289737   1.60512664  2.83077052 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 00:30:37 2024]  Iteration number: 0 with current cost as 0.2764559454460585 and parameters 
[-4.45008135  2.23743475 -2.12427941 -0.11653103  0.55388697 -2.77010909
  3.06858498  2.18960145  1.18551998 -1.06648331  0.60271522  1.14432445
  1.31029887 -1.87354691  0.72965058  2.88578408 -0.54534346 -0.47522485
 -2.02654263  0.72897358  1.60512652  2.83077084 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 00:36:43 2024]  Iteration number: 0 with current cost as 0.34884890222912857 and parameters 
[-7.58077526  2.23743464 -2.12427901 -0.1165304   0.5538877  -2.77010959
  3.06858436  2.18960207  1.18552061 -1.06648371  0.60271635  1.14432445
  1.31029899 -1.87354618  0.72965018  2.88578419 -0.54534397 -0.47522423
 -2.02654303  0.72897307  1.60512664  2.83077045 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1851.6494390964508 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 00:42:57 2024]  Iteration number: 0 with current cost as 0.3085586392976748 and parameters 
[-4.44578292  2.23743464 -2.12427913 -0.11653103  0.55388708 -2.77010948
  3.06858473  2.18960145  1.18551998 -1.06648359  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965055  2.88578394 -0.5453436  -0.47522485
 -2.02654266  0.72897344  1.60512639  2.83077057 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337105  2.54856958 -0.67550812 -2.69002227]. 
Working on 0.4 fold... 
[Fri Apr  5 00:48:30 2024]  Iteration number: 0 with current cost as 0.3205518678191448 and parameters 
[-4.32247437  2.23743481 -2.12427912 -0.11653085  0.55388725 -2.7701088
  3.06858516  2.18960162  1.18552016 -1.06648291  0.60271545  1.14432462
  1.31029916 -1.87354663  0.7296508   2.88578437 -0.54534352 -0.47522451
 -2.0265424   0.72897387  1.60512664  2.8307709  -1.26456692 -0.25136087
 -2.39279201 -2.27309757  3.13337155  2.54856976 -0.67550787 -2.69002184]. 
Working on 0.6 fold... 
[Fri Apr  5 00:55:14 2024]  Iteration number: 0 with current cost as 0.34250054921095563 and parameters 
[-4.33926417  2.23743464 -2.12427909 -0.11653048  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648336  0.60271565  1.14432445
  1.31029899 -1.8735468   0.72965025  2.88578419 -0.54534335 -0.47522485
 -2.02654268  0.7289737   1.60512664  2.83077052 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 01:01:21 2024]  Iteration number: 0 with current cost as 0.2764559454460585 and parameters 
[-4.45008135  2.23743475 -2.12427941 -0.11653103  0.55388697 -2.77010909
  3.06858498  2.18960145  1.18551998 -1.06648331  0.60271522  1.14432445
  1.31029887 -1.87354691  0.72965058  2.88578408 -0.54534346 -0.47522485
 -2.02654263  0.72897358  1.60512652  2.83077084 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 01:07:34 2024]  Iteration number: 0 with current cost as 0.34884890222912857 and parameters 
[-7.58077526  2.23743464 -2.12427901 -0.1165304   0.5538877  -2.77010959
  3.06858436  2.18960207  1.18552061 -1.06648371  0.60271635  1.14432445
  1.31029899 -1.87354618  0.72965018  2.88578419 -0.54534397 -0.47522423
 -2.02654303  0.72897307  1.60512664  2.83077045 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1848.4451878070831 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 01:13:42 2024]  Iteration number: 0 with current cost as 0.3085586392976748 and parameters 
[-4.44578292  2.23743464 -2.12427913 -0.11653103  0.55388708 -2.77010948
  3.06858473  2.18960145  1.18551998 -1.06648359  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965055  2.88578394 -0.5453436  -0.47522485
 -2.02654266  0.72897344  1.60512639  2.83077057 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337105  2.54856958 -0.67550812 -2.69002227]. 
Working on 0.4 fold... 
[Fri Apr  5 01:19:16 2024]  Iteration number: 0 with current cost as 0.3205518678191448 and parameters 
[-4.32247437  2.23743481 -2.12427912 -0.11653085  0.55388725 -2.7701088
  3.06858516  2.18960162  1.18552016 -1.06648291  0.60271545  1.14432462
  1.31029916 -1.87354663  0.7296508   2.88578437 -0.54534352 -0.47522451
 -2.0265424   0.72897387  1.60512664  2.8307709  -1.26456692 -0.25136087
 -2.39279201 -2.27309757  3.13337155  2.54856976 -0.67550787 -2.69002184]. 
Working on 0.6 fold... 
[Fri Apr  5 01:26:03 2024]  Iteration number: 0 with current cost as 0.34250054921095563 and parameters 
[-4.33926417  2.23743464 -2.12427909 -0.11653048  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648336  0.60271565  1.14432445
  1.31029899 -1.8735468   0.72965025  2.88578419 -0.54534335 -0.47522485
 -2.02654268  0.7289737   1.60512664  2.83077052 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 01:32:12 2024]  Iteration number: 0 with current cost as 0.2764559454460585 and parameters 
[-4.45008135  2.23743475 -2.12427941 -0.11653103  0.55388697 -2.77010909
  3.06858498  2.18960145  1.18551998 -1.06648331  0.60271522  1.14432445
  1.31029887 -1.87354691  0.72965058  2.88578408 -0.54534346 -0.47522485
 -2.02654263  0.72897358  1.60512652  2.83077084 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 01:38:22 2024]  Iteration number: 0 with current cost as 0.34884890222912857 and parameters 
[-7.58077526  2.23743464 -2.12427901 -0.1165304   0.5538877  -2.77010959
  3.06858436  2.18960207  1.18552061 -1.06648371  0.60271635  1.14432445
  1.31029899 -1.87354618  0.72965018  2.88578419 -0.54534397 -0.47522423
 -2.02654303  0.72897307  1.60512664  2.83077045 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1848.6750943660736 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 01:44:31 2024]  Iteration number: 0 with current cost as 0.3085586392976748 and parameters 
[-4.44578292  2.23743464 -2.12427913 -0.11653103  0.55388708 -2.77010948
  3.06858473  2.18960145  1.18551998 -1.06648359  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965055  2.88578394 -0.5453436  -0.47522485
 -2.02654266  0.72897344  1.60512639  2.83077057 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337105  2.54856958 -0.67550812 -2.69002227]. 
Working on 0.4 fold... 
[Fri Apr  5 01:50:05 2024]  Iteration number: 0 with current cost as 0.3205518678191448 and parameters 
[-4.32247437  2.23743481 -2.12427912 -0.11653085  0.55388725 -2.7701088
  3.06858516  2.18960162  1.18552016 -1.06648291  0.60271545  1.14432462
  1.31029916 -1.87354663  0.7296508   2.88578437 -0.54534352 -0.47522451
 -2.0265424   0.72897387  1.60512664  2.8307709  -1.26456692 -0.25136087
 -2.39279201 -2.27309757  3.13337155  2.54856976 -0.67550787 -2.69002184]. 
Working on 0.6 fold... 
[Fri Apr  5 01:56:46 2024]  Iteration number: 0 with current cost as 0.34250054921095563 and parameters 
[-4.33926417  2.23743464 -2.12427909 -0.11653048  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648336  0.60271565  1.14432445
  1.31029899 -1.8735468   0.72965025  2.88578419 -0.54534335 -0.47522485
 -2.02654268  0.7289737   1.60512664  2.83077052 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 02:02:53 2024]  Iteration number: 0 with current cost as 0.2764559454460585 and parameters 
[-4.45008135  2.23743475 -2.12427941 -0.11653103  0.55388697 -2.77010909
  3.06858498  2.18960145  1.18551998 -1.06648331  0.60271522  1.14432445
  1.31029887 -1.87354691  0.72965058  2.88578408 -0.54534346 -0.47522485
 -2.02654263  0.72897358  1.60512652  2.83077084 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 02:08:59 2024]  Iteration number: 0 with current cost as 0.34884890222912857 and parameters 
[-7.58077526  2.23743464 -2.12427901 -0.1165304   0.5538877  -2.77010959
  3.06858436  2.18960207  1.18552061 -1.06648371  0.60271635  1.14432445
  1.31029899 -1.87354618  0.72965018  2.88578419 -0.54534397 -0.47522423
 -2.02654303  0.72897307  1.60512664  2.83077045 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1837.564189195633 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 02:15:09 2024]  Iteration number: 0 with current cost as 0.3085586392976748 and parameters 
[-4.44578292  2.23743464 -2.12427913 -0.11653103  0.55388708 -2.77010948
  3.06858473  2.18960145  1.18551998 -1.06648359  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965055  2.88578394 -0.5453436  -0.47522485
 -2.02654266  0.72897344  1.60512639  2.83077057 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337105  2.54856958 -0.67550812 -2.69002227]. 
Working on 0.4 fold... 
[Fri Apr  5 02:20:44 2024]  Iteration number: 0 with current cost as 0.3205518678191448 and parameters 
[-4.32247437  2.23743481 -2.12427912 -0.11653085  0.55388725 -2.7701088
  3.06858516  2.18960162  1.18552016 -1.06648291  0.60271545  1.14432462
  1.31029916 -1.87354663  0.7296508   2.88578437 -0.54534352 -0.47522451
 -2.0265424   0.72897387  1.60512664  2.8307709  -1.26456692 -0.25136087
 -2.39279201 -2.27309757  3.13337155  2.54856976 -0.67550787 -2.69002184]. 
Working on 0.6 fold... 
[Fri Apr  5 02:27:30 2024]  Iteration number: 0 with current cost as 0.34250054921095563 and parameters 
[-4.33926417  2.23743464 -2.12427909 -0.11653048  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648336  0.60271565  1.14432445
  1.31029899 -1.8735468   0.72965025  2.88578419 -0.54534335 -0.47522485
 -2.02654268  0.7289737   1.60512664  2.83077052 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 02:33:40 2024]  Iteration number: 0 with current cost as 0.2764559454460585 and parameters 
[-4.45008135  2.23743475 -2.12427941 -0.11653103  0.55388697 -2.77010909
  3.06858498  2.18960145  1.18551998 -1.06648331  0.60271522  1.14432445
  1.31029887 -1.87354691  0.72965058  2.88578408 -0.54534346 -0.47522485
 -2.02654263  0.72897358  1.60512652  2.83077084 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 02:39:50 2024]  Iteration number: 0 with current cost as 0.34884890222912857 and parameters 
[-7.58077526  2.23743464 -2.12427901 -0.1165304   0.5538877  -2.77010959
  3.06858436  2.18960207  1.18552061 -1.06648371  0.60271635  1.14432445
  1.31029899 -1.87354618  0.72965018  2.88578419 -0.54534397 -0.47522423
 -2.02654303  0.72897307  1.60512664  2.83077045 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1851.4352011680603 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 02:45:59 2024]  Iteration number: 0 with current cost as 0.3085586392976748 and parameters 
[-4.44578292  2.23743464 -2.12427913 -0.11653103  0.55388708 -2.77010948
  3.06858473  2.18960145  1.18551998 -1.06648359  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965055  2.88578394 -0.5453436  -0.47522485
 -2.02654266  0.72897344  1.60512639  2.83077057 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337105  2.54856958 -0.67550812 -2.69002227]. 
Working on 0.4 fold... 
[Fri Apr  5 02:51:33 2024]  Iteration number: 0 with current cost as 0.3205518678191448 and parameters 
[-4.32247437  2.23743481 -2.12427912 -0.11653085  0.55388725 -2.7701088
  3.06858516  2.18960162  1.18552016 -1.06648291  0.60271545  1.14432462
  1.31029916 -1.87354663  0.7296508   2.88578437 -0.54534352 -0.47522451
 -2.0265424   0.72897387  1.60512664  2.8307709  -1.26456692 -0.25136087
 -2.39279201 -2.27309757  3.13337155  2.54856976 -0.67550787 -2.69002184]. 
Working on 0.6 fold... 
[Fri Apr  5 02:58:24 2024]  Iteration number: 0 with current cost as 0.34250054921095563 and parameters 
[-4.33926417  2.23743464 -2.12427909 -0.11653048  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648336  0.60271565  1.14432445
  1.31029899 -1.8735468   0.72965025  2.88578419 -0.54534335 -0.47522485
 -2.02654268  0.7289737   1.60512664  2.83077052 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 03:04:33 2024]  Iteration number: 0 with current cost as 0.2764559454460585 and parameters 
[-4.45008135  2.23743475 -2.12427941 -0.11653103  0.55388697 -2.77010909
  3.06858498  2.18960145  1.18551998 -1.06648331  0.60271522  1.14432445
  1.31029887 -1.87354691  0.72965058  2.88578408 -0.54534346 -0.47522485
 -2.02654263  0.72897358  1.60512652  2.83077084 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 03:10:44 2024]  Iteration number: 0 with current cost as 0.34884890222912857 and parameters 
[-7.58077526  2.23743464 -2.12427901 -0.1165304   0.5538877  -2.77010959
  3.06858436  2.18960207  1.18552061 -1.06648371  0.60271635  1.14432445
  1.31029899 -1.87354618  0.72965018  2.88578419 -0.54534397 -0.47522423
 -2.02654303  0.72897307  1.60512664  2.83077045 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1854.336550951004 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 03:16:53 2024]  Iteration number: 0 with current cost as 0.3085586392976748 and parameters 
[-4.44578292  2.23743464 -2.12427913 -0.11653103  0.55388708 -2.77010948
  3.06858473  2.18960145  1.18551998 -1.06648359  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965055  2.88578394 -0.5453436  -0.47522485
 -2.02654266  0.72897344  1.60512639  2.83077057 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337105  2.54856958 -0.67550812 -2.69002227]. 
Working on 0.4 fold... 
[Fri Apr  5 03:22:26 2024]  Iteration number: 0 with current cost as 0.3205518678191448 and parameters 
[-4.32247437  2.23743481 -2.12427912 -0.11653085  0.55388725 -2.7701088
  3.06858516  2.18960162  1.18552016 -1.06648291  0.60271545  1.14432462
  1.31029916 -1.87354663  0.7296508   2.88578437 -0.54534352 -0.47522451
 -2.0265424   0.72897387  1.60512664  2.8307709  -1.26456692 -0.25136087
 -2.39279201 -2.27309757  3.13337155  2.54856976 -0.67550787 -2.69002184]. 
Working on 0.6 fold... 
[Fri Apr  5 03:29:14 2024]  Iteration number: 0 with current cost as 0.34250054921095563 and parameters 
[-4.33926417  2.23743464 -2.12427909 -0.11653048  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648336  0.60271565  1.14432445
  1.31029899 -1.8735468   0.72965025  2.88578419 -0.54534335 -0.47522485
 -2.02654268  0.7289737   1.60512664  2.83077052 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 03:35:23 2024]  Iteration number: 0 with current cost as 0.2764559454460585 and parameters 
[-4.45008135  2.23743475 -2.12427941 -0.11653103  0.55388697 -2.77010909
  3.06858498  2.18960145  1.18551998 -1.06648331  0.60271522  1.14432445
  1.31029887 -1.87354691  0.72965058  2.88578408 -0.54534346 -0.47522485
 -2.02654263  0.72897358  1.60512652  2.83077084 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 03:41:33 2024]  Iteration number: 0 with current cost as 0.34884890222912857 and parameters 
[-7.58077526  2.23743464 -2.12427901 -0.1165304   0.5538877  -2.77010959
  3.06858436  2.18960207  1.18552061 -1.06648371  0.60271635  1.14432445
  1.31029899 -1.87354618  0.72965018  2.88578419 -0.54534397 -0.47522423
 -2.02654303  0.72897307  1.60512664  2.83077045 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1848.3034980297089 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 03:47:42 2024]  Iteration number: 0 with current cost as 0.3085586392976748 and parameters 
[-4.44578292  2.23743464 -2.12427913 -0.11653103  0.55388708 -2.77010948
  3.06858473  2.18960145  1.18551998 -1.06648359  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965055  2.88578394 -0.5453436  -0.47522485
 -2.02654266  0.72897344  1.60512639  2.83077057 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337105  2.54856958 -0.67550812 -2.69002227]. 
Working on 0.4 fold... 
[Fri Apr  5 03:53:15 2024]  Iteration number: 0 with current cost as 0.3205518678191448 and parameters 
[-4.32247437  2.23743481 -2.12427912 -0.11653085  0.55388725 -2.7701088
  3.06858516  2.18960162  1.18552016 -1.06648291  0.60271545  1.14432462
  1.31029916 -1.87354663  0.7296508   2.88578437 -0.54534352 -0.47522451
 -2.0265424   0.72897387  1.60512664  2.8307709  -1.26456692 -0.25136087
 -2.39279201 -2.27309757  3.13337155  2.54856976 -0.67550787 -2.69002184]. 
Working on 0.6 fold... 
[Fri Apr  5 04:00:00 2024]  Iteration number: 0 with current cost as 0.34250054921095563 and parameters 
[-4.33926417  2.23743464 -2.12427909 -0.11653048  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648336  0.60271565  1.14432445
  1.31029899 -1.8735468   0.72965025  2.88578419 -0.54534335 -0.47522485
 -2.02654268  0.7289737   1.60512664  2.83077052 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 04:06:10 2024]  Iteration number: 0 with current cost as 0.2764559454460585 and parameters 
[-4.45008135  2.23743475 -2.12427941 -0.11653103  0.55388697 -2.77010909
  3.06858498  2.18960145  1.18551998 -1.06648331  0.60271522  1.14432445
  1.31029887 -1.87354691  0.72965058  2.88578408 -0.54534346 -0.47522485
 -2.02654263  0.72897358  1.60512652  2.83077084 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 04:12:24 2024]  Iteration number: 0 with current cost as 0.34884890222912857 and parameters 
[-7.58077526  2.23743464 -2.12427901 -0.1165304   0.5538877  -2.77010959
  3.06858436  2.18960207  1.18552061 -1.06648371  0.60271635  1.14432445
  1.31029899 -1.87354618  0.72965018  2.88578419 -0.54534397 -0.47522423
 -2.02654303  0.72897307  1.60512664  2.83077045 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1851.8515849113464 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 04:18:36 2024]  Iteration number: 0 with current cost as 0.3085586392976748 and parameters 
[-4.44578292  2.23743464 -2.12427913 -0.11653103  0.55388708 -2.77010948
  3.06858473  2.18960145  1.18551998 -1.06648359  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965055  2.88578394 -0.5453436  -0.47522485
 -2.02654266  0.72897344  1.60512639  2.83077057 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337105  2.54856958 -0.67550812 -2.69002227]. 
Working on 0.4 fold... 
[Fri Apr  5 04:24:13 2024]  Iteration number: 0 with current cost as 0.3205518678191448 and parameters 
[-4.32247437  2.23743481 -2.12427912 -0.11653085  0.55388725 -2.7701088
  3.06858516  2.18960162  1.18552016 -1.06648291  0.60271545  1.14432462
  1.31029916 -1.87354663  0.7296508   2.88578437 -0.54534352 -0.47522451
 -2.0265424   0.72897387  1.60512664  2.8307709  -1.26456692 -0.25136087
 -2.39279201 -2.27309757  3.13337155  2.54856976 -0.67550787 -2.69002184]. 
Working on 0.6 fold... 
[Fri Apr  5 04:31:00 2024]  Iteration number: 0 with current cost as 0.34250054921095563 and parameters 
[-4.33926417  2.23743464 -2.12427909 -0.11653048  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648336  0.60271565  1.14432445
  1.31029899 -1.8735468   0.72965025  2.88578419 -0.54534335 -0.47522485
 -2.02654268  0.7289737   1.60512664  2.83077052 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 04:37:15 2024]  Iteration number: 0 with current cost as 0.2764559454460585 and parameters 
[-4.45008135  2.23743475 -2.12427941 -0.11653103  0.55388697 -2.77010909
  3.06858498  2.18960145  1.18551998 -1.06648331  0.60271522  1.14432445
  1.31029887 -1.87354691  0.72965058  2.88578408 -0.54534346 -0.47522485
 -2.02654263  0.72897358  1.60512652  2.83077084 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 04:43:27 2024]  Iteration number: 0 with current cost as 0.34884890222912857 and parameters 
[-7.58077526  2.23743464 -2.12427901 -0.1165304   0.5538877  -2.77010959
  3.06858436  2.18960207  1.18552061 -1.06648371  0.60271635  1.14432445
  1.31029899 -1.87354618  0.72965018  2.88578419 -0.54534397 -0.47522423
 -2.02654303  0.72897307  1.60512664  2.83077045 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1863.4538378715515 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 04:49:39 2024]  Iteration number: 0 with current cost as 0.3085586392976748 and parameters 
[-4.44578292  2.23743464 -2.12427913 -0.11653103  0.55388708 -2.77010948
  3.06858473  2.18960145  1.18551998 -1.06648359  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965055  2.88578394 -0.5453436  -0.47522485
 -2.02654266  0.72897344  1.60512639  2.83077057 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337105  2.54856958 -0.67550812 -2.69002227]. 
Working on 0.4 fold... 
[Fri Apr  5 04:55:13 2024]  Iteration number: 0 with current cost as 0.3205518678191448 and parameters 
[-4.32247437  2.23743481 -2.12427912 -0.11653085  0.55388725 -2.7701088
  3.06858516  2.18960162  1.18552016 -1.06648291  0.60271545  1.14432462
  1.31029916 -1.87354663  0.7296508   2.88578437 -0.54534352 -0.47522451
 -2.0265424   0.72897387  1.60512664  2.8307709  -1.26456692 -0.25136087
 -2.39279201 -2.27309757  3.13337155  2.54856976 -0.67550787 -2.69002184]. 
Working on 0.6 fold... 
[Fri Apr  5 05:02:02 2024]  Iteration number: 0 with current cost as 0.34250054921095563 and parameters 
[-4.33926417  2.23743464 -2.12427909 -0.11653048  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648336  0.60271565  1.14432445
  1.31029899 -1.8735468   0.72965025  2.88578419 -0.54534335 -0.47522485
 -2.02654268  0.7289737   1.60512664  2.83077052 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 05:08:12 2024]  Iteration number: 0 with current cost as 0.2764559454460585 and parameters 
[-4.45008135  2.23743475 -2.12427941 -0.11653103  0.55388697 -2.77010909
  3.06858498  2.18960145  1.18551998 -1.06648331  0.60271522  1.14432445
  1.31029887 -1.87354691  0.72965058  2.88578408 -0.54534346 -0.47522485
 -2.02654263  0.72897358  1.60512652  2.83077084 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 05:14:21 2024]  Iteration number: 0 with current cost as 0.34884890222912857 and parameters 
[-7.58077526  2.23743464 -2.12427901 -0.1165304   0.5538877  -2.77010959
  3.06858436  2.18960207  1.18552061 -1.06648371  0.60271635  1.14432445
  1.31029899 -1.87354618  0.72965018  2.88578419 -0.54534397 -0.47522423
 -2.02654303  0.72897307  1.60512664  2.83077045 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1852.1384472846985 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 05:20:32 2024]  Iteration number: 0 with current cost as 0.3085586392976748 and parameters 
[-4.44578292  2.23743464 -2.12427913 -0.11653103  0.55388708 -2.77010948
  3.06858473  2.18960145  1.18551998 -1.06648359  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965055  2.88578394 -0.5453436  -0.47522485
 -2.02654266  0.72897344  1.60512639  2.83077057 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337105  2.54856958 -0.67550812 -2.69002227]. 
Working on 0.4 fold... 
[Fri Apr  5 05:26:08 2024]  Iteration number: 0 with current cost as 0.3205518678191448 and parameters 
[-4.32247437  2.23743481 -2.12427912 -0.11653085  0.55388725 -2.7701088
  3.06858516  2.18960162  1.18552016 -1.06648291  0.60271545  1.14432462
  1.31029916 -1.87354663  0.7296508   2.88578437 -0.54534352 -0.47522451
 -2.0265424   0.72897387  1.60512664  2.8307709  -1.26456692 -0.25136087
 -2.39279201 -2.27309757  3.13337155  2.54856976 -0.67550787 -2.69002184]. 
Working on 0.6 fold... 
[Fri Apr  5 05:33:00 2024]  Iteration number: 0 with current cost as 0.34250054921095563 and parameters 
[-4.33926417  2.23743464 -2.12427909 -0.11653048  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648336  0.60271565  1.14432445
  1.31029899 -1.8735468   0.72965025  2.88578419 -0.54534335 -0.47522485
 -2.02654268  0.7289737   1.60512664  2.83077052 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 05:39:13 2024]  Iteration number: 0 with current cost as 0.2764559454460585 and parameters 
[-4.45008135  2.23743475 -2.12427941 -0.11653103  0.55388697 -2.77010909
  3.06858498  2.18960145  1.18551998 -1.06648331  0.60271522  1.14432445
  1.31029887 -1.87354691  0.72965058  2.88578408 -0.54534346 -0.47522485
 -2.02654263  0.72897358  1.60512652  2.83077084 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 05:45:28 2024]  Iteration number: 0 with current cost as 0.34884890222912857 and parameters 
[-7.58077526  2.23743464 -2.12427901 -0.1165304   0.5538877  -2.77010959
  3.06858436  2.18960207  1.18552061 -1.06648371  0.60271635  1.14432445
  1.31029899 -1.87354618  0.72965018  2.88578419 -0.54534397 -0.47522423
 -2.02654303  0.72897307  1.60512664  2.83077045 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1867.2014813423157 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 05:51:38 2024]  Iteration number: 0 with current cost as 0.3085586392976748 and parameters 
[-4.44578292  2.23743464 -2.12427913 -0.11653103  0.55388708 -2.77010948
  3.06858473  2.18960145  1.18551998 -1.06648359  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965055  2.88578394 -0.5453436  -0.47522485
 -2.02654266  0.72897344  1.60512639  2.83077057 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337105  2.54856958 -0.67550812 -2.69002227]. 
Working on 0.4 fold... 
[Fri Apr  5 05:57:12 2024]  Iteration number: 0 with current cost as 0.3205518678191448 and parameters 
[-4.32247437  2.23743481 -2.12427912 -0.11653085  0.55388725 -2.7701088
  3.06858516  2.18960162  1.18552016 -1.06648291  0.60271545  1.14432462
  1.31029916 -1.87354663  0.7296508   2.88578437 -0.54534352 -0.47522451
 -2.0265424   0.72897387  1.60512664  2.8307709  -1.26456692 -0.25136087
 -2.39279201 -2.27309757  3.13337155  2.54856976 -0.67550787 -2.69002184]. 
Working on 0.6 fold... 
[Fri Apr  5 06:04:03 2024]  Iteration number: 0 with current cost as 0.34250054921095563 and parameters 
[-4.33926417  2.23743464 -2.12427909 -0.11653048  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648336  0.60271565  1.14432445
  1.31029899 -1.8735468   0.72965025  2.88578419 -0.54534335 -0.47522485
 -2.02654268  0.7289737   1.60512664  2.83077052 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 06:10:12 2024]  Iteration number: 0 with current cost as 0.2764559454460585 and parameters 
[-4.45008135  2.23743475 -2.12427941 -0.11653103  0.55388697 -2.77010909
  3.06858498  2.18960145  1.18551998 -1.06648331  0.60271522  1.14432445
  1.31029887 -1.87354691  0.72965058  2.88578408 -0.54534346 -0.47522485
 -2.02654263  0.72897358  1.60512652  2.83077084 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 06:16:26 2024]  Iteration number: 0 with current cost as 0.34884890222912857 and parameters 
[-7.58077526  2.23743464 -2.12427901 -0.1165304   0.5538877  -2.77010959
  3.06858436  2.18960207  1.18552061 -1.06648371  0.60271635  1.14432445
  1.31029899 -1.87354618  0.72965018  2.88578419 -0.54534397 -0.47522423
 -2.02654303  0.72897307  1.60512664  2.83077045 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1858.761739730835 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 06:22:42 2024]  Iteration number: 0 with current cost as 0.3085586392976748 and parameters 
[-4.44578292  2.23743464 -2.12427913 -0.11653103  0.55388708 -2.77010948
  3.06858473  2.18960145  1.18551998 -1.06648359  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965055  2.88578394 -0.5453436  -0.47522485
 -2.02654266  0.72897344  1.60512639  2.83077057 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337105  2.54856958 -0.67550812 -2.69002227]. 
Working on 0.4 fold... 
[Fri Apr  5 06:28:19 2024]  Iteration number: 0 with current cost as 0.3205518678191448 and parameters 
[-4.32247437  2.23743481 -2.12427912 -0.11653085  0.55388725 -2.7701088
  3.06858516  2.18960162  1.18552016 -1.06648291  0.60271545  1.14432462
  1.31029916 -1.87354663  0.7296508   2.88578437 -0.54534352 -0.47522451
 -2.0265424   0.72897387  1.60512664  2.8307709  -1.26456692 -0.25136087
 -2.39279201 -2.27309757  3.13337155  2.54856976 -0.67550787 -2.69002184]. 
Working on 0.6 fold... 
[Fri Apr  5 06:35:13 2024]  Iteration number: 0 with current cost as 0.34250054921095563 and parameters 
[-4.33926417  2.23743464 -2.12427909 -0.11653048  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648336  0.60271565  1.14432445
  1.31029899 -1.8735468   0.72965025  2.88578419 -0.54534335 -0.47522485
 -2.02654268  0.7289737   1.60512664  2.83077052 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 06:41:27 2024]  Iteration number: 0 with current cost as 0.2764559454460585 and parameters 
[-4.45008135  2.23743475 -2.12427941 -0.11653103  0.55388697 -2.77010909
  3.06858498  2.18960145  1.18551998 -1.06648331  0.60271522  1.14432445
  1.31029887 -1.87354691  0.72965058  2.88578408 -0.54534346 -0.47522485
 -2.02654263  0.72897358  1.60512652  2.83077084 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 06:47:38 2024]  Iteration number: 0 with current cost as 0.34884890222912857 and parameters 
[-7.58077526  2.23743464 -2.12427901 -0.1165304   0.5538877  -2.77010959
  3.06858436  2.18960207  1.18552061 -1.06648371  0.60271635  1.14432445
  1.31029899 -1.87354618  0.72965018  2.88578419 -0.54534397 -0.47522423
 -2.02654303  0.72897307  1.60512664  2.83077045 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1870.9228677749634 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 06:53:46 2024]  Iteration number: 0 with current cost as 0.3085586392976748 and parameters 
[-4.44578292  2.23743464 -2.12427913 -0.11653103  0.55388708 -2.77010948
  3.06858473  2.18960145  1.18551998 -1.06648359  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965055  2.88578394 -0.5453436  -0.47522485
 -2.02654266  0.72897344  1.60512639  2.83077057 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337105  2.54856958 -0.67550812 -2.69002227]. 
Working on 0.4 fold... 
[Fri Apr  5 06:59:19 2024]  Iteration number: 0 with current cost as 0.3205518678191448 and parameters 
[-4.32247437  2.23743481 -2.12427912 -0.11653085  0.55388725 -2.7701088
  3.06858516  2.18960162  1.18552016 -1.06648291  0.60271545  1.14432462
  1.31029916 -1.87354663  0.7296508   2.88578437 -0.54534352 -0.47522451
 -2.0265424   0.72897387  1.60512664  2.8307709  -1.26456692 -0.25136087
 -2.39279201 -2.27309757  3.13337155  2.54856976 -0.67550787 -2.69002184]. 
Working on 0.6 fold... 
[Fri Apr  5 07:06:02 2024]  Iteration number: 0 with current cost as 0.34250054921095563 and parameters 
[-4.33926417  2.23743464 -2.12427909 -0.11653048  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648336  0.60271565  1.14432445
  1.31029899 -1.8735468   0.72965025  2.88578419 -0.54534335 -0.47522485
 -2.02654268  0.7289737   1.60512664  2.83077052 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 07:12:09 2024]  Iteration number: 0 with current cost as 0.2764559454460585 and parameters 
[-4.45008135  2.23743475 -2.12427941 -0.11653103  0.55388697 -2.77010909
  3.06858498  2.18960145  1.18551998 -1.06648331  0.60271522  1.14432445
  1.31029887 -1.87354691  0.72965058  2.88578408 -0.54534346 -0.47522485
 -2.02654263  0.72897358  1.60512652  2.83077084 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 07:18:18 2024]  Iteration number: 0 with current cost as 0.34884890222912857 and parameters 
[-7.58077526  2.23743464 -2.12427901 -0.1165304   0.5538877  -2.77010959
  3.06858436  2.18960207  1.18552061 -1.06648371  0.60271635  1.14432445
  1.31029899 -1.87354618  0.72965018  2.88578419 -0.54534397 -0.47522423
 -2.02654303  0.72897307  1.60512664  2.83077045 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1840.191110610962 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 07:24:26 2024]  Iteration number: 0 with current cost as 0.3085586392976748 and parameters 
[-4.44578292  2.23743464 -2.12427913 -0.11653103  0.55388708 -2.77010948
  3.06858473  2.18960145  1.18551998 -1.06648359  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965055  2.88578394 -0.5453436  -0.47522485
 -2.02654266  0.72897344  1.60512639  2.83077057 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337105  2.54856958 -0.67550812 -2.69002227]. 
Working on 0.4 fold... 
[Fri Apr  5 07:29:56 2024]  Iteration number: 0 with current cost as 0.3205518678191448 and parameters 
[-4.32247437  2.23743481 -2.12427912 -0.11653085  0.55388725 -2.7701088
  3.06858516  2.18960162  1.18552016 -1.06648291  0.60271545  1.14432462
  1.31029916 -1.87354663  0.7296508   2.88578437 -0.54534352 -0.47522451
 -2.0265424   0.72897387  1.60512664  2.8307709  -1.26456692 -0.25136087
 -2.39279201 -2.27309757  3.13337155  2.54856976 -0.67550787 -2.69002184]. 
Working on 0.6 fold... 
[Fri Apr  5 07:36:42 2024]  Iteration number: 0 with current cost as 0.34250054921095563 and parameters 
[-4.33926417  2.23743464 -2.12427909 -0.11653048  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648336  0.60271565  1.14432445
  1.31029899 -1.8735468   0.72965025  2.88578419 -0.54534335 -0.47522485
 -2.02654268  0.7289737   1.60512664  2.83077052 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 07:42:52 2024]  Iteration number: 0 with current cost as 0.2764559454460585 and parameters 
[-4.45008135  2.23743475 -2.12427941 -0.11653103  0.55388697 -2.77010909
  3.06858498  2.18960145  1.18551998 -1.06648331  0.60271522  1.14432445
  1.31029887 -1.87354691  0.72965058  2.88578408 -0.54534346 -0.47522485
 -2.02654263  0.72897358  1.60512652  2.83077084 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 07:49:02 2024]  Iteration number: 0 with current cost as 0.34884890222912857 and parameters 
[-7.58077526  2.23743464 -2.12427901 -0.1165304   0.5538877  -2.77010959
  3.06858436  2.18960207  1.18552061 -1.06648371  0.60271635  1.14432445
  1.31029899 -1.87354618  0.72965018  2.88578419 -0.54534397 -0.47522423
 -2.02654303  0.72897307  1.60512664  2.83077045 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1842.7052671909332 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 07:55:11 2024]  Iteration number: 0 with current cost as 0.3085586392976748 and parameters 
[-4.44578292  2.23743464 -2.12427913 -0.11653103  0.55388708 -2.77010948
  3.06858473  2.18960145  1.18551998 -1.06648359  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965055  2.88578394 -0.5453436  -0.47522485
 -2.02654266  0.72897344  1.60512639  2.83077057 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337105  2.54856958 -0.67550812 -2.69002227]. 
Working on 0.4 fold... 
[Fri Apr  5 08:00:46 2024]  Iteration number: 0 with current cost as 0.3205518678191448 and parameters 
[-4.32247437  2.23743481 -2.12427912 -0.11653085  0.55388725 -2.7701088
  3.06858516  2.18960162  1.18552016 -1.06648291  0.60271545  1.14432462
  1.31029916 -1.87354663  0.7296508   2.88578437 -0.54534352 -0.47522451
 -2.0265424   0.72897387  1.60512664  2.8307709  -1.26456692 -0.25136087
 -2.39279201 -2.27309757  3.13337155  2.54856976 -0.67550787 -2.69002184]. 
Working on 0.6 fold... 
[Fri Apr  5 08:07:35 2024]  Iteration number: 0 with current cost as 0.34250054921095563 and parameters 
[-4.33926417  2.23743464 -2.12427909 -0.11653048  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648336  0.60271565  1.14432445
  1.31029899 -1.8735468   0.72965025  2.88578419 -0.54534335 -0.47522485
 -2.02654268  0.7289737   1.60512664  2.83077052 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 08:13:56 2024]  Iteration number: 0 with current cost as 0.2764559454460585 and parameters 
[-4.45008135  2.23743475 -2.12427941 -0.11653103  0.55388697 -2.77010909
  3.06858498  2.18960145  1.18551998 -1.06648331  0.60271522  1.14432445
  1.31029887 -1.87354691  0.72965058  2.88578408 -0.54534346 -0.47522485
 -2.02654263  0.72897358  1.60512652  2.83077084 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 08:20:14 2024]  Iteration number: 0 with current cost as 0.34884890222912857 and parameters 
[-7.58077526  2.23743464 -2.12427901 -0.1165304   0.5538877  -2.77010959
  3.06858436  2.18960207  1.18552061 -1.06648371  0.60271635  1.14432445
  1.31029899 -1.87354618  0.72965018  2.88578419 -0.54534397 -0.47522423
 -2.02654303  0.72897307  1.60512664  2.83077045 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1878.159301996231 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 08:26:30 2024]  Iteration number: 0 with current cost as 0.3085586392976748 and parameters 
[-4.44578292  2.23743464 -2.12427913 -0.11653103  0.55388708 -2.77010948
  3.06858473  2.18960145  1.18551998 -1.06648359  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965055  2.88578394 -0.5453436  -0.47522485
 -2.02654266  0.72897344  1.60512639  2.83077057 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337105  2.54856958 -0.67550812 -2.69002227]. 
Working on 0.4 fold... 
[Fri Apr  5 08:32:08 2024]  Iteration number: 0 with current cost as 0.3205518678191448 and parameters 
[-4.32247437  2.23743481 -2.12427912 -0.11653085  0.55388725 -2.7701088
  3.06858516  2.18960162  1.18552016 -1.06648291  0.60271545  1.14432462
  1.31029916 -1.87354663  0.7296508   2.88578437 -0.54534352 -0.47522451
 -2.0265424   0.72897387  1.60512664  2.8307709  -1.26456692 -0.25136087
 -2.39279201 -2.27309757  3.13337155  2.54856976 -0.67550787 -2.69002184]. 
Working on 0.6 fold... 
[Fri Apr  5 08:39:03 2024]  Iteration number: 0 with current cost as 0.34250054921095563 and parameters 
[-4.33926417  2.23743464 -2.12427909 -0.11653048  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648336  0.60271565  1.14432445
  1.31029899 -1.8735468   0.72965025  2.88578419 -0.54534335 -0.47522485
 -2.02654268  0.7289737   1.60512664  2.83077052 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 08:45:19 2024]  Iteration number: 0 with current cost as 0.2764559454460585 and parameters 
[-4.45008135  2.23743475 -2.12427941 -0.11653103  0.55388697 -2.77010909
  3.06858498  2.18960145  1.18551998 -1.06648331  0.60271522  1.14432445
  1.31029887 -1.87354691  0.72965058  2.88578408 -0.54534346 -0.47522485
 -2.02654263  0.72897358  1.60512652  2.83077084 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 08:51:34 2024]  Iteration number: 0 with current cost as 0.34884890222912857 and parameters 
[-7.58077526  2.23743464 -2.12427901 -0.1165304   0.5538877  -2.77010959
  3.06858436  2.18960207  1.18552061 -1.06648371  0.60271635  1.14432445
  1.31029899 -1.87354618  0.72965018  2.88578419 -0.54534397 -0.47522423
 -2.02654303  0.72897307  1.60512664  2.83077045 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1877.1747796535492 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 08:57:51 2024]  Iteration number: 0 with current cost as 0.3085586392976748 and parameters 
[-4.44578292  2.23743464 -2.12427913 -0.11653103  0.55388708 -2.77010948
  3.06858473  2.18960145  1.18551998 -1.06648359  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965055  2.88578394 -0.5453436  -0.47522485
 -2.02654266  0.72897344  1.60512639  2.83077057 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337105  2.54856958 -0.67550812 -2.69002227]. 
Working on 0.4 fold... 
[Fri Apr  5 09:03:35 2024]  Iteration number: 0 with current cost as 0.3205518678191448 and parameters 
[-4.32247437  2.23743481 -2.12427912 -0.11653085  0.55388725 -2.7701088
  3.06858516  2.18960162  1.18552016 -1.06648291  0.60271545  1.14432462
  1.31029916 -1.87354663  0.7296508   2.88578437 -0.54534352 -0.47522451
 -2.0265424   0.72897387  1.60512664  2.8307709  -1.26456692 -0.25136087
 -2.39279201 -2.27309757  3.13337155  2.54856976 -0.67550787 -2.69002184]. 
Working on 0.6 fold... 
[Fri Apr  5 09:10:36 2024]  Iteration number: 0 with current cost as 0.34250054921095563 and parameters 
[-4.33926417  2.23743464 -2.12427909 -0.11653048  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648336  0.60271565  1.14432445
  1.31029899 -1.8735468   0.72965025  2.88578419 -0.54534335 -0.47522485
 -2.02654268  0.7289737   1.60512664  2.83077052 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 09:17:13 2024]  Iteration number: 0 with current cost as 0.2764559454460585 and parameters 
[-4.45008135  2.23743475 -2.12427941 -0.11653103  0.55388697 -2.77010909
  3.06858498  2.18960145  1.18551998 -1.06648331  0.60271522  1.14432445
  1.31029887 -1.87354691  0.72965058  2.88578408 -0.54534346 -0.47522485
 -2.02654263  0.72897358  1.60512652  2.83077084 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 09:23:40 2024]  Iteration number: 0 with current cost as 0.34884890222912857 and parameters 
[-7.58077526  2.23743464 -2.12427901 -0.1165304   0.5538877  -2.77010959
  3.06858436  2.18960207  1.18552061 -1.06648371  0.60271635  1.14432445
  1.31029899 -1.87354618  0.72965018  2.88578419 -0.54534397 -0.47522423
 -2.02654303  0.72897307  1.60512664  2.83077045 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1927.1062200069427 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 09:29:57 2024]  Iteration number: 0 with current cost as 0.3085586392976748 and parameters 
[-4.44578292  2.23743464 -2.12427913 -0.11653103  0.55388708 -2.77010948
  3.06858473  2.18960145  1.18551998 -1.06648359  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965055  2.88578394 -0.5453436  -0.47522485
 -2.02654266  0.72897344  1.60512639  2.83077057 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337105  2.54856958 -0.67550812 -2.69002227]. 
Working on 0.4 fold... 
[Fri Apr  5 09:35:40 2024]  Iteration number: 0 with current cost as 0.3205518678191448 and parameters 
[-4.32247437  2.23743481 -2.12427912 -0.11653085  0.55388725 -2.7701088
  3.06858516  2.18960162  1.18552016 -1.06648291  0.60271545  1.14432462
  1.31029916 -1.87354663  0.7296508   2.88578437 -0.54534352 -0.47522451
 -2.0265424   0.72897387  1.60512664  2.8307709  -1.26456692 -0.25136087
 -2.39279201 -2.27309757  3.13337155  2.54856976 -0.67550787 -2.69002184]. 
Working on 0.6 fold... 
[Fri Apr  5 09:42:33 2024]  Iteration number: 0 with current cost as 0.34250054921095563 and parameters 
[-4.33926417  2.23743464 -2.12427909 -0.11653048  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648336  0.60271565  1.14432445
  1.31029899 -1.8735468   0.72965025  2.88578419 -0.54534335 -0.47522485
 -2.02654268  0.7289737   1.60512664  2.83077052 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 09:48:47 2024]  Iteration number: 0 with current cost as 0.2764559454460585 and parameters 
[-4.45008135  2.23743475 -2.12427941 -0.11653103  0.55388697 -2.77010909
  3.06858498  2.18960145  1.18551998 -1.06648331  0.60271522  1.14432445
  1.31029887 -1.87354691  0.72965058  2.88578408 -0.54534346 -0.47522485
 -2.02654263  0.72897358  1.60512652  2.83077084 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 09:55:04 2024]  Iteration number: 0 with current cost as 0.34884890222912857 and parameters 
[-7.58077526  2.23743464 -2.12427901 -0.1165304   0.5538877  -2.77010959
  3.06858436  2.18960207  1.18552061 -1.06648371  0.60271635  1.14432445
  1.31029899 -1.87354618  0.72965018  2.88578419 -0.54534397 -0.47522423
 -2.02654303  0.72897307  1.60512664  2.83077045 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1881.5474858283997 seconds. 
Discarding model... 

Training complete taking 46582.73729920387 total seconds. 
Now scoring model... 
Scoring complete taking 2.2229790687561035 seconds. 
Saved predicted values as A2-A2-CZ_Full-CRZ_predicted_values.csv
Model scores: {'MSE_train': (0.5897807152291664,), 'R2_train': -0.17439224891255245, 'MAE_train': 0.6694770426306345, 'MSE_test': 0.6324817571130065, 'R2_test': -0.17955630876886053, 'MAE_test': 0.6679460592061834}. 
Saved model results as A2-A2-CZ_Full-CRZ_results.json. 
