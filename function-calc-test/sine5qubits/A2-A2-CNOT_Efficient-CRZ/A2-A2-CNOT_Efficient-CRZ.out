/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /scratch/j/jacobsen/gjones/sine5qubits/sine_train.bin... 
Successfully loaded /scratch/j/jacobsen/gjones/sine5qubits/sine_train.bin into X and y data. 
Loading dataset from /scratch/j/jacobsen/gjones/sine5qubits/sine_test.bin... 
Successfully loaded /scratch/j/jacobsen/gjones/sine5qubits/sine_test.bin into X and y data. 
Training model with dataset /scratch/j/jacobsen/gjones/sine5qubits/sine_train.bin 
 at time Thu Apr  4 22:03:30 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 22:05:37 2024]  Iteration number: 0 with current cost as 0.28045946209385286 and parameters 
[ 1.37382131  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960128  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Apr  4 22:08:36 2024]  Iteration number: 0 with current cost as 0.3423991943977621 and parameters 
[-1.67804059  2.23743458 -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960134  1.18551998 -1.0664832   0.6027151   1.14432433
  1.31029893 -1.87354686]. 
Working on 0.6 fold... 
[Thu Apr  4 22:11:15 2024]  Iteration number: 0 with current cost as 0.33601862473788136 and parameters 
[-1.57381367  2.23743464 -2.12427952 -0.11653097  0.55388714 -2.77010897
  3.06858498  2.18960145  1.18552004 -1.06648314  0.60271516  1.14432451
  1.31029904 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 22:13:38 2024]  Iteration number: 0 with current cost as 0.3318226495654909 and parameters 
[-1.63687967  2.2374347  -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960145  1.18551998 -1.06648315  0.60271516  1.14432445
  1.31029905 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 22:15:56 2024]  Iteration number: 0 with current cost as 0.32663897328298214 and parameters 
[-1.510412    2.23743457 -2.12427951 -0.11653103  0.55388702 -2.77010903
  3.06858492  2.18960145  1.18552005 -1.06648315  0.60271516  1.14432451
  1.31029899 -1.8735468 ]. 
Training complete taking 799.5210101604462 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 22:18:57 2024]  Iteration number: 0 with current cost as 0.28045946209385286 and parameters 
[ 1.37382131  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960128  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Apr  4 22:21:52 2024]  Iteration number: 0 with current cost as 0.3423991943977621 and parameters 
[-1.67804059  2.23743458 -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960134  1.18551998 -1.0664832   0.6027151   1.14432433
  1.31029893 -1.87354686]. 
Working on 0.6 fold... 
[Thu Apr  4 22:24:29 2024]  Iteration number: 0 with current cost as 0.33601862473788136 and parameters 
[-1.57381367  2.23743464 -2.12427952 -0.11653097  0.55388714 -2.77010897
  3.06858498  2.18960145  1.18552004 -1.06648314  0.60271516  1.14432451
  1.31029904 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 22:26:50 2024]  Iteration number: 0 with current cost as 0.3318226495654909 and parameters 
[-1.63687967  2.2374347  -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960145  1.18551998 -1.06648315  0.60271516  1.14432445
  1.31029905 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 22:29:10 2024]  Iteration number: 0 with current cost as 0.32663897328298214 and parameters 
[-1.510412    2.23743457 -2.12427951 -0.11653103  0.55388702 -2.77010903
  3.06858492  2.18960145  1.18552005 -1.06648315  0.60271516  1.14432451
  1.31029899 -1.8735468 ]. 
Training complete taking 792.4016573429108 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 22:32:05 2024]  Iteration number: 0 with current cost as 0.28045946209385286 and parameters 
[ 1.37382131  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960128  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Apr  4 22:35:03 2024]  Iteration number: 0 with current cost as 0.3423991943977621 and parameters 
[-1.67804059  2.23743458 -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960134  1.18551998 -1.0664832   0.6027151   1.14432433
  1.31029893 -1.87354686]. 
Working on 0.6 fold... 
[Thu Apr  4 22:37:42 2024]  Iteration number: 0 with current cost as 0.33601862473788136 and parameters 
[-1.57381367  2.23743464 -2.12427952 -0.11653097  0.55388714 -2.77010897
  3.06858498  2.18960145  1.18552004 -1.06648314  0.60271516  1.14432451
  1.31029904 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 22:40:04 2024]  Iteration number: 0 with current cost as 0.3318226495654909 and parameters 
[-1.63687967  2.2374347  -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960145  1.18551998 -1.06648315  0.60271516  1.14432445
  1.31029905 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 22:42:27 2024]  Iteration number: 0 with current cost as 0.32663897328298214 and parameters 
[-1.510412    2.23743457 -2.12427951 -0.11653103  0.55388702 -2.77010903
  3.06858492  2.18960145  1.18552005 -1.06648315  0.60271516  1.14432451
  1.31029899 -1.8735468 ]. 
Training complete taking 802.4287993907928 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 22:45:27 2024]  Iteration number: 0 with current cost as 0.28045946209385286 and parameters 
[ 1.37382131  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960128  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Apr  4 22:48:19 2024]  Iteration number: 0 with current cost as 0.3423991943977621 and parameters 
[-1.67804059  2.23743458 -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960134  1.18551998 -1.0664832   0.6027151   1.14432433
  1.31029893 -1.87354686]. 
Working on 0.6 fold... 
[Thu Apr  4 22:50:57 2024]  Iteration number: 0 with current cost as 0.33601862473788136 and parameters 
[-1.57381367  2.23743464 -2.12427952 -0.11653097  0.55388714 -2.77010897
  3.06858498  2.18960145  1.18552004 -1.06648314  0.60271516  1.14432451
  1.31029904 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 22:53:16 2024]  Iteration number: 0 with current cost as 0.3318226495654909 and parameters 
[-1.63687967  2.2374347  -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960145  1.18551998 -1.06648315  0.60271516  1.14432445
  1.31029905 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 22:55:33 2024]  Iteration number: 0 with current cost as 0.32663897328298214 and parameters 
[-1.510412    2.23743457 -2.12427951 -0.11653103  0.55388702 -2.77010903
  3.06858492  2.18960145  1.18552005 -1.06648315  0.60271516  1.14432451
  1.31029899 -1.8735468 ]. 
Training complete taking 779.9284701347351 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 22:58:25 2024]  Iteration number: 0 with current cost as 0.28045946209385286 and parameters 
[ 1.37382131  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960128  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Apr  4 23:01:28 2024]  Iteration number: 0 with current cost as 0.3423991943977621 and parameters 
[-1.67804059  2.23743458 -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960134  1.18551998 -1.0664832   0.6027151   1.14432433
  1.31029893 -1.87354686]. 
Working on 0.6 fold... 
[Thu Apr  4 23:04:01 2024]  Iteration number: 0 with current cost as 0.33601862473788136 and parameters 
[-1.57381367  2.23743464 -2.12427952 -0.11653097  0.55388714 -2.77010897
  3.06858498  2.18960145  1.18552004 -1.06648314  0.60271516  1.14432451
  1.31029904 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 23:06:23 2024]  Iteration number: 0 with current cost as 0.3318226495654909 and parameters 
[-1.63687967  2.2374347  -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960145  1.18551998 -1.06648315  0.60271516  1.14432445
  1.31029905 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 23:08:40 2024]  Iteration number: 0 with current cost as 0.32663897328298214 and parameters 
[-1.510412    2.23743457 -2.12427951 -0.11653103  0.55388702 -2.77010903
  3.06858492  2.18960145  1.18552005 -1.06648315  0.60271516  1.14432451
  1.31029899 -1.8735468 ]. 
Training complete taking 786.8937482833862 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 23:11:35 2024]  Iteration number: 0 with current cost as 0.28045946209385286 and parameters 
[ 1.37382131  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960128  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Apr  4 23:14:31 2024]  Iteration number: 0 with current cost as 0.3423991943977621 and parameters 
[-1.67804059  2.23743458 -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960134  1.18551998 -1.0664832   0.6027151   1.14432433
  1.31029893 -1.87354686]. 
Working on 0.6 fold... 
[Thu Apr  4 23:17:04 2024]  Iteration number: 0 with current cost as 0.33601862473788136 and parameters 
[-1.57381367  2.23743464 -2.12427952 -0.11653097  0.55388714 -2.77010897
  3.06858498  2.18960145  1.18552004 -1.06648314  0.60271516  1.14432451
  1.31029904 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 23:19:22 2024]  Iteration number: 0 with current cost as 0.3318226495654909 and parameters 
[-1.63687967  2.2374347  -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960145  1.18551998 -1.06648315  0.60271516  1.14432445
  1.31029905 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 23:21:39 2024]  Iteration number: 0 with current cost as 0.32663897328298214 and parameters 
[-1.510412    2.23743457 -2.12427951 -0.11653103  0.55388702 -2.77010903
  3.06858492  2.18960145  1.18552005 -1.06648315  0.60271516  1.14432451
  1.31029899 -1.8735468 ]. 
Training complete taking 781.3202629089355 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 23:24:39 2024]  Iteration number: 0 with current cost as 0.28045946209385286 and parameters 
[ 1.37382131  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960128  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Apr  4 23:27:35 2024]  Iteration number: 0 with current cost as 0.3423991943977621 and parameters 
[-1.67804059  2.23743458 -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960134  1.18551998 -1.0664832   0.6027151   1.14432433
  1.31029893 -1.87354686]. 
Working on 0.6 fold... 
[Thu Apr  4 23:30:16 2024]  Iteration number: 0 with current cost as 0.33601862473788136 and parameters 
[-1.57381367  2.23743464 -2.12427952 -0.11653097  0.55388714 -2.77010897
  3.06858498  2.18960145  1.18552004 -1.06648314  0.60271516  1.14432451
  1.31029904 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 23:32:36 2024]  Iteration number: 0 with current cost as 0.3318226495654909 and parameters 
[-1.63687967  2.2374347  -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960145  1.18551998 -1.06648315  0.60271516  1.14432445
  1.31029905 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 23:34:59 2024]  Iteration number: 0 with current cost as 0.32663897328298214 and parameters 
[-1.510412    2.23743457 -2.12427951 -0.11653103  0.55388702 -2.77010903
  3.06858492  2.18960145  1.18552005 -1.06648315  0.60271516  1.14432451
  1.31029899 -1.8735468 ]. 
Training complete taking 798.9694163799286 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 23:37:58 2024]  Iteration number: 0 with current cost as 0.28045946209385286 and parameters 
[ 1.37382131  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960128  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Apr  4 23:40:59 2024]  Iteration number: 0 with current cost as 0.3423991943977621 and parameters 
[-1.67804059  2.23743458 -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960134  1.18551998 -1.0664832   0.6027151   1.14432433
  1.31029893 -1.87354686]. 
Working on 0.6 fold... 
[Thu Apr  4 23:43:41 2024]  Iteration number: 0 with current cost as 0.33601862473788136 and parameters 
[-1.57381367  2.23743464 -2.12427952 -0.11653097  0.55388714 -2.77010897
  3.06858498  2.18960145  1.18552004 -1.06648314  0.60271516  1.14432451
  1.31029904 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 23:46:10 2024]  Iteration number: 0 with current cost as 0.3318226495654909 and parameters 
[-1.63687967  2.2374347  -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960145  1.18551998 -1.06648315  0.60271516  1.14432445
  1.31029905 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 23:48:31 2024]  Iteration number: 0 with current cost as 0.32663897328298214 and parameters 
[-1.510412    2.23743457 -2.12427951 -0.11653103  0.55388702 -2.77010903
  3.06858492  2.18960145  1.18552005 -1.06648315  0.60271516  1.14432451
  1.31029899 -1.8735468 ]. 
Training complete taking 811.92906498909 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 23:51:29 2024]  Iteration number: 0 with current cost as 0.28045946209385286 and parameters 
[ 1.37382131  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960128  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Apr  4 23:54:28 2024]  Iteration number: 0 with current cost as 0.3423991943977621 and parameters 
[-1.67804059  2.23743458 -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960134  1.18551998 -1.0664832   0.6027151   1.14432433
  1.31029893 -1.87354686]. 
Working on 0.6 fold... 
[Thu Apr  4 23:57:08 2024]  Iteration number: 0 with current cost as 0.33601862473788136 and parameters 
[-1.57381367  2.23743464 -2.12427952 -0.11653097  0.55388714 -2.77010897
  3.06858498  2.18960145  1.18552004 -1.06648314  0.60271516  1.14432451
  1.31029904 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 23:59:35 2024]  Iteration number: 0 with current cost as 0.3318226495654909 and parameters 
[-1.63687967  2.2374347  -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960145  1.18551998 -1.06648315  0.60271516  1.14432445
  1.31029905 -1.8735468 ]. 
Working on 1.0 fold... 
[Fri Apr  5 00:02:07 2024]  Iteration number: 0 with current cost as 0.32663897328298214 and parameters 
[-1.510412    2.23743457 -2.12427951 -0.11653103  0.55388702 -2.77010903
  3.06858492  2.18960145  1.18552005 -1.06648315  0.60271516  1.14432451
  1.31029899 -1.8735468 ]. 
Training complete taking 817.8608129024506 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 00:05:10 2024]  Iteration number: 0 with current cost as 0.28045946209385286 and parameters 
[ 1.37382131  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960128  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Fri Apr  5 00:08:10 2024]  Iteration number: 0 with current cost as 0.3423991943977621 and parameters 
[-1.67804059  2.23743458 -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960134  1.18551998 -1.0664832   0.6027151   1.14432433
  1.31029893 -1.87354686]. 
Working on 0.6 fold... 
[Fri Apr  5 00:10:57 2024]  Iteration number: 0 with current cost as 0.33601862473788136 and parameters 
[-1.57381367  2.23743464 -2.12427952 -0.11653097  0.55388714 -2.77010897
  3.06858498  2.18960145  1.18552004 -1.06648314  0.60271516  1.14432451
  1.31029904 -1.8735468 ]. 
Working on 0.8 fold... 
[Fri Apr  5 00:13:27 2024]  Iteration number: 0 with current cost as 0.3318226495654909 and parameters 
[-1.63687967  2.2374347  -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960145  1.18551998 -1.06648315  0.60271516  1.14432445
  1.31029905 -1.8735468 ]. 
Working on 1.0 fold... 
[Fri Apr  5 00:15:52 2024]  Iteration number: 0 with current cost as 0.32663897328298214 and parameters 
[-1.510412    2.23743457 -2.12427951 -0.11653103  0.55388702 -2.77010903
  3.06858492  2.18960145  1.18552005 -1.06648315  0.60271516  1.14432451
  1.31029899 -1.8735468 ]. 
Training complete taking 824.6132438182831 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 00:18:53 2024]  Iteration number: 0 with current cost as 0.28045946209385286 and parameters 
[ 1.37382131  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960128  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Fri Apr  5 00:21:58 2024]  Iteration number: 0 with current cost as 0.3423991943977621 and parameters 
[-1.67804059  2.23743458 -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960134  1.18551998 -1.0664832   0.6027151   1.14432433
  1.31029893 -1.87354686]. 
Working on 0.6 fold... 
[Fri Apr  5 00:24:46 2024]  Iteration number: 0 with current cost as 0.33601862473788136 and parameters 
[-1.57381367  2.23743464 -2.12427952 -0.11653097  0.55388714 -2.77010897
  3.06858498  2.18960145  1.18552004 -1.06648314  0.60271516  1.14432451
  1.31029904 -1.8735468 ]. 
Working on 0.8 fold... 
[Fri Apr  5 00:27:13 2024]  Iteration number: 0 with current cost as 0.3318226495654909 and parameters 
[-1.63687967  2.2374347  -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960145  1.18551998 -1.06648315  0.60271516  1.14432445
  1.31029905 -1.8735468 ]. 
Working on 1.0 fold... 
[Fri Apr  5 00:29:54 2024]  Iteration number: 0 with current cost as 0.32663897328298214 and parameters 
[-1.510412    2.23743457 -2.12427951 -0.11653103  0.55388702 -2.77010903
  3.06858492  2.18960145  1.18552005 -1.06648315  0.60271516  1.14432451
  1.31029899 -1.8735468 ]. 
Training complete taking 848.8881280422211 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 00:33:07 2024]  Iteration number: 0 with current cost as 0.28045946209385286 and parameters 
[ 1.37382131  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960128  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Fri Apr  5 00:36:11 2024]  Iteration number: 0 with current cost as 0.3423991943977621 and parameters 
[-1.67804059  2.23743458 -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960134  1.18551998 -1.0664832   0.6027151   1.14432433
  1.31029893 -1.87354686]. 
Working on 0.6 fold... 
[Fri Apr  5 00:38:55 2024]  Iteration number: 0 with current cost as 0.33601862473788136 and parameters 
[-1.57381367  2.23743464 -2.12427952 -0.11653097  0.55388714 -2.77010897
  3.06858498  2.18960145  1.18552004 -1.06648314  0.60271516  1.14432451
  1.31029904 -1.8735468 ]. 
Working on 0.8 fold... 
[Fri Apr  5 00:41:26 2024]  Iteration number: 0 with current cost as 0.3318226495654909 and parameters 
[-1.63687967  2.2374347  -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960145  1.18551998 -1.06648315  0.60271516  1.14432445
  1.31029905 -1.8735468 ]. 
Working on 1.0 fold... 
[Fri Apr  5 00:44:01 2024]  Iteration number: 0 with current cost as 0.32663897328298214 and parameters 
[-1.510412    2.23743457 -2.12427951 -0.11653103  0.55388702 -2.77010903
  3.06858492  2.18960145  1.18552005 -1.06648315  0.60271516  1.14432451
  1.31029899 -1.8735468 ]. 
Training complete taking 847.8731880187988 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 00:47:12 2024]  Iteration number: 0 with current cost as 0.28045946209385286 and parameters 
[ 1.37382131  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960128  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Fri Apr  5 00:50:22 2024]  Iteration number: 0 with current cost as 0.3423991943977621 and parameters 
[-1.67804059  2.23743458 -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960134  1.18551998 -1.0664832   0.6027151   1.14432433
  1.31029893 -1.87354686]. 
Working on 0.6 fold... 
[Fri Apr  5 00:53:06 2024]  Iteration number: 0 with current cost as 0.33601862473788136 and parameters 
[-1.57381367  2.23743464 -2.12427952 -0.11653097  0.55388714 -2.77010897
  3.06858498  2.18960145  1.18552004 -1.06648314  0.60271516  1.14432451
  1.31029904 -1.8735468 ]. 
Working on 0.8 fold... 
[Fri Apr  5 00:55:45 2024]  Iteration number: 0 with current cost as 0.3318226495654909 and parameters 
[-1.63687967  2.2374347  -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960145  1.18551998 -1.06648315  0.60271516  1.14432445
  1.31029905 -1.8735468 ]. 
Working on 1.0 fold... 
[Fri Apr  5 00:58:20 2024]  Iteration number: 0 with current cost as 0.32663897328298214 and parameters 
[-1.510412    2.23743457 -2.12427951 -0.11653103  0.55388702 -2.77010903
  3.06858492  2.18960145  1.18552005 -1.06648315  0.60271516  1.14432451
  1.31029899 -1.8735468 ]. 
Training complete taking 850.7316133975983 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 01:01:24 2024]  Iteration number: 0 with current cost as 0.28045946209385286 and parameters 
[ 1.37382131  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960128  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Fri Apr  5 01:04:26 2024]  Iteration number: 0 with current cost as 0.3423991943977621 and parameters 
[-1.67804059  2.23743458 -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960134  1.18551998 -1.0664832   0.6027151   1.14432433
  1.31029893 -1.87354686]. 
Working on 0.6 fold... 
[Fri Apr  5 01:07:11 2024]  Iteration number: 0 with current cost as 0.33601862473788136 and parameters 
[-1.57381367  2.23743464 -2.12427952 -0.11653097  0.55388714 -2.77010897
  3.06858498  2.18960145  1.18552004 -1.06648314  0.60271516  1.14432451
  1.31029904 -1.8735468 ]. 
Working on 0.8 fold... 
[Fri Apr  5 01:09:34 2024]  Iteration number: 0 with current cost as 0.3318226495654909 and parameters 
[-1.63687967  2.2374347  -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960145  1.18551998 -1.06648315  0.60271516  1.14432445
  1.31029905 -1.8735468 ]. 
Working on 1.0 fold... 
[Fri Apr  5 01:11:55 2024]  Iteration number: 0 with current cost as 0.32663897328298214 and parameters 
[-1.510412    2.23743457 -2.12427951 -0.11653103  0.55388702 -2.77010903
  3.06858492  2.18960145  1.18552005 -1.06648315  0.60271516  1.14432451
  1.31029899 -1.8735468 ]. 
Training complete taking 814.6826295852661 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 01:14:48 2024]  Iteration number: 0 with current cost as 0.28045946209385286 and parameters 
[ 1.37382131  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960128  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Fri Apr  5 01:17:46 2024]  Iteration number: 0 with current cost as 0.3423991943977621 and parameters 
[-1.67804059  2.23743458 -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960134  1.18551998 -1.0664832   0.6027151   1.14432433
  1.31029893 -1.87354686]. 
Working on 0.6 fold... 
[Fri Apr  5 01:20:26 2024]  Iteration number: 0 with current cost as 0.33601862473788136 and parameters 
[-1.57381367  2.23743464 -2.12427952 -0.11653097  0.55388714 -2.77010897
  3.06858498  2.18960145  1.18552004 -1.06648314  0.60271516  1.14432451
  1.31029904 -1.8735468 ]. 
Working on 0.8 fold... 
[Fri Apr  5 01:23:00 2024]  Iteration number: 0 with current cost as 0.3318226495654909 and parameters 
[-1.63687967  2.2374347  -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960145  1.18551998 -1.06648315  0.60271516  1.14432445
  1.31029905 -1.8735468 ]. 
Working on 1.0 fold... 
[Fri Apr  5 01:25:23 2024]  Iteration number: 0 with current cost as 0.32663897328298214 and parameters 
[-1.510412    2.23743457 -2.12427951 -0.11653103  0.55388702 -2.77010903
  3.06858492  2.18960145  1.18552005 -1.06648315  0.60271516  1.14432451
  1.31029899 -1.8735468 ]. 
Training complete taking 808.932519197464 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 01:28:30 2024]  Iteration number: 0 with current cost as 0.28045946209385286 and parameters 
[ 1.37382131  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960128  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Fri Apr  5 01:31:35 2024]  Iteration number: 0 with current cost as 0.3423991943977621 and parameters 
[-1.67804059  2.23743458 -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960134  1.18551998 -1.0664832   0.6027151   1.14432433
  1.31029893 -1.87354686]. 
Working on 0.6 fold... 
[Fri Apr  5 01:34:18 2024]  Iteration number: 0 with current cost as 0.33601862473788136 and parameters 
[-1.57381367  2.23743464 -2.12427952 -0.11653097  0.55388714 -2.77010897
  3.06858498  2.18960145  1.18552004 -1.06648314  0.60271516  1.14432451
  1.31029904 -1.8735468 ]. 
Working on 0.8 fold... 
[Fri Apr  5 01:36:51 2024]  Iteration number: 0 with current cost as 0.3318226495654909 and parameters 
[-1.63687967  2.2374347  -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960145  1.18551998 -1.06648315  0.60271516  1.14432445
  1.31029905 -1.8735468 ]. 
Working on 1.0 fold... 
[Fri Apr  5 01:39:23 2024]  Iteration number: 0 with current cost as 0.32663897328298214 and parameters 
[-1.510412    2.23743457 -2.12427951 -0.11653103  0.55388702 -2.77010903
  3.06858492  2.18960145  1.18552005 -1.06648315  0.60271516  1.14432451
  1.31029899 -1.8735468 ]. 
Training complete taking 839.8176863193512 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 01:42:40 2024]  Iteration number: 0 with current cost as 0.28045946209385286 and parameters 
[ 1.37382131  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960128  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Fri Apr  5 01:45:54 2024]  Iteration number: 0 with current cost as 0.3423991943977621 and parameters 
[-1.67804059  2.23743458 -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960134  1.18551998 -1.0664832   0.6027151   1.14432433
  1.31029893 -1.87354686]. 
Working on 0.6 fold... 
[Fri Apr  5 01:48:39 2024]  Iteration number: 0 with current cost as 0.33601862473788136 and parameters 
[-1.57381367  2.23743464 -2.12427952 -0.11653097  0.55388714 -2.77010897
  3.06858498  2.18960145  1.18552004 -1.06648314  0.60271516  1.14432451
  1.31029904 -1.8735468 ]. 
Working on 0.8 fold... 
[Fri Apr  5 01:51:04 2024]  Iteration number: 0 with current cost as 0.3318226495654909 and parameters 
[-1.63687967  2.2374347  -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960145  1.18551998 -1.06648315  0.60271516  1.14432445
  1.31029905 -1.8735468 ]. 
Working on 1.0 fold... 
[Fri Apr  5 01:53:29 2024]  Iteration number: 0 with current cost as 0.32663897328298214 and parameters 
[-1.510412    2.23743457 -2.12427951 -0.11653103  0.55388702 -2.77010903
  3.06858492  2.18960145  1.18552005 -1.06648315  0.60271516  1.14432451
  1.31029899 -1.8735468 ]. 
Training complete taking 844.7649672031403 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 01:56:26 2024]  Iteration number: 0 with current cost as 0.28045946209385286 and parameters 
[ 1.37382131  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960128  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Fri Apr  5 01:59:31 2024]  Iteration number: 0 with current cost as 0.3423991943977621 and parameters 
[-1.67804059  2.23743458 -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960134  1.18551998 -1.0664832   0.6027151   1.14432433
  1.31029893 -1.87354686]. 
Working on 0.6 fold... 
[Fri Apr  5 02:02:11 2024]  Iteration number: 0 with current cost as 0.33601862473788136 and parameters 
[-1.57381367  2.23743464 -2.12427952 -0.11653097  0.55388714 -2.77010897
  3.06858498  2.18960145  1.18552004 -1.06648314  0.60271516  1.14432451
  1.31029904 -1.8735468 ]. 
Working on 0.8 fold... 
[Fri Apr  5 02:04:35 2024]  Iteration number: 0 with current cost as 0.3318226495654909 and parameters 
[-1.63687967  2.2374347  -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960145  1.18551998 -1.06648315  0.60271516  1.14432445
  1.31029905 -1.8735468 ]. 
Working on 1.0 fold... 
[Fri Apr  5 02:06:56 2024]  Iteration number: 0 with current cost as 0.32663897328298214 and parameters 
[-1.510412    2.23743457 -2.12427951 -0.11653103  0.55388702 -2.77010903
  3.06858492  2.18960145  1.18552005 -1.06648315  0.60271516  1.14432451
  1.31029899 -1.8735468 ]. 
Training complete taking 807.7779579162598 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 02:09:57 2024]  Iteration number: 0 with current cost as 0.28045946209385286 and parameters 
[ 1.37382131  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960128  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Fri Apr  5 02:12:54 2024]  Iteration number: 0 with current cost as 0.3423991943977621 and parameters 
[-1.67804059  2.23743458 -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960134  1.18551998 -1.0664832   0.6027151   1.14432433
  1.31029893 -1.87354686]. 
Working on 0.6 fold... 
[Fri Apr  5 02:15:34 2024]  Iteration number: 0 with current cost as 0.33601862473788136 and parameters 
[-1.57381367  2.23743464 -2.12427952 -0.11653097  0.55388714 -2.77010897
  3.06858498  2.18960145  1.18552004 -1.06648314  0.60271516  1.14432451
  1.31029904 -1.8735468 ]. 
Working on 0.8 fold... 
[Fri Apr  5 02:17:54 2024]  Iteration number: 0 with current cost as 0.3318226495654909 and parameters 
[-1.63687967  2.2374347  -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960145  1.18551998 -1.06648315  0.60271516  1.14432445
  1.31029905 -1.8735468 ]. 
Working on 1.0 fold... 
[Fri Apr  5 02:20:13 2024]  Iteration number: 0 with current cost as 0.32663897328298214 and parameters 
[-1.510412    2.23743457 -2.12427951 -0.11653103  0.55388702 -2.77010903
  3.06858492  2.18960145  1.18552005 -1.06648315  0.60271516  1.14432451
  1.31029899 -1.8735468 ]. 
Training complete taking 795.1090025901794 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 02:23:08 2024]  Iteration number: 0 with current cost as 0.28045946209385286 and parameters 
[ 1.37382131  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960128  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Fri Apr  5 02:26:07 2024]  Iteration number: 0 with current cost as 0.3423991943977621 and parameters 
[-1.67804059  2.23743458 -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960134  1.18551998 -1.0664832   0.6027151   1.14432433
  1.31029893 -1.87354686]. 
Working on 0.6 fold... 
[Fri Apr  5 02:28:42 2024]  Iteration number: 0 with current cost as 0.33601862473788136 and parameters 
[-1.57381367  2.23743464 -2.12427952 -0.11653097  0.55388714 -2.77010897
  3.06858498  2.18960145  1.18552004 -1.06648314  0.60271516  1.14432451
  1.31029904 -1.8735468 ]. 
Working on 0.8 fold... 
[Fri Apr  5 02:31:01 2024]  Iteration number: 0 with current cost as 0.3318226495654909 and parameters 
[-1.63687967  2.2374347  -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960145  1.18551998 -1.06648315  0.60271516  1.14432445
  1.31029905 -1.8735468 ]. 
Working on 1.0 fold... 
[Fri Apr  5 02:33:17 2024]  Iteration number: 0 with current cost as 0.32663897328298214 and parameters 
[-1.510412    2.23743457 -2.12427951 -0.11653103  0.55388702 -2.77010903
  3.06858492  2.18960145  1.18552005 -1.06648315  0.60271516  1.14432451
  1.31029899 -1.8735468 ]. 
Training complete taking 785.8932642936707 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 02:36:12 2024]  Iteration number: 0 with current cost as 0.28045946209385286 and parameters 
[ 1.37382131  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960128  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Fri Apr  5 02:39:09 2024]  Iteration number: 0 with current cost as 0.3423991943977621 and parameters 
[-1.67804059  2.23743458 -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960134  1.18551998 -1.0664832   0.6027151   1.14432433
  1.31029893 -1.87354686]. 
Working on 0.6 fold... 
[Fri Apr  5 02:41:45 2024]  Iteration number: 0 with current cost as 0.33601862473788136 and parameters 
[-1.57381367  2.23743464 -2.12427952 -0.11653097  0.55388714 -2.77010897
  3.06858498  2.18960145  1.18552004 -1.06648314  0.60271516  1.14432451
  1.31029904 -1.8735468 ]. 
Working on 0.8 fold... 
[Fri Apr  5 02:44:06 2024]  Iteration number: 0 with current cost as 0.3318226495654909 and parameters 
[-1.63687967  2.2374347  -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960145  1.18551998 -1.06648315  0.60271516  1.14432445
  1.31029905 -1.8735468 ]. 
Working on 1.0 fold... 
[Fri Apr  5 02:46:27 2024]  Iteration number: 0 with current cost as 0.32663897328298214 and parameters 
[-1.510412    2.23743457 -2.12427951 -0.11653103  0.55388702 -2.77010903
  3.06858492  2.18960145  1.18552005 -1.06648315  0.60271516  1.14432451
  1.31029899 -1.8735468 ]. 
Training complete taking 789.1620044708252 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 02:49:26 2024]  Iteration number: 0 with current cost as 0.28045946209385286 and parameters 
[ 1.37382131  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960128  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Fri Apr  5 02:52:18 2024]  Iteration number: 0 with current cost as 0.3423991943977621 and parameters 
[-1.67804059  2.23743458 -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960134  1.18551998 -1.0664832   0.6027151   1.14432433
  1.31029893 -1.87354686]. 
Working on 0.6 fold... 
[Fri Apr  5 02:54:57 2024]  Iteration number: 0 with current cost as 0.33601862473788136 and parameters 
[-1.57381367  2.23743464 -2.12427952 -0.11653097  0.55388714 -2.77010897
  3.06858498  2.18960145  1.18552004 -1.06648314  0.60271516  1.14432451
  1.31029904 -1.8735468 ]. 
Working on 0.8 fold... 
[Fri Apr  5 02:57:19 2024]  Iteration number: 0 with current cost as 0.3318226495654909 and parameters 
[-1.63687967  2.2374347  -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960145  1.18551998 -1.06648315  0.60271516  1.14432445
  1.31029905 -1.8735468 ]. 
Working on 1.0 fold... 
[Fri Apr  5 02:59:39 2024]  Iteration number: 0 with current cost as 0.32663897328298214 and parameters 
[-1.510412    2.23743457 -2.12427951 -0.11653103  0.55388702 -2.77010903
  3.06858492  2.18960145  1.18552005 -1.06648315  0.60271516  1.14432451
  1.31029899 -1.8735468 ]. 
Training complete taking 794.3535795211792 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 03:02:38 2024]  Iteration number: 0 with current cost as 0.28045946209385286 and parameters 
[ 1.37382131  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960128  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Fri Apr  5 03:05:36 2024]  Iteration number: 0 with current cost as 0.3423991943977621 and parameters 
[-1.67804059  2.23743458 -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960134  1.18551998 -1.0664832   0.6027151   1.14432433
  1.31029893 -1.87354686]. 
Working on 0.6 fold... 
[Fri Apr  5 03:08:21 2024]  Iteration number: 0 with current cost as 0.33601862473788136 and parameters 
[-1.57381367  2.23743464 -2.12427952 -0.11653097  0.55388714 -2.77010897
  3.06858498  2.18960145  1.18552004 -1.06648314  0.60271516  1.14432451
  1.31029904 -1.8735468 ]. 
Working on 0.8 fold... 
[Fri Apr  5 03:10:40 2024]  Iteration number: 0 with current cost as 0.3318226495654909 and parameters 
[-1.63687967  2.2374347  -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960145  1.18551998 -1.06648315  0.60271516  1.14432445
  1.31029905 -1.8735468 ]. 
Working on 1.0 fold... 
[Fri Apr  5 03:13:04 2024]  Iteration number: 0 with current cost as 0.32663897328298214 and parameters 
[-1.510412    2.23743457 -2.12427951 -0.11653103  0.55388702 -2.77010903
  3.06858492  2.18960145  1.18552005 -1.06648315  0.60271516  1.14432451
  1.31029899 -1.8735468 ]. 
Training complete taking 799.5776858329773 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 03:15:55 2024]  Iteration number: 0 with current cost as 0.28045946209385286 and parameters 
[ 1.37382131  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960128  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Fri Apr  5 03:18:59 2024]  Iteration number: 0 with current cost as 0.3423991943977621 and parameters 
[-1.67804059  2.23743458 -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960134  1.18551998 -1.0664832   0.6027151   1.14432433
  1.31029893 -1.87354686]. 
Working on 0.6 fold... 
[Fri Apr  5 03:21:33 2024]  Iteration number: 0 with current cost as 0.33601862473788136 and parameters 
[-1.57381367  2.23743464 -2.12427952 -0.11653097  0.55388714 -2.77010897
  3.06858498  2.18960145  1.18552004 -1.06648314  0.60271516  1.14432451
  1.31029904 -1.8735468 ]. 
Working on 0.8 fold... 
[Fri Apr  5 03:23:56 2024]  Iteration number: 0 with current cost as 0.3318226495654909 and parameters 
[-1.63687967  2.2374347  -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960145  1.18551998 -1.06648315  0.60271516  1.14432445
  1.31029905 -1.8735468 ]. 
Working on 1.0 fold... 
[Fri Apr  5 03:26:22 2024]  Iteration number: 0 with current cost as 0.32663897328298214 and parameters 
[-1.510412    2.23743457 -2.12427951 -0.11653103  0.55388702 -2.77010903
  3.06858492  2.18960145  1.18552005 -1.06648315  0.60271516  1.14432451
  1.31029899 -1.8735468 ]. 
Training complete taking 802.7987375259399 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 03:29:22 2024]  Iteration number: 0 with current cost as 0.28045946209385286 and parameters 
[ 1.37382131  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960128  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Fri Apr  5 03:32:15 2024]  Iteration number: 0 with current cost as 0.3423991943977621 and parameters 
[-1.67804059  2.23743458 -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960134  1.18551998 -1.0664832   0.6027151   1.14432433
  1.31029893 -1.87354686]. 
Working on 0.6 fold... 
[Fri Apr  5 03:34:54 2024]  Iteration number: 0 with current cost as 0.33601862473788136 and parameters 
[-1.57381367  2.23743464 -2.12427952 -0.11653097  0.55388714 -2.77010897
  3.06858498  2.18960145  1.18552004 -1.06648314  0.60271516  1.14432451
  1.31029904 -1.8735468 ]. 
Working on 0.8 fold... 
[Fri Apr  5 03:37:14 2024]  Iteration number: 0 with current cost as 0.3318226495654909 and parameters 
[-1.63687967  2.2374347  -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960145  1.18551998 -1.06648315  0.60271516  1.14432445
  1.31029905 -1.8735468 ]. 
Working on 1.0 fold... 
[Fri Apr  5 03:39:31 2024]  Iteration number: 0 with current cost as 0.32663897328298214 and parameters 
[-1.510412    2.23743457 -2.12427951 -0.11653103  0.55388702 -2.77010903
  3.06858492  2.18960145  1.18552005 -1.06648315  0.60271516  1.14432451
  1.31029899 -1.8735468 ]. 
Training complete taking 785.457973241806 seconds. 
Discarding model... 

Training complete taking 20211.688472747803 total seconds. 
Now scoring model... 
Scoring complete taking 2.044157028198242 seconds. 
Saved predicted values as A2-A2-CNOT_Efficient-CRZ_predicted_values.csv
Model scores: {'MSE_train': (0.5573102822785987,), 'R2_train': -0.10973597278937608, 'MAE_train': 0.6369679395489956, 'MSE_test': 0.6076453798763491, 'R2_test': -0.133237335728144, 'MAE_test': 0.6974916280833723}. 
Saved model results as A2-A2-CNOT_Efficient-CRZ_results.json. 
