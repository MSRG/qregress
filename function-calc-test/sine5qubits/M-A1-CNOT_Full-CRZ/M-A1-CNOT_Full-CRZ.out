/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /scratch/j/jacobsen/gjones/sine5qubits/sine_train.bin... 
Successfully loaded /scratch/j/jacobsen/gjones/sine5qubits/sine_train.bin into X and y data. 
Loading dataset from /scratch/j/jacobsen/gjones/sine5qubits/sine_test.bin... 
Successfully loaded /scratch/j/jacobsen/gjones/sine5qubits/sine_test.bin into X and y data. 
Training model with dataset /scratch/j/jacobsen/gjones/sine5qubits/sine_train.bin 
 at time Thu Apr  4 21:37:45 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 21:41:40 2024]  Iteration number: 0 with current cost as 0.6016901184184594 and parameters 
[-0.18336986  2.23743448 -2.12427948 -0.11653103  0.55388708 -2.77010929
  3.06858483  2.1896013   1.18551983 -1.06648355  0.6027151   1.14432445
  1.31029867 -1.8735468   0.72965065  2.88578388 -0.54534366 -0.47522485
 -2.02654256  0.7289737   1.60512648  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856958 -0.67550787 -2.69002217]. 
Working on 0.4 fold... 
[Thu Apr  4 21:51:27 2024]  Iteration number: 0 with current cost as 0.3672680875242503 and parameters 
[-1.3683026   2.23743464 -2.12427938 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522472
 -2.02654253  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 21:59:11 2024]  Iteration number: 0 with current cost as 0.3708577333667045 and parameters 
[-1.45052441  2.23743442 -2.12427953 -0.11653103  0.55388697 -2.77010919
  3.06858487  2.18960134  1.18551998 -1.0664833   0.60271499  1.14432434
  1.31029888 -1.87354691  0.72965059  2.88578398 -0.54534357 -0.47522485
 -2.02654262  0.72897359  1.60512653  2.83077085 -1.2645672  -0.25136115
 -2.39279229 -2.27309785  3.13337144  2.54856948 -0.67550798 -2.69002212]. 
Working on 0.8 fold... 
[Thu Apr  4 22:07:59 2024]  Iteration number: 0 with current cost as 0.3404778404115016 and parameters 
[-1.43259807  2.23743464 -2.12427953 -0.11653103  0.55388708 -2.77010908
  3.06858498  2.18960124  1.18551998 -1.0664833   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296507   2.88578408 -0.54534357 -0.47522485
 -2.02654251  0.7289737   1.60512653  2.83077085 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 22:16:58 2024]  Iteration number: 0 with current cost as 0.35470298755274515 and parameters 
[-1.42087438  2.23743464 -2.12427946 -0.11653094  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552016 -1.06648308  0.6027151   1.14432454
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522467
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2530.1280307769775 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 22:23:37 2024]  Iteration number: 0 with current cost as 0.6016901184184594 and parameters 
[-0.18336986  2.23743448 -2.12427948 -0.11653103  0.55388708 -2.77010929
  3.06858483  2.1896013   1.18551983 -1.06648355  0.6027151   1.14432445
  1.31029867 -1.8735468   0.72965065  2.88578388 -0.54534366 -0.47522485
 -2.02654256  0.7289737   1.60512648  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856958 -0.67550787 -2.69002217]. 
Working on 0.4 fold... 
[Thu Apr  4 22:32:29 2024]  Iteration number: 0 with current cost as 0.3672680875242503 and parameters 
[-1.3683026   2.23743464 -2.12427938 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522472
 -2.02654253  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 22:39:42 2024]  Iteration number: 0 with current cost as 0.3708577333667045 and parameters 
[-1.45052441  2.23743442 -2.12427953 -0.11653103  0.55388697 -2.77010919
  3.06858487  2.18960134  1.18551998 -1.0664833   0.60271499  1.14432434
  1.31029888 -1.87354691  0.72965059  2.88578398 -0.54534357 -0.47522485
 -2.02654262  0.72897359  1.60512653  2.83077085 -1.2645672  -0.25136115
 -2.39279229 -2.27309785  3.13337144  2.54856948 -0.67550798 -2.69002212]. 
Working on 0.8 fold... 
[Thu Apr  4 22:48:11 2024]  Iteration number: 0 with current cost as 0.3404778404115016 and parameters 
[-1.43259807  2.23743464 -2.12427953 -0.11653103  0.55388708 -2.77010908
  3.06858498  2.18960124  1.18551998 -1.0664833   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296507   2.88578408 -0.54534357 -0.47522485
 -2.02654251  0.7289737   1.60512653  2.83077085 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 22:56:42 2024]  Iteration number: 0 with current cost as 0.35470298755274515 and parameters 
[-1.42087438  2.23743464 -2.12427946 -0.11653094  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552016 -1.06648308  0.6027151   1.14432454
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522467
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2369.275288105011 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 23:03:07 2024]  Iteration number: 0 with current cost as 0.6016901184184594 and parameters 
[-0.18336986  2.23743448 -2.12427948 -0.11653103  0.55388708 -2.77010929
  3.06858483  2.1896013   1.18551983 -1.06648355  0.6027151   1.14432445
  1.31029867 -1.8735468   0.72965065  2.88578388 -0.54534366 -0.47522485
 -2.02654256  0.7289737   1.60512648  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856958 -0.67550787 -2.69002217]. 
Working on 0.4 fold... 
[Thu Apr  4 23:12:32 2024]  Iteration number: 0 with current cost as 0.3672680875242503 and parameters 
[-1.3683026   2.23743464 -2.12427938 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522472
 -2.02654253  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 23:20:24 2024]  Iteration number: 0 with current cost as 0.3708577333667045 and parameters 
[-1.45052441  2.23743442 -2.12427953 -0.11653103  0.55388697 -2.77010919
  3.06858487  2.18960134  1.18551998 -1.0664833   0.60271499  1.14432434
  1.31029888 -1.87354691  0.72965059  2.88578398 -0.54534357 -0.47522485
 -2.02654262  0.72897359  1.60512653  2.83077085 -1.2645672  -0.25136115
 -2.39279229 -2.27309785  3.13337144  2.54856948 -0.67550798 -2.69002212]. 
Working on 0.8 fold... 
[Thu Apr  4 23:29:24 2024]  Iteration number: 0 with current cost as 0.3404778404115016 and parameters 
[-1.43259807  2.23743464 -2.12427953 -0.11653103  0.55388708 -2.77010908
  3.06858498  2.18960124  1.18551998 -1.0664833   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296507   2.88578408 -0.54534357 -0.47522485
 -2.02654251  0.7289737   1.60512653  2.83077085 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 23:38:20 2024]  Iteration number: 0 with current cost as 0.35470298755274515 and parameters 
[-1.42087438  2.23743464 -2.12427946 -0.11653094  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552016 -1.06648308  0.6027151   1.14432454
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522467
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2505.1107952594757 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 23:45:02 2024]  Iteration number: 0 with current cost as 0.6016901184184594 and parameters 
[-0.18336986  2.23743448 -2.12427948 -0.11653103  0.55388708 -2.77010929
  3.06858483  2.1896013   1.18551983 -1.06648355  0.6027151   1.14432445
  1.31029867 -1.8735468   0.72965065  2.88578388 -0.54534366 -0.47522485
 -2.02654256  0.7289737   1.60512648  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856958 -0.67550787 -2.69002217]. 
Working on 0.4 fold... 
[Thu Apr  4 23:54:48 2024]  Iteration number: 0 with current cost as 0.3672680875242503 and parameters 
[-1.3683026   2.23743464 -2.12427938 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522472
 -2.02654253  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 00:02:41 2024]  Iteration number: 0 with current cost as 0.3708577333667045 and parameters 
[-1.45052441  2.23743442 -2.12427953 -0.11653103  0.55388697 -2.77010919
  3.06858487  2.18960134  1.18551998 -1.0664833   0.60271499  1.14432434
  1.31029888 -1.87354691  0.72965059  2.88578398 -0.54534357 -0.47522485
 -2.02654262  0.72897359  1.60512653  2.83077085 -1.2645672  -0.25136115
 -2.39279229 -2.27309785  3.13337144  2.54856948 -0.67550798 -2.69002212]. 
Working on 0.8 fold... 
[Fri Apr  5 00:11:41 2024]  Iteration number: 0 with current cost as 0.3404778404115016 and parameters 
[-1.43259807  2.23743464 -2.12427953 -0.11653103  0.55388708 -2.77010908
  3.06858498  2.18960124  1.18551998 -1.0664833   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296507   2.88578408 -0.54534357 -0.47522485
 -2.02654251  0.7289737   1.60512653  2.83077085 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 00:20:25 2024]  Iteration number: 0 with current cost as 0.35470298755274515 and parameters 
[-1.42087438  2.23743464 -2.12427946 -0.11653094  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552016 -1.06648308  0.6027151   1.14432454
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522467
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2536.3794662952423 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 00:27:23 2024]  Iteration number: 0 with current cost as 0.6016901184184594 and parameters 
[-0.18336986  2.23743448 -2.12427948 -0.11653103  0.55388708 -2.77010929
  3.06858483  2.1896013   1.18551983 -1.06648355  0.6027151   1.14432445
  1.31029867 -1.8735468   0.72965065  2.88578388 -0.54534366 -0.47522485
 -2.02654256  0.7289737   1.60512648  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856958 -0.67550787 -2.69002217]. 
Working on 0.4 fold... 
[Fri Apr  5 00:37:17 2024]  Iteration number: 0 with current cost as 0.3672680875242503 and parameters 
[-1.3683026   2.23743464 -2.12427938 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522472
 -2.02654253  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 00:45:03 2024]  Iteration number: 0 with current cost as 0.3708577333667045 and parameters 
[-1.45052441  2.23743442 -2.12427953 -0.11653103  0.55388697 -2.77010919
  3.06858487  2.18960134  1.18551998 -1.0664833   0.60271499  1.14432434
  1.31029888 -1.87354691  0.72965059  2.88578398 -0.54534357 -0.47522485
 -2.02654262  0.72897359  1.60512653  2.83077085 -1.2645672  -0.25136115
 -2.39279229 -2.27309785  3.13337144  2.54856948 -0.67550798 -2.69002212]. 
Working on 0.8 fold... 
[Fri Apr  5 00:53:36 2024]  Iteration number: 0 with current cost as 0.3404778404115016 and parameters 
[-1.43259807  2.23743464 -2.12427953 -0.11653103  0.55388708 -2.77010908
  3.06858498  2.18960124  1.18551998 -1.0664833   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296507   2.88578408 -0.54534357 -0.47522485
 -2.02654251  0.7289737   1.60512653  2.83077085 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 01:02:26 2024]  Iteration number: 0 with current cost as 0.35470298755274515 and parameters 
[-1.42087438  2.23743464 -2.12427946 -0.11653094  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552016 -1.06648308  0.6027151   1.14432454
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522467
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2520.446865797043 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 01:09:08 2024]  Iteration number: 0 with current cost as 0.6016901184184594 and parameters 
[-0.18336986  2.23743448 -2.12427948 -0.11653103  0.55388708 -2.77010929
  3.06858483  2.1896013   1.18551983 -1.06648355  0.6027151   1.14432445
  1.31029867 -1.8735468   0.72965065  2.88578388 -0.54534366 -0.47522485
 -2.02654256  0.7289737   1.60512648  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856958 -0.67550787 -2.69002217]. 
Working on 0.4 fold... 
[Fri Apr  5 01:19:04 2024]  Iteration number: 0 with current cost as 0.3672680875242503 and parameters 
[-1.3683026   2.23743464 -2.12427938 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522472
 -2.02654253  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 01:27:16 2024]  Iteration number: 0 with current cost as 0.3708577333667045 and parameters 
[-1.45052441  2.23743442 -2.12427953 -0.11653103  0.55388697 -2.77010919
  3.06858487  2.18960134  1.18551998 -1.0664833   0.60271499  1.14432434
  1.31029888 -1.87354691  0.72965059  2.88578398 -0.54534357 -0.47522485
 -2.02654262  0.72897359  1.60512653  2.83077085 -1.2645672  -0.25136115
 -2.39279229 -2.27309785  3.13337144  2.54856948 -0.67550798 -2.69002212]. 
Working on 0.8 fold... 
[Fri Apr  5 01:36:13 2024]  Iteration number: 0 with current cost as 0.3404778404115016 and parameters 
[-1.43259807  2.23743464 -2.12427953 -0.11653103  0.55388708 -2.77010908
  3.06858498  2.18960124  1.18551998 -1.0664833   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296507   2.88578408 -0.54534357 -0.47522485
 -2.02654251  0.7289737   1.60512653  2.83077085 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 01:44:55 2024]  Iteration number: 0 with current cost as 0.35470298755274515 and parameters 
[-1.42087438  2.23743464 -2.12427946 -0.11653094  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552016 -1.06648308  0.6027151   1.14432454
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522467
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2552.744521379471 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 01:52:02 2024]  Iteration number: 0 with current cost as 0.6016901184184594 and parameters 
[-0.18336986  2.23743448 -2.12427948 -0.11653103  0.55388708 -2.77010929
  3.06858483  2.1896013   1.18551983 -1.06648355  0.6027151   1.14432445
  1.31029867 -1.8735468   0.72965065  2.88578388 -0.54534366 -0.47522485
 -2.02654256  0.7289737   1.60512648  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856958 -0.67550787 -2.69002217]. 
Working on 0.4 fold... 
[Fri Apr  5 02:02:03 2024]  Iteration number: 0 with current cost as 0.3672680875242503 and parameters 
[-1.3683026   2.23743464 -2.12427938 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522472
 -2.02654253  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 02:10:19 2024]  Iteration number: 0 with current cost as 0.3708577333667045 and parameters 
[-1.45052441  2.23743442 -2.12427953 -0.11653103  0.55388697 -2.77010919
  3.06858487  2.18960134  1.18551998 -1.0664833   0.60271499  1.14432434
  1.31029888 -1.87354691  0.72965059  2.88578398 -0.54534357 -0.47522485
 -2.02654262  0.72897359  1.60512653  2.83077085 -1.2645672  -0.25136115
 -2.39279229 -2.27309785  3.13337144  2.54856948 -0.67550798 -2.69002212]. 
Working on 0.8 fold... 
[Fri Apr  5 02:19:15 2024]  Iteration number: 0 with current cost as 0.3404778404115016 and parameters 
[-1.43259807  2.23743464 -2.12427953 -0.11653103  0.55388708 -2.77010908
  3.06858498  2.18960124  1.18551998 -1.0664833   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296507   2.88578408 -0.54534357 -0.47522485
 -2.02654251  0.7289737   1.60512653  2.83077085 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 02:28:00 2024]  Iteration number: 0 with current cost as 0.35470298755274515 and parameters 
[-1.42087438  2.23743464 -2.12427946 -0.11653094  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552016 -1.06648308  0.6027151   1.14432454
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522467
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2578.146980047226 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 02:34:58 2024]  Iteration number: 0 with current cost as 0.6016901184184594 and parameters 
[-0.18336986  2.23743448 -2.12427948 -0.11653103  0.55388708 -2.77010929
  3.06858483  2.1896013   1.18551983 -1.06648355  0.6027151   1.14432445
  1.31029867 -1.8735468   0.72965065  2.88578388 -0.54534366 -0.47522485
 -2.02654256  0.7289737   1.60512648  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856958 -0.67550787 -2.69002217]. 
Working on 0.4 fold... 
[Fri Apr  5 02:44:44 2024]  Iteration number: 0 with current cost as 0.3672680875242503 and parameters 
[-1.3683026   2.23743464 -2.12427938 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522472
 -2.02654253  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 02:52:23 2024]  Iteration number: 0 with current cost as 0.3708577333667045 and parameters 
[-1.45052441  2.23743442 -2.12427953 -0.11653103  0.55388697 -2.77010919
  3.06858487  2.18960134  1.18551998 -1.0664833   0.60271499  1.14432434
  1.31029888 -1.87354691  0.72965059  2.88578398 -0.54534357 -0.47522485
 -2.02654262  0.72897359  1.60512653  2.83077085 -1.2645672  -0.25136115
 -2.39279229 -2.27309785  3.13337144  2.54856948 -0.67550798 -2.69002212]. 
Working on 0.8 fold... 
[Fri Apr  5 03:01:21 2024]  Iteration number: 0 with current cost as 0.3404778404115016 and parameters 
[-1.43259807  2.23743464 -2.12427953 -0.11653103  0.55388708 -2.77010908
  3.06858498  2.18960124  1.18551998 -1.0664833   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296507   2.88578408 -0.54534357 -0.47522485
 -2.02654251  0.7289737   1.60512653  2.83077085 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 03:10:26 2024]  Iteration number: 0 with current cost as 0.35470298755274515 and parameters 
[-1.42087438  2.23743464 -2.12427946 -0.11653094  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552016 -1.06648308  0.6027151   1.14432454
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522467
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2543.9022884368896 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 03:17:20 2024]  Iteration number: 0 with current cost as 0.6016901184184594 and parameters 
[-0.18336986  2.23743448 -2.12427948 -0.11653103  0.55388708 -2.77010929
  3.06858483  2.1896013   1.18551983 -1.06648355  0.6027151   1.14432445
  1.31029867 -1.8735468   0.72965065  2.88578388 -0.54534366 -0.47522485
 -2.02654256  0.7289737   1.60512648  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856958 -0.67550787 -2.69002217]. 
Working on 0.4 fold... 
[Fri Apr  5 03:27:10 2024]  Iteration number: 0 with current cost as 0.3672680875242503 and parameters 
[-1.3683026   2.23743464 -2.12427938 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522472
 -2.02654253  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 03:35:05 2024]  Iteration number: 0 with current cost as 0.3708577333667045 and parameters 
[-1.45052441  2.23743442 -2.12427953 -0.11653103  0.55388697 -2.77010919
  3.06858487  2.18960134  1.18551998 -1.0664833   0.60271499  1.14432434
  1.31029888 -1.87354691  0.72965059  2.88578398 -0.54534357 -0.47522485
 -2.02654262  0.72897359  1.60512653  2.83077085 -1.2645672  -0.25136115
 -2.39279229 -2.27309785  3.13337144  2.54856948 -0.67550798 -2.69002212]. 
Working on 0.8 fold... 
[Fri Apr  5 03:43:48 2024]  Iteration number: 0 with current cost as 0.3404778404115016 and parameters 
[-1.43259807  2.23743464 -2.12427953 -0.11653103  0.55388708 -2.77010908
  3.06858498  2.18960124  1.18551998 -1.0664833   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296507   2.88578408 -0.54534357 -0.47522485
 -2.02654251  0.7289737   1.60512653  2.83077085 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 03:52:45 2024]  Iteration number: 0 with current cost as 0.35470298755274515 and parameters 
[-1.42087438  2.23743464 -2.12427946 -0.11653094  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552016 -1.06648308  0.6027151   1.14432454
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522467
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2544.343075990677 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 03:59:29 2024]  Iteration number: 0 with current cost as 0.6016901184184594 and parameters 
[-0.18336986  2.23743448 -2.12427948 -0.11653103  0.55388708 -2.77010929
  3.06858483  2.1896013   1.18551983 -1.06648355  0.6027151   1.14432445
  1.31029867 -1.8735468   0.72965065  2.88578388 -0.54534366 -0.47522485
 -2.02654256  0.7289737   1.60512648  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856958 -0.67550787 -2.69002217]. 
Working on 0.4 fold... 
[Fri Apr  5 04:09:07 2024]  Iteration number: 0 with current cost as 0.3672680875242503 and parameters 
[-1.3683026   2.23743464 -2.12427938 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522472
 -2.02654253  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 04:16:52 2024]  Iteration number: 0 with current cost as 0.3708577333667045 and parameters 
[-1.45052441  2.23743442 -2.12427953 -0.11653103  0.55388697 -2.77010919
  3.06858487  2.18960134  1.18551998 -1.0664833   0.60271499  1.14432434
  1.31029888 -1.87354691  0.72965059  2.88578398 -0.54534357 -0.47522485
 -2.02654262  0.72897359  1.60512653  2.83077085 -1.2645672  -0.25136115
 -2.39279229 -2.27309785  3.13337144  2.54856948 -0.67550798 -2.69002212]. 
Working on 0.8 fold... 
[Fri Apr  5 04:25:40 2024]  Iteration number: 0 with current cost as 0.3404778404115016 and parameters 
[-1.43259807  2.23743464 -2.12427953 -0.11653103  0.55388708 -2.77010908
  3.06858498  2.18960124  1.18551998 -1.0664833   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296507   2.88578408 -0.54534357 -0.47522485
 -2.02654251  0.7289737   1.60512653  2.83077085 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 04:34:49 2024]  Iteration number: 0 with current cost as 0.35470298755274515 and parameters 
[-1.42087438  2.23743464 -2.12427946 -0.11653094  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552016 -1.06648308  0.6027151   1.14432454
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522467
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2511.923557281494 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 04:41:42 2024]  Iteration number: 0 with current cost as 0.6016901184184594 and parameters 
[-0.18336986  2.23743448 -2.12427948 -0.11653103  0.55388708 -2.77010929
  3.06858483  2.1896013   1.18551983 -1.06648355  0.6027151   1.14432445
  1.31029867 -1.8735468   0.72965065  2.88578388 -0.54534366 -0.47522485
 -2.02654256  0.7289737   1.60512648  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856958 -0.67550787 -2.69002217]. 
Working on 0.4 fold... 
[Fri Apr  5 04:52:01 2024]  Iteration number: 0 with current cost as 0.3672680875242503 and parameters 
[-1.3683026   2.23743464 -2.12427938 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522472
 -2.02654253  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 05:00:17 2024]  Iteration number: 0 with current cost as 0.3708577333667045 and parameters 
[-1.45052441  2.23743442 -2.12427953 -0.11653103  0.55388697 -2.77010919
  3.06858487  2.18960134  1.18551998 -1.0664833   0.60271499  1.14432434
  1.31029888 -1.87354691  0.72965059  2.88578398 -0.54534357 -0.47522485
 -2.02654262  0.72897359  1.60512653  2.83077085 -1.2645672  -0.25136115
 -2.39279229 -2.27309785  3.13337144  2.54856948 -0.67550798 -2.69002212]. 
Working on 0.8 fold... 
[Fri Apr  5 05:08:55 2024]  Iteration number: 0 with current cost as 0.3404778404115016 and parameters 
[-1.43259807  2.23743464 -2.12427953 -0.11653103  0.55388708 -2.77010908
  3.06858498  2.18960124  1.18551998 -1.0664833   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296507   2.88578408 -0.54534357 -0.47522485
 -2.02654251  0.7289737   1.60512653  2.83077085 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 05:17:26 2024]  Iteration number: 0 with current cost as 0.35470298755274515 and parameters 
[-1.42087438  2.23743464 -2.12427946 -0.11653094  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552016 -1.06648308  0.6027151   1.14432454
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522467
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2567.0840363502502 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 05:24:20 2024]  Iteration number: 0 with current cost as 0.6016901184184594 and parameters 
[-0.18336986  2.23743448 -2.12427948 -0.11653103  0.55388708 -2.77010929
  3.06858483  2.1896013   1.18551983 -1.06648355  0.6027151   1.14432445
  1.31029867 -1.8735468   0.72965065  2.88578388 -0.54534366 -0.47522485
 -2.02654256  0.7289737   1.60512648  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856958 -0.67550787 -2.69002217]. 
Working on 0.4 fold... 
[Fri Apr  5 05:33:48 2024]  Iteration number: 0 with current cost as 0.3672680875242503 and parameters 
[-1.3683026   2.23743464 -2.12427938 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522472
 -2.02654253  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 05:41:39 2024]  Iteration number: 0 with current cost as 0.3708577333667045 and parameters 
[-1.45052441  2.23743442 -2.12427953 -0.11653103  0.55388697 -2.77010919
  3.06858487  2.18960134  1.18551998 -1.0664833   0.60271499  1.14432434
  1.31029888 -1.87354691  0.72965059  2.88578398 -0.54534357 -0.47522485
 -2.02654262  0.72897359  1.60512653  2.83077085 -1.2645672  -0.25136115
 -2.39279229 -2.27309785  3.13337144  2.54856948 -0.67550798 -2.69002212]. 
Working on 0.8 fold... 
[Fri Apr  5 05:50:23 2024]  Iteration number: 0 with current cost as 0.3404778404115016 and parameters 
[-1.43259807  2.23743464 -2.12427953 -0.11653103  0.55388708 -2.77010908
  3.06858498  2.18960124  1.18551998 -1.0664833   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296507   2.88578408 -0.54534357 -0.47522485
 -2.02654251  0.7289737   1.60512653  2.83077085 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 05:59:29 2024]  Iteration number: 0 with current cost as 0.35470298755274515 and parameters 
[-1.42087438  2.23743464 -2.12427946 -0.11653094  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552016 -1.06648308  0.6027151   1.14432454
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522467
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2511.7709681987762 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 06:06:11 2024]  Iteration number: 0 with current cost as 0.6016901184184594 and parameters 
[-0.18336986  2.23743448 -2.12427948 -0.11653103  0.55388708 -2.77010929
  3.06858483  2.1896013   1.18551983 -1.06648355  0.6027151   1.14432445
  1.31029867 -1.8735468   0.72965065  2.88578388 -0.54534366 -0.47522485
 -2.02654256  0.7289737   1.60512648  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856958 -0.67550787 -2.69002217]. 
Working on 0.4 fold... 
[Fri Apr  5 06:16:01 2024]  Iteration number: 0 with current cost as 0.3672680875242503 and parameters 
[-1.3683026   2.23743464 -2.12427938 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522472
 -2.02654253  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 06:23:35 2024]  Iteration number: 0 with current cost as 0.3708577333667045 and parameters 
[-1.45052441  2.23743442 -2.12427953 -0.11653103  0.55388697 -2.77010919
  3.06858487  2.18960134  1.18551998 -1.0664833   0.60271499  1.14432434
  1.31029888 -1.87354691  0.72965059  2.88578398 -0.54534357 -0.47522485
 -2.02654262  0.72897359  1.60512653  2.83077085 -1.2645672  -0.25136115
 -2.39279229 -2.27309785  3.13337144  2.54856948 -0.67550798 -2.69002212]. 
Working on 0.8 fold... 
[Fri Apr  5 06:31:55 2024]  Iteration number: 0 with current cost as 0.3404778404115016 and parameters 
[-1.43259807  2.23743464 -2.12427953 -0.11653103  0.55388708 -2.77010908
  3.06858498  2.18960124  1.18551998 -1.0664833   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296507   2.88578408 -0.54534357 -0.47522485
 -2.02654251  0.7289737   1.60512653  2.83077085 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 06:40:13 2024]  Iteration number: 0 with current cost as 0.35470298755274515 and parameters 
[-1.42087438  2.23743464 -2.12427946 -0.11653094  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552016 -1.06648308  0.6027151   1.14432454
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522467
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2449.357052564621 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 06:47:02 2024]  Iteration number: 0 with current cost as 0.6016901184184594 and parameters 
[-0.18336986  2.23743448 -2.12427948 -0.11653103  0.55388708 -2.77010929
  3.06858483  2.1896013   1.18551983 -1.06648355  0.6027151   1.14432445
  1.31029867 -1.8735468   0.72965065  2.88578388 -0.54534366 -0.47522485
 -2.02654256  0.7289737   1.60512648  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856958 -0.67550787 -2.69002217]. 
Working on 0.4 fold... 
[Fri Apr  5 06:57:03 2024]  Iteration number: 0 with current cost as 0.3672680875242503 and parameters 
[-1.3683026   2.23743464 -2.12427938 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522472
 -2.02654253  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 07:04:21 2024]  Iteration number: 0 with current cost as 0.3708577333667045 and parameters 
[-1.45052441  2.23743442 -2.12427953 -0.11653103  0.55388697 -2.77010919
  3.06858487  2.18960134  1.18551998 -1.0664833   0.60271499  1.14432434
  1.31029888 -1.87354691  0.72965059  2.88578398 -0.54534357 -0.47522485
 -2.02654262  0.72897359  1.60512653  2.83077085 -1.2645672  -0.25136115
 -2.39279229 -2.27309785  3.13337144  2.54856948 -0.67550798 -2.69002212]. 
Working on 0.8 fold... 
[Fri Apr  5 07:13:05 2024]  Iteration number: 0 with current cost as 0.3404778404115016 and parameters 
[-1.43259807  2.23743464 -2.12427953 -0.11653103  0.55388708 -2.77010908
  3.06858498  2.18960124  1.18551998 -1.0664833   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296507   2.88578408 -0.54534357 -0.47522485
 -2.02654251  0.7289737   1.60512653  2.83077085 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 07:21:53 2024]  Iteration number: 0 with current cost as 0.35470298755274515 and parameters 
[-1.42087438  2.23743464 -2.12427946 -0.11653094  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552016 -1.06648308  0.6027151   1.14432454
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522467
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2495.323179244995 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 07:28:41 2024]  Iteration number: 0 with current cost as 0.6016901184184594 and parameters 
[-0.18336986  2.23743448 -2.12427948 -0.11653103  0.55388708 -2.77010929
  3.06858483  2.1896013   1.18551983 -1.06648355  0.6027151   1.14432445
  1.31029867 -1.8735468   0.72965065  2.88578388 -0.54534366 -0.47522485
 -2.02654256  0.7289737   1.60512648  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856958 -0.67550787 -2.69002217]. 
Working on 0.4 fold... 
[Fri Apr  5 07:38:03 2024]  Iteration number: 0 with current cost as 0.3672680875242503 and parameters 
[-1.3683026   2.23743464 -2.12427938 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522472
 -2.02654253  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 07:45:52 2024]  Iteration number: 0 with current cost as 0.3708577333667045 and parameters 
[-1.45052441  2.23743442 -2.12427953 -0.11653103  0.55388697 -2.77010919
  3.06858487  2.18960134  1.18551998 -1.0664833   0.60271499  1.14432434
  1.31029888 -1.87354691  0.72965059  2.88578398 -0.54534357 -0.47522485
 -2.02654262  0.72897359  1.60512653  2.83077085 -1.2645672  -0.25136115
 -2.39279229 -2.27309785  3.13337144  2.54856948 -0.67550798 -2.69002212]. 
Working on 0.8 fold... 
[Fri Apr  5 07:54:23 2024]  Iteration number: 0 with current cost as 0.3404778404115016 and parameters 
[-1.43259807  2.23743464 -2.12427953 -0.11653103  0.55388708 -2.77010908
  3.06858498  2.18960124  1.18551998 -1.0664833   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296507   2.88578408 -0.54534357 -0.47522485
 -2.02654251  0.7289737   1.60512653  2.83077085 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 08:02:48 2024]  Iteration number: 0 with current cost as 0.35470298755274515 and parameters 
[-1.42087438  2.23743464 -2.12427946 -0.11653094  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552016 -1.06648308  0.6027151   1.14432454
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522467
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2457.661709547043 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 08:09:29 2024]  Iteration number: 0 with current cost as 0.6016901184184594 and parameters 
[-0.18336986  2.23743448 -2.12427948 -0.11653103  0.55388708 -2.77010929
  3.06858483  2.1896013   1.18551983 -1.06648355  0.6027151   1.14432445
  1.31029867 -1.8735468   0.72965065  2.88578388 -0.54534366 -0.47522485
 -2.02654256  0.7289737   1.60512648  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856958 -0.67550787 -2.69002217]. 
Working on 0.4 fold... 
[Fri Apr  5 08:19:14 2024]  Iteration number: 0 with current cost as 0.3672680875242503 and parameters 
[-1.3683026   2.23743464 -2.12427938 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522472
 -2.02654253  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 08:26:59 2024]  Iteration number: 0 with current cost as 0.3708577333667045 and parameters 
[-1.45052441  2.23743442 -2.12427953 -0.11653103  0.55388697 -2.77010919
  3.06858487  2.18960134  1.18551998 -1.0664833   0.60271499  1.14432434
  1.31029888 -1.87354691  0.72965059  2.88578398 -0.54534357 -0.47522485
 -2.02654262  0.72897359  1.60512653  2.83077085 -1.2645672  -0.25136115
 -2.39279229 -2.27309785  3.13337144  2.54856948 -0.67550798 -2.69002212]. 
Working on 0.8 fold... 
[Fri Apr  5 08:35:28 2024]  Iteration number: 0 with current cost as 0.3404778404115016 and parameters 
[-1.43259807  2.23743464 -2.12427953 -0.11653103  0.55388708 -2.77010908
  3.06858498  2.18960124  1.18551998 -1.0664833   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296507   2.88578408 -0.54534357 -0.47522485
 -2.02654251  0.7289737   1.60512653  2.83077085 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 08:43:46 2024]  Iteration number: 0 with current cost as 0.35470298755274515 and parameters 
[-1.42087438  2.23743464 -2.12427946 -0.11653094  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552016 -1.06648308  0.6027151   1.14432454
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522467
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2463.1437788009644 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 08:50:35 2024]  Iteration number: 0 with current cost as 0.6016901184184594 and parameters 
[-0.18336986  2.23743448 -2.12427948 -0.11653103  0.55388708 -2.77010929
  3.06858483  2.1896013   1.18551983 -1.06648355  0.6027151   1.14432445
  1.31029867 -1.8735468   0.72965065  2.88578388 -0.54534366 -0.47522485
 -2.02654256  0.7289737   1.60512648  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856958 -0.67550787 -2.69002217]. 
Working on 0.4 fold... 
[Fri Apr  5 09:00:18 2024]  Iteration number: 0 with current cost as 0.3672680875242503 and parameters 
[-1.3683026   2.23743464 -2.12427938 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522472
 -2.02654253  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 09:07:33 2024]  Iteration number: 0 with current cost as 0.3708577333667045 and parameters 
[-1.45052441  2.23743442 -2.12427953 -0.11653103  0.55388697 -2.77010919
  3.06858487  2.18960134  1.18551998 -1.0664833   0.60271499  1.14432434
  1.31029888 -1.87354691  0.72965059  2.88578398 -0.54534357 -0.47522485
 -2.02654262  0.72897359  1.60512653  2.83077085 -1.2645672  -0.25136115
 -2.39279229 -2.27309785  3.13337144  2.54856948 -0.67550798 -2.69002212]. 
Working on 0.8 fold... 
[Fri Apr  5 09:15:46 2024]  Iteration number: 0 with current cost as 0.3404778404115016 and parameters 
[-1.43259807  2.23743464 -2.12427953 -0.11653103  0.55388708 -2.77010908
  3.06858498  2.18960124  1.18551998 -1.0664833   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296507   2.88578408 -0.54534357 -0.47522485
 -2.02654251  0.7289737   1.60512653  2.83077085 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 09:24:03 2024]  Iteration number: 0 with current cost as 0.35470298755274515 and parameters 
[-1.42087438  2.23743464 -2.12427946 -0.11653094  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552016 -1.06648308  0.6027151   1.14432454
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522467
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2403.357498884201 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 09:30:18 2024]  Iteration number: 0 with current cost as 0.6016901184184594 and parameters 
[-0.18336986  2.23743448 -2.12427948 -0.11653103  0.55388708 -2.77010929
  3.06858483  2.1896013   1.18551983 -1.06648355  0.6027151   1.14432445
  1.31029867 -1.8735468   0.72965065  2.88578388 -0.54534366 -0.47522485
 -2.02654256  0.7289737   1.60512648  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856958 -0.67550787 -2.69002217]. 
Working on 0.4 fold... 
[Fri Apr  5 09:39:13 2024]  Iteration number: 0 with current cost as 0.3672680875242503 and parameters 
[-1.3683026   2.23743464 -2.12427938 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522472
 -2.02654253  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 09:46:26 2024]  Iteration number: 0 with current cost as 0.3708577333667045 and parameters 
[-1.45052441  2.23743442 -2.12427953 -0.11653103  0.55388697 -2.77010919
  3.06858487  2.18960134  1.18551998 -1.0664833   0.60271499  1.14432434
  1.31029888 -1.87354691  0.72965059  2.88578398 -0.54534357 -0.47522485
 -2.02654262  0.72897359  1.60512653  2.83077085 -1.2645672  -0.25136115
 -2.39279229 -2.27309785  3.13337144  2.54856948 -0.67550798 -2.69002212]. 
Working on 0.8 fold... 
[Fri Apr  5 09:54:52 2024]  Iteration number: 0 with current cost as 0.3404778404115016 and parameters 
[-1.43259807  2.23743464 -2.12427953 -0.11653103  0.55388708 -2.77010908
  3.06858498  2.18960124  1.18551998 -1.0664833   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296507   2.88578408 -0.54534357 -0.47522485
 -2.02654251  0.7289737   1.60512653  2.83077085 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 10:03:25 2024]  Iteration number: 0 with current cost as 0.35470298755274515 and parameters 
[-1.42087438  2.23743464 -2.12427946 -0.11653094  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552016 -1.06648308  0.6027151   1.14432454
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522467
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2368.558614253998 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 10:09:57 2024]  Iteration number: 0 with current cost as 0.6016901184184594 and parameters 
[-0.18336986  2.23743448 -2.12427948 -0.11653103  0.55388708 -2.77010929
  3.06858483  2.1896013   1.18551983 -1.06648355  0.6027151   1.14432445
  1.31029867 -1.8735468   0.72965065  2.88578388 -0.54534366 -0.47522485
 -2.02654256  0.7289737   1.60512648  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856958 -0.67550787 -2.69002217]. 
Working on 0.4 fold... 
[Fri Apr  5 10:19:32 2024]  Iteration number: 0 with current cost as 0.3672680875242503 and parameters 
[-1.3683026   2.23743464 -2.12427938 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522472
 -2.02654253  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 10:26:43 2024]  Iteration number: 0 with current cost as 0.3708577333667045 and parameters 
[-1.45052441  2.23743442 -2.12427953 -0.11653103  0.55388697 -2.77010919
  3.06858487  2.18960134  1.18551998 -1.0664833   0.60271499  1.14432434
  1.31029888 -1.87354691  0.72965059  2.88578398 -0.54534357 -0.47522485
 -2.02654262  0.72897359  1.60512653  2.83077085 -1.2645672  -0.25136115
 -2.39279229 -2.27309785  3.13337144  2.54856948 -0.67550798 -2.69002212]. 
Working on 0.8 fold... 
[Fri Apr  5 10:34:56 2024]  Iteration number: 0 with current cost as 0.3404778404115016 and parameters 
[-1.43259807  2.23743464 -2.12427953 -0.11653103  0.55388708 -2.77010908
  3.06858498  2.18960124  1.18551998 -1.0664833   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296507   2.88578408 -0.54534357 -0.47522485
 -2.02654251  0.7289737   1.60512653  2.83077085 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 10:43:15 2024]  Iteration number: 0 with current cost as 0.35470298755274515 and parameters 
[-1.42087438  2.23743464 -2.12427946 -0.11653094  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552016 -1.06648308  0.6027151   1.14432454
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522467
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2392.7543942928314 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 10:49:48 2024]  Iteration number: 0 with current cost as 0.6016901184184594 and parameters 
[-0.18336986  2.23743448 -2.12427948 -0.11653103  0.55388708 -2.77010929
  3.06858483  2.1896013   1.18551983 -1.06648355  0.6027151   1.14432445
  1.31029867 -1.8735468   0.72965065  2.88578388 -0.54534366 -0.47522485
 -2.02654256  0.7289737   1.60512648  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856958 -0.67550787 -2.69002217]. 
Working on 0.4 fold... 
[Fri Apr  5 10:59:07 2024]  Iteration number: 0 with current cost as 0.3672680875242503 and parameters 
[-1.3683026   2.23743464 -2.12427938 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522472
 -2.02654253  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 11:06:46 2024]  Iteration number: 0 with current cost as 0.3708577333667045 and parameters 
[-1.45052441  2.23743442 -2.12427953 -0.11653103  0.55388697 -2.77010919
  3.06858487  2.18960134  1.18551998 -1.0664833   0.60271499  1.14432434
  1.31029888 -1.87354691  0.72965059  2.88578398 -0.54534357 -0.47522485
 -2.02654262  0.72897359  1.60512653  2.83077085 -1.2645672  -0.25136115
 -2.39279229 -2.27309785  3.13337144  2.54856948 -0.67550798 -2.69002212]. 
Working on 0.8 fold... 
[Fri Apr  5 11:15:40 2024]  Iteration number: 0 with current cost as 0.3404778404115016 and parameters 
[-1.43259807  2.23743464 -2.12427953 -0.11653103  0.55388708 -2.77010908
  3.06858498  2.18960124  1.18551998 -1.0664833   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296507   2.88578408 -0.54534357 -0.47522485
 -2.02654251  0.7289737   1.60512653  2.83077085 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 11:24:14 2024]  Iteration number: 0 with current cost as 0.35470298755274515 and parameters 
[-1.42087438  2.23743464 -2.12427946 -0.11653094  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552016 -1.06648308  0.6027151   1.14432454
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522467
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2467.028636932373 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 11:31:05 2024]  Iteration number: 0 with current cost as 0.6016901184184594 and parameters 
[-0.18336986  2.23743448 -2.12427948 -0.11653103  0.55388708 -2.77010929
  3.06858483  2.1896013   1.18551983 -1.06648355  0.6027151   1.14432445
  1.31029867 -1.8735468   0.72965065  2.88578388 -0.54534366 -0.47522485
 -2.02654256  0.7289737   1.60512648  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856958 -0.67550787 -2.69002217]. 
Working on 0.4 fold... 
[Fri Apr  5 11:40:39 2024]  Iteration number: 0 with current cost as 0.3672680875242503 and parameters 
[-1.3683026   2.23743464 -2.12427938 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522472
 -2.02654253  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 11:48:18 2024]  Iteration number: 0 with current cost as 0.3708577333667045 and parameters 
[-1.45052441  2.23743442 -2.12427953 -0.11653103  0.55388697 -2.77010919
  3.06858487  2.18960134  1.18551998 -1.0664833   0.60271499  1.14432434
  1.31029888 -1.87354691  0.72965059  2.88578398 -0.54534357 -0.47522485
 -2.02654262  0.72897359  1.60512653  2.83077085 -1.2645672  -0.25136115
 -2.39279229 -2.27309785  3.13337144  2.54856948 -0.67550798 -2.69002212]. 
Working on 0.8 fold... 
[Fri Apr  5 11:56:53 2024]  Iteration number: 0 with current cost as 0.3404778404115016 and parameters 
[-1.43259807  2.23743464 -2.12427953 -0.11653103  0.55388708 -2.77010908
  3.06858498  2.18960124  1.18551998 -1.0664833   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296507   2.88578408 -0.54534357 -0.47522485
 -2.02654251  0.7289737   1.60512653  2.83077085 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 12:05:20 2024]  Iteration number: 0 with current cost as 0.35470298755274515 and parameters 
[-1.42087438  2.23743464 -2.12427946 -0.11653094  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552016 -1.06648308  0.6027151   1.14432454
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522467
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2451.5920054912567 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 12:11:56 2024]  Iteration number: 0 with current cost as 0.6016901184184594 and parameters 
[-0.18336986  2.23743448 -2.12427948 -0.11653103  0.55388708 -2.77010929
  3.06858483  2.1896013   1.18551983 -1.06648355  0.6027151   1.14432445
  1.31029867 -1.8735468   0.72965065  2.88578388 -0.54534366 -0.47522485
 -2.02654256  0.7289737   1.60512648  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856958 -0.67550787 -2.69002217]. 
Working on 0.4 fold... 
[Fri Apr  5 12:21:34 2024]  Iteration number: 0 with current cost as 0.3672680875242503 and parameters 
[-1.3683026   2.23743464 -2.12427938 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522472
 -2.02654253  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 12:28:53 2024]  Iteration number: 0 with current cost as 0.3708577333667045 and parameters 
[-1.45052441  2.23743442 -2.12427953 -0.11653103  0.55388697 -2.77010919
  3.06858487  2.18960134  1.18551998 -1.0664833   0.60271499  1.14432434
  1.31029888 -1.87354691  0.72965059  2.88578398 -0.54534357 -0.47522485
 -2.02654262  0.72897359  1.60512653  2.83077085 -1.2645672  -0.25136115
 -2.39279229 -2.27309785  3.13337144  2.54856948 -0.67550798 -2.69002212]. 
Working on 0.8 fold... 
[Fri Apr  5 12:37:02 2024]  Iteration number: 0 with current cost as 0.3404778404115016 and parameters 
[-1.43259807  2.23743464 -2.12427953 -0.11653103  0.55388708 -2.77010908
  3.06858498  2.18960124  1.18551998 -1.0664833   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296507   2.88578408 -0.54534357 -0.47522485
 -2.02654251  0.7289737   1.60512653  2.83077085 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 12:45:27 2024]  Iteration number: 0 with current cost as 0.35470298755274515 and parameters 
[-1.42087438  2.23743464 -2.12427946 -0.11653094  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552016 -1.06648308  0.6027151   1.14432454
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522467
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2409.458373785019 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 12:52:05 2024]  Iteration number: 0 with current cost as 0.6016901184184594 and parameters 
[-0.18336986  2.23743448 -2.12427948 -0.11653103  0.55388708 -2.77010929
  3.06858483  2.1896013   1.18551983 -1.06648355  0.6027151   1.14432445
  1.31029867 -1.8735468   0.72965065  2.88578388 -0.54534366 -0.47522485
 -2.02654256  0.7289737   1.60512648  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856958 -0.67550787 -2.69002217]. 
Working on 0.4 fold... 
[Fri Apr  5 13:01:20 2024]  Iteration number: 0 with current cost as 0.3672680875242503 and parameters 
[-1.3683026   2.23743464 -2.12427938 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522472
 -2.02654253  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 13:08:39 2024]  Iteration number: 0 with current cost as 0.3708577333667045 and parameters 
[-1.45052441  2.23743442 -2.12427953 -0.11653103  0.55388697 -2.77010919
  3.06858487  2.18960134  1.18551998 -1.0664833   0.60271499  1.14432434
  1.31029888 -1.87354691  0.72965059  2.88578398 -0.54534357 -0.47522485
 -2.02654262  0.72897359  1.60512653  2.83077085 -1.2645672  -0.25136115
 -2.39279229 -2.27309785  3.13337144  2.54856948 -0.67550798 -2.69002212]. 
Working on 0.8 fold... 
[Fri Apr  5 13:16:53 2024]  Iteration number: 0 with current cost as 0.3404778404115016 and parameters 
[-1.43259807  2.23743464 -2.12427953 -0.11653103  0.55388708 -2.77010908
  3.06858498  2.18960124  1.18551998 -1.0664833   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296507   2.88578408 -0.54534357 -0.47522485
 -2.02654251  0.7289737   1.60512653  2.83077085 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 13:25:13 2024]  Iteration number: 0 with current cost as 0.35470298755274515 and parameters 
[-1.42087438  2.23743464 -2.12427946 -0.11653094  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552016 -1.06648308  0.6027151   1.14432454
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522467
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2381.546289205551 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 13:31:20 2024]  Iteration number: 0 with current cost as 0.6016901184184594 and parameters 
[-0.18336986  2.23743448 -2.12427948 -0.11653103  0.55388708 -2.77010929
  3.06858483  2.1896013   1.18551983 -1.06648355  0.6027151   1.14432445
  1.31029867 -1.8735468   0.72965065  2.88578388 -0.54534366 -0.47522485
 -2.02654256  0.7289737   1.60512648  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856958 -0.67550787 -2.69002217]. 
Working on 0.4 fold... 
[Fri Apr  5 13:40:23 2024]  Iteration number: 0 with current cost as 0.3672680875242503 and parameters 
[-1.3683026   2.23743464 -2.12427938 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522472
 -2.02654253  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 13:48:02 2024]  Iteration number: 0 with current cost as 0.3708577333667045 and parameters 
[-1.45052441  2.23743442 -2.12427953 -0.11653103  0.55388697 -2.77010919
  3.06858487  2.18960134  1.18551998 -1.0664833   0.60271499  1.14432434
  1.31029888 -1.87354691  0.72965059  2.88578398 -0.54534357 -0.47522485
 -2.02654262  0.72897359  1.60512653  2.83077085 -1.2645672  -0.25136115
 -2.39279229 -2.27309785  3.13337144  2.54856948 -0.67550798 -2.69002212]. 
Working on 0.8 fold... 
[Fri Apr  5 13:56:15 2024]  Iteration number: 0 with current cost as 0.3404778404115016 and parameters 
[-1.43259807  2.23743464 -2.12427953 -0.11653103  0.55388708 -2.77010908
  3.06858498  2.18960124  1.18551998 -1.0664833   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296507   2.88578408 -0.54534357 -0.47522485
 -2.02654251  0.7289737   1.60512653  2.83077085 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 14:04:19 2024]  Iteration number: 0 with current cost as 0.35470298755274515 and parameters 
[-1.42087438  2.23743464 -2.12427946 -0.11653094  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552016 -1.06648308  0.6027151   1.14432454
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522467
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2343.9950773715973 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 14:10:39 2024]  Iteration number: 0 with current cost as 0.6016901184184594 and parameters 
[-0.18336986  2.23743448 -2.12427948 -0.11653103  0.55388708 -2.77010929
  3.06858483  2.1896013   1.18551983 -1.06648355  0.6027151   1.14432445
  1.31029867 -1.8735468   0.72965065  2.88578388 -0.54534366 -0.47522485
 -2.02654256  0.7289737   1.60512648  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856958 -0.67550787 -2.69002217]. 
Working on 0.4 fold... 
[Fri Apr  5 14:19:54 2024]  Iteration number: 0 with current cost as 0.3672680875242503 and parameters 
[-1.3683026   2.23743464 -2.12427938 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522472
 -2.02654253  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 14:27:30 2024]  Iteration number: 0 with current cost as 0.3708577333667045 and parameters 
[-1.45052441  2.23743442 -2.12427953 -0.11653103  0.55388697 -2.77010919
  3.06858487  2.18960134  1.18551998 -1.0664833   0.60271499  1.14432434
  1.31029888 -1.87354691  0.72965059  2.88578398 -0.54534357 -0.47522485
 -2.02654262  0.72897359  1.60512653  2.83077085 -1.2645672  -0.25136115
 -2.39279229 -2.27309785  3.13337144  2.54856948 -0.67550798 -2.69002212]. 
Working on 0.8 fold... 
[Fri Apr  5 14:35:38 2024]  Iteration number: 0 with current cost as 0.3404778404115016 and parameters 
[-1.43259807  2.23743464 -2.12427953 -0.11653103  0.55388708 -2.77010908
  3.06858498  2.18960124  1.18551998 -1.0664833   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296507   2.88578408 -0.54534357 -0.47522485
 -2.02654251  0.7289737   1.60512653  2.83077085 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 14:43:53 2024]  Iteration number: 0 with current cost as 0.35470298755274515 and parameters 
[-1.42087438  2.23743464 -2.12427946 -0.11653094  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552016 -1.06648308  0.6027151   1.14432454
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522467
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2386.1097083091736 seconds. 
Discarding model... 

Training complete taking 61741.142816782 total seconds. 
Now scoring model... 
Scoring complete taking 2.6391031742095947 seconds. 
Saved predicted values as M-A1-CNOT_Full-CRZ_predicted_values.csv
Model scores: {'MSE_train': (0.6017804311695842,), 'R2_train': -0.19828650829691252, 'MAE_train': 0.7025604331236417, 'MSE_test': 0.576309197112489, 'R2_test': -0.07479645319492478, 'MAE_test': 0.7124948124116629}. 
Saved model results as M-A1-CNOT_Full-CRZ_results.json. 
