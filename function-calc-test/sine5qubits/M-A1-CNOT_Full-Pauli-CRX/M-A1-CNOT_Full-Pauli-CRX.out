/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /scratch/j/jacobsen/gjones/sine5qubits/sine_train.bin... 
Successfully loaded /scratch/j/jacobsen/gjones/sine5qubits/sine_train.bin into X and y data. 
Loading dataset from /scratch/j/jacobsen/gjones/sine5qubits/sine_test.bin... 
Successfully loaded /scratch/j/jacobsen/gjones/sine5qubits/sine_test.bin into X and y data. 
Training model with dataset /scratch/j/jacobsen/gjones/sine5qubits/sine_train.bin 
 at time Thu Apr  4 21:37:31 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 21:37:56 2024]  Iteration number: 0 with current cost as 0.3539189306777692 and parameters 
[-2.95222574  2.38716345 -2.13263087 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.64855999  1.14432446
  1.37419648 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.4 fold... 
[Thu Apr  4 21:48:09 2024]  Iteration number: 0 with current cost as 0.28253016280257603 and parameters 
[-3.03901238  3.09874187 -2.11048984 -0.11653101  0.55388708 -2.77010895
  3.06858498  2.18960147  1.18551998 -1.06648308  0.7035692   1.14432447
  1.44609077 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654237  0.72897371  1.60512664  2.83077107 -1.2645671  -0.25136101]. 
Working on 0.6 fold... 
[Thu Apr  4 21:58:55 2024]  Iteration number: 0 with current cost as 0.277713446590252 and parameters 
[-3.06332934  3.15208724 -2.11218664 -0.11653106  0.55388704 -2.77010901
  3.06858495  2.18960145  1.18551998 -1.06648312  0.71682652  1.14432445
  1.47001617 -1.87354684  0.72965078  2.88578417 -0.54534339 -0.47522489
 -2.0265424   0.7289737   1.6051266   2.83077103 -1.26456713 -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 22:08:59 2024]  Iteration number: 0 with current cost as 0.34498589705479904 and parameters 
[-2.93298718  2.3739501  -2.12704254 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.63236567  1.14432445
  1.35004175 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 22:19:25 2024]  Iteration number: 0 with current cost as 0.35873834156250306 and parameters 
[-2.95446034  2.40169931 -2.13165276 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.64955553  1.14432446
  1.37425821 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 3038.448967218399 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 22:28:34 2024]  Iteration number: 0 with current cost as 0.3539189306777692 and parameters 
[-2.95222574  2.38716345 -2.13263087 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.64855999  1.14432446
  1.37419648 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.4 fold... 
[Thu Apr  4 22:39:19 2024]  Iteration number: 0 with current cost as 0.28253016280257603 and parameters 
[-3.03901238  3.09874187 -2.11048984 -0.11653101  0.55388708 -2.77010895
  3.06858498  2.18960147  1.18551998 -1.06648308  0.7035692   1.14432447
  1.44609077 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654237  0.72897371  1.60512664  2.83077107 -1.2645671  -0.25136101]. 
Working on 0.6 fold... 
[Thu Apr  4 22:50:11 2024]  Iteration number: 0 with current cost as 0.277713446590252 and parameters 
[-3.06332934  3.15208724 -2.11218664 -0.11653106  0.55388704 -2.77010901
  3.06858495  2.18960145  1.18551998 -1.06648312  0.71682652  1.14432445
  1.47001617 -1.87354684  0.72965078  2.88578417 -0.54534339 -0.47522489
 -2.0265424   0.7289737   1.6051266   2.83077103 -1.26456713 -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 23:00:12 2024]  Iteration number: 0 with current cost as 0.34498589705479904 and parameters 
[-2.93298718  2.3739501  -2.12704254 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.63236567  1.14432445
  1.35004175 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 23:10:31 2024]  Iteration number: 0 with current cost as 0.35873834156250306 and parameters 
[-2.95446034  2.40169931 -2.13165276 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.64955553  1.14432446
  1.37425821 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 3068.235023498535 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 23:19:42 2024]  Iteration number: 0 with current cost as 0.3539189306777692 and parameters 
[-2.95222574  2.38716345 -2.13263087 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.64855999  1.14432446
  1.37419648 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.4 fold... 
[Thu Apr  4 23:29:59 2024]  Iteration number: 0 with current cost as 0.28253016280257603 and parameters 
[-3.03901238  3.09874187 -2.11048984 -0.11653101  0.55388708 -2.77010895
  3.06858498  2.18960147  1.18551998 -1.06648308  0.7035692   1.14432447
  1.44609077 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654237  0.72897371  1.60512664  2.83077107 -1.2645671  -0.25136101]. 
Working on 0.6 fold... 
[Thu Apr  4 23:40:35 2024]  Iteration number: 0 with current cost as 0.277713446590252 and parameters 
[-3.06332934  3.15208724 -2.11218664 -0.11653106  0.55388704 -2.77010901
  3.06858495  2.18960145  1.18551998 -1.06648312  0.71682652  1.14432445
  1.47001617 -1.87354684  0.72965078  2.88578417 -0.54534339 -0.47522489
 -2.0265424   0.7289737   1.6051266   2.83077103 -1.26456713 -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 23:50:31 2024]  Iteration number: 0 with current cost as 0.34498589705479904 and parameters 
[-2.93298718  2.3739501  -2.12704254 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.63236567  1.14432445
  1.35004175 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 00:00:42 2024]  Iteration number: 0 with current cost as 0.35873834156250306 and parameters 
[-2.95446034  2.40169931 -2.13165276 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.64955553  1.14432446
  1.37425821 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 3012.5191552639008 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 00:09:55 2024]  Iteration number: 0 with current cost as 0.3539189306777692 and parameters 
[-2.95222574  2.38716345 -2.13263087 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.64855999  1.14432446
  1.37419648 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.4 fold... 
[Fri Apr  5 00:20:24 2024]  Iteration number: 0 with current cost as 0.28253016280257603 and parameters 
[-3.03901238  3.09874187 -2.11048984 -0.11653101  0.55388708 -2.77010895
  3.06858498  2.18960147  1.18551998 -1.06648308  0.7035692   1.14432447
  1.44609077 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654237  0.72897371  1.60512664  2.83077107 -1.2645671  -0.25136101]. 
Working on 0.6 fold... 
[Fri Apr  5 00:31:08 2024]  Iteration number: 0 with current cost as 0.277713446590252 and parameters 
[-3.06332934  3.15208724 -2.11218664 -0.11653106  0.55388704 -2.77010901
  3.06858495  2.18960145  1.18551998 -1.06648312  0.71682652  1.14432445
  1.47001617 -1.87354684  0.72965078  2.88578417 -0.54534339 -0.47522489
 -2.0265424   0.7289737   1.6051266   2.83077103 -1.26456713 -0.25136105]. 
Working on 0.8 fold... 
[Fri Apr  5 00:41:20 2024]  Iteration number: 0 with current cost as 0.34498589705479904 and parameters 
[-2.93298718  2.3739501  -2.12704254 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.63236567  1.14432445
  1.35004175 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 00:51:32 2024]  Iteration number: 0 with current cost as 0.35873834156250306 and parameters 
[-2.95446034  2.40169931 -2.13165276 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.64955553  1.14432446
  1.37425821 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 3044.7477221488953 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 01:00:39 2024]  Iteration number: 0 with current cost as 0.3539189306777692 and parameters 
[-2.95222574  2.38716345 -2.13263087 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.64855999  1.14432446
  1.37419648 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.4 fold... 
[Fri Apr  5 01:10:51 2024]  Iteration number: 0 with current cost as 0.28253016280257603 and parameters 
[-3.03901238  3.09874187 -2.11048984 -0.11653101  0.55388708 -2.77010895
  3.06858498  2.18960147  1.18551998 -1.06648308  0.7035692   1.14432447
  1.44609077 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654237  0.72897371  1.60512664  2.83077107 -1.2645671  -0.25136101]. 
Working on 0.6 fold... 
[Fri Apr  5 01:21:32 2024]  Iteration number: 0 with current cost as 0.277713446590252 and parameters 
[-3.06332934  3.15208724 -2.11218664 -0.11653106  0.55388704 -2.77010901
  3.06858495  2.18960145  1.18551998 -1.06648312  0.71682652  1.14432445
  1.47001617 -1.87354684  0.72965078  2.88578417 -0.54534339 -0.47522489
 -2.0265424   0.7289737   1.6051266   2.83077103 -1.26456713 -0.25136105]. 
Working on 0.8 fold... 
[Fri Apr  5 01:31:32 2024]  Iteration number: 0 with current cost as 0.34498589705479904 and parameters 
[-2.93298718  2.3739501  -2.12704254 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.63236567  1.14432445
  1.35004175 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 01:41:51 2024]  Iteration number: 0 with current cost as 0.35873834156250306 and parameters 
[-2.95446034  2.40169931 -2.13165276 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.64955553  1.14432446
  1.37425821 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 3023.2725870609283 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 01:51:03 2024]  Iteration number: 0 with current cost as 0.3539189306777692 and parameters 
[-2.95222574  2.38716345 -2.13263087 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.64855999  1.14432446
  1.37419648 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.4 fold... 
[Fri Apr  5 02:01:14 2024]  Iteration number: 0 with current cost as 0.28253016280257603 and parameters 
[-3.03901238  3.09874187 -2.11048984 -0.11653101  0.55388708 -2.77010895
  3.06858498  2.18960147  1.18551998 -1.06648308  0.7035692   1.14432447
  1.44609077 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654237  0.72897371  1.60512664  2.83077107 -1.2645671  -0.25136101]. 
Working on 0.6 fold... 
[Fri Apr  5 02:11:47 2024]  Iteration number: 0 with current cost as 0.277713446590252 and parameters 
[-3.06332934  3.15208724 -2.11218664 -0.11653106  0.55388704 -2.77010901
  3.06858495  2.18960145  1.18551998 -1.06648312  0.71682652  1.14432445
  1.47001617 -1.87354684  0.72965078  2.88578417 -0.54534339 -0.47522489
 -2.0265424   0.7289737   1.6051266   2.83077103 -1.26456713 -0.25136105]. 
Working on 0.8 fold... 
[Fri Apr  5 02:21:41 2024]  Iteration number: 0 with current cost as 0.34498589705479904 and parameters 
[-2.93298718  2.3739501  -2.12704254 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.63236567  1.14432445
  1.35004175 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 02:32:08 2024]  Iteration number: 0 with current cost as 0.35873834156250306 and parameters 
[-2.95446034  2.40169931 -2.13165276 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.64955553  1.14432446
  1.37425821 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 3011.715055704117 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 02:41:14 2024]  Iteration number: 0 with current cost as 0.3539189306777692 and parameters 
[-2.95222574  2.38716345 -2.13263087 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.64855999  1.14432446
  1.37419648 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.4 fold... 
[Fri Apr  5 02:51:33 2024]  Iteration number: 0 with current cost as 0.28253016280257603 and parameters 
[-3.03901238  3.09874187 -2.11048984 -0.11653101  0.55388708 -2.77010895
  3.06858498  2.18960147  1.18551998 -1.06648308  0.7035692   1.14432447
  1.44609077 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654237  0.72897371  1.60512664  2.83077107 -1.2645671  -0.25136101]. 
Working on 0.6 fold... 
[Fri Apr  5 03:02:29 2024]  Iteration number: 0 with current cost as 0.277713446590252 and parameters 
[-3.06332934  3.15208724 -2.11218664 -0.11653106  0.55388704 -2.77010901
  3.06858495  2.18960145  1.18551998 -1.06648312  0.71682652  1.14432445
  1.47001617 -1.87354684  0.72965078  2.88578417 -0.54534339 -0.47522489
 -2.0265424   0.7289737   1.6051266   2.83077103 -1.26456713 -0.25136105]. 
Working on 0.8 fold... 
[Fri Apr  5 03:13:20 2024]  Iteration number: 0 with current cost as 0.34498589705479904 and parameters 
[-2.93298718  2.3739501  -2.12704254 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.63236567  1.14432445
  1.35004175 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 03:23:37 2024]  Iteration number: 0 with current cost as 0.35873834156250306 and parameters 
[-2.95446034  2.40169931 -2.13165276 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.64955553  1.14432446
  1.37425821 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 3115.7037806510925 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 03:33:10 2024]  Iteration number: 0 with current cost as 0.3539189306777692 and parameters 
[-2.95222574  2.38716345 -2.13263087 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.64855999  1.14432446
  1.37419648 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.4 fold... 
[Fri Apr  5 03:43:31 2024]  Iteration number: 0 with current cost as 0.28253016280257603 and parameters 
[-3.03901238  3.09874187 -2.11048984 -0.11653101  0.55388708 -2.77010895
  3.06858498  2.18960147  1.18551998 -1.06648308  0.7035692   1.14432447
  1.44609077 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654237  0.72897371  1.60512664  2.83077107 -1.2645671  -0.25136101]. 
Working on 0.6 fold... 
[Fri Apr  5 03:54:28 2024]  Iteration number: 0 with current cost as 0.277713446590252 and parameters 
[-3.06332934  3.15208724 -2.11218664 -0.11653106  0.55388704 -2.77010901
  3.06858495  2.18960145  1.18551998 -1.06648312  0.71682652  1.14432445
  1.47001617 -1.87354684  0.72965078  2.88578417 -0.54534339 -0.47522489
 -2.0265424   0.7289737   1.6051266   2.83077103 -1.26456713 -0.25136105]. 
Working on 0.8 fold... 
[Fri Apr  5 04:04:27 2024]  Iteration number: 0 with current cost as 0.34498589705479904 and parameters 
[-2.93298718  2.3739501  -2.12704254 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.63236567  1.14432445
  1.35004175 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 04:14:36 2024]  Iteration number: 0 with current cost as 0.35873834156250306 and parameters 
[-2.95446034  2.40169931 -2.13165276 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.64955553  1.14432446
  1.37425821 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 3062.919157743454 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 04:24:16 2024]  Iteration number: 0 with current cost as 0.3539189306777692 and parameters 
[-2.95222574  2.38716345 -2.13263087 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.64855999  1.14432446
  1.37419648 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.4 fold... 
[Fri Apr  5 04:34:33 2024]  Iteration number: 0 with current cost as 0.28253016280257603 and parameters 
[-3.03901238  3.09874187 -2.11048984 -0.11653101  0.55388708 -2.77010895
  3.06858498  2.18960147  1.18551998 -1.06648308  0.7035692   1.14432447
  1.44609077 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654237  0.72897371  1.60512664  2.83077107 -1.2645671  -0.25136101]. 
Working on 0.6 fold... 
[Fri Apr  5 04:45:16 2024]  Iteration number: 0 with current cost as 0.277713446590252 and parameters 
[-3.06332934  3.15208724 -2.11218664 -0.11653106  0.55388704 -2.77010901
  3.06858495  2.18960145  1.18551998 -1.06648312  0.71682652  1.14432445
  1.47001617 -1.87354684  0.72965078  2.88578417 -0.54534339 -0.47522489
 -2.0265424   0.7289737   1.6051266   2.83077103 -1.26456713 -0.25136105]. 
Working on 0.8 fold... 
[Fri Apr  5 04:55:16 2024]  Iteration number: 0 with current cost as 0.34498589705479904 and parameters 
[-2.93298718  2.3739501  -2.12704254 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.63236567  1.14432445
  1.35004175 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 05:05:23 2024]  Iteration number: 0 with current cost as 0.35873834156250306 and parameters 
[-2.95446034  2.40169931 -2.13165276 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.64955553  1.14432446
  1.37425821 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 3033.8129873275757 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 05:14:47 2024]  Iteration number: 0 with current cost as 0.3539189306777692 and parameters 
[-2.95222574  2.38716345 -2.13263087 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.64855999  1.14432446
  1.37419648 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.4 fold... 
[Fri Apr  5 05:25:30 2024]  Iteration number: 0 with current cost as 0.28253016280257603 and parameters 
[-3.03901238  3.09874187 -2.11048984 -0.11653101  0.55388708 -2.77010895
  3.06858498  2.18960147  1.18551998 -1.06648308  0.7035692   1.14432447
  1.44609077 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654237  0.72897371  1.60512664  2.83077107 -1.2645671  -0.25136101]. 
Working on 0.6 fold... 
[Fri Apr  5 05:36:19 2024]  Iteration number: 0 with current cost as 0.277713446590252 and parameters 
[-3.06332934  3.15208724 -2.11218664 -0.11653106  0.55388704 -2.77010901
  3.06858495  2.18960145  1.18551998 -1.06648312  0.71682652  1.14432445
  1.47001617 -1.87354684  0.72965078  2.88578417 -0.54534339 -0.47522489
 -2.0265424   0.7289737   1.6051266   2.83077103 -1.26456713 -0.25136105]. 
Working on 0.8 fold... 
[Fri Apr  5 05:46:22 2024]  Iteration number: 0 with current cost as 0.34498589705479904 and parameters 
[-2.93298718  2.3739501  -2.12704254 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.63236567  1.14432445
  1.35004175 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 05:56:38 2024]  Iteration number: 0 with current cost as 0.35873834156250306 and parameters 
[-2.95446034  2.40169931 -2.13165276 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.64955553  1.14432446
  1.37425821 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 3077.0658190250397 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 06:06:04 2024]  Iteration number: 0 with current cost as 0.3539189306777692 and parameters 
[-2.95222574  2.38716345 -2.13263087 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.64855999  1.14432446
  1.37419648 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.4 fold... 
[Fri Apr  5 06:16:19 2024]  Iteration number: 0 with current cost as 0.28253016280257603 and parameters 
[-3.03901238  3.09874187 -2.11048984 -0.11653101  0.55388708 -2.77010895
  3.06858498  2.18960147  1.18551998 -1.06648308  0.7035692   1.14432447
  1.44609077 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654237  0.72897371  1.60512664  2.83077107 -1.2645671  -0.25136101]. 
Working on 0.6 fold... 
[Fri Apr  5 06:26:49 2024]  Iteration number: 0 with current cost as 0.277713446590252 and parameters 
[-3.06332934  3.15208724 -2.11218664 -0.11653106  0.55388704 -2.77010901
  3.06858495  2.18960145  1.18551998 -1.06648312  0.71682652  1.14432445
  1.47001617 -1.87354684  0.72965078  2.88578417 -0.54534339 -0.47522489
 -2.0265424   0.7289737   1.6051266   2.83077103 -1.26456713 -0.25136105]. 
Working on 0.8 fold... 
[Fri Apr  5 06:36:46 2024]  Iteration number: 0 with current cost as 0.34498589705479904 and parameters 
[-2.93298718  2.3739501  -2.12704254 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.63236567  1.14432445
  1.35004175 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 06:47:23 2024]  Iteration number: 0 with current cost as 0.35873834156250306 and parameters 
[-2.95446034  2.40169931 -2.13165276 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.64955553  1.14432446
  1.37425821 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 3038.6418175697327 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 06:56:41 2024]  Iteration number: 0 with current cost as 0.3539189306777692 and parameters 
[-2.95222574  2.38716345 -2.13263087 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.64855999  1.14432446
  1.37419648 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.4 fold... 
[Fri Apr  5 07:06:54 2024]  Iteration number: 0 with current cost as 0.28253016280257603 and parameters 
[-3.03901238  3.09874187 -2.11048984 -0.11653101  0.55388708 -2.77010895
  3.06858498  2.18960147  1.18551998 -1.06648308  0.7035692   1.14432447
  1.44609077 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654237  0.72897371  1.60512664  2.83077107 -1.2645671  -0.25136101]. 
Working on 0.6 fold... 
[Fri Apr  5 07:17:28 2024]  Iteration number: 0 with current cost as 0.277713446590252 and parameters 
[-3.06332934  3.15208724 -2.11218664 -0.11653106  0.55388704 -2.77010901
  3.06858495  2.18960145  1.18551998 -1.06648312  0.71682652  1.14432445
  1.47001617 -1.87354684  0.72965078  2.88578417 -0.54534339 -0.47522489
 -2.0265424   0.7289737   1.6051266   2.83077103 -1.26456713 -0.25136105]. 
Working on 0.8 fold... 
[Fri Apr  5 07:27:31 2024]  Iteration number: 0 with current cost as 0.34498589705479904 and parameters 
[-2.93298718  2.3739501  -2.12704254 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.63236567  1.14432445
  1.35004175 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 07:37:51 2024]  Iteration number: 0 with current cost as 0.35873834156250306 and parameters 
[-2.95446034  2.40169931 -2.13165276 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.64955553  1.14432446
  1.37425821 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 3014.5268416404724 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 07:46:57 2024]  Iteration number: 0 with current cost as 0.3539189306777692 and parameters 
[-2.95222574  2.38716345 -2.13263087 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.64855999  1.14432446
  1.37419648 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.4 fold... 
[Fri Apr  5 07:57:09 2024]  Iteration number: 0 with current cost as 0.28253016280257603 and parameters 
[-3.03901238  3.09874187 -2.11048984 -0.11653101  0.55388708 -2.77010895
  3.06858498  2.18960147  1.18551998 -1.06648308  0.7035692   1.14432447
  1.44609077 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654237  0.72897371  1.60512664  2.83077107 -1.2645671  -0.25136101]. 
Working on 0.6 fold... 
[Fri Apr  5 08:07:54 2024]  Iteration number: 0 with current cost as 0.277713446590252 and parameters 
[-3.06332934  3.15208724 -2.11218664 -0.11653106  0.55388704 -2.77010901
  3.06858495  2.18960145  1.18551998 -1.06648312  0.71682652  1.14432445
  1.47001617 -1.87354684  0.72965078  2.88578417 -0.54534339 -0.47522489
 -2.0265424   0.7289737   1.6051266   2.83077103 -1.26456713 -0.25136105]. 
Working on 0.8 fold... 
[Fri Apr  5 08:18:10 2024]  Iteration number: 0 with current cost as 0.34498589705479904 and parameters 
[-2.93298718  2.3739501  -2.12704254 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.63236567  1.14432445
  1.35004175 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 08:28:31 2024]  Iteration number: 0 with current cost as 0.35873834156250306 and parameters 
[-2.95446034  2.40169931 -2.13165276 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.64955553  1.14432446
  1.37425821 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 3052.4502601623535 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 08:37:48 2024]  Iteration number: 0 with current cost as 0.3539189306777692 and parameters 
[-2.95222574  2.38716345 -2.13263087 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.64855999  1.14432446
  1.37419648 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.4 fold... 
[Fri Apr  5 08:48:18 2024]  Iteration number: 0 with current cost as 0.28253016280257603 and parameters 
[-3.03901238  3.09874187 -2.11048984 -0.11653101  0.55388708 -2.77010895
  3.06858498  2.18960147  1.18551998 -1.06648308  0.7035692   1.14432447
  1.44609077 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654237  0.72897371  1.60512664  2.83077107 -1.2645671  -0.25136101]. 
Working on 0.6 fold... 
[Fri Apr  5 08:59:04 2024]  Iteration number: 0 with current cost as 0.277713446590252 and parameters 
[-3.06332934  3.15208724 -2.11218664 -0.11653106  0.55388704 -2.77010901
  3.06858495  2.18960145  1.18551998 -1.06648312  0.71682652  1.14432445
  1.47001617 -1.87354684  0.72965078  2.88578417 -0.54534339 -0.47522489
 -2.0265424   0.7289737   1.6051266   2.83077103 -1.26456713 -0.25136105]. 
Working on 0.8 fold... 
[Fri Apr  5 09:09:23 2024]  Iteration number: 0 with current cost as 0.34498589705479904 and parameters 
[-2.93298718  2.3739501  -2.12704254 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.63236567  1.14432445
  1.35004175 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 09:19:33 2024]  Iteration number: 0 with current cost as 0.35873834156250306 and parameters 
[-2.95446034  2.40169931 -2.13165276 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.64955553  1.14432446
  1.37425821 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 3055.1362674236298 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 09:28:45 2024]  Iteration number: 0 with current cost as 0.3539189306777692 and parameters 
[-2.95222574  2.38716345 -2.13263087 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.64855999  1.14432446
  1.37419648 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.4 fold... 
[Fri Apr  5 09:38:54 2024]  Iteration number: 0 with current cost as 0.28253016280257603 and parameters 
[-3.03901238  3.09874187 -2.11048984 -0.11653101  0.55388708 -2.77010895
  3.06858498  2.18960147  1.18551998 -1.06648308  0.7035692   1.14432447
  1.44609077 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654237  0.72897371  1.60512664  2.83077107 -1.2645671  -0.25136101]. 
Working on 0.6 fold... 
[Fri Apr  5 09:49:41 2024]  Iteration number: 0 with current cost as 0.277713446590252 and parameters 
[-3.06332934  3.15208724 -2.11218664 -0.11653106  0.55388704 -2.77010901
  3.06858495  2.18960145  1.18551998 -1.06648312  0.71682652  1.14432445
  1.47001617 -1.87354684  0.72965078  2.88578417 -0.54534339 -0.47522489
 -2.0265424   0.7289737   1.6051266   2.83077103 -1.26456713 -0.25136105]. 
Working on 0.8 fold... 
[Fri Apr  5 09:59:52 2024]  Iteration number: 0 with current cost as 0.34498589705479904 and parameters 
[-2.93298718  2.3739501  -2.12704254 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.63236567  1.14432445
  1.35004175 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 10:10:41 2024]  Iteration number: 0 with current cost as 0.35873834156250306 and parameters 
[-2.95446034  2.40169931 -2.13165276 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.64955553  1.14432446
  1.37425821 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 3069.9752593040466 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 10:19:53 2024]  Iteration number: 0 with current cost as 0.3539189306777692 and parameters 
[-2.95222574  2.38716345 -2.13263087 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.64855999  1.14432446
  1.37419648 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.4 fold... 
[Fri Apr  5 10:30:00 2024]  Iteration number: 0 with current cost as 0.28253016280257603 and parameters 
[-3.03901238  3.09874187 -2.11048984 -0.11653101  0.55388708 -2.77010895
  3.06858498  2.18960147  1.18551998 -1.06648308  0.7035692   1.14432447
  1.44609077 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654237  0.72897371  1.60512664  2.83077107 -1.2645671  -0.25136101]. 
Working on 0.6 fold... 
[Fri Apr  5 10:40:33 2024]  Iteration number: 0 with current cost as 0.277713446590252 and parameters 
[-3.06332934  3.15208724 -2.11218664 -0.11653106  0.55388704 -2.77010901
  3.06858495  2.18960145  1.18551998 -1.06648312  0.71682652  1.14432445
  1.47001617 -1.87354684  0.72965078  2.88578417 -0.54534339 -0.47522489
 -2.0265424   0.7289737   1.6051266   2.83077103 -1.26456713 -0.25136105]. 
Working on 0.8 fold... 
[Fri Apr  5 10:50:31 2024]  Iteration number: 0 with current cost as 0.34498589705479904 and parameters 
[-2.93298718  2.3739501  -2.12704254 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.63236567  1.14432445
  1.35004175 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 11:00:41 2024]  Iteration number: 0 with current cost as 0.35873834156250306 and parameters 
[-2.95446034  2.40169931 -2.13165276 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.64955553  1.14432446
  1.37425821 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2995.519942522049 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 11:09:50 2024]  Iteration number: 0 with current cost as 0.3539189306777692 and parameters 
[-2.95222574  2.38716345 -2.13263087 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.64855999  1.14432446
  1.37419648 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.4 fold... 
[Fri Apr  5 11:19:58 2024]  Iteration number: 0 with current cost as 0.28253016280257603 and parameters 
[-3.03901238  3.09874187 -2.11048984 -0.11653101  0.55388708 -2.77010895
  3.06858498  2.18960147  1.18551998 -1.06648308  0.7035692   1.14432447
  1.44609077 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654237  0.72897371  1.60512664  2.83077107 -1.2645671  -0.25136101]. 
Working on 0.6 fold... 
[Fri Apr  5 11:30:38 2024]  Iteration number: 0 with current cost as 0.277713446590252 and parameters 
[-3.06332934  3.15208724 -2.11218664 -0.11653106  0.55388704 -2.77010901
  3.06858495  2.18960145  1.18551998 -1.06648312  0.71682652  1.14432445
  1.47001617 -1.87354684  0.72965078  2.88578417 -0.54534339 -0.47522489
 -2.0265424   0.7289737   1.6051266   2.83077103 -1.26456713 -0.25136105]. 
Working on 0.8 fold... 
[Fri Apr  5 11:40:36 2024]  Iteration number: 0 with current cost as 0.34498589705479904 and parameters 
[-2.93298718  2.3739501  -2.12704254 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.63236567  1.14432445
  1.35004175 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 11:50:42 2024]  Iteration number: 0 with current cost as 0.35873834156250306 and parameters 
[-2.95446034  2.40169931 -2.13165276 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.64955553  1.14432446
  1.37425821 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 3001.557089805603 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 11:59:52 2024]  Iteration number: 0 with current cost as 0.3539189306777692 and parameters 
[-2.95222574  2.38716345 -2.13263087 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.64855999  1.14432446
  1.37419648 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.4 fold... 
[Fri Apr  5 12:10:02 2024]  Iteration number: 0 with current cost as 0.28253016280257603 and parameters 
[-3.03901238  3.09874187 -2.11048984 -0.11653101  0.55388708 -2.77010895
  3.06858498  2.18960147  1.18551998 -1.06648308  0.7035692   1.14432447
  1.44609077 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654237  0.72897371  1.60512664  2.83077107 -1.2645671  -0.25136101]. 
Working on 0.6 fold... 
[Fri Apr  5 12:20:39 2024]  Iteration number: 0 with current cost as 0.277713446590252 and parameters 
[-3.06332934  3.15208724 -2.11218664 -0.11653106  0.55388704 -2.77010901
  3.06858495  2.18960145  1.18551998 -1.06648312  0.71682652  1.14432445
  1.47001617 -1.87354684  0.72965078  2.88578417 -0.54534339 -0.47522489
 -2.0265424   0.7289737   1.6051266   2.83077103 -1.26456713 -0.25136105]. 
Working on 0.8 fold... 
[Fri Apr  5 12:31:03 2024]  Iteration number: 0 with current cost as 0.34498589705479904 and parameters 
[-2.93298718  2.3739501  -2.12704254 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.63236567  1.14432445
  1.35004175 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 12:41:59 2024]  Iteration number: 0 with current cost as 0.35873834156250306 and parameters 
[-2.95446034  2.40169931 -2.13165276 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.64955553  1.14432446
  1.37425821 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 3113.741730451584 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 12:51:45 2024]  Iteration number: 0 with current cost as 0.3539189306777692 and parameters 
[-2.95222574  2.38716345 -2.13263087 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.64855999  1.14432446
  1.37419648 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.4 fold... 
[Fri Apr  5 13:01:58 2024]  Iteration number: 0 with current cost as 0.28253016280257603 and parameters 
[-3.03901238  3.09874187 -2.11048984 -0.11653101  0.55388708 -2.77010895
  3.06858498  2.18960147  1.18551998 -1.06648308  0.7035692   1.14432447
  1.44609077 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654237  0.72897371  1.60512664  2.83077107 -1.2645671  -0.25136101]. 
Working on 0.6 fold... 
[Fri Apr  5 13:12:32 2024]  Iteration number: 0 with current cost as 0.277713446590252 and parameters 
[-3.06332934  3.15208724 -2.11218664 -0.11653106  0.55388704 -2.77010901
  3.06858495  2.18960145  1.18551998 -1.06648312  0.71682652  1.14432445
  1.47001617 -1.87354684  0.72965078  2.88578417 -0.54534339 -0.47522489
 -2.0265424   0.7289737   1.6051266   2.83077103 -1.26456713 -0.25136105]. 
Working on 0.8 fold... 
[Fri Apr  5 13:22:30 2024]  Iteration number: 0 with current cost as 0.34498589705479904 and parameters 
[-2.93298718  2.3739501  -2.12704254 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.63236567  1.14432445
  1.35004175 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 13:32:44 2024]  Iteration number: 0 with current cost as 0.35873834156250306 and parameters 
[-2.95446034  2.40169931 -2.13165276 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.64955553  1.14432446
  1.37425821 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 3015.478905916214 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 13:42:01 2024]  Iteration number: 0 with current cost as 0.3539189306777692 and parameters 
[-2.95222574  2.38716345 -2.13263087 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.64855999  1.14432446
  1.37419648 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.4 fold... 
[Fri Apr  5 13:52:20 2024]  Iteration number: 0 with current cost as 0.28253016280257603 and parameters 
[-3.03901238  3.09874187 -2.11048984 -0.11653101  0.55388708 -2.77010895
  3.06858498  2.18960147  1.18551998 -1.06648308  0.7035692   1.14432447
  1.44609077 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654237  0.72897371  1.60512664  2.83077107 -1.2645671  -0.25136101]. 
Working on 0.6 fold... 
[Fri Apr  5 14:03:05 2024]  Iteration number: 0 with current cost as 0.277713446590252 and parameters 
[-3.06332934  3.15208724 -2.11218664 -0.11653106  0.55388704 -2.77010901
  3.06858495  2.18960145  1.18551998 -1.06648312  0.71682652  1.14432445
  1.47001617 -1.87354684  0.72965078  2.88578417 -0.54534339 -0.47522489
 -2.0265424   0.7289737   1.6051266   2.83077103 -1.26456713 -0.25136105]. 
Working on 0.8 fold... 
[Fri Apr  5 14:13:28 2024]  Iteration number: 0 with current cost as 0.34498589705479904 and parameters 
[-2.93298718  2.3739501  -2.12704254 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.63236567  1.14432445
  1.35004175 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 14:23:40 2024]  Iteration number: 0 with current cost as 0.35873834156250306 and parameters 
[-2.95446034  2.40169931 -2.13165276 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.64955553  1.14432446
  1.37425821 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 3051.356496334076 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 14:32:52 2024]  Iteration number: 0 with current cost as 0.3539189306777692 and parameters 
[-2.95222574  2.38716345 -2.13263087 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.64855999  1.14432446
  1.37419648 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.4 fold... 
[Fri Apr  5 14:43:05 2024]  Iteration number: 0 with current cost as 0.28253016280257603 and parameters 
[-3.03901238  3.09874187 -2.11048984 -0.11653101  0.55388708 -2.77010895
  3.06858498  2.18960147  1.18551998 -1.06648308  0.7035692   1.14432447
  1.44609077 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654237  0.72897371  1.60512664  2.83077107 -1.2645671  -0.25136101]. 
Working on 0.6 fold... 
[Fri Apr  5 14:54:10 2024]  Iteration number: 0 with current cost as 0.277713446590252 and parameters 
[-3.06332934  3.15208724 -2.11218664 -0.11653106  0.55388704 -2.77010901
  3.06858495  2.18960145  1.18551998 -1.06648312  0.71682652  1.14432445
  1.47001617 -1.87354684  0.72965078  2.88578417 -0.54534339 -0.47522489
 -2.0265424   0.7289737   1.6051266   2.83077103 -1.26456713 -0.25136105]. 
Working on 0.8 fold... 
[Fri Apr  5 15:04:13 2024]  Iteration number: 0 with current cost as 0.34498589705479904 and parameters 
[-2.93298718  2.3739501  -2.12704254 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.63236567  1.14432445
  1.35004175 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 15:14:30 2024]  Iteration number: 0 with current cost as 0.35873834156250306 and parameters 
[-2.95446034  2.40169931 -2.13165276 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.64955553  1.14432446
  1.37425821 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 3054.549831390381 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 15:23:46 2024]  Iteration number: 0 with current cost as 0.3539189306777692 and parameters 
[-2.95222574  2.38716345 -2.13263087 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.64855999  1.14432446
  1.37419648 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.4 fold... 
[Fri Apr  5 15:33:58 2024]  Iteration number: 0 with current cost as 0.28253016280257603 and parameters 
[-3.03901238  3.09874187 -2.11048984 -0.11653101  0.55388708 -2.77010895
  3.06858498  2.18960147  1.18551998 -1.06648308  0.7035692   1.14432447
  1.44609077 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654237  0.72897371  1.60512664  2.83077107 -1.2645671  -0.25136101]. 
Working on 0.6 fold... 
[Fri Apr  5 15:44:44 2024]  Iteration number: 0 with current cost as 0.277713446590252 and parameters 
[-3.06332934  3.15208724 -2.11218664 -0.11653106  0.55388704 -2.77010901
  3.06858495  2.18960145  1.18551998 -1.06648312  0.71682652  1.14432445
  1.47001617 -1.87354684  0.72965078  2.88578417 -0.54534339 -0.47522489
 -2.0265424   0.7289737   1.6051266   2.83077103 -1.26456713 -0.25136105]. 
Working on 0.8 fold... 
[Fri Apr  5 15:54:46 2024]  Iteration number: 0 with current cost as 0.34498589705479904 and parameters 
[-2.93298718  2.3739501  -2.12704254 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.63236567  1.14432445
  1.35004175 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 16:04:57 2024]  Iteration number: 0 with current cost as 0.35873834156250306 and parameters 
[-2.95446034  2.40169931 -2.13165276 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.64955553  1.14432446
  1.37425821 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 3037.0799503326416 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 16:14:24 2024]  Iteration number: 0 with current cost as 0.3539189306777692 and parameters 
[-2.95222574  2.38716345 -2.13263087 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.64855999  1.14432446
  1.37419648 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.4 fold... 
[Fri Apr  5 16:24:34 2024]  Iteration number: 0 with current cost as 0.28253016280257603 and parameters 
[-3.03901238  3.09874187 -2.11048984 -0.11653101  0.55388708 -2.77010895
  3.06858498  2.18960147  1.18551998 -1.06648308  0.7035692   1.14432447
  1.44609077 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654237  0.72897371  1.60512664  2.83077107 -1.2645671  -0.25136101]. 
Working on 0.6 fold... 
[Fri Apr  5 16:35:09 2024]  Iteration number: 0 with current cost as 0.277713446590252 and parameters 
[-3.06332934  3.15208724 -2.11218664 -0.11653106  0.55388704 -2.77010901
  3.06858495  2.18960145  1.18551998 -1.06648312  0.71682652  1.14432445
  1.47001617 -1.87354684  0.72965078  2.88578417 -0.54534339 -0.47522489
 -2.0265424   0.7289737   1.6051266   2.83077103 -1.26456713 -0.25136105]. 
Working on 0.8 fold... 
[Fri Apr  5 16:45:06 2024]  Iteration number: 0 with current cost as 0.34498589705479904 and parameters 
[-2.93298718  2.3739501  -2.12704254 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.63236567  1.14432445
  1.35004175 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 16:55:17 2024]  Iteration number: 0 with current cost as 0.35873834156250306 and parameters 
[-2.95446034  2.40169931 -2.13165276 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.64955553  1.14432446
  1.37425821 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 3003.129063129425 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 17:04:27 2024]  Iteration number: 0 with current cost as 0.3539189306777692 and parameters 
[-2.95222574  2.38716345 -2.13263087 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.64855999  1.14432446
  1.37419648 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.4 fold... 
[Fri Apr  5 17:14:36 2024]  Iteration number: 0 with current cost as 0.28253016280257603 and parameters 
[-3.03901238  3.09874187 -2.11048984 -0.11653101  0.55388708 -2.77010895
  3.06858498  2.18960147  1.18551998 -1.06648308  0.7035692   1.14432447
  1.44609077 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654237  0.72897371  1.60512664  2.83077107 -1.2645671  -0.25136101]. 
Working on 0.6 fold... 
[Fri Apr  5 17:25:12 2024]  Iteration number: 0 with current cost as 0.277713446590252 and parameters 
[-3.06332934  3.15208724 -2.11218664 -0.11653106  0.55388704 -2.77010901
  3.06858495  2.18960145  1.18551998 -1.06648312  0.71682652  1.14432445
  1.47001617 -1.87354684  0.72965078  2.88578417 -0.54534339 -0.47522489
 -2.0265424   0.7289737   1.6051266   2.83077103 -1.26456713 -0.25136105]. 
Working on 0.8 fold... 
[Fri Apr  5 17:35:12 2024]  Iteration number: 0 with current cost as 0.34498589705479904 and parameters 
[-2.93298718  2.3739501  -2.12704254 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.63236567  1.14432445
  1.35004175 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 17:45:36 2024]  Iteration number: 0 with current cost as 0.35873834156250306 and parameters 
[-2.95446034  2.40169931 -2.13165276 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.64955553  1.14432446
  1.37425821 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 3017.969803571701 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 17:54:45 2024]  Iteration number: 0 with current cost as 0.3539189306777692 and parameters 
[-2.95222574  2.38716345 -2.13263087 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.64855999  1.14432446
  1.37419648 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.4 fold... 
[Fri Apr  5 18:04:56 2024]  Iteration number: 0 with current cost as 0.28253016280257603 and parameters 
[-3.03901238  3.09874187 -2.11048984 -0.11653101  0.55388708 -2.77010895
  3.06858498  2.18960147  1.18551998 -1.06648308  0.7035692   1.14432447
  1.44609077 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654237  0.72897371  1.60512664  2.83077107 -1.2645671  -0.25136101]. 
Working on 0.6 fold... 
[Fri Apr  5 18:15:42 2024]  Iteration number: 0 with current cost as 0.277713446590252 and parameters 
[-3.06332934  3.15208724 -2.11218664 -0.11653106  0.55388704 -2.77010901
  3.06858495  2.18960145  1.18551998 -1.06648312  0.71682652  1.14432445
  1.47001617 -1.87354684  0.72965078  2.88578417 -0.54534339 -0.47522489
 -2.0265424   0.7289737   1.6051266   2.83077103 -1.26456713 -0.25136105]. 
Working on 0.8 fold... 
[Fri Apr  5 18:25:40 2024]  Iteration number: 0 with current cost as 0.34498589705479904 and parameters 
[-2.93298718  2.3739501  -2.12704254 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.63236567  1.14432445
  1.35004175 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 18:35:58 2024]  Iteration number: 0 with current cost as 0.35873834156250306 and parameters 
[-2.95446034  2.40169931 -2.13165276 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.64955553  1.14432446
  1.37425821 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 3041.913368701935 seconds. 
Discarding model... 

Training complete taking 76051.46888566017 total seconds. 
Now scoring model... 
Scoring complete taking 1.220719814300537 seconds. 
Saved predicted values as M-A1-CNOT_Full-Pauli-CRX_predicted_values.csv
Model scores: {'MSE_train': (0.31957719454174904,), 'R2_train': 0.3636465715003616, 'MAE_train': 0.49031910363222186, 'MSE_test': 0.37583779575207776, 'R2_test': 0.29907568389527983, 'MAE_test': 0.5396178627734027}. 
Saved model results as M-A1-CNOT_Full-Pauli-CRX_results.json. 
