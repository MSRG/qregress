/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /scratch/j/jacobsen/gjones/sine5qubits/sine_train.bin... 
Successfully loaded /scratch/j/jacobsen/gjones/sine5qubits/sine_train.bin into X and y data. 
Loading dataset from /scratch/j/jacobsen/gjones/sine5qubits/sine_test.bin... 
Successfully loaded /scratch/j/jacobsen/gjones/sine5qubits/sine_test.bin into X and y data. 
Training model with dataset /scratch/j/jacobsen/gjones/sine5qubits/sine_train.bin 
 at time Thu Apr  4 21:37:45 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 21:40:37 2024]  Iteration number: 0 with current cost as 0.32710204119873 and parameters 
[-1.76826899  2.23743459 -2.12427959 -0.11653103  0.55388703 -2.77010902
  3.06858493  2.1896014   1.18552003 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965075  2.88578419 -0.5453434  -0.47522485
 -2.0265425   0.72897365  1.60512659  2.83077097 -1.26456715 -0.25136105
 -2.39279218 -2.27309774  3.1333715   2.54856958 -0.67550787 -2.69002207]. 
Working on 0.4 fold... 
[Thu Apr  4 21:46:53 2024]  Iteration number: 0 with current cost as 0.32931751136549337 and parameters 
[-1.81187897  2.23743464 -2.12427956 -0.116531    0.55388708 -2.770109
  3.06858498  2.18960143  1.18552004 -1.06648308  0.60271513  1.14432445
  1.31029904 -1.87354675  0.72965078  2.88578419 -0.54534335 -0.47522483
 -2.02654243  0.72897372  1.60512666  2.83077104 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 21:54:08 2024]  Iteration number: 0 with current cost as 0.302921871290067 and parameters 
[-1.61223999  2.2374346  -2.12427955 -0.11653106  0.55388705 -2.77010903
  3.06858489  2.18960142  1.18552005 -1.06648312  0.60271507  1.14432442
  1.31029902 -1.87354677  0.72965071  2.88578416 -0.54534338 -0.47522488
 -2.02654247  0.72897366  1.60512661  2.83077104 -1.26456713 -0.25136105
 -2.39279218 -2.27309774  3.13337152  2.54856955 -0.67550787 -2.69002205]. 
Working on 0.8 fold... 
[Thu Apr  4 21:58:14 2024]  Iteration number: 0 with current cost as 0.31411556266347024 and parameters 
[-1.73764243  2.23743464 -2.12427953 -0.11653097  0.55388705 -2.77010903
  3.06858498  2.18960145  1.18552004 -1.06648311  0.6027151   1.14432445
  1.31029904 -1.87354675  0.72965075  2.88578419 -0.54534335 -0.4752248
 -2.02654243  0.7289737   1.60512661  2.83077102 -1.26456712 -0.25136105
 -2.39279218 -2.27309774  3.13337152  2.54856958 -0.67550787 -2.69002204]. 
Working on 1.0 fold... 
[Thu Apr  4 22:04:24 2024]  Iteration number: 0 with current cost as 0.327780467801287 and parameters 
[-1.79022485  2.23743464 -2.12427954 -0.11653098  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18552003 -1.06648308  0.6027151   1.14432445
  1.31029901 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.4752248
 -2.02654243  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1800.6880509853363 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 22:10:25 2024]  Iteration number: 0 with current cost as 0.32710204119873 and parameters 
[-1.76826899  2.23743459 -2.12427959 -0.11653103  0.55388703 -2.77010902
  3.06858493  2.1896014   1.18552003 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965075  2.88578419 -0.5453434  -0.47522485
 -2.0265425   0.72897365  1.60512659  2.83077097 -1.26456715 -0.25136105
 -2.39279218 -2.27309774  3.1333715   2.54856958 -0.67550787 -2.69002207]. 
Working on 0.4 fold... 
[Thu Apr  4 22:16:38 2024]  Iteration number: 0 with current cost as 0.32931751136549337 and parameters 
[-1.81187897  2.23743464 -2.12427956 -0.116531    0.55388708 -2.770109
  3.06858498  2.18960143  1.18552004 -1.06648308  0.60271513  1.14432445
  1.31029904 -1.87354675  0.72965078  2.88578419 -0.54534335 -0.47522483
 -2.02654243  0.72897372  1.60512666  2.83077104 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 22:24:25 2024]  Iteration number: 0 with current cost as 0.302921871290067 and parameters 
[-1.61223999  2.2374346  -2.12427955 -0.11653106  0.55388705 -2.77010903
  3.06858489  2.18960142  1.18552005 -1.06648312  0.60271507  1.14432442
  1.31029902 -1.87354677  0.72965071  2.88578416 -0.54534338 -0.47522488
 -2.02654247  0.72897366  1.60512661  2.83077104 -1.26456713 -0.25136105
 -2.39279218 -2.27309774  3.13337152  2.54856955 -0.67550787 -2.69002205]. 
Working on 0.8 fold... 
[Thu Apr  4 22:28:55 2024]  Iteration number: 0 with current cost as 0.31411556266347024 and parameters 
[-1.73764243  2.23743464 -2.12427953 -0.11653097  0.55388705 -2.77010903
  3.06858498  2.18960145  1.18552004 -1.06648311  0.6027151   1.14432445
  1.31029904 -1.87354675  0.72965075  2.88578419 -0.54534335 -0.4752248
 -2.02654243  0.7289737   1.60512661  2.83077102 -1.26456712 -0.25136105
 -2.39279218 -2.27309774  3.13337152  2.54856958 -0.67550787 -2.69002204]. 
Working on 1.0 fold... 
[Thu Apr  4 22:35:28 2024]  Iteration number: 0 with current cost as 0.327780467801287 and parameters 
[-1.79022485  2.23743464 -2.12427954 -0.11653098  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18552003 -1.06648308  0.6027151   1.14432445
  1.31029901 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.4752248
 -2.02654243  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1885.2614071369171 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 22:42:02 2024]  Iteration number: 0 with current cost as 0.32710204119873 and parameters 
[-1.76826899  2.23743459 -2.12427959 -0.11653103  0.55388703 -2.77010902
  3.06858493  2.1896014   1.18552003 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965075  2.88578419 -0.5453434  -0.47522485
 -2.0265425   0.72897365  1.60512659  2.83077097 -1.26456715 -0.25136105
 -2.39279218 -2.27309774  3.1333715   2.54856958 -0.67550787 -2.69002207]. 
Working on 0.4 fold... 
[Thu Apr  4 22:47:55 2024]  Iteration number: 0 with current cost as 0.32931751136549337 and parameters 
[-1.81187897  2.23743464 -2.12427956 -0.116531    0.55388708 -2.770109
  3.06858498  2.18960143  1.18552004 -1.06648308  0.60271513  1.14432445
  1.31029904 -1.87354675  0.72965078  2.88578419 -0.54534335 -0.47522483
 -2.02654243  0.72897372  1.60512666  2.83077104 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 22:55:20 2024]  Iteration number: 0 with current cost as 0.302921871290067 and parameters 
[-1.61223999  2.2374346  -2.12427955 -0.11653106  0.55388705 -2.77010903
  3.06858489  2.18960142  1.18552005 -1.06648312  0.60271507  1.14432442
  1.31029902 -1.87354677  0.72965071  2.88578416 -0.54534338 -0.47522488
 -2.02654247  0.72897366  1.60512661  2.83077104 -1.26456713 -0.25136105
 -2.39279218 -2.27309774  3.13337152  2.54856955 -0.67550787 -2.69002205]. 
Working on 0.8 fold... 
[Thu Apr  4 22:59:16 2024]  Iteration number: 0 with current cost as 0.31411556266347024 and parameters 
[-1.73764243  2.23743464 -2.12427953 -0.11653097  0.55388705 -2.77010903
  3.06858498  2.18960145  1.18552004 -1.06648311  0.6027151   1.14432445
  1.31029904 -1.87354675  0.72965075  2.88578419 -0.54534335 -0.4752248
 -2.02654243  0.7289737   1.60512661  2.83077102 -1.26456712 -0.25136105
 -2.39279218 -2.27309774  3.13337152  2.54856958 -0.67550787 -2.69002204]. 
Working on 1.0 fold... 
[Thu Apr  4 23:05:25 2024]  Iteration number: 0 with current cost as 0.327780467801287 and parameters 
[-1.79022485  2.23743464 -2.12427954 -0.11653098  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18552003 -1.06648308  0.6027151   1.14432445
  1.31029901 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.4752248
 -2.02654243  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1780.363406419754 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 23:11:34 2024]  Iteration number: 0 with current cost as 0.32710204119873 and parameters 
[-1.76826899  2.23743459 -2.12427959 -0.11653103  0.55388703 -2.77010902
  3.06858493  2.1896014   1.18552003 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965075  2.88578419 -0.5453434  -0.47522485
 -2.0265425   0.72897365  1.60512659  2.83077097 -1.26456715 -0.25136105
 -2.39279218 -2.27309774  3.1333715   2.54856958 -0.67550787 -2.69002207]. 
Working on 0.4 fold... 
[Thu Apr  4 23:17:40 2024]  Iteration number: 0 with current cost as 0.32931751136549337 and parameters 
[-1.81187897  2.23743464 -2.12427956 -0.116531    0.55388708 -2.770109
  3.06858498  2.18960143  1.18552004 -1.06648308  0.60271513  1.14432445
  1.31029904 -1.87354675  0.72965078  2.88578419 -0.54534335 -0.47522483
 -2.02654243  0.72897372  1.60512666  2.83077104 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 23:24:57 2024]  Iteration number: 0 with current cost as 0.302921871290067 and parameters 
[-1.61223999  2.2374346  -2.12427955 -0.11653106  0.55388705 -2.77010903
  3.06858489  2.18960142  1.18552005 -1.06648312  0.60271507  1.14432442
  1.31029902 -1.87354677  0.72965071  2.88578416 -0.54534338 -0.47522488
 -2.02654247  0.72897366  1.60512661  2.83077104 -1.26456713 -0.25136105
 -2.39279218 -2.27309774  3.13337152  2.54856955 -0.67550787 -2.69002205]. 
Working on 0.8 fold... 
[Thu Apr  4 23:28:46 2024]  Iteration number: 0 with current cost as 0.31411556266347024 and parameters 
[-1.73764243  2.23743464 -2.12427953 -0.11653097  0.55388705 -2.77010903
  3.06858498  2.18960145  1.18552004 -1.06648311  0.6027151   1.14432445
  1.31029904 -1.87354675  0.72965075  2.88578419 -0.54534335 -0.4752248
 -2.02654243  0.7289737   1.60512661  2.83077102 -1.26456712 -0.25136105
 -2.39279218 -2.27309774  3.13337152  2.54856958 -0.67550787 -2.69002204]. 
Working on 1.0 fold... 
[Thu Apr  4 23:34:45 2024]  Iteration number: 0 with current cost as 0.327780467801287 and parameters 
[-1.79022485  2.23743464 -2.12427954 -0.11653098  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18552003 -1.06648308  0.6027151   1.14432445
  1.31029901 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.4752248
 -2.02654243  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1750.094155073166 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 23:40:42 2024]  Iteration number: 0 with current cost as 0.32710204119873 and parameters 
[-1.76826899  2.23743459 -2.12427959 -0.11653103  0.55388703 -2.77010902
  3.06858493  2.1896014   1.18552003 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965075  2.88578419 -0.5453434  -0.47522485
 -2.0265425   0.72897365  1.60512659  2.83077097 -1.26456715 -0.25136105
 -2.39279218 -2.27309774  3.1333715   2.54856958 -0.67550787 -2.69002207]. 
Working on 0.4 fold... 
[Thu Apr  4 23:46:32 2024]  Iteration number: 0 with current cost as 0.32931751136549337 and parameters 
[-1.81187897  2.23743464 -2.12427956 -0.116531    0.55388708 -2.770109
  3.06858498  2.18960143  1.18552004 -1.06648308  0.60271513  1.14432445
  1.31029904 -1.87354675  0.72965078  2.88578419 -0.54534335 -0.47522483
 -2.02654243  0.72897372  1.60512666  2.83077104 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 23:53:36 2024]  Iteration number: 0 with current cost as 0.302921871290067 and parameters 
[-1.61223999  2.2374346  -2.12427955 -0.11653106  0.55388705 -2.77010903
  3.06858489  2.18960142  1.18552005 -1.06648312  0.60271507  1.14432442
  1.31029902 -1.87354677  0.72965071  2.88578416 -0.54534338 -0.47522488
 -2.02654247  0.72897366  1.60512661  2.83077104 -1.26456713 -0.25136105
 -2.39279218 -2.27309774  3.13337152  2.54856955 -0.67550787 -2.69002205]. 
Working on 0.8 fold... 
[Thu Apr  4 23:57:23 2024]  Iteration number: 0 with current cost as 0.31411556266347024 and parameters 
[-1.73764243  2.23743464 -2.12427953 -0.11653097  0.55388705 -2.77010903
  3.06858498  2.18960145  1.18552004 -1.06648311  0.6027151   1.14432445
  1.31029904 -1.87354675  0.72965075  2.88578419 -0.54534335 -0.4752248
 -2.02654243  0.7289737   1.60512661  2.83077102 -1.26456712 -0.25136105
 -2.39279218 -2.27309774  3.13337152  2.54856958 -0.67550787 -2.69002204]. 
Working on 1.0 fold... 
[Fri Apr  5 00:03:08 2024]  Iteration number: 0 with current cost as 0.327780467801287 and parameters 
[-1.79022485  2.23743464 -2.12427954 -0.11653098  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18552003 -1.06648308  0.6027151   1.14432445
  1.31029901 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.4752248
 -2.02654243  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1702.3233292102814 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 00:09:01 2024]  Iteration number: 0 with current cost as 0.32710204119873 and parameters 
[-1.76826899  2.23743459 -2.12427959 -0.11653103  0.55388703 -2.77010902
  3.06858493  2.1896014   1.18552003 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965075  2.88578419 -0.5453434  -0.47522485
 -2.0265425   0.72897365  1.60512659  2.83077097 -1.26456715 -0.25136105
 -2.39279218 -2.27309774  3.1333715   2.54856958 -0.67550787 -2.69002207]. 
Working on 0.4 fold... 
[Fri Apr  5 00:14:44 2024]  Iteration number: 0 with current cost as 0.32931751136549337 and parameters 
[-1.81187897  2.23743464 -2.12427956 -0.116531    0.55388708 -2.770109
  3.06858498  2.18960143  1.18552004 -1.06648308  0.60271513  1.14432445
  1.31029904 -1.87354675  0.72965078  2.88578419 -0.54534335 -0.47522483
 -2.02654243  0.72897372  1.60512666  2.83077104 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 00:21:49 2024]  Iteration number: 0 with current cost as 0.302921871290067 and parameters 
[-1.61223999  2.2374346  -2.12427955 -0.11653106  0.55388705 -2.77010903
  3.06858489  2.18960142  1.18552005 -1.06648312  0.60271507  1.14432442
  1.31029902 -1.87354677  0.72965071  2.88578416 -0.54534338 -0.47522488
 -2.02654247  0.72897366  1.60512661  2.83077104 -1.26456713 -0.25136105
 -2.39279218 -2.27309774  3.13337152  2.54856955 -0.67550787 -2.69002205]. 
Working on 0.8 fold... 
[Fri Apr  5 00:25:52 2024]  Iteration number: 0 with current cost as 0.31411556266347024 and parameters 
[-1.73764243  2.23743464 -2.12427953 -0.11653097  0.55388705 -2.77010903
  3.06858498  2.18960145  1.18552004 -1.06648311  0.6027151   1.14432445
  1.31029904 -1.87354675  0.72965075  2.88578419 -0.54534335 -0.4752248
 -2.02654243  0.7289737   1.60512661  2.83077102 -1.26456712 -0.25136105
 -2.39279218 -2.27309774  3.13337152  2.54856958 -0.67550787 -2.69002204]. 
Working on 1.0 fold... 
[Fri Apr  5 00:31:41 2024]  Iteration number: 0 with current cost as 0.327780467801287 and parameters 
[-1.79022485  2.23743464 -2.12427954 -0.11653098  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18552003 -1.06648308  0.6027151   1.14432445
  1.31029901 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.4752248
 -2.02654243  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1705.1421756744385 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 00:37:28 2024]  Iteration number: 0 with current cost as 0.32710204119873 and parameters 
[-1.76826899  2.23743459 -2.12427959 -0.11653103  0.55388703 -2.77010902
  3.06858493  2.1896014   1.18552003 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965075  2.88578419 -0.5453434  -0.47522485
 -2.0265425   0.72897365  1.60512659  2.83077097 -1.26456715 -0.25136105
 -2.39279218 -2.27309774  3.1333715   2.54856958 -0.67550787 -2.69002207]. 
Working on 0.4 fold... 
[Fri Apr  5 00:43:24 2024]  Iteration number: 0 with current cost as 0.32931751136549337 and parameters 
[-1.81187897  2.23743464 -2.12427956 -0.116531    0.55388708 -2.770109
  3.06858498  2.18960143  1.18552004 -1.06648308  0.60271513  1.14432445
  1.31029904 -1.87354675  0.72965078  2.88578419 -0.54534335 -0.47522483
 -2.02654243  0.72897372  1.60512666  2.83077104 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 00:50:31 2024]  Iteration number: 0 with current cost as 0.302921871290067 and parameters 
[-1.61223999  2.2374346  -2.12427955 -0.11653106  0.55388705 -2.77010903
  3.06858489  2.18960142  1.18552005 -1.06648312  0.60271507  1.14432442
  1.31029902 -1.87354677  0.72965071  2.88578416 -0.54534338 -0.47522488
 -2.02654247  0.72897366  1.60512661  2.83077104 -1.26456713 -0.25136105
 -2.39279218 -2.27309774  3.13337152  2.54856955 -0.67550787 -2.69002205]. 
Working on 0.8 fold... 
[Fri Apr  5 00:54:22 2024]  Iteration number: 0 with current cost as 0.31411556266347024 and parameters 
[-1.73764243  2.23743464 -2.12427953 -0.11653097  0.55388705 -2.77010903
  3.06858498  2.18960145  1.18552004 -1.06648311  0.6027151   1.14432445
  1.31029904 -1.87354675  0.72965075  2.88578419 -0.54534335 -0.4752248
 -2.02654243  0.7289737   1.60512661  2.83077102 -1.26456712 -0.25136105
 -2.39279218 -2.27309774  3.13337152  2.54856958 -0.67550787 -2.69002204]. 
Working on 1.0 fold... 
[Fri Apr  5 01:00:14 2024]  Iteration number: 0 with current cost as 0.327780467801287 and parameters 
[-1.79022485  2.23743464 -2.12427954 -0.11653098  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18552003 -1.06648308  0.6027151   1.14432445
  1.31029901 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.4752248
 -2.02654243  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1722.2376523017883 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 01:06:03 2024]  Iteration number: 0 with current cost as 0.32710204119873 and parameters 
[-1.76826899  2.23743459 -2.12427959 -0.11653103  0.55388703 -2.77010902
  3.06858493  2.1896014   1.18552003 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965075  2.88578419 -0.5453434  -0.47522485
 -2.0265425   0.72897365  1.60512659  2.83077097 -1.26456715 -0.25136105
 -2.39279218 -2.27309774  3.1333715   2.54856958 -0.67550787 -2.69002207]. 
Working on 0.4 fold... 
[Fri Apr  5 01:11:56 2024]  Iteration number: 0 with current cost as 0.32931751136549337 and parameters 
[-1.81187897  2.23743464 -2.12427956 -0.116531    0.55388708 -2.770109
  3.06858498  2.18960143  1.18552004 -1.06648308  0.60271513  1.14432445
  1.31029904 -1.87354675  0.72965078  2.88578419 -0.54534335 -0.47522483
 -2.02654243  0.72897372  1.60512666  2.83077104 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 01:18:57 2024]  Iteration number: 0 with current cost as 0.302921871290067 and parameters 
[-1.61223999  2.2374346  -2.12427955 -0.11653106  0.55388705 -2.77010903
  3.06858489  2.18960142  1.18552005 -1.06648312  0.60271507  1.14432442
  1.31029902 -1.87354677  0.72965071  2.88578416 -0.54534338 -0.47522488
 -2.02654247  0.72897366  1.60512661  2.83077104 -1.26456713 -0.25136105
 -2.39279218 -2.27309774  3.13337152  2.54856955 -0.67550787 -2.69002205]. 
Working on 0.8 fold... 
[Fri Apr  5 01:22:54 2024]  Iteration number: 0 with current cost as 0.31411556266347024 and parameters 
[-1.73764243  2.23743464 -2.12427953 -0.11653097  0.55388705 -2.77010903
  3.06858498  2.18960145  1.18552004 -1.06648311  0.6027151   1.14432445
  1.31029904 -1.87354675  0.72965075  2.88578419 -0.54534335 -0.4752248
 -2.02654243  0.7289737   1.60512661  2.83077102 -1.26456712 -0.25136105
 -2.39279218 -2.27309774  3.13337152  2.54856958 -0.67550787 -2.69002204]. 
Working on 1.0 fold... 
[Fri Apr  5 01:28:39 2024]  Iteration number: 0 with current cost as 0.327780467801287 and parameters 
[-1.79022485  2.23743464 -2.12427954 -0.11653098  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18552003 -1.06648308  0.6027151   1.14432445
  1.31029901 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.4752248
 -2.02654243  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1699.488776922226 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 01:34:20 2024]  Iteration number: 0 with current cost as 0.32710204119873 and parameters 
[-1.76826899  2.23743459 -2.12427959 -0.11653103  0.55388703 -2.77010902
  3.06858493  2.1896014   1.18552003 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965075  2.88578419 -0.5453434  -0.47522485
 -2.0265425   0.72897365  1.60512659  2.83077097 -1.26456715 -0.25136105
 -2.39279218 -2.27309774  3.1333715   2.54856958 -0.67550787 -2.69002207]. 
Working on 0.4 fold... 
[Fri Apr  5 01:40:17 2024]  Iteration number: 0 with current cost as 0.32931751136549337 and parameters 
[-1.81187897  2.23743464 -2.12427956 -0.116531    0.55388708 -2.770109
  3.06858498  2.18960143  1.18552004 -1.06648308  0.60271513  1.14432445
  1.31029904 -1.87354675  0.72965078  2.88578419 -0.54534335 -0.47522483
 -2.02654243  0.72897372  1.60512666  2.83077104 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 01:47:43 2024]  Iteration number: 0 with current cost as 0.302921871290067 and parameters 
[-1.61223999  2.2374346  -2.12427955 -0.11653106  0.55388705 -2.77010903
  3.06858489  2.18960142  1.18552005 -1.06648312  0.60271507  1.14432442
  1.31029902 -1.87354677  0.72965071  2.88578416 -0.54534338 -0.47522488
 -2.02654247  0.72897366  1.60512661  2.83077104 -1.26456713 -0.25136105
 -2.39279218 -2.27309774  3.13337152  2.54856955 -0.67550787 -2.69002205]. 
Working on 0.8 fold... 
[Fri Apr  5 01:51:34 2024]  Iteration number: 0 with current cost as 0.31411556266347024 and parameters 
[-1.73764243  2.23743464 -2.12427953 -0.11653097  0.55388705 -2.77010903
  3.06858498  2.18960145  1.18552004 -1.06648311  0.6027151   1.14432445
  1.31029904 -1.87354675  0.72965075  2.88578419 -0.54534335 -0.4752248
 -2.02654243  0.7289737   1.60512661  2.83077102 -1.26456712 -0.25136105
 -2.39279218 -2.27309774  3.13337152  2.54856958 -0.67550787 -2.69002204]. 
Working on 1.0 fold... 
[Fri Apr  5 01:57:25 2024]  Iteration number: 0 with current cost as 0.327780467801287 and parameters 
[-1.79022485  2.23743464 -2.12427954 -0.11653098  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18552003 -1.06648308  0.6027151   1.14432445
  1.31029901 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.4752248
 -2.02654243  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1729.2473602294922 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 02:03:17 2024]  Iteration number: 0 with current cost as 0.32710204119873 and parameters 
[-1.76826899  2.23743459 -2.12427959 -0.11653103  0.55388703 -2.77010902
  3.06858493  2.1896014   1.18552003 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965075  2.88578419 -0.5453434  -0.47522485
 -2.0265425   0.72897365  1.60512659  2.83077097 -1.26456715 -0.25136105
 -2.39279218 -2.27309774  3.1333715   2.54856958 -0.67550787 -2.69002207]. 
Working on 0.4 fold... 
[Fri Apr  5 02:09:05 2024]  Iteration number: 0 with current cost as 0.32931751136549337 and parameters 
[-1.81187897  2.23743464 -2.12427956 -0.116531    0.55388708 -2.770109
  3.06858498  2.18960143  1.18552004 -1.06648308  0.60271513  1.14432445
  1.31029904 -1.87354675  0.72965078  2.88578419 -0.54534335 -0.47522483
 -2.02654243  0.72897372  1.60512666  2.83077104 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 02:16:18 2024]  Iteration number: 0 with current cost as 0.302921871290067 and parameters 
[-1.61223999  2.2374346  -2.12427955 -0.11653106  0.55388705 -2.77010903
  3.06858489  2.18960142  1.18552005 -1.06648312  0.60271507  1.14432442
  1.31029902 -1.87354677  0.72965071  2.88578416 -0.54534338 -0.47522488
 -2.02654247  0.72897366  1.60512661  2.83077104 -1.26456713 -0.25136105
 -2.39279218 -2.27309774  3.13337152  2.54856955 -0.67550787 -2.69002205]. 
Working on 0.8 fold... 
[Fri Apr  5 02:20:14 2024]  Iteration number: 0 with current cost as 0.31411556266347024 and parameters 
[-1.73764243  2.23743464 -2.12427953 -0.11653097  0.55388705 -2.77010903
  3.06858498  2.18960145  1.18552004 -1.06648311  0.6027151   1.14432445
  1.31029904 -1.87354675  0.72965075  2.88578419 -0.54534335 -0.4752248
 -2.02654243  0.7289737   1.60512661  2.83077102 -1.26456712 -0.25136105
 -2.39279218 -2.27309774  3.13337152  2.54856958 -0.67550787 -2.69002204]. 
Working on 1.0 fold... 
[Fri Apr  5 02:26:00 2024]  Iteration number: 0 with current cost as 0.327780467801287 and parameters 
[-1.79022485  2.23743464 -2.12427954 -0.11653098  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18552003 -1.06648308  0.6027151   1.14432445
  1.31029901 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.4752248
 -2.02654243  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1712.3036019802094 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 02:31:49 2024]  Iteration number: 0 with current cost as 0.32710204119873 and parameters 
[-1.76826899  2.23743459 -2.12427959 -0.11653103  0.55388703 -2.77010902
  3.06858493  2.1896014   1.18552003 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965075  2.88578419 -0.5453434  -0.47522485
 -2.0265425   0.72897365  1.60512659  2.83077097 -1.26456715 -0.25136105
 -2.39279218 -2.27309774  3.1333715   2.54856958 -0.67550787 -2.69002207]. 
Working on 0.4 fold... 
[Fri Apr  5 02:37:44 2024]  Iteration number: 0 with current cost as 0.32931751136549337 and parameters 
[-1.81187897  2.23743464 -2.12427956 -0.116531    0.55388708 -2.770109
  3.06858498  2.18960143  1.18552004 -1.06648308  0.60271513  1.14432445
  1.31029904 -1.87354675  0.72965078  2.88578419 -0.54534335 -0.47522483
 -2.02654243  0.72897372  1.60512666  2.83077104 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 02:45:03 2024]  Iteration number: 0 with current cost as 0.302921871290067 and parameters 
[-1.61223999  2.2374346  -2.12427955 -0.11653106  0.55388705 -2.77010903
  3.06858489  2.18960142  1.18552005 -1.06648312  0.60271507  1.14432442
  1.31029902 -1.87354677  0.72965071  2.88578416 -0.54534338 -0.47522488
 -2.02654247  0.72897366  1.60512661  2.83077104 -1.26456713 -0.25136105
 -2.39279218 -2.27309774  3.13337152  2.54856955 -0.67550787 -2.69002205]. 
Working on 0.8 fold... 
[Fri Apr  5 02:49:02 2024]  Iteration number: 0 with current cost as 0.31411556266347024 and parameters 
[-1.73764243  2.23743464 -2.12427953 -0.11653097  0.55388705 -2.77010903
  3.06858498  2.18960145  1.18552004 -1.06648311  0.6027151   1.14432445
  1.31029904 -1.87354675  0.72965075  2.88578419 -0.54534335 -0.4752248
 -2.02654243  0.7289737   1.60512661  2.83077102 -1.26456712 -0.25136105
 -2.39279218 -2.27309774  3.13337152  2.54856958 -0.67550787 -2.69002204]. 
Working on 1.0 fold... 
[Fri Apr  5 02:54:57 2024]  Iteration number: 0 with current cost as 0.327780467801287 and parameters 
[-1.79022485  2.23743464 -2.12427954 -0.11653098  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18552003 -1.06648308  0.6027151   1.14432445
  1.31029901 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.4752248
 -2.02654243  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1736.2721428871155 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 03:00:42 2024]  Iteration number: 0 with current cost as 0.32710204119873 and parameters 
[-1.76826899  2.23743459 -2.12427959 -0.11653103  0.55388703 -2.77010902
  3.06858493  2.1896014   1.18552003 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965075  2.88578419 -0.5453434  -0.47522485
 -2.0265425   0.72897365  1.60512659  2.83077097 -1.26456715 -0.25136105
 -2.39279218 -2.27309774  3.1333715   2.54856958 -0.67550787 -2.69002207]. 
Working on 0.4 fold... 
[Fri Apr  5 03:06:41 2024]  Iteration number: 0 with current cost as 0.32931751136549337 and parameters 
[-1.81187897  2.23743464 -2.12427956 -0.116531    0.55388708 -2.770109
  3.06858498  2.18960143  1.18552004 -1.06648308  0.60271513  1.14432445
  1.31029904 -1.87354675  0.72965078  2.88578419 -0.54534335 -0.47522483
 -2.02654243  0.72897372  1.60512666  2.83077104 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 03:13:47 2024]  Iteration number: 0 with current cost as 0.302921871290067 and parameters 
[-1.61223999  2.2374346  -2.12427955 -0.11653106  0.55388705 -2.77010903
  3.06858489  2.18960142  1.18552005 -1.06648312  0.60271507  1.14432442
  1.31029902 -1.87354677  0.72965071  2.88578416 -0.54534338 -0.47522488
 -2.02654247  0.72897366  1.60512661  2.83077104 -1.26456713 -0.25136105
 -2.39279218 -2.27309774  3.13337152  2.54856955 -0.67550787 -2.69002205]. 
Working on 0.8 fold... 
[Fri Apr  5 03:17:37 2024]  Iteration number: 0 with current cost as 0.31411556266347024 and parameters 
[-1.73764243  2.23743464 -2.12427953 -0.11653097  0.55388705 -2.77010903
  3.06858498  2.18960145  1.18552004 -1.06648311  0.6027151   1.14432445
  1.31029904 -1.87354675  0.72965075  2.88578419 -0.54534335 -0.4752248
 -2.02654243  0.7289737   1.60512661  2.83077102 -1.26456712 -0.25136105
 -2.39279218 -2.27309774  3.13337152  2.54856958 -0.67550787 -2.69002204]. 
Working on 1.0 fold... 
[Fri Apr  5 03:23:30 2024]  Iteration number: 0 with current cost as 0.327780467801287 and parameters 
[-1.79022485  2.23743464 -2.12427954 -0.11653098  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18552003 -1.06648308  0.6027151   1.14432445
  1.31029901 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.4752248
 -2.02654243  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1721.1630067825317 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 03:29:23 2024]  Iteration number: 0 with current cost as 0.32710204119873 and parameters 
[-1.76826899  2.23743459 -2.12427959 -0.11653103  0.55388703 -2.77010902
  3.06858493  2.1896014   1.18552003 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965075  2.88578419 -0.5453434  -0.47522485
 -2.0265425   0.72897365  1.60512659  2.83077097 -1.26456715 -0.25136105
 -2.39279218 -2.27309774  3.1333715   2.54856958 -0.67550787 -2.69002207]. 
Working on 0.4 fold... 
[Fri Apr  5 03:35:18 2024]  Iteration number: 0 with current cost as 0.32931751136549337 and parameters 
[-1.81187897  2.23743464 -2.12427956 -0.116531    0.55388708 -2.770109
  3.06858498  2.18960143  1.18552004 -1.06648308  0.60271513  1.14432445
  1.31029904 -1.87354675  0.72965078  2.88578419 -0.54534335 -0.47522483
 -2.02654243  0.72897372  1.60512666  2.83077104 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 03:42:27 2024]  Iteration number: 0 with current cost as 0.302921871290067 and parameters 
[-1.61223999  2.2374346  -2.12427955 -0.11653106  0.55388705 -2.77010903
  3.06858489  2.18960142  1.18552005 -1.06648312  0.60271507  1.14432442
  1.31029902 -1.87354677  0.72965071  2.88578416 -0.54534338 -0.47522488
 -2.02654247  0.72897366  1.60512661  2.83077104 -1.26456713 -0.25136105
 -2.39279218 -2.27309774  3.13337152  2.54856955 -0.67550787 -2.69002205]. 
Working on 0.8 fold... 
[Fri Apr  5 03:46:18 2024]  Iteration number: 0 with current cost as 0.31411556266347024 and parameters 
[-1.73764243  2.23743464 -2.12427953 -0.11653097  0.55388705 -2.77010903
  3.06858498  2.18960145  1.18552004 -1.06648311  0.6027151   1.14432445
  1.31029904 -1.87354675  0.72965075  2.88578419 -0.54534335 -0.4752248
 -2.02654243  0.7289737   1.60512661  2.83077102 -1.26456712 -0.25136105
 -2.39279218 -2.27309774  3.13337152  2.54856958 -0.67550787 -2.69002204]. 
Working on 1.0 fold... 
[Fri Apr  5 03:52:12 2024]  Iteration number: 0 with current cost as 0.327780467801287 and parameters 
[-1.79022485  2.23743464 -2.12427954 -0.11653098  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18552003 -1.06648308  0.6027151   1.14432445
  1.31029901 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.4752248
 -2.02654243  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1712.795345067978 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 03:57:56 2024]  Iteration number: 0 with current cost as 0.32710204119873 and parameters 
[-1.76826899  2.23743459 -2.12427959 -0.11653103  0.55388703 -2.77010902
  3.06858493  2.1896014   1.18552003 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965075  2.88578419 -0.5453434  -0.47522485
 -2.0265425   0.72897365  1.60512659  2.83077097 -1.26456715 -0.25136105
 -2.39279218 -2.27309774  3.1333715   2.54856958 -0.67550787 -2.69002207]. 
Working on 0.4 fold... 
[Fri Apr  5 04:03:46 2024]  Iteration number: 0 with current cost as 0.32931751136549337 and parameters 
[-1.81187897  2.23743464 -2.12427956 -0.116531    0.55388708 -2.770109
  3.06858498  2.18960143  1.18552004 -1.06648308  0.60271513  1.14432445
  1.31029904 -1.87354675  0.72965078  2.88578419 -0.54534335 -0.47522483
 -2.02654243  0.72897372  1.60512666  2.83077104 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 04:10:53 2024]  Iteration number: 0 with current cost as 0.302921871290067 and parameters 
[-1.61223999  2.2374346  -2.12427955 -0.11653106  0.55388705 -2.77010903
  3.06858489  2.18960142  1.18552005 -1.06648312  0.60271507  1.14432442
  1.31029902 -1.87354677  0.72965071  2.88578416 -0.54534338 -0.47522488
 -2.02654247  0.72897366  1.60512661  2.83077104 -1.26456713 -0.25136105
 -2.39279218 -2.27309774  3.13337152  2.54856955 -0.67550787 -2.69002205]. 
Working on 0.8 fold... 
[Fri Apr  5 04:14:54 2024]  Iteration number: 0 with current cost as 0.31411556266347024 and parameters 
[-1.73764243  2.23743464 -2.12427953 -0.11653097  0.55388705 -2.77010903
  3.06858498  2.18960145  1.18552004 -1.06648311  0.6027151   1.14432445
  1.31029904 -1.87354675  0.72965075  2.88578419 -0.54534335 -0.4752248
 -2.02654243  0.7289737   1.60512661  2.83077102 -1.26456712 -0.25136105
 -2.39279218 -2.27309774  3.13337152  2.54856958 -0.67550787 -2.69002204]. 
Working on 1.0 fold... 
[Fri Apr  5 04:20:42 2024]  Iteration number: 0 with current cost as 0.327780467801287 and parameters 
[-1.79022485  2.23743464 -2.12427954 -0.11653098  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18552003 -1.06648308  0.6027151   1.14432445
  1.31029901 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.4752248
 -2.02654243  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1713.6471185684204 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 04:26:36 2024]  Iteration number: 0 with current cost as 0.32710204119873 and parameters 
[-1.76826899  2.23743459 -2.12427959 -0.11653103  0.55388703 -2.77010902
  3.06858493  2.1896014   1.18552003 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965075  2.88578419 -0.5453434  -0.47522485
 -2.0265425   0.72897365  1.60512659  2.83077097 -1.26456715 -0.25136105
 -2.39279218 -2.27309774  3.1333715   2.54856958 -0.67550787 -2.69002207]. 
Working on 0.4 fold... 
[Fri Apr  5 04:32:21 2024]  Iteration number: 0 with current cost as 0.32931751136549337 and parameters 
[-1.81187897  2.23743464 -2.12427956 -0.116531    0.55388708 -2.770109
  3.06858498  2.18960143  1.18552004 -1.06648308  0.60271513  1.14432445
  1.31029904 -1.87354675  0.72965078  2.88578419 -0.54534335 -0.47522483
 -2.02654243  0.72897372  1.60512666  2.83077104 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 04:39:26 2024]  Iteration number: 0 with current cost as 0.302921871290067 and parameters 
[-1.61223999  2.2374346  -2.12427955 -0.11653106  0.55388705 -2.77010903
  3.06858489  2.18960142  1.18552005 -1.06648312  0.60271507  1.14432442
  1.31029902 -1.87354677  0.72965071  2.88578416 -0.54534338 -0.47522488
 -2.02654247  0.72897366  1.60512661  2.83077104 -1.26456713 -0.25136105
 -2.39279218 -2.27309774  3.13337152  2.54856955 -0.67550787 -2.69002205]. 
Working on 0.8 fold... 
[Fri Apr  5 04:43:19 2024]  Iteration number: 0 with current cost as 0.31411556266347024 and parameters 
[-1.73764243  2.23743464 -2.12427953 -0.11653097  0.55388705 -2.77010903
  3.06858498  2.18960145  1.18552004 -1.06648311  0.6027151   1.14432445
  1.31029904 -1.87354675  0.72965075  2.88578419 -0.54534335 -0.4752248
 -2.02654243  0.7289737   1.60512661  2.83077102 -1.26456712 -0.25136105
 -2.39279218 -2.27309774  3.13337152  2.54856958 -0.67550787 -2.69002204]. 
Working on 1.0 fold... 
[Fri Apr  5 04:49:14 2024]  Iteration number: 0 with current cost as 0.327780467801287 and parameters 
[-1.79022485  2.23743464 -2.12427954 -0.11653098  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18552003 -1.06648308  0.6027151   1.14432445
  1.31029901 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.4752248
 -2.02654243  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1720.8187274932861 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 04:55:13 2024]  Iteration number: 0 with current cost as 0.32710204119873 and parameters 
[-1.76826899  2.23743459 -2.12427959 -0.11653103  0.55388703 -2.77010902
  3.06858493  2.1896014   1.18552003 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965075  2.88578419 -0.5453434  -0.47522485
 -2.0265425   0.72897365  1.60512659  2.83077097 -1.26456715 -0.25136105
 -2.39279218 -2.27309774  3.1333715   2.54856958 -0.67550787 -2.69002207]. 
Working on 0.4 fold... 
[Fri Apr  5 05:01:01 2024]  Iteration number: 0 with current cost as 0.32931751136549337 and parameters 
[-1.81187897  2.23743464 -2.12427956 -0.116531    0.55388708 -2.770109
  3.06858498  2.18960143  1.18552004 -1.06648308  0.60271513  1.14432445
  1.31029904 -1.87354675  0.72965078  2.88578419 -0.54534335 -0.47522483
 -2.02654243  0.72897372  1.60512666  2.83077104 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 05:08:17 2024]  Iteration number: 0 with current cost as 0.302921871290067 and parameters 
[-1.61223999  2.2374346  -2.12427955 -0.11653106  0.55388705 -2.77010903
  3.06858489  2.18960142  1.18552005 -1.06648312  0.60271507  1.14432442
  1.31029902 -1.87354677  0.72965071  2.88578416 -0.54534338 -0.47522488
 -2.02654247  0.72897366  1.60512661  2.83077104 -1.26456713 -0.25136105
 -2.39279218 -2.27309774  3.13337152  2.54856955 -0.67550787 -2.69002205]. 
Working on 0.8 fold... 
[Fri Apr  5 05:12:13 2024]  Iteration number: 0 with current cost as 0.31411556266347024 and parameters 
[-1.73764243  2.23743464 -2.12427953 -0.11653097  0.55388705 -2.77010903
  3.06858498  2.18960145  1.18552004 -1.06648311  0.6027151   1.14432445
  1.31029904 -1.87354675  0.72965075  2.88578419 -0.54534335 -0.4752248
 -2.02654243  0.7289737   1.60512661  2.83077102 -1.26456712 -0.25136105
 -2.39279218 -2.27309774  3.13337152  2.54856958 -0.67550787 -2.69002204]. 
Working on 1.0 fold... 
[Fri Apr  5 05:17:58 2024]  Iteration number: 0 with current cost as 0.327780467801287 and parameters 
[-1.79022485  2.23743464 -2.12427954 -0.11653098  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18552003 -1.06648308  0.6027151   1.14432445
  1.31029901 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.4752248
 -2.02654243  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1718.3124628067017 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 05:23:50 2024]  Iteration number: 0 with current cost as 0.32710204119873 and parameters 
[-1.76826899  2.23743459 -2.12427959 -0.11653103  0.55388703 -2.77010902
  3.06858493  2.1896014   1.18552003 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965075  2.88578419 -0.5453434  -0.47522485
 -2.0265425   0.72897365  1.60512659  2.83077097 -1.26456715 -0.25136105
 -2.39279218 -2.27309774  3.1333715   2.54856958 -0.67550787 -2.69002207]. 
Working on 0.4 fold... 
[Fri Apr  5 05:29:33 2024]  Iteration number: 0 with current cost as 0.32931751136549337 and parameters 
[-1.81187897  2.23743464 -2.12427956 -0.116531    0.55388708 -2.770109
  3.06858498  2.18960143  1.18552004 -1.06648308  0.60271513  1.14432445
  1.31029904 -1.87354675  0.72965078  2.88578419 -0.54534335 -0.47522483
 -2.02654243  0.72897372  1.60512666  2.83077104 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 05:36:40 2024]  Iteration number: 0 with current cost as 0.302921871290067 and parameters 
[-1.61223999  2.2374346  -2.12427955 -0.11653106  0.55388705 -2.77010903
  3.06858489  2.18960142  1.18552005 -1.06648312  0.60271507  1.14432442
  1.31029902 -1.87354677  0.72965071  2.88578416 -0.54534338 -0.47522488
 -2.02654247  0.72897366  1.60512661  2.83077104 -1.26456713 -0.25136105
 -2.39279218 -2.27309774  3.13337152  2.54856955 -0.67550787 -2.69002205]. 
Working on 0.8 fold... 
[Fri Apr  5 05:40:34 2024]  Iteration number: 0 with current cost as 0.31411556266347024 and parameters 
[-1.73764243  2.23743464 -2.12427953 -0.11653097  0.55388705 -2.77010903
  3.06858498  2.18960145  1.18552004 -1.06648311  0.6027151   1.14432445
  1.31029904 -1.87354675  0.72965075  2.88578419 -0.54534335 -0.4752248
 -2.02654243  0.7289737   1.60512661  2.83077102 -1.26456712 -0.25136105
 -2.39279218 -2.27309774  3.13337152  2.54856958 -0.67550787 -2.69002204]. 
Working on 1.0 fold... 
[Fri Apr  5 05:46:17 2024]  Iteration number: 0 with current cost as 0.327780467801287 and parameters 
[-1.79022485  2.23743464 -2.12427954 -0.11653098  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18552003 -1.06648308  0.6027151   1.14432445
  1.31029901 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.4752248
 -2.02654243  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1693.9887137413025 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 05:52:05 2024]  Iteration number: 0 with current cost as 0.32710204119873 and parameters 
[-1.76826899  2.23743459 -2.12427959 -0.11653103  0.55388703 -2.77010902
  3.06858493  2.1896014   1.18552003 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965075  2.88578419 -0.5453434  -0.47522485
 -2.0265425   0.72897365  1.60512659  2.83077097 -1.26456715 -0.25136105
 -2.39279218 -2.27309774  3.1333715   2.54856958 -0.67550787 -2.69002207]. 
Working on 0.4 fold... 
[Fri Apr  5 05:57:59 2024]  Iteration number: 0 with current cost as 0.32931751136549337 and parameters 
[-1.81187897  2.23743464 -2.12427956 -0.116531    0.55388708 -2.770109
  3.06858498  2.18960143  1.18552004 -1.06648308  0.60271513  1.14432445
  1.31029904 -1.87354675  0.72965078  2.88578419 -0.54534335 -0.47522483
 -2.02654243  0.72897372  1.60512666  2.83077104 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 06:05:00 2024]  Iteration number: 0 with current cost as 0.302921871290067 and parameters 
[-1.61223999  2.2374346  -2.12427955 -0.11653106  0.55388705 -2.77010903
  3.06858489  2.18960142  1.18552005 -1.06648312  0.60271507  1.14432442
  1.31029902 -1.87354677  0.72965071  2.88578416 -0.54534338 -0.47522488
 -2.02654247  0.72897366  1.60512661  2.83077104 -1.26456713 -0.25136105
 -2.39279218 -2.27309774  3.13337152  2.54856955 -0.67550787 -2.69002205]. 
Working on 0.8 fold... 
[Fri Apr  5 06:08:56 2024]  Iteration number: 0 with current cost as 0.31411556266347024 and parameters 
[-1.73764243  2.23743464 -2.12427953 -0.11653097  0.55388705 -2.77010903
  3.06858498  2.18960145  1.18552004 -1.06648311  0.6027151   1.14432445
  1.31029904 -1.87354675  0.72965075  2.88578419 -0.54534335 -0.4752248
 -2.02654243  0.7289737   1.60512661  2.83077102 -1.26456712 -0.25136105
 -2.39279218 -2.27309774  3.13337152  2.54856958 -0.67550787 -2.69002204]. 
Working on 1.0 fold... 
[Fri Apr  5 06:14:52 2024]  Iteration number: 0 with current cost as 0.327780467801287 and parameters 
[-1.79022485  2.23743464 -2.12427954 -0.11653098  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18552003 -1.06648308  0.6027151   1.14432445
  1.31029901 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.4752248
 -2.02654243  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1715.0609016418457 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 06:20:37 2024]  Iteration number: 0 with current cost as 0.32710204119873 and parameters 
[-1.76826899  2.23743459 -2.12427959 -0.11653103  0.55388703 -2.77010902
  3.06858493  2.1896014   1.18552003 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965075  2.88578419 -0.5453434  -0.47522485
 -2.0265425   0.72897365  1.60512659  2.83077097 -1.26456715 -0.25136105
 -2.39279218 -2.27309774  3.1333715   2.54856958 -0.67550787 -2.69002207]. 
Working on 0.4 fold... 
[Fri Apr  5 06:26:23 2024]  Iteration number: 0 with current cost as 0.32931751136549337 and parameters 
[-1.81187897  2.23743464 -2.12427956 -0.116531    0.55388708 -2.770109
  3.06858498  2.18960143  1.18552004 -1.06648308  0.60271513  1.14432445
  1.31029904 -1.87354675  0.72965078  2.88578419 -0.54534335 -0.47522483
 -2.02654243  0.72897372  1.60512666  2.83077104 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 06:33:38 2024]  Iteration number: 0 with current cost as 0.302921871290067 and parameters 
[-1.61223999  2.2374346  -2.12427955 -0.11653106  0.55388705 -2.77010903
  3.06858489  2.18960142  1.18552005 -1.06648312  0.60271507  1.14432442
  1.31029902 -1.87354677  0.72965071  2.88578416 -0.54534338 -0.47522488
 -2.02654247  0.72897366  1.60512661  2.83077104 -1.26456713 -0.25136105
 -2.39279218 -2.27309774  3.13337152  2.54856955 -0.67550787 -2.69002205]. 
Working on 0.8 fold... 
[Fri Apr  5 06:37:26 2024]  Iteration number: 0 with current cost as 0.31411556266347024 and parameters 
[-1.73764243  2.23743464 -2.12427953 -0.11653097  0.55388705 -2.77010903
  3.06858498  2.18960145  1.18552004 -1.06648311  0.6027151   1.14432445
  1.31029904 -1.87354675  0.72965075  2.88578419 -0.54534335 -0.4752248
 -2.02654243  0.7289737   1.60512661  2.83077102 -1.26456712 -0.25136105
 -2.39279218 -2.27309774  3.13337152  2.54856958 -0.67550787 -2.69002204]. 
Working on 1.0 fold... 
[Fri Apr  5 06:43:10 2024]  Iteration number: 0 with current cost as 0.327780467801287 and parameters 
[-1.79022485  2.23743464 -2.12427954 -0.11653098  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18552003 -1.06648308  0.6027151   1.14432445
  1.31029901 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.4752248
 -2.02654243  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1695.1379160881042 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 06:48:52 2024]  Iteration number: 0 with current cost as 0.32710204119873 and parameters 
[-1.76826899  2.23743459 -2.12427959 -0.11653103  0.55388703 -2.77010902
  3.06858493  2.1896014   1.18552003 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965075  2.88578419 -0.5453434  -0.47522485
 -2.0265425   0.72897365  1.60512659  2.83077097 -1.26456715 -0.25136105
 -2.39279218 -2.27309774  3.1333715   2.54856958 -0.67550787 -2.69002207]. 
Working on 0.4 fold... 
[Fri Apr  5 06:54:42 2024]  Iteration number: 0 with current cost as 0.32931751136549337 and parameters 
[-1.81187897  2.23743464 -2.12427956 -0.116531    0.55388708 -2.770109
  3.06858498  2.18960143  1.18552004 -1.06648308  0.60271513  1.14432445
  1.31029904 -1.87354675  0.72965078  2.88578419 -0.54534335 -0.47522483
 -2.02654243  0.72897372  1.60512666  2.83077104 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 07:01:56 2024]  Iteration number: 0 with current cost as 0.302921871290067 and parameters 
[-1.61223999  2.2374346  -2.12427955 -0.11653106  0.55388705 -2.77010903
  3.06858489  2.18960142  1.18552005 -1.06648312  0.60271507  1.14432442
  1.31029902 -1.87354677  0.72965071  2.88578416 -0.54534338 -0.47522488
 -2.02654247  0.72897366  1.60512661  2.83077104 -1.26456713 -0.25136105
 -2.39279218 -2.27309774  3.13337152  2.54856955 -0.67550787 -2.69002205]. 
Working on 0.8 fold... 
[Fri Apr  5 07:05:47 2024]  Iteration number: 0 with current cost as 0.31411556266347024 and parameters 
[-1.73764243  2.23743464 -2.12427953 -0.11653097  0.55388705 -2.77010903
  3.06858498  2.18960145  1.18552004 -1.06648311  0.6027151   1.14432445
  1.31029904 -1.87354675  0.72965075  2.88578419 -0.54534335 -0.4752248
 -2.02654243  0.7289737   1.60512661  2.83077102 -1.26456712 -0.25136105
 -2.39279218 -2.27309774  3.13337152  2.54856958 -0.67550787 -2.69002204]. 
Working on 1.0 fold... 
[Fri Apr  5 07:11:33 2024]  Iteration number: 0 with current cost as 0.327780467801287 and parameters 
[-1.79022485  2.23743464 -2.12427954 -0.11653098  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18552003 -1.06648308  0.6027151   1.14432445
  1.31029901 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.4752248
 -2.02654243  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1707.7057764530182 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 07:17:30 2024]  Iteration number: 0 with current cost as 0.32710204119873 and parameters 
[-1.76826899  2.23743459 -2.12427959 -0.11653103  0.55388703 -2.77010902
  3.06858493  2.1896014   1.18552003 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965075  2.88578419 -0.5453434  -0.47522485
 -2.0265425   0.72897365  1.60512659  2.83077097 -1.26456715 -0.25136105
 -2.39279218 -2.27309774  3.1333715   2.54856958 -0.67550787 -2.69002207]. 
Working on 0.4 fold... 
[Fri Apr  5 07:23:16 2024]  Iteration number: 0 with current cost as 0.32931751136549337 and parameters 
[-1.81187897  2.23743464 -2.12427956 -0.116531    0.55388708 -2.770109
  3.06858498  2.18960143  1.18552004 -1.06648308  0.60271513  1.14432445
  1.31029904 -1.87354675  0.72965078  2.88578419 -0.54534335 -0.47522483
 -2.02654243  0.72897372  1.60512666  2.83077104 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 07:30:21 2024]  Iteration number: 0 with current cost as 0.302921871290067 and parameters 
[-1.61223999  2.2374346  -2.12427955 -0.11653106  0.55388705 -2.77010903
  3.06858489  2.18960142  1.18552005 -1.06648312  0.60271507  1.14432442
  1.31029902 -1.87354677  0.72965071  2.88578416 -0.54534338 -0.47522488
 -2.02654247  0.72897366  1.60512661  2.83077104 -1.26456713 -0.25136105
 -2.39279218 -2.27309774  3.13337152  2.54856955 -0.67550787 -2.69002205]. 
Working on 0.8 fold... 
[Fri Apr  5 07:34:19 2024]  Iteration number: 0 with current cost as 0.31411556266347024 and parameters 
[-1.73764243  2.23743464 -2.12427953 -0.11653097  0.55388705 -2.77010903
  3.06858498  2.18960145  1.18552004 -1.06648311  0.6027151   1.14432445
  1.31029904 -1.87354675  0.72965075  2.88578419 -0.54534335 -0.4752248
 -2.02654243  0.7289737   1.60512661  2.83077102 -1.26456712 -0.25136105
 -2.39279218 -2.27309774  3.13337152  2.54856958 -0.67550787 -2.69002204]. 
Working on 1.0 fold... 
[Fri Apr  5 07:40:03 2024]  Iteration number: 0 with current cost as 0.327780467801287 and parameters 
[-1.79022485  2.23743464 -2.12427954 -0.11653098  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18552003 -1.06648308  0.6027151   1.14432445
  1.31029901 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.4752248
 -2.02654243  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1713.45845413208 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 07:45:51 2024]  Iteration number: 0 with current cost as 0.32710204119873 and parameters 
[-1.76826899  2.23743459 -2.12427959 -0.11653103  0.55388703 -2.77010902
  3.06858493  2.1896014   1.18552003 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965075  2.88578419 -0.5453434  -0.47522485
 -2.0265425   0.72897365  1.60512659  2.83077097 -1.26456715 -0.25136105
 -2.39279218 -2.27309774  3.1333715   2.54856958 -0.67550787 -2.69002207]. 
Working on 0.4 fold... 
[Fri Apr  5 07:51:32 2024]  Iteration number: 0 with current cost as 0.32931751136549337 and parameters 
[-1.81187897  2.23743464 -2.12427956 -0.116531    0.55388708 -2.770109
  3.06858498  2.18960143  1.18552004 -1.06648308  0.60271513  1.14432445
  1.31029904 -1.87354675  0.72965078  2.88578419 -0.54534335 -0.47522483
 -2.02654243  0.72897372  1.60512666  2.83077104 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 07:58:35 2024]  Iteration number: 0 with current cost as 0.302921871290067 and parameters 
[-1.61223999  2.2374346  -2.12427955 -0.11653106  0.55388705 -2.77010903
  3.06858489  2.18960142  1.18552005 -1.06648312  0.60271507  1.14432442
  1.31029902 -1.87354677  0.72965071  2.88578416 -0.54534338 -0.47522488
 -2.02654247  0.72897366  1.60512661  2.83077104 -1.26456713 -0.25136105
 -2.39279218 -2.27309774  3.13337152  2.54856955 -0.67550787 -2.69002205]. 
Working on 0.8 fold... 
[Fri Apr  5 08:02:25 2024]  Iteration number: 0 with current cost as 0.31411556266347024 and parameters 
[-1.73764243  2.23743464 -2.12427953 -0.11653097  0.55388705 -2.77010903
  3.06858498  2.18960145  1.18552004 -1.06648311  0.6027151   1.14432445
  1.31029904 -1.87354675  0.72965075  2.88578419 -0.54534335 -0.4752248
 -2.02654243  0.7289737   1.60512661  2.83077102 -1.26456712 -0.25136105
 -2.39279218 -2.27309774  3.13337152  2.54856958 -0.67550787 -2.69002204]. 
Working on 1.0 fold... 
[Fri Apr  5 08:08:17 2024]  Iteration number: 0 with current cost as 0.327780467801287 and parameters 
[-1.79022485  2.23743464 -2.12427954 -0.11653098  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18552003 -1.06648308  0.6027151   1.14432445
  1.31029901 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.4752248
 -2.02654243  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1690.9523103237152 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 08:14:08 2024]  Iteration number: 0 with current cost as 0.32710204119873 and parameters 
[-1.76826899  2.23743459 -2.12427959 -0.11653103  0.55388703 -2.77010902
  3.06858493  2.1896014   1.18552003 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965075  2.88578419 -0.5453434  -0.47522485
 -2.0265425   0.72897365  1.60512659  2.83077097 -1.26456715 -0.25136105
 -2.39279218 -2.27309774  3.1333715   2.54856958 -0.67550787 -2.69002207]. 
Working on 0.4 fold... 
[Fri Apr  5 08:20:10 2024]  Iteration number: 0 with current cost as 0.32931751136549337 and parameters 
[-1.81187897  2.23743464 -2.12427956 -0.116531    0.55388708 -2.770109
  3.06858498  2.18960143  1.18552004 -1.06648308  0.60271513  1.14432445
  1.31029904 -1.87354675  0.72965078  2.88578419 -0.54534335 -0.47522483
 -2.02654243  0.72897372  1.60512666  2.83077104 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 08:27:26 2024]  Iteration number: 0 with current cost as 0.302921871290067 and parameters 
[-1.61223999  2.2374346  -2.12427955 -0.11653106  0.55388705 -2.77010903
  3.06858489  2.18960142  1.18552005 -1.06648312  0.60271507  1.14432442
  1.31029902 -1.87354677  0.72965071  2.88578416 -0.54534338 -0.47522488
 -2.02654247  0.72897366  1.60512661  2.83077104 -1.26456713 -0.25136105
 -2.39279218 -2.27309774  3.13337152  2.54856955 -0.67550787 -2.69002205]. 
Working on 0.8 fold... 
[Fri Apr  5 08:31:16 2024]  Iteration number: 0 with current cost as 0.31411556266347024 and parameters 
[-1.73764243  2.23743464 -2.12427953 -0.11653097  0.55388705 -2.77010903
  3.06858498  2.18960145  1.18552004 -1.06648311  0.6027151   1.14432445
  1.31029904 -1.87354675  0.72965075  2.88578419 -0.54534335 -0.4752248
 -2.02654243  0.7289737   1.60512661  2.83077102 -1.26456712 -0.25136105
 -2.39279218 -2.27309774  3.13337152  2.54856958 -0.67550787 -2.69002204]. 
Working on 1.0 fold... 
[Fri Apr  5 08:37:09 2024]  Iteration number: 0 with current cost as 0.327780467801287 and parameters 
[-1.79022485  2.23743464 -2.12427954 -0.11653098  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18552003 -1.06648308  0.6027151   1.14432445
  1.31029901 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.4752248
 -2.02654243  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1737.6905143260956 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 08:43:12 2024]  Iteration number: 0 with current cost as 0.32710204119873 and parameters 
[-1.76826899  2.23743459 -2.12427959 -0.11653103  0.55388703 -2.77010902
  3.06858493  2.1896014   1.18552003 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965075  2.88578419 -0.5453434  -0.47522485
 -2.0265425   0.72897365  1.60512659  2.83077097 -1.26456715 -0.25136105
 -2.39279218 -2.27309774  3.1333715   2.54856958 -0.67550787 -2.69002207]. 
Working on 0.4 fold... 
[Fri Apr  5 08:49:11 2024]  Iteration number: 0 with current cost as 0.32931751136549337 and parameters 
[-1.81187897  2.23743464 -2.12427956 -0.116531    0.55388708 -2.770109
  3.06858498  2.18960143  1.18552004 -1.06648308  0.60271513  1.14432445
  1.31029904 -1.87354675  0.72965078  2.88578419 -0.54534335 -0.47522483
 -2.02654243  0.72897372  1.60512666  2.83077104 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 08:56:12 2024]  Iteration number: 0 with current cost as 0.302921871290067 and parameters 
[-1.61223999  2.2374346  -2.12427955 -0.11653106  0.55388705 -2.77010903
  3.06858489  2.18960142  1.18552005 -1.06648312  0.60271507  1.14432442
  1.31029902 -1.87354677  0.72965071  2.88578416 -0.54534338 -0.47522488
 -2.02654247  0.72897366  1.60512661  2.83077104 -1.26456713 -0.25136105
 -2.39279218 -2.27309774  3.13337152  2.54856955 -0.67550787 -2.69002205]. 
Working on 0.8 fold... 
[Fri Apr  5 09:00:01 2024]  Iteration number: 0 with current cost as 0.31411556266347024 and parameters 
[-1.73764243  2.23743464 -2.12427953 -0.11653097  0.55388705 -2.77010903
  3.06858498  2.18960145  1.18552004 -1.06648311  0.6027151   1.14432445
  1.31029904 -1.87354675  0.72965075  2.88578419 -0.54534335 -0.4752248
 -2.02654243  0.7289737   1.60512661  2.83077102 -1.26456712 -0.25136105
 -2.39279218 -2.27309774  3.13337152  2.54856958 -0.67550787 -2.69002204]. 
Working on 1.0 fold... 
[Fri Apr  5 09:06:01 2024]  Iteration number: 0 with current cost as 0.327780467801287 and parameters 
[-1.79022485  2.23743464 -2.12427954 -0.11653098  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18552003 -1.06648308  0.6027151   1.14432445
  1.31029901 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.4752248
 -2.02654243  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1728.4365842342377 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 09:11:53 2024]  Iteration number: 0 with current cost as 0.32710204119873 and parameters 
[-1.76826899  2.23743459 -2.12427959 -0.11653103  0.55388703 -2.77010902
  3.06858493  2.1896014   1.18552003 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965075  2.88578419 -0.5453434  -0.47522485
 -2.0265425   0.72897365  1.60512659  2.83077097 -1.26456715 -0.25136105
 -2.39279218 -2.27309774  3.1333715   2.54856958 -0.67550787 -2.69002207]. 
Working on 0.4 fold... 
[Fri Apr  5 09:17:44 2024]  Iteration number: 0 with current cost as 0.32931751136549337 and parameters 
[-1.81187897  2.23743464 -2.12427956 -0.116531    0.55388708 -2.770109
  3.06858498  2.18960143  1.18552004 -1.06648308  0.60271513  1.14432445
  1.31029904 -1.87354675  0.72965078  2.88578419 -0.54534335 -0.47522483
 -2.02654243  0.72897372  1.60512666  2.83077104 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 09:24:50 2024]  Iteration number: 0 with current cost as 0.302921871290067 and parameters 
[-1.61223999  2.2374346  -2.12427955 -0.11653106  0.55388705 -2.77010903
  3.06858489  2.18960142  1.18552005 -1.06648312  0.60271507  1.14432442
  1.31029902 -1.87354677  0.72965071  2.88578416 -0.54534338 -0.47522488
 -2.02654247  0.72897366  1.60512661  2.83077104 -1.26456713 -0.25136105
 -2.39279218 -2.27309774  3.13337152  2.54856955 -0.67550787 -2.69002205]. 
Working on 0.8 fold... 
[Fri Apr  5 09:28:45 2024]  Iteration number: 0 with current cost as 0.31411556266347024 and parameters 
[-1.73764243  2.23743464 -2.12427953 -0.11653097  0.55388705 -2.77010903
  3.06858498  2.18960145  1.18552004 -1.06648311  0.6027151   1.14432445
  1.31029904 -1.87354675  0.72965075  2.88578419 -0.54534335 -0.4752248
 -2.02654243  0.7289737   1.60512661  2.83077102 -1.26456712 -0.25136105
 -2.39279218 -2.27309774  3.13337152  2.54856958 -0.67550787 -2.69002204]. 
Working on 1.0 fold... 
[Fri Apr  5 09:34:36 2024]  Iteration number: 0 with current cost as 0.327780467801287 and parameters 
[-1.79022485  2.23743464 -2.12427954 -0.11653098  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18552003 -1.06648308  0.6027151   1.14432445
  1.31029901 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.4752248
 -2.02654243  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1712.6101305484772 seconds. 
Discarding model... 

Training complete taking 43205.2009100914 total seconds. 
Now scoring model... 
Scoring complete taking 2.3366730213165283 seconds. 
Saved predicted values as A1_Full-CRZ_predicted_values.csv
Model scores: {'MSE_train': (0.5020917393387362,), 'R2_train': 0.00021714563298869116, 'MAE_train': 0.6187141092135444, 'MSE_test': 0.5742272470832982, 'R2_test': -0.07091368936204678, 'MAE_test': 0.6784097615164988}. 
Saved model results as A1_Full-CRZ_results.json. 
