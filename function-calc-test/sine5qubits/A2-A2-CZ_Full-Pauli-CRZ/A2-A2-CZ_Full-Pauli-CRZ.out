/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /scratch/j/jacobsen/gjones/sine5qubits/sine_train.bin... 
Successfully loaded /scratch/j/jacobsen/gjones/sine5qubits/sine_train.bin into X and y data. 
Loading dataset from /scratch/j/jacobsen/gjones/sine5qubits/sine_test.bin... 
Successfully loaded /scratch/j/jacobsen/gjones/sine5qubits/sine_test.bin into X and y data. 
Training model with dataset /scratch/j/jacobsen/gjones/sine5qubits/sine_train.bin 
 at time Thu Apr  4 21:37:41 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 21:38:05 2024]  Iteration number: 0 with current cost as 0.287952094512578 and parameters 
[-3.09077914  2.19107978 -2.08463564 -0.11653103  0.55388707 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57030207  1.14432445
  1.5281719  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 21:46:56 2024]  Iteration number: 0 with current cost as 0.29655114807874794 and parameters 
[-3.00498127  2.20557025 -2.08598814 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57815834  1.14432445
  1.43140122 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 21:55:33 2024]  Iteration number: 0 with current cost as 0.31443591183951636 and parameters 
[-3.0142499   2.19413561 -2.08061683 -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551998 -1.06648308  0.56938212  1.14432446
  1.44528909 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.8 fold... 
[Thu Apr  4 22:05:06 2024]  Iteration number: 0 with current cost as 0.2522366903570601 and parameters 
[-3.06950997  2.20786821 -2.09352706 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.58368183  1.14432445
  1.49884748 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 22:12:45 2024]  Iteration number: 0 with current cost as 0.28510484547741355 and parameters 
[-3.05756541  2.18781743 -2.08140626 -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551998 -1.06648308  0.56590616  1.14432446
  1.49172566 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2408.4103648662567 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 22:18:13 2024]  Iteration number: 0 with current cost as 0.287952094512578 and parameters 
[-3.09077914  2.19107978 -2.08463564 -0.11653103  0.55388707 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57030207  1.14432445
  1.5281719  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 22:26:55 2024]  Iteration number: 0 with current cost as 0.29655114807874794 and parameters 
[-3.00498127  2.20557025 -2.08598814 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57815834  1.14432445
  1.43140122 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 22:35:41 2024]  Iteration number: 0 with current cost as 0.31443591183951636 and parameters 
[-3.0142499   2.19413561 -2.08061683 -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551998 -1.06648308  0.56938212  1.14432446
  1.44528909 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.8 fold... 
[Thu Apr  4 22:45:10 2024]  Iteration number: 0 with current cost as 0.2522366903570601 and parameters 
[-3.06950997  2.20786821 -2.09352706 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.58368183  1.14432445
  1.49884748 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 22:52:47 2024]  Iteration number: 0 with current cost as 0.28510484547741355 and parameters 
[-3.05756541  2.18781743 -2.08140626 -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551998 -1.06648308  0.56590616  1.14432446
  1.49172566 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2390.3651678562164 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 22:58:04 2024]  Iteration number: 0 with current cost as 0.287952094512578 and parameters 
[-3.09077914  2.19107978 -2.08463564 -0.11653103  0.55388707 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57030207  1.14432445
  1.5281719  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 23:06:51 2024]  Iteration number: 0 with current cost as 0.29655114807874794 and parameters 
[-3.00498127  2.20557025 -2.08598814 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57815834  1.14432445
  1.43140122 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 23:15:33 2024]  Iteration number: 0 with current cost as 0.31443591183951636 and parameters 
[-3.0142499   2.19413561 -2.08061683 -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551998 -1.06648308  0.56938212  1.14432446
  1.44528909 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.8 fold... 
[Thu Apr  4 23:25:06 2024]  Iteration number: 0 with current cost as 0.2522366903570601 and parameters 
[-3.06950997  2.20786821 -2.09352706 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.58368183  1.14432445
  1.49884748 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 23:32:48 2024]  Iteration number: 0 with current cost as 0.28510484547741355 and parameters 
[-3.05756541  2.18781743 -2.08140626 -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551998 -1.06648308  0.56590616  1.14432446
  1.49172566 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2404.5733087062836 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 23:38:08 2024]  Iteration number: 0 with current cost as 0.287952094512578 and parameters 
[-3.09077914  2.19107978 -2.08463564 -0.11653103  0.55388707 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57030207  1.14432445
  1.5281719  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 23:47:16 2024]  Iteration number: 0 with current cost as 0.29655114807874794 and parameters 
[-3.00498127  2.20557025 -2.08598814 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57815834  1.14432445
  1.43140122 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 23:55:52 2024]  Iteration number: 0 with current cost as 0.31443591183951636 and parameters 
[-3.0142499   2.19413561 -2.08061683 -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551998 -1.06648308  0.56938212  1.14432446
  1.44528909 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.8 fold... 
[Fri Apr  5 00:05:29 2024]  Iteration number: 0 with current cost as 0.2522366903570601 and parameters 
[-3.06950997  2.20786821 -2.09352706 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.58368183  1.14432445
  1.49884748 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 00:13:20 2024]  Iteration number: 0 with current cost as 0.28510484547741355 and parameters 
[-3.05756541  2.18781743 -2.08140626 -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551998 -1.06648308  0.56590616  1.14432446
  1.49172566 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2437.672068119049 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 00:18:46 2024]  Iteration number: 0 with current cost as 0.287952094512578 and parameters 
[-3.09077914  2.19107978 -2.08463564 -0.11653103  0.55388707 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57030207  1.14432445
  1.5281719  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Fri Apr  5 00:28:13 2024]  Iteration number: 0 with current cost as 0.29655114807874794 and parameters 
[-3.00498127  2.20557025 -2.08598814 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57815834  1.14432445
  1.43140122 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Fri Apr  5 00:37:04 2024]  Iteration number: 0 with current cost as 0.31443591183951636 and parameters 
[-3.0142499   2.19413561 -2.08061683 -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551998 -1.06648308  0.56938212  1.14432446
  1.44528909 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.8 fold... 
[Fri Apr  5 00:46:55 2024]  Iteration number: 0 with current cost as 0.2522366903570601 and parameters 
[-3.06950997  2.20786821 -2.09352706 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.58368183  1.14432445
  1.49884748 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 00:54:35 2024]  Iteration number: 0 with current cost as 0.28510484547741355 and parameters 
[-3.05756541  2.18781743 -2.08140626 -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551998 -1.06648308  0.56590616  1.14432446
  1.49172566 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2472.4035835266113 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 00:59:59 2024]  Iteration number: 0 with current cost as 0.287952094512578 and parameters 
[-3.09077914  2.19107978 -2.08463564 -0.11653103  0.55388707 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57030207  1.14432445
  1.5281719  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Fri Apr  5 01:09:26 2024]  Iteration number: 0 with current cost as 0.29655114807874794 and parameters 
[-3.00498127  2.20557025 -2.08598814 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57815834  1.14432445
  1.43140122 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Fri Apr  5 01:18:45 2024]  Iteration number: 0 with current cost as 0.31443591183951636 and parameters 
[-3.0142499   2.19413561 -2.08061683 -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551998 -1.06648308  0.56938212  1.14432446
  1.44528909 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.8 fold... 
[Fri Apr  5 01:29:04 2024]  Iteration number: 0 with current cost as 0.2522366903570601 and parameters 
[-3.06950997  2.20786821 -2.09352706 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.58368183  1.14432445
  1.49884748 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 01:36:49 2024]  Iteration number: 0 with current cost as 0.28510484547741355 and parameters 
[-3.05756541  2.18781743 -2.08140626 -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551998 -1.06648308  0.56590616  1.14432446
  1.49172566 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2528.7852008342743 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 01:42:06 2024]  Iteration number: 0 with current cost as 0.287952094512578 and parameters 
[-3.09077914  2.19107978 -2.08463564 -0.11653103  0.55388707 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57030207  1.14432445
  1.5281719  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Fri Apr  5 01:50:58 2024]  Iteration number: 0 with current cost as 0.29655114807874794 and parameters 
[-3.00498127  2.20557025 -2.08598814 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57815834  1.14432445
  1.43140122 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Fri Apr  5 01:59:48 2024]  Iteration number: 0 with current cost as 0.31443591183951636 and parameters 
[-3.0142499   2.19413561 -2.08061683 -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551998 -1.06648308  0.56938212  1.14432446
  1.44528909 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.8 fold... 
[Fri Apr  5 02:09:26 2024]  Iteration number: 0 with current cost as 0.2522366903570601 and parameters 
[-3.06950997  2.20786821 -2.09352706 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.58368183  1.14432445
  1.49884748 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 02:17:11 2024]  Iteration number: 0 with current cost as 0.28510484547741355 and parameters 
[-3.05756541  2.18781743 -2.08140626 -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551998 -1.06648308  0.56590616  1.14432446
  1.49172566 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2453.56090259552 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 02:23:01 2024]  Iteration number: 0 with current cost as 0.287952094512578 and parameters 
[-3.09077914  2.19107978 -2.08463564 -0.11653103  0.55388707 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57030207  1.14432445
  1.5281719  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Fri Apr  5 02:31:55 2024]  Iteration number: 0 with current cost as 0.29655114807874794 and parameters 
[-3.00498127  2.20557025 -2.08598814 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57815834  1.14432445
  1.43140122 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Fri Apr  5 02:41:07 2024]  Iteration number: 0 with current cost as 0.31443591183951636 and parameters 
[-3.0142499   2.19413561 -2.08061683 -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551998 -1.06648308  0.56938212  1.14432446
  1.44528909 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.8 fold... 
[Fri Apr  5 02:50:42 2024]  Iteration number: 0 with current cost as 0.2522366903570601 and parameters 
[-3.06950997  2.20786821 -2.09352706 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.58368183  1.14432445
  1.49884748 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 02:58:24 2024]  Iteration number: 0 with current cost as 0.28510484547741355 and parameters 
[-3.05756541  2.18781743 -2.08140626 -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551998 -1.06648308  0.56590616  1.14432446
  1.49172566 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2440.5899839401245 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 03:03:40 2024]  Iteration number: 0 with current cost as 0.287952094512578 and parameters 
[-3.09077914  2.19107978 -2.08463564 -0.11653103  0.55388707 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57030207  1.14432445
  1.5281719  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Fri Apr  5 03:12:29 2024]  Iteration number: 0 with current cost as 0.29655114807874794 and parameters 
[-3.00498127  2.20557025 -2.08598814 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57815834  1.14432445
  1.43140122 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Fri Apr  5 03:21:06 2024]  Iteration number: 0 with current cost as 0.31443591183951636 and parameters 
[-3.0142499   2.19413561 -2.08061683 -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551998 -1.06648308  0.56938212  1.14432446
  1.44528909 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.8 fold... 
[Fri Apr  5 03:30:49 2024]  Iteration number: 0 with current cost as 0.2522366903570601 and parameters 
[-3.06950997  2.20786821 -2.09352706 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.58368183  1.14432445
  1.49884748 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 03:38:32 2024]  Iteration number: 0 with current cost as 0.28510484547741355 and parameters 
[-3.05756541  2.18781743 -2.08140626 -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551998 -1.06648308  0.56590616  1.14432446
  1.49172566 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2409.196316719055 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 03:43:50 2024]  Iteration number: 0 with current cost as 0.287952094512578 and parameters 
[-3.09077914  2.19107978 -2.08463564 -0.11653103  0.55388707 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57030207  1.14432445
  1.5281719  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Fri Apr  5 03:53:15 2024]  Iteration number: 0 with current cost as 0.29655114807874794 and parameters 
[-3.00498127  2.20557025 -2.08598814 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57815834  1.14432445
  1.43140122 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Fri Apr  5 04:01:51 2024]  Iteration number: 0 with current cost as 0.31443591183951636 and parameters 
[-3.0142499   2.19413561 -2.08061683 -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551998 -1.06648308  0.56938212  1.14432446
  1.44528909 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.8 fold... 
[Fri Apr  5 04:11:46 2024]  Iteration number: 0 with current cost as 0.2522366903570601 and parameters 
[-3.06950997  2.20786821 -2.09352706 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.58368183  1.14432445
  1.49884748 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 04:19:24 2024]  Iteration number: 0 with current cost as 0.28510484547741355 and parameters 
[-3.05756541  2.18781743 -2.08140626 -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551998 -1.06648308  0.56590616  1.14432446
  1.49172566 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2452.885722875595 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 04:24:43 2024]  Iteration number: 0 with current cost as 0.287952094512578 and parameters 
[-3.09077914  2.19107978 -2.08463564 -0.11653103  0.55388707 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57030207  1.14432445
  1.5281719  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Fri Apr  5 04:33:37 2024]  Iteration number: 0 with current cost as 0.29655114807874794 and parameters 
[-3.00498127  2.20557025 -2.08598814 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57815834  1.14432445
  1.43140122 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Fri Apr  5 04:42:14 2024]  Iteration number: 0 with current cost as 0.31443591183951636 and parameters 
[-3.0142499   2.19413561 -2.08061683 -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551998 -1.06648308  0.56938212  1.14432446
  1.44528909 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.8 fold... 
[Fri Apr  5 04:52:02 2024]  Iteration number: 0 with current cost as 0.2522366903570601 and parameters 
[-3.06950997  2.20786821 -2.09352706 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.58368183  1.14432445
  1.49884748 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 04:59:46 2024]  Iteration number: 0 with current cost as 0.28510484547741355 and parameters 
[-3.05756541  2.18781743 -2.08140626 -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551998 -1.06648308  0.56590616  1.14432446
  1.49172566 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2421.3965747356415 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 05:05:06 2024]  Iteration number: 0 with current cost as 0.287952094512578 and parameters 
[-3.09077914  2.19107978 -2.08463564 -0.11653103  0.55388707 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57030207  1.14432445
  1.5281719  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Fri Apr  5 05:14:17 2024]  Iteration number: 0 with current cost as 0.29655114807874794 and parameters 
[-3.00498127  2.20557025 -2.08598814 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57815834  1.14432445
  1.43140122 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Fri Apr  5 05:23:04 2024]  Iteration number: 0 with current cost as 0.31443591183951636 and parameters 
[-3.0142499   2.19413561 -2.08061683 -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551998 -1.06648308  0.56938212  1.14432446
  1.44528909 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.8 fold... 
[Fri Apr  5 05:32:38 2024]  Iteration number: 0 with current cost as 0.2522366903570601 and parameters 
[-3.06950997  2.20786821 -2.09352706 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.58368183  1.14432445
  1.49884748 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 05:40:13 2024]  Iteration number: 0 with current cost as 0.28510484547741355 and parameters 
[-3.05756541  2.18781743 -2.08140626 -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551998 -1.06648308  0.56590616  1.14432446
  1.49172566 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2461.0090367794037 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 05:46:05 2024]  Iteration number: 0 with current cost as 0.287952094512578 and parameters 
[-3.09077914  2.19107978 -2.08463564 -0.11653103  0.55388707 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57030207  1.14432445
  1.5281719  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Fri Apr  5 05:54:54 2024]  Iteration number: 0 with current cost as 0.29655114807874794 and parameters 
[-3.00498127  2.20557025 -2.08598814 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57815834  1.14432445
  1.43140122 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Fri Apr  5 06:03:36 2024]  Iteration number: 0 with current cost as 0.31443591183951636 and parameters 
[-3.0142499   2.19413561 -2.08061683 -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551998 -1.06648308  0.56938212  1.14432446
  1.44528909 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.8 fold... 
[Fri Apr  5 06:14:03 2024]  Iteration number: 0 with current cost as 0.2522366903570601 and parameters 
[-3.06950997  2.20786821 -2.09352706 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.58368183  1.14432445
  1.49884748 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 06:21:44 2024]  Iteration number: 0 with current cost as 0.28510484547741355 and parameters 
[-3.05756541  2.18781743 -2.08140626 -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551998 -1.06648308  0.56590616  1.14432446
  1.49172566 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2482.0738735198975 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 06:27:27 2024]  Iteration number: 0 with current cost as 0.287952094512578 and parameters 
[-3.09077914  2.19107978 -2.08463564 -0.11653103  0.55388707 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57030207  1.14432445
  1.5281719  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Fri Apr  5 06:36:21 2024]  Iteration number: 0 with current cost as 0.29655114807874794 and parameters 
[-3.00498127  2.20557025 -2.08598814 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57815834  1.14432445
  1.43140122 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Fri Apr  5 06:45:41 2024]  Iteration number: 0 with current cost as 0.31443591183951636 and parameters 
[-3.0142499   2.19413561 -2.08061683 -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551998 -1.06648308  0.56938212  1.14432446
  1.44528909 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.8 fold... 
[Fri Apr  5 06:55:30 2024]  Iteration number: 0 with current cost as 0.2522366903570601 and parameters 
[-3.06950997  2.20786821 -2.09352706 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.58368183  1.14432445
  1.49884748 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 07:03:23 2024]  Iteration number: 0 with current cost as 0.28510484547741355 and parameters 
[-3.05756541  2.18781743 -2.08140626 -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551998 -1.06648308  0.56590616  1.14432446
  1.49172566 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2472.779760837555 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 07:08:41 2024]  Iteration number: 0 with current cost as 0.287952094512578 and parameters 
[-3.09077914  2.19107978 -2.08463564 -0.11653103  0.55388707 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57030207  1.14432445
  1.5281719  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Fri Apr  5 07:17:51 2024]  Iteration number: 0 with current cost as 0.29655114807874794 and parameters 
[-3.00498127  2.20557025 -2.08598814 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57815834  1.14432445
  1.43140122 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Fri Apr  5 07:26:26 2024]  Iteration number: 0 with current cost as 0.31443591183951636 and parameters 
[-3.0142499   2.19413561 -2.08061683 -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551998 -1.06648308  0.56938212  1.14432446
  1.44528909 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.8 fold... 
[Fri Apr  5 07:36:08 2024]  Iteration number: 0 with current cost as 0.2522366903570601 and parameters 
[-3.06950997  2.20786821 -2.09352706 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.58368183  1.14432445
  1.49884748 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 07:43:52 2024]  Iteration number: 0 with current cost as 0.28510484547741355 and parameters 
[-3.05756541  2.18781743 -2.08140626 -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551998 -1.06648308  0.56590616  1.14432446
  1.49172566 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2427.764725923538 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 07:49:08 2024]  Iteration number: 0 with current cost as 0.287952094512578 and parameters 
[-3.09077914  2.19107978 -2.08463564 -0.11653103  0.55388707 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57030207  1.14432445
  1.5281719  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Fri Apr  5 07:57:52 2024]  Iteration number: 0 with current cost as 0.29655114807874794 and parameters 
[-3.00498127  2.20557025 -2.08598814 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57815834  1.14432445
  1.43140122 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Fri Apr  5 08:06:35 2024]  Iteration number: 0 with current cost as 0.31443591183951636 and parameters 
[-3.0142499   2.19413561 -2.08061683 -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551998 -1.06648308  0.56938212  1.14432446
  1.44528909 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.8 fold... 
[Fri Apr  5 08:16:16 2024]  Iteration number: 0 with current cost as 0.2522366903570601 and parameters 
[-3.06950997  2.20786821 -2.09352706 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.58368183  1.14432445
  1.49884748 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 08:23:52 2024]  Iteration number: 0 with current cost as 0.28510484547741355 and parameters 
[-3.05756541  2.18781743 -2.08140626 -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551998 -1.06648308  0.56590616  1.14432446
  1.49172566 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2403.8180685043335 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 08:29:12 2024]  Iteration number: 0 with current cost as 0.287952094512578 and parameters 
[-3.09077914  2.19107978 -2.08463564 -0.11653103  0.55388707 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57030207  1.14432445
  1.5281719  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Fri Apr  5 08:38:02 2024]  Iteration number: 0 with current cost as 0.29655114807874794 and parameters 
[-3.00498127  2.20557025 -2.08598814 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57815834  1.14432445
  1.43140122 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Fri Apr  5 08:46:43 2024]  Iteration number: 0 with current cost as 0.31443591183951636 and parameters 
[-3.0142499   2.19413561 -2.08061683 -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551998 -1.06648308  0.56938212  1.14432446
  1.44528909 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.8 fold... 
[Fri Apr  5 08:56:27 2024]  Iteration number: 0 with current cost as 0.2522366903570601 and parameters 
[-3.06950997  2.20786821 -2.09352706 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.58368183  1.14432445
  1.49884748 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 09:04:19 2024]  Iteration number: 0 with current cost as 0.28510484547741355 and parameters 
[-3.05756541  2.18781743 -2.08140626 -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551998 -1.06648308  0.56590616  1.14432446
  1.49172566 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2422.9724435806274 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 09:09:34 2024]  Iteration number: 0 with current cost as 0.287952094512578 and parameters 
[-3.09077914  2.19107978 -2.08463564 -0.11653103  0.55388707 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57030207  1.14432445
  1.5281719  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Fri Apr  5 09:18:23 2024]  Iteration number: 0 with current cost as 0.29655114807874794 and parameters 
[-3.00498127  2.20557025 -2.08598814 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57815834  1.14432445
  1.43140122 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Fri Apr  5 09:27:08 2024]  Iteration number: 0 with current cost as 0.31443591183951636 and parameters 
[-3.0142499   2.19413561 -2.08061683 -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551998 -1.06648308  0.56938212  1.14432446
  1.44528909 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.8 fold... 
[Fri Apr  5 09:36:44 2024]  Iteration number: 0 with current cost as 0.2522366903570601 and parameters 
[-3.06950997  2.20786821 -2.09352706 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.58368183  1.14432445
  1.49884748 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 09:44:22 2024]  Iteration number: 0 with current cost as 0.28510484547741355 and parameters 
[-3.05756541  2.18781743 -2.08140626 -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551998 -1.06648308  0.56590616  1.14432446
  1.49172566 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2413.0033371448517 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 09:50:05 2024]  Iteration number: 0 with current cost as 0.287952094512578 and parameters 
[-3.09077914  2.19107978 -2.08463564 -0.11653103  0.55388707 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57030207  1.14432445
  1.5281719  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Fri Apr  5 09:58:57 2024]  Iteration number: 0 with current cost as 0.29655114807874794 and parameters 
[-3.00498127  2.20557025 -2.08598814 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57815834  1.14432445
  1.43140122 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Fri Apr  5 10:07:33 2024]  Iteration number: 0 with current cost as 0.31443591183951636 and parameters 
[-3.0142499   2.19413561 -2.08061683 -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551998 -1.06648308  0.56938212  1.14432446
  1.44528909 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.8 fold... 
[Fri Apr  5 10:17:24 2024]  Iteration number: 0 with current cost as 0.2522366903570601 and parameters 
[-3.06950997  2.20786821 -2.09352706 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.58368183  1.14432445
  1.49884748 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 10:25:01 2024]  Iteration number: 0 with current cost as 0.28510484547741355 and parameters 
[-3.05756541  2.18781743 -2.08140626 -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551998 -1.06648308  0.56590616  1.14432446
  1.49172566 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2428.4971764087677 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 10:30:17 2024]  Iteration number: 0 with current cost as 0.287952094512578 and parameters 
[-3.09077914  2.19107978 -2.08463564 -0.11653103  0.55388707 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57030207  1.14432445
  1.5281719  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Fri Apr  5 10:39:15 2024]  Iteration number: 0 with current cost as 0.29655114807874794 and parameters 
[-3.00498127  2.20557025 -2.08598814 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57815834  1.14432445
  1.43140122 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Fri Apr  5 10:47:50 2024]  Iteration number: 0 with current cost as 0.31443591183951636 and parameters 
[-3.0142499   2.19413561 -2.08061683 -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551998 -1.06648308  0.56938212  1.14432446
  1.44528909 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.8 fold... 
[Fri Apr  5 10:57:27 2024]  Iteration number: 0 with current cost as 0.2522366903570601 and parameters 
[-3.06950997  2.20786821 -2.09352706 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.58368183  1.14432445
  1.49884748 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 11:05:03 2024]  Iteration number: 0 with current cost as 0.28510484547741355 and parameters 
[-3.05756541  2.18781743 -2.08140626 -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551998 -1.06648308  0.56590616  1.14432446
  1.49172566 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2407.028249502182 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 11:10:27 2024]  Iteration number: 0 with current cost as 0.287952094512578 and parameters 
[-3.09077914  2.19107978 -2.08463564 -0.11653103  0.55388707 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57030207  1.14432445
  1.5281719  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Fri Apr  5 11:19:42 2024]  Iteration number: 0 with current cost as 0.29655114807874794 and parameters 
[-3.00498127  2.20557025 -2.08598814 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57815834  1.14432445
  1.43140122 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Fri Apr  5 11:28:25 2024]  Iteration number: 0 with current cost as 0.31443591183951636 and parameters 
[-3.0142499   2.19413561 -2.08061683 -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551998 -1.06648308  0.56938212  1.14432446
  1.44528909 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.8 fold... 
[Fri Apr  5 11:38:00 2024]  Iteration number: 0 with current cost as 0.2522366903570601 and parameters 
[-3.06950997  2.20786821 -2.09352706 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.58368183  1.14432445
  1.49884748 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 11:45:34 2024]  Iteration number: 0 with current cost as 0.28510484547741355 and parameters 
[-3.05756541  2.18781743 -2.08140626 -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551998 -1.06648308  0.56590616  1.14432446
  1.49172566 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2425.004057407379 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 11:50:49 2024]  Iteration number: 0 with current cost as 0.287952094512578 and parameters 
[-3.09077914  2.19107978 -2.08463564 -0.11653103  0.55388707 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57030207  1.14432445
  1.5281719  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Fri Apr  5 11:59:31 2024]  Iteration number: 0 with current cost as 0.29655114807874794 and parameters 
[-3.00498127  2.20557025 -2.08598814 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57815834  1.14432445
  1.43140122 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Fri Apr  5 12:08:01 2024]  Iteration number: 0 with current cost as 0.31443591183951636 and parameters 
[-3.0142499   2.19413561 -2.08061683 -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551998 -1.06648308  0.56938212  1.14432446
  1.44528909 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.8 fold... 
[Fri Apr  5 12:17:32 2024]  Iteration number: 0 with current cost as 0.2522366903570601 and parameters 
[-3.06950997  2.20786821 -2.09352706 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.58368183  1.14432445
  1.49884748 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 12:25:04 2024]  Iteration number: 0 with current cost as 0.28510484547741355 and parameters 
[-3.05756541  2.18781743 -2.08140626 -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551998 -1.06648308  0.56590616  1.14432446
  1.49172566 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2370.043166399002 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 12:30:18 2024]  Iteration number: 0 with current cost as 0.287952094512578 and parameters 
[-3.09077914  2.19107978 -2.08463564 -0.11653103  0.55388707 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57030207  1.14432445
  1.5281719  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Fri Apr  5 12:39:14 2024]  Iteration number: 0 with current cost as 0.29655114807874794 and parameters 
[-3.00498127  2.20557025 -2.08598814 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57815834  1.14432445
  1.43140122 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Fri Apr  5 12:48:29 2024]  Iteration number: 0 with current cost as 0.31443591183951636 and parameters 
[-3.0142499   2.19413561 -2.08061683 -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551998 -1.06648308  0.56938212  1.14432446
  1.44528909 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.8 fold... 
[Fri Apr  5 12:59:21 2024]  Iteration number: 0 with current cost as 0.2522366903570601 and parameters 
[-3.06950997  2.20786821 -2.09352706 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.58368183  1.14432445
  1.49884748 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 13:07:00 2024]  Iteration number: 0 with current cost as 0.28510484547741355 and parameters 
[-3.05756541  2.18781743 -2.08140626 -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551998 -1.06648308  0.56590616  1.14432446
  1.49172566 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2525.109211921692 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 13:12:25 2024]  Iteration number: 0 with current cost as 0.287952094512578 and parameters 
[-3.09077914  2.19107978 -2.08463564 -0.11653103  0.55388707 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57030207  1.14432445
  1.5281719  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Fri Apr  5 13:21:14 2024]  Iteration number: 0 with current cost as 0.29655114807874794 and parameters 
[-3.00498127  2.20557025 -2.08598814 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57815834  1.14432445
  1.43140122 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Fri Apr  5 13:29:48 2024]  Iteration number: 0 with current cost as 0.31443591183951636 and parameters 
[-3.0142499   2.19413561 -2.08061683 -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551998 -1.06648308  0.56938212  1.14432446
  1.44528909 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.8 fold... 
[Fri Apr  5 13:39:33 2024]  Iteration number: 0 with current cost as 0.2522366903570601 and parameters 
[-3.06950997  2.20786821 -2.09352706 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.58368183  1.14432445
  1.49884748 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 13:47:42 2024]  Iteration number: 0 with current cost as 0.28510484547741355 and parameters 
[-3.05756541  2.18781743 -2.08140626 -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551998 -1.06648308  0.56590616  1.14432446
  1.49172566 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2435.0693707466125 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 13:52:59 2024]  Iteration number: 0 with current cost as 0.287952094512578 and parameters 
[-3.09077914  2.19107978 -2.08463564 -0.11653103  0.55388707 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57030207  1.14432445
  1.5281719  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Fri Apr  5 14:01:46 2024]  Iteration number: 0 with current cost as 0.29655114807874794 and parameters 
[-3.00498127  2.20557025 -2.08598814 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57815834  1.14432445
  1.43140122 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Fri Apr  5 14:10:18 2024]  Iteration number: 0 with current cost as 0.31443591183951636 and parameters 
[-3.0142499   2.19413561 -2.08061683 -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551998 -1.06648308  0.56938212  1.14432446
  1.44528909 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.8 fold... 
[Fri Apr  5 14:20:45 2024]  Iteration number: 0 with current cost as 0.2522366903570601 and parameters 
[-3.06950997  2.20786821 -2.09352706 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.58368183  1.14432445
  1.49884748 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 14:28:54 2024]  Iteration number: 0 with current cost as 0.28510484547741355 and parameters 
[-3.05756541  2.18781743 -2.08140626 -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551998 -1.06648308  0.56590616  1.14432446
  1.49172566 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2471.4575197696686 seconds. 
Discarding model... 

Training complete taking 60965.47061395645 total seconds. 
Now scoring model... 
Scoring complete taking 1.0078246593475342 seconds. 
Saved predicted values as A2-A2-CZ_Full-Pauli-CRZ_predicted_values.csv
Model scores: {'MSE_train': (0.27579077724969353,), 'R2_train': 0.4508356364318248, 'MAE_train': 0.44729975269299926, 'MSE_test': 0.24660962715137424, 'R2_test': 0.5400816889370514, 'MAE_test': 0.45500794296415065}. 
Saved model results as A2-A2-CZ_Full-Pauli-CRZ_results.json. 
