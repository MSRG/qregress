/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /scratch/j/jacobsen/gjones/sine5qubits/sine_train.bin... 
Successfully loaded /scratch/j/jacobsen/gjones/sine5qubits/sine_train.bin into X and y data. 
Loading dataset from /scratch/j/jacobsen/gjones/sine5qubits/sine_test.bin... 
Successfully loaded /scratch/j/jacobsen/gjones/sine5qubits/sine_test.bin into X and y data. 
Training model with dataset /scratch/j/jacobsen/gjones/sine5qubits/sine_train.bin 
 at time Thu Apr  4 21:37:31 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 21:37:57 2024]  Iteration number: 0 with current cost as 0.3079862553791395 and parameters 
[-2.97688193  2.2955549  -2.15344249 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648309  0.62825145  1.14432445
  1.39788999 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.4 fold... 
[Thu Apr  4 21:49:09 2024]  Iteration number: 0 with current cost as 0.31847564551206087 and parameters 
[-2.96832273  2.31217529 -2.14905041 -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.6099314   1.14432446
  1.38125672 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Thu Apr  4 22:00:28 2024]  Iteration number: 0 with current cost as 0.3242472413006292 and parameters 
[-2.97511169  2.32216738 -2.14972332 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61400148  1.14432445
  1.3886929  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.8 fold... 
[Thu Apr  4 22:10:20 2024]  Iteration number: 0 with current cost as 0.30354059107081766 and parameters 
[-2.96405605  2.28629616 -2.15318238 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.61058467  1.14432445
  1.38295214 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 1.0 fold... 
[Thu Apr  4 22:21:14 2024]  Iteration number: 0 with current cost as 0.3017753338594201 and parameters 
[-2.97008084  2.30289244 -2.14802918 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.62229712  1.14432445
  1.3851151  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 3320.330466270447 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 22:33:17 2024]  Iteration number: 0 with current cost as 0.3079862553791395 and parameters 
[-2.97688193  2.2955549  -2.15344249 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648309  0.62825145  1.14432445
  1.39788999 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.4 fold... 
[Thu Apr  4 22:44:23 2024]  Iteration number: 0 with current cost as 0.31847564551206087 and parameters 
[-2.96832273  2.31217529 -2.14905041 -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.6099314   1.14432446
  1.38125672 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Thu Apr  4 22:55:46 2024]  Iteration number: 0 with current cost as 0.3242472413006292 and parameters 
[-2.97511169  2.32216738 -2.14972332 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61400148  1.14432445
  1.3886929  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.8 fold... 
[Thu Apr  4 23:05:39 2024]  Iteration number: 0 with current cost as 0.30354059107081766 and parameters 
[-2.96405605  2.28629616 -2.15318238 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.61058467  1.14432445
  1.38295214 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 1.0 fold... 
[Thu Apr  4 23:16:28 2024]  Iteration number: 0 with current cost as 0.3017753338594201 and parameters 
[-2.97008084  2.30289244 -2.14802918 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.62229712  1.14432445
  1.3851151  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 3308.9264039993286 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 23:28:26 2024]  Iteration number: 0 with current cost as 0.3079862553791395 and parameters 
[-2.97688193  2.2955549  -2.15344249 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648309  0.62825145  1.14432445
  1.39788999 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.4 fold... 
[Thu Apr  4 23:39:36 2024]  Iteration number: 0 with current cost as 0.31847564551206087 and parameters 
[-2.96832273  2.31217529 -2.14905041 -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.6099314   1.14432446
  1.38125672 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Thu Apr  4 23:50:46 2024]  Iteration number: 0 with current cost as 0.3242472413006292 and parameters 
[-2.97511169  2.32216738 -2.14972332 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61400148  1.14432445
  1.3886929  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.8 fold... 
[Fri Apr  5 00:00:51 2024]  Iteration number: 0 with current cost as 0.30354059107081766 and parameters 
[-2.96405605  2.28629616 -2.15318238 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.61058467  1.14432445
  1.38295214 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 1.0 fold... 
[Fri Apr  5 00:11:35 2024]  Iteration number: 0 with current cost as 0.3017753338594201 and parameters 
[-2.97008084  2.30289244 -2.14802918 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.62229712  1.14432445
  1.3851151  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 3305.7582376003265 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 00:23:39 2024]  Iteration number: 0 with current cost as 0.3079862553791395 and parameters 
[-2.97688193  2.2955549  -2.15344249 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648309  0.62825145  1.14432445
  1.39788999 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.4 fold... 
[Fri Apr  5 00:34:44 2024]  Iteration number: 0 with current cost as 0.31847564551206087 and parameters 
[-2.96832273  2.31217529 -2.14905041 -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.6099314   1.14432446
  1.38125672 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Fri Apr  5 00:46:16 2024]  Iteration number: 0 with current cost as 0.3242472413006292 and parameters 
[-2.97511169  2.32216738 -2.14972332 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61400148  1.14432445
  1.3886929  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.8 fold... 
[Fri Apr  5 00:55:58 2024]  Iteration number: 0 with current cost as 0.30354059107081766 and parameters 
[-2.96405605  2.28629616 -2.15318238 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.61058467  1.14432445
  1.38295214 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 1.0 fold... 
[Fri Apr  5 01:06:51 2024]  Iteration number: 0 with current cost as 0.3017753338594201 and parameters 
[-2.97008084  2.30289244 -2.14802918 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.62229712  1.14432445
  1.3851151  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 3329.6565606594086 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 01:19:18 2024]  Iteration number: 0 with current cost as 0.3079862553791395 and parameters 
[-2.97688193  2.2955549  -2.15344249 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648309  0.62825145  1.14432445
  1.39788999 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.4 fold... 
[Fri Apr  5 01:30:40 2024]  Iteration number: 0 with current cost as 0.31847564551206087 and parameters 
[-2.96832273  2.31217529 -2.14905041 -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.6099314   1.14432446
  1.38125672 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Fri Apr  5 01:41:56 2024]  Iteration number: 0 with current cost as 0.3242472413006292 and parameters 
[-2.97511169  2.32216738 -2.14972332 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61400148  1.14432445
  1.3886929  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.8 fold... 
[Fri Apr  5 01:51:34 2024]  Iteration number: 0 with current cost as 0.30354059107081766 and parameters 
[-2.96405605  2.28629616 -2.15318238 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.61058467  1.14432445
  1.38295214 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 1.0 fold... 
[Fri Apr  5 02:02:18 2024]  Iteration number: 0 with current cost as 0.3017753338594201 and parameters 
[-2.97008084  2.30289244 -2.14802918 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.62229712  1.14432445
  1.3851151  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 3314.0903236865997 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 02:14:15 2024]  Iteration number: 0 with current cost as 0.3079862553791395 and parameters 
[-2.97688193  2.2955549  -2.15344249 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648309  0.62825145  1.14432445
  1.39788999 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.4 fold... 
[Fri Apr  5 02:25:32 2024]  Iteration number: 0 with current cost as 0.31847564551206087 and parameters 
[-2.96832273  2.31217529 -2.14905041 -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.6099314   1.14432446
  1.38125672 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Fri Apr  5 02:36:57 2024]  Iteration number: 0 with current cost as 0.3242472413006292 and parameters 
[-2.97511169  2.32216738 -2.14972332 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61400148  1.14432445
  1.3886929  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.8 fold... 
[Fri Apr  5 02:47:08 2024]  Iteration number: 0 with current cost as 0.30354059107081766 and parameters 
[-2.96405605  2.28629616 -2.15318238 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.61058467  1.14432445
  1.38295214 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 1.0 fold... 
[Fri Apr  5 02:57:54 2024]  Iteration number: 0 with current cost as 0.3017753338594201 and parameters 
[-2.97008084  2.30289244 -2.14802918 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.62229712  1.14432445
  1.3851151  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 3326.9509410858154 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 03:09:43 2024]  Iteration number: 0 with current cost as 0.3079862553791395 and parameters 
[-2.97688193  2.2955549  -2.15344249 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648309  0.62825145  1.14432445
  1.39788999 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.4 fold... 
[Fri Apr  5 03:20:43 2024]  Iteration number: 0 with current cost as 0.31847564551206087 and parameters 
[-2.96832273  2.31217529 -2.14905041 -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.6099314   1.14432446
  1.38125672 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Fri Apr  5 03:32:04 2024]  Iteration number: 0 with current cost as 0.3242472413006292 and parameters 
[-2.97511169  2.32216738 -2.14972332 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61400148  1.14432445
  1.3886929  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.8 fold... 
[Fri Apr  5 03:42:00 2024]  Iteration number: 0 with current cost as 0.30354059107081766 and parameters 
[-2.96405605  2.28629616 -2.15318238 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.61058467  1.14432445
  1.38295214 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 1.0 fold... 
[Fri Apr  5 03:53:12 2024]  Iteration number: 0 with current cost as 0.3017753338594201 and parameters 
[-2.97008084  2.30289244 -2.14802918 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.62229712  1.14432445
  1.3851151  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 3350.253029346466 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 04:05:40 2024]  Iteration number: 0 with current cost as 0.3079862553791395 and parameters 
[-2.97688193  2.2955549  -2.15344249 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648309  0.62825145  1.14432445
  1.39788999 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.4 fold... 
[Fri Apr  5 04:16:34 2024]  Iteration number: 0 with current cost as 0.31847564551206087 and parameters 
[-2.96832273  2.31217529 -2.14905041 -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.6099314   1.14432446
  1.38125672 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Fri Apr  5 04:27:45 2024]  Iteration number: 0 with current cost as 0.3242472413006292 and parameters 
[-2.97511169  2.32216738 -2.14972332 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61400148  1.14432445
  1.3886929  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.8 fold... 
[Fri Apr  5 04:37:29 2024]  Iteration number: 0 with current cost as 0.30354059107081766 and parameters 
[-2.96405605  2.28629616 -2.15318238 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.61058467  1.14432445
  1.38295214 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 1.0 fold... 
[Fri Apr  5 04:48:29 2024]  Iteration number: 0 with current cost as 0.3017753338594201 and parameters 
[-2.97008084  2.30289244 -2.14802918 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.62229712  1.14432445
  1.3851151  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 3287.240211725235 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 05:00:20 2024]  Iteration number: 0 with current cost as 0.3079862553791395 and parameters 
[-2.97688193  2.2955549  -2.15344249 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648309  0.62825145  1.14432445
  1.39788999 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.4 fold... 
[Fri Apr  5 05:11:20 2024]  Iteration number: 0 with current cost as 0.31847564551206087 and parameters 
[-2.96832273  2.31217529 -2.14905041 -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.6099314   1.14432446
  1.38125672 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Fri Apr  5 05:22:32 2024]  Iteration number: 0 with current cost as 0.3242472413006292 and parameters 
[-2.97511169  2.32216738 -2.14972332 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61400148  1.14432445
  1.3886929  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.8 fold... 
[Fri Apr  5 05:32:14 2024]  Iteration number: 0 with current cost as 0.30354059107081766 and parameters 
[-2.96405605  2.28629616 -2.15318238 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.61058467  1.14432445
  1.38295214 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 1.0 fold... 
[Fri Apr  5 05:43:05 2024]  Iteration number: 0 with current cost as 0.3017753338594201 and parameters 
[-2.97008084  2.30289244 -2.14802918 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.62229712  1.14432445
  1.3851151  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 3279.339602947235 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 05:54:58 2024]  Iteration number: 0 with current cost as 0.3079862553791395 and parameters 
[-2.97688193  2.2955549  -2.15344249 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648309  0.62825145  1.14432445
  1.39788999 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.4 fold... 
[Fri Apr  5 06:06:36 2024]  Iteration number: 0 with current cost as 0.31847564551206087 and parameters 
[-2.96832273  2.31217529 -2.14905041 -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.6099314   1.14432446
  1.38125672 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Fri Apr  5 06:17:47 2024]  Iteration number: 0 with current cost as 0.3242472413006292 and parameters 
[-2.97511169  2.32216738 -2.14972332 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61400148  1.14432445
  1.3886929  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.8 fold... 
[Fri Apr  5 06:27:23 2024]  Iteration number: 0 with current cost as 0.30354059107081766 and parameters 
[-2.96405605  2.28629616 -2.15318238 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.61058467  1.14432445
  1.38295214 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 1.0 fold... 
[Fri Apr  5 06:38:15 2024]  Iteration number: 0 with current cost as 0.3017753338594201 and parameters 
[-2.97008084  2.30289244 -2.14802918 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.62229712  1.14432445
  1.3851151  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 3309.5652780532837 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 06:50:09 2024]  Iteration number: 0 with current cost as 0.3079862553791395 and parameters 
[-2.97688193  2.2955549  -2.15344249 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648309  0.62825145  1.14432445
  1.39788999 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.4 fold... 
[Fri Apr  5 07:01:14 2024]  Iteration number: 0 with current cost as 0.31847564551206087 and parameters 
[-2.96832273  2.31217529 -2.14905041 -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.6099314   1.14432446
  1.38125672 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Fri Apr  5 07:12:28 2024]  Iteration number: 0 with current cost as 0.3242472413006292 and parameters 
[-2.97511169  2.32216738 -2.14972332 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61400148  1.14432445
  1.3886929  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.8 fold... 
[Fri Apr  5 07:22:09 2024]  Iteration number: 0 with current cost as 0.30354059107081766 and parameters 
[-2.96405605  2.28629616 -2.15318238 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.61058467  1.14432445
  1.38295214 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 1.0 fold... 
[Fri Apr  5 07:32:54 2024]  Iteration number: 0 with current cost as 0.3017753338594201 and parameters 
[-2.97008084  2.30289244 -2.14802918 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.62229712  1.14432445
  1.3851151  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 3287.806646347046 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 07:44:57 2024]  Iteration number: 0 with current cost as 0.3079862553791395 and parameters 
[-2.97688193  2.2955549  -2.15344249 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648309  0.62825145  1.14432445
  1.39788999 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.4 fold... 
[Fri Apr  5 07:56:03 2024]  Iteration number: 0 with current cost as 0.31847564551206087 and parameters 
[-2.96832273  2.31217529 -2.14905041 -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.6099314   1.14432446
  1.38125672 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Fri Apr  5 08:07:27 2024]  Iteration number: 0 with current cost as 0.3242472413006292 and parameters 
[-2.97511169  2.32216738 -2.14972332 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61400148  1.14432445
  1.3886929  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.8 fold... 
[Fri Apr  5 08:17:07 2024]  Iteration number: 0 with current cost as 0.30354059107081766 and parameters 
[-2.96405605  2.28629616 -2.15318238 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.61058467  1.14432445
  1.38295214 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 1.0 fold... 
[Fri Apr  5 08:27:58 2024]  Iteration number: 0 with current cost as 0.3017753338594201 and parameters 
[-2.97008084  2.30289244 -2.14802918 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.62229712  1.14432445
  1.3851151  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 3300.330148458481 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 08:39:57 2024]  Iteration number: 0 with current cost as 0.3079862553791395 and parameters 
[-2.97688193  2.2955549  -2.15344249 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648309  0.62825145  1.14432445
  1.39788999 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.4 fold... 
[Fri Apr  5 08:51:00 2024]  Iteration number: 0 with current cost as 0.31847564551206087 and parameters 
[-2.96832273  2.31217529 -2.14905041 -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.6099314   1.14432446
  1.38125672 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Fri Apr  5 09:02:18 2024]  Iteration number: 0 with current cost as 0.3242472413006292 and parameters 
[-2.97511169  2.32216738 -2.14972332 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61400148  1.14432445
  1.3886929  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.8 fold... 
[Fri Apr  5 09:12:21 2024]  Iteration number: 0 with current cost as 0.30354059107081766 and parameters 
[-2.96405605  2.28629616 -2.15318238 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.61058467  1.14432445
  1.38295214 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 1.0 fold... 
[Fri Apr  5 09:23:39 2024]  Iteration number: 0 with current cost as 0.3017753338594201 and parameters 
[-2.97008084  2.30289244 -2.14802918 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.62229712  1.14432445
  1.3851151  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 3373.1179962158203 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 09:36:10 2024]  Iteration number: 0 with current cost as 0.3079862553791395 and parameters 
[-2.97688193  2.2955549  -2.15344249 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648309  0.62825145  1.14432445
  1.39788999 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.4 fold... 
[Fri Apr  5 09:47:13 2024]  Iteration number: 0 with current cost as 0.31847564551206087 and parameters 
[-2.96832273  2.31217529 -2.14905041 -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.6099314   1.14432446
  1.38125672 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Fri Apr  5 09:58:29 2024]  Iteration number: 0 with current cost as 0.3242472413006292 and parameters 
[-2.97511169  2.32216738 -2.14972332 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61400148  1.14432445
  1.3886929  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.8 fold... 
[Fri Apr  5 10:08:12 2024]  Iteration number: 0 with current cost as 0.30354059107081766 and parameters 
[-2.96405605  2.28629616 -2.15318238 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.61058467  1.14432445
  1.38295214 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 1.0 fold... 
[Fri Apr  5 10:19:39 2024]  Iteration number: 0 with current cost as 0.3017753338594201 and parameters 
[-2.97008084  2.30289244 -2.14802918 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.62229712  1.14432445
  1.3851151  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 3364.7530450820923 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 10:32:14 2024]  Iteration number: 0 with current cost as 0.3079862553791395 and parameters 
[-2.97688193  2.2955549  -2.15344249 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648309  0.62825145  1.14432445
  1.39788999 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.4 fold... 
[Fri Apr  5 10:43:20 2024]  Iteration number: 0 with current cost as 0.31847564551206087 and parameters 
[-2.96832273  2.31217529 -2.14905041 -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.6099314   1.14432446
  1.38125672 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Fri Apr  5 10:54:50 2024]  Iteration number: 0 with current cost as 0.3242472413006292 and parameters 
[-2.97511169  2.32216738 -2.14972332 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61400148  1.14432445
  1.3886929  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.8 fold... 
[Fri Apr  5 11:04:33 2024]  Iteration number: 0 with current cost as 0.30354059107081766 and parameters 
[-2.96405605  2.28629616 -2.15318238 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.61058467  1.14432445
  1.38295214 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 1.0 fold... 
[Fri Apr  5 11:15:36 2024]  Iteration number: 0 with current cost as 0.3017753338594201 and parameters 
[-2.97008084  2.30289244 -2.14802918 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.62229712  1.14432445
  1.3851151  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 3339.9231371879578 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 11:27:55 2024]  Iteration number: 0 with current cost as 0.3079862553791395 and parameters 
[-2.97688193  2.2955549  -2.15344249 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648309  0.62825145  1.14432445
  1.39788999 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.4 fold... 
[Fri Apr  5 11:38:57 2024]  Iteration number: 0 with current cost as 0.31847564551206087 and parameters 
[-2.96832273  2.31217529 -2.14905041 -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.6099314   1.14432446
  1.38125672 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Fri Apr  5 11:50:08 2024]  Iteration number: 0 with current cost as 0.3242472413006292 and parameters 
[-2.97511169  2.32216738 -2.14972332 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61400148  1.14432445
  1.3886929  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.8 fold... 
[Fri Apr  5 11:59:46 2024]  Iteration number: 0 with current cost as 0.30354059107081766 and parameters 
[-2.96405605  2.28629616 -2.15318238 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.61058467  1.14432445
  1.38295214 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 1.0 fold... 
[Fri Apr  5 12:10:53 2024]  Iteration number: 0 with current cost as 0.3017753338594201 and parameters 
[-2.97008084  2.30289244 -2.14802918 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.62229712  1.14432445
  1.3851151  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 3299.308560848236 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 12:22:54 2024]  Iteration number: 0 with current cost as 0.3079862553791395 and parameters 
[-2.97688193  2.2955549  -2.15344249 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648309  0.62825145  1.14432445
  1.39788999 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.4 fold... 
[Fri Apr  5 12:34:43 2024]  Iteration number: 0 with current cost as 0.31847564551206087 and parameters 
[-2.96832273  2.31217529 -2.14905041 -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.6099314   1.14432446
  1.38125672 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Fri Apr  5 12:45:59 2024]  Iteration number: 0 with current cost as 0.3242472413006292 and parameters 
[-2.97511169  2.32216738 -2.14972332 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61400148  1.14432445
  1.3886929  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.8 fold... 
[Fri Apr  5 12:55:38 2024]  Iteration number: 0 with current cost as 0.30354059107081766 and parameters 
[-2.96405605  2.28629616 -2.15318238 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.61058467  1.14432445
  1.38295214 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 1.0 fold... 
[Fri Apr  5 13:06:22 2024]  Iteration number: 0 with current cost as 0.3017753338594201 and parameters 
[-2.97008084  2.30289244 -2.14802918 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.62229712  1.14432445
  1.3851151  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 3323.2106459140778 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 13:18:22 2024]  Iteration number: 0 with current cost as 0.3079862553791395 and parameters 
[-2.97688193  2.2955549  -2.15344249 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648309  0.62825145  1.14432445
  1.39788999 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.4 fold... 
[Fri Apr  5 13:29:25 2024]  Iteration number: 0 with current cost as 0.31847564551206087 and parameters 
[-2.96832273  2.31217529 -2.14905041 -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.6099314   1.14432446
  1.38125672 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Fri Apr  5 13:40:40 2024]  Iteration number: 0 with current cost as 0.3242472413006292 and parameters 
[-2.97511169  2.32216738 -2.14972332 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61400148  1.14432445
  1.3886929  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.8 fold... 
[Fri Apr  5 13:50:23 2024]  Iteration number: 0 with current cost as 0.30354059107081766 and parameters 
[-2.96405605  2.28629616 -2.15318238 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.61058467  1.14432445
  1.38295214 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 1.0 fold... 
[Fri Apr  5 14:01:14 2024]  Iteration number: 0 with current cost as 0.3017753338594201 and parameters 
[-2.97008084  2.30289244 -2.14802918 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.62229712  1.14432445
  1.3851151  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 3320.4901099205017 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 14:13:38 2024]  Iteration number: 0 with current cost as 0.3079862553791395 and parameters 
[-2.97688193  2.2955549  -2.15344249 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648309  0.62825145  1.14432445
  1.39788999 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.4 fold... 
[Fri Apr  5 14:24:40 2024]  Iteration number: 0 with current cost as 0.31847564551206087 and parameters 
[-2.96832273  2.31217529 -2.14905041 -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.6099314   1.14432446
  1.38125672 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Fri Apr  5 14:36:14 2024]  Iteration number: 0 with current cost as 0.3242472413006292 and parameters 
[-2.97511169  2.32216738 -2.14972332 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61400148  1.14432445
  1.3886929  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.8 fold... 
[Fri Apr  5 14:46:16 2024]  Iteration number: 0 with current cost as 0.30354059107081766 and parameters 
[-2.96405605  2.28629616 -2.15318238 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.61058467  1.14432445
  1.38295214 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 1.0 fold... 
[Fri Apr  5 14:57:18 2024]  Iteration number: 0 with current cost as 0.3017753338594201 and parameters 
[-2.97008084  2.30289244 -2.14802918 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.62229712  1.14432445
  1.3851151  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 3392.8820612430573 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 15:10:11 2024]  Iteration number: 0 with current cost as 0.3079862553791395 and parameters 
[-2.97688193  2.2955549  -2.15344249 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648309  0.62825145  1.14432445
  1.39788999 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.4 fold... 
[Fri Apr  5 15:21:08 2024]  Iteration number: 0 with current cost as 0.31847564551206087 and parameters 
[-2.96832273  2.31217529 -2.14905041 -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.6099314   1.14432446
  1.38125672 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Fri Apr  5 15:32:24 2024]  Iteration number: 0 with current cost as 0.3242472413006292 and parameters 
[-2.97511169  2.32216738 -2.14972332 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61400148  1.14432445
  1.3886929  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.8 fold... 
[Fri Apr  5 15:42:41 2024]  Iteration number: 0 with current cost as 0.30354059107081766 and parameters 
[-2.96405605  2.28629616 -2.15318238 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.61058467  1.14432445
  1.38295214 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 1.0 fold... 
[Fri Apr  5 15:53:49 2024]  Iteration number: 0 with current cost as 0.3017753338594201 and parameters 
[-2.97008084  2.30289244 -2.14802918 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.62229712  1.14432445
  1.3851151  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 3350.0530095100403 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 16:06:03 2024]  Iteration number: 0 with current cost as 0.3079862553791395 and parameters 
[-2.97688193  2.2955549  -2.15344249 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648309  0.62825145  1.14432445
  1.39788999 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.4 fold... 
[Fri Apr  5 16:17:48 2024]  Iteration number: 0 with current cost as 0.31847564551206087 and parameters 
[-2.96832273  2.31217529 -2.14905041 -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.6099314   1.14432446
  1.38125672 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Fri Apr  5 16:29:51 2024]  Iteration number: 0 with current cost as 0.3242472413006292 and parameters 
[-2.97511169  2.32216738 -2.14972332 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61400148  1.14432445
  1.3886929  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.8 fold... 
[Fri Apr  5 16:39:46 2024]  Iteration number: 0 with current cost as 0.30354059107081766 and parameters 
[-2.96405605  2.28629616 -2.15318238 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.61058467  1.14432445
  1.38295214 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 1.0 fold... 
[Fri Apr  5 16:50:50 2024]  Iteration number: 0 with current cost as 0.3017753338594201 and parameters 
[-2.97008084  2.30289244 -2.14802918 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.62229712  1.14432445
  1.3851151  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 3439.318848848343 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 17:03:21 2024]  Iteration number: 0 with current cost as 0.3079862553791395 and parameters 
[-2.97688193  2.2955549  -2.15344249 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648309  0.62825145  1.14432445
  1.39788999 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.4 fold... 
[Fri Apr  5 17:14:57 2024]  Iteration number: 0 with current cost as 0.31847564551206087 and parameters 
[-2.96832273  2.31217529 -2.14905041 -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.6099314   1.14432446
  1.38125672 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Fri Apr  5 17:26:33 2024]  Iteration number: 0 with current cost as 0.3242472413006292 and parameters 
[-2.97511169  2.32216738 -2.14972332 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61400148  1.14432445
  1.3886929  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.8 fold... 
[Fri Apr  5 17:36:22 2024]  Iteration number: 0 with current cost as 0.30354059107081766 and parameters 
[-2.96405605  2.28629616 -2.15318238 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.61058467  1.14432445
  1.38295214 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 1.0 fold... 
[Fri Apr  5 17:48:12 2024]  Iteration number: 0 with current cost as 0.3017753338594201 and parameters 
[-2.97008084  2.30289244 -2.14802918 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.62229712  1.14432445
  1.3851151  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 3414.7531917095184 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 18:00:14 2024]  Iteration number: 0 with current cost as 0.3079862553791395 and parameters 
[-2.97688193  2.2955549  -2.15344249 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648309  0.62825145  1.14432445
  1.39788999 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.4 fold... 
[Fri Apr  5 18:11:23 2024]  Iteration number: 0 with current cost as 0.31847564551206087 and parameters 
[-2.96832273  2.31217529 -2.14905041 -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.6099314   1.14432446
  1.38125672 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Fri Apr  5 18:23:25 2024]  Iteration number: 0 with current cost as 0.3242472413006292 and parameters 
[-2.97511169  2.32216738 -2.14972332 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61400148  1.14432445
  1.3886929  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.8 fold... 
[Fri Apr  5 18:33:05 2024]  Iteration number: 0 with current cost as 0.30354059107081766 and parameters 
[-2.96405605  2.28629616 -2.15318238 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.61058467  1.14432445
  1.38295214 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 1.0 fold... 
[Fri Apr  5 18:43:51 2024]  Iteration number: 0 with current cost as 0.3017753338594201 and parameters 
[-2.97008084  2.30289244 -2.14802918 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.62229712  1.14432445
  1.3851151  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 3352.695558309555 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 18:56:08 2024]  Iteration number: 0 with current cost as 0.3079862553791395 and parameters 
[-2.97688193  2.2955549  -2.15344249 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648309  0.62825145  1.14432445
  1.39788999 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.4 fold... 
[Fri Apr  5 19:07:15 2024]  Iteration number: 0 with current cost as 0.31847564551206087 and parameters 
[-2.96832273  2.31217529 -2.14905041 -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.6099314   1.14432446
  1.38125672 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Fri Apr  5 19:18:35 2024]  Iteration number: 0 with current cost as 0.3242472413006292 and parameters 
[-2.97511169  2.32216738 -2.14972332 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61400148  1.14432445
  1.3886929  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.8 fold... 
[Fri Apr  5 19:28:18 2024]  Iteration number: 0 with current cost as 0.30354059107081766 and parameters 
[-2.96405605  2.28629616 -2.15318238 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.61058467  1.14432445
  1.38295214 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 1.0 fold... 
[Fri Apr  5 19:39:16 2024]  Iteration number: 0 with current cost as 0.3017753338594201 and parameters 
[-2.97008084  2.30289244 -2.14802918 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.62229712  1.14432445
  1.3851151  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 3305.2022132873535 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 19:51:13 2024]  Iteration number: 0 with current cost as 0.3079862553791395 and parameters 
[-2.97688193  2.2955549  -2.15344249 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648309  0.62825145  1.14432445
  1.39788999 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.4 fold... 
[Fri Apr  5 20:02:24 2024]  Iteration number: 0 with current cost as 0.31847564551206087 and parameters 
[-2.96832273  2.31217529 -2.14905041 -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.6099314   1.14432446
  1.38125672 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Fri Apr  5 20:13:41 2024]  Iteration number: 0 with current cost as 0.3242472413006292 and parameters 
[-2.97511169  2.32216738 -2.14972332 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61400148  1.14432445
  1.3886929  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.8 fold... 
[Fri Apr  5 20:23:28 2024]  Iteration number: 0 with current cost as 0.30354059107081766 and parameters 
[-2.96405605  2.28629616 -2.15318238 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.61058467  1.14432445
  1.38295214 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 1.0 fold... 
[Fri Apr  5 20:34:22 2024]  Iteration number: 0 with current cost as 0.3017753338594201 and parameters 
[-2.97008084  2.30289244 -2.14802918 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.62229712  1.14432445
  1.3851151  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 3325.061290502548 seconds. 
Discarding model... 

Training complete taking 83321.01958680153 total seconds. 
Now scoring model... 
Scoring complete taking 1.180492639541626 seconds. 
Saved predicted values as M-M-CNOT_Full-Pauli-CRX_predicted_values.csv
Model scores: {'MSE_train': (0.3561820116788551,), 'R2_train': 0.290757763154068, 'MAE_train': 0.5127219410858297, 'MSE_test': 0.3948711350587809, 'R2_test': 0.2635791731996382, 'MAE_test': 0.5467939087847212}. 
Saved model results as M-M-CNOT_Full-Pauli-CRX_results.json. 
