/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /scratch/j/jacobsen/gjones/sine5qubits/sine_train.bin... 
Successfully loaded /scratch/j/jacobsen/gjones/sine5qubits/sine_train.bin into X and y data. 
Loading dataset from /scratch/j/jacobsen/gjones/sine5qubits/sine_test.bin... 
Successfully loaded /scratch/j/jacobsen/gjones/sine5qubits/sine_test.bin into X and y data. 
Training model with dataset /scratch/j/jacobsen/gjones/sine5qubits/sine_train.bin 
 at time Thu Apr  4 22:03:30 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 22:04:40 2024]  Iteration number: 0 with current cost as 0.6752914652729203 and parameters 
[-0.07783389  2.23743464 -2.12427945 -0.11653103  0.55388698 -2.77010907
  3.06858479  2.18960145  1.18552008 -1.06648318  0.6027151   1.14432436
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Apr  4 22:09:11 2024]  Iteration number: 0 with current cost as 0.30564058383849557 and parameters 
[ 1.45292769  2.23743464 -2.1242793  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552032 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 22:11:43 2024]  Iteration number: 0 with current cost as 0.31224371193036243 and parameters 
[ 1.38271467  2.23743448 -2.12427948 -0.11653118  0.55388692 -2.77010929
  3.06858483  2.18960145  1.18551998 -1.06648324  0.60271495  1.14432429
  1.31029883 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 22:14:26 2024]  Iteration number: 0 with current cost as 0.29855781656868585 and parameters 
[ 1.33710453  2.23743464 -2.12427932 -0.11653118  0.55388692 -2.77010897
  3.06858483  2.18960161  1.18552014 -1.06648308  0.60271526  1.14432445
  1.31029914 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 22:17:05 2024]  Iteration number: 0 with current cost as 0.3032996818698616 and parameters 
[ 1.35806314  2.23743464 -2.12427933 -0.11653103  0.55388693 -2.77010912
  3.06858483  2.18960145  1.18552014 -1.06648308  0.6027151   1.1443243
  1.31029899 -1.8735468 ]. 
Training complete taking 903.5848610401154 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 22:19:45 2024]  Iteration number: 0 with current cost as 0.6752914652729203 and parameters 
[-0.07783389  2.23743464 -2.12427945 -0.11653103  0.55388698 -2.77010907
  3.06858479  2.18960145  1.18552008 -1.06648318  0.6027151   1.14432436
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Apr  4 22:24:18 2024]  Iteration number: 0 with current cost as 0.30564058383849557 and parameters 
[ 1.45292769  2.23743464 -2.1242793  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552032 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 22:26:47 2024]  Iteration number: 0 with current cost as 0.31224371193036243 and parameters 
[ 1.38271467  2.23743448 -2.12427948 -0.11653118  0.55388692 -2.77010929
  3.06858483  2.18960145  1.18551998 -1.06648324  0.60271495  1.14432429
  1.31029883 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 22:29:35 2024]  Iteration number: 0 with current cost as 0.29855781656868585 and parameters 
[ 1.33710453  2.23743464 -2.12427932 -0.11653118  0.55388692 -2.77010897
  3.06858483  2.18960161  1.18552014 -1.06648308  0.60271526  1.14432445
  1.31029914 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 22:32:15 2024]  Iteration number: 0 with current cost as 0.3032996818698616 and parameters 
[ 1.35806314  2.23743464 -2.12427933 -0.11653103  0.55388693 -2.77010912
  3.06858483  2.18960145  1.18552014 -1.06648308  0.6027151   1.1443243
  1.31029899 -1.8735468 ]. 
Training complete taking 919.9099328517914 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 22:35:09 2024]  Iteration number: 0 with current cost as 0.6752914652729203 and parameters 
[-0.07783389  2.23743464 -2.12427945 -0.11653103  0.55388698 -2.77010907
  3.06858479  2.18960145  1.18552008 -1.06648318  0.6027151   1.14432436
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Apr  4 22:39:53 2024]  Iteration number: 0 with current cost as 0.30564058383849557 and parameters 
[ 1.45292769  2.23743464 -2.1242793  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552032 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 22:42:18 2024]  Iteration number: 0 with current cost as 0.31224371193036243 and parameters 
[ 1.38271467  2.23743448 -2.12427948 -0.11653118  0.55388692 -2.77010929
  3.06858483  2.18960145  1.18551998 -1.06648324  0.60271495  1.14432429
  1.31029883 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 22:45:06 2024]  Iteration number: 0 with current cost as 0.29855781656868585 and parameters 
[ 1.33710453  2.23743464 -2.12427932 -0.11653118  0.55388692 -2.77010897
  3.06858483  2.18960161  1.18552014 -1.06648308  0.60271526  1.14432445
  1.31029914 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 22:47:39 2024]  Iteration number: 0 with current cost as 0.3032996818698616 and parameters 
[ 1.35806314  2.23743464 -2.12427933 -0.11653103  0.55388693 -2.77010912
  3.06858483  2.18960145  1.18552014 -1.06648308  0.6027151   1.1443243
  1.31029899 -1.8735468 ]. 
Training complete taking 913.5680255889893 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 22:50:20 2024]  Iteration number: 0 with current cost as 0.6752914652729203 and parameters 
[-0.07783389  2.23743464 -2.12427945 -0.11653103  0.55388698 -2.77010907
  3.06858479  2.18960145  1.18552008 -1.06648318  0.6027151   1.14432436
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Apr  4 22:54:57 2024]  Iteration number: 0 with current cost as 0.30564058383849557 and parameters 
[ 1.45292769  2.23743464 -2.1242793  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552032 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 22:57:27 2024]  Iteration number: 0 with current cost as 0.31224371193036243 and parameters 
[ 1.38271467  2.23743448 -2.12427948 -0.11653118  0.55388692 -2.77010929
  3.06858483  2.18960145  1.18551998 -1.06648324  0.60271495  1.14432429
  1.31029883 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 23:00:13 2024]  Iteration number: 0 with current cost as 0.29855781656868585 and parameters 
[ 1.33710453  2.23743464 -2.12427932 -0.11653118  0.55388692 -2.77010897
  3.06858483  2.18960161  1.18552014 -1.06648308  0.60271526  1.14432445
  1.31029914 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 23:02:57 2024]  Iteration number: 0 with current cost as 0.3032996818698616 and parameters 
[ 1.35806314  2.23743464 -2.12427933 -0.11653103  0.55388693 -2.77010912
  3.06858483  2.18960145  1.18552014 -1.06648308  0.6027151   1.1443243
  1.31029899 -1.8735468 ]. 
Training complete taking 928.828774690628 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 23:05:57 2024]  Iteration number: 0 with current cost as 0.6752914652729203 and parameters 
[-0.07783389  2.23743464 -2.12427945 -0.11653103  0.55388698 -2.77010907
  3.06858479  2.18960145  1.18552008 -1.06648318  0.6027151   1.14432436
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Apr  4 23:10:29 2024]  Iteration number: 0 with current cost as 0.30564058383849557 and parameters 
[ 1.45292769  2.23743464 -2.1242793  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552032 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 23:12:54 2024]  Iteration number: 0 with current cost as 0.31224371193036243 and parameters 
[ 1.38271467  2.23743448 -2.12427948 -0.11653118  0.55388692 -2.77010929
  3.06858483  2.18960145  1.18551998 -1.06648324  0.60271495  1.14432429
  1.31029883 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 23:15:39 2024]  Iteration number: 0 with current cost as 0.29855781656868585 and parameters 
[ 1.33710453  2.23743464 -2.12427932 -0.11653118  0.55388692 -2.77010897
  3.06858483  2.18960161  1.18552014 -1.06648308  0.60271526  1.14432445
  1.31029914 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 23:18:25 2024]  Iteration number: 0 with current cost as 0.3032996818698616 and parameters 
[ 1.35806314  2.23743464 -2.12427933 -0.11653103  0.55388693 -2.77010912
  3.06858483  2.18960145  1.18552014 -1.06648308  0.6027151   1.1443243
  1.31029899 -1.8735468 ]. 
Training complete taking 911.4254570007324 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 23:20:59 2024]  Iteration number: 0 with current cost as 0.6752914652729203 and parameters 
[-0.07783389  2.23743464 -2.12427945 -0.11653103  0.55388698 -2.77010907
  3.06858479  2.18960145  1.18552008 -1.06648318  0.6027151   1.14432436
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Apr  4 23:25:17 2024]  Iteration number: 0 with current cost as 0.30564058383849557 and parameters 
[ 1.45292769  2.23743464 -2.1242793  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552032 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 23:27:36 2024]  Iteration number: 0 with current cost as 0.31224371193036243 and parameters 
[ 1.38271467  2.23743448 -2.12427948 -0.11653118  0.55388692 -2.77010929
  3.06858483  2.18960145  1.18551998 -1.06648324  0.60271495  1.14432429
  1.31029883 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 23:30:08 2024]  Iteration number: 0 with current cost as 0.29855781656868585 and parameters 
[ 1.33710453  2.23743464 -2.12427932 -0.11653118  0.55388692 -2.77010897
  3.06858483  2.18960161  1.18552014 -1.06648308  0.60271526  1.14432445
  1.31029914 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 23:32:44 2024]  Iteration number: 0 with current cost as 0.3032996818698616 and parameters 
[ 1.35806314  2.23743464 -2.12427933 -0.11653103  0.55388693 -2.77010912
  3.06858483  2.18960145  1.18552014 -1.06648308  0.6027151   1.1443243
  1.31029899 -1.8735468 ]. 
Training complete taking 864.6431119441986 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 23:35:22 2024]  Iteration number: 0 with current cost as 0.6752914652729203 and parameters 
[-0.07783389  2.23743464 -2.12427945 -0.11653103  0.55388698 -2.77010907
  3.06858479  2.18960145  1.18552008 -1.06648318  0.6027151   1.14432436
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Apr  4 23:39:45 2024]  Iteration number: 0 with current cost as 0.30564058383849557 and parameters 
[ 1.45292769  2.23743464 -2.1242793  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552032 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 23:42:10 2024]  Iteration number: 0 with current cost as 0.31224371193036243 and parameters 
[ 1.38271467  2.23743448 -2.12427948 -0.11653118  0.55388692 -2.77010929
  3.06858483  2.18960145  1.18551998 -1.06648324  0.60271495  1.14432429
  1.31029883 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 23:44:49 2024]  Iteration number: 0 with current cost as 0.29855781656868585 and parameters 
[ 1.33710453  2.23743464 -2.12427932 -0.11653118  0.55388692 -2.77010897
  3.06858483  2.18960161  1.18552014 -1.06648308  0.60271526  1.14432445
  1.31029914 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 23:47:24 2024]  Iteration number: 0 with current cost as 0.3032996818698616 and parameters 
[ 1.35806314  2.23743464 -2.12427933 -0.11653103  0.55388693 -2.77010912
  3.06858483  2.18960145  1.18552014 -1.06648308  0.6027151   1.1443243
  1.31029899 -1.8735468 ]. 
Training complete taking 880.7725021839142 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 23:50:03 2024]  Iteration number: 0 with current cost as 0.6752914652729203 and parameters 
[-0.07783389  2.23743464 -2.12427945 -0.11653103  0.55388698 -2.77010907
  3.06858479  2.18960145  1.18552008 -1.06648318  0.6027151   1.14432436
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Apr  4 23:54:25 2024]  Iteration number: 0 with current cost as 0.30564058383849557 and parameters 
[ 1.45292769  2.23743464 -2.1242793  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552032 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 23:56:42 2024]  Iteration number: 0 with current cost as 0.31224371193036243 and parameters 
[ 1.38271467  2.23743448 -2.12427948 -0.11653118  0.55388692 -2.77010929
  3.06858483  2.18960145  1.18551998 -1.06648324  0.60271495  1.14432429
  1.31029883 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 23:59:19 2024]  Iteration number: 0 with current cost as 0.29855781656868585 and parameters 
[ 1.33710453  2.23743464 -2.12427932 -0.11653118  0.55388692 -2.77010897
  3.06858483  2.18960161  1.18552014 -1.06648308  0.60271526  1.14432445
  1.31029914 -1.8735468 ]. 
Working on 1.0 fold... 
[Fri Apr  5 00:02:06 2024]  Iteration number: 0 with current cost as 0.3032996818698616 and parameters 
[ 1.35806314  2.23743464 -2.12427933 -0.11653103  0.55388693 -2.77010912
  3.06858483  2.18960145  1.18552014 -1.06648308  0.6027151   1.1443243
  1.31029899 -1.8735468 ]. 
Training complete taking 884.654789686203 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 00:04:51 2024]  Iteration number: 0 with current cost as 0.6752914652729203 and parameters 
[-0.07783389  2.23743464 -2.12427945 -0.11653103  0.55388698 -2.77010907
  3.06858479  2.18960145  1.18552008 -1.06648318  0.6027151   1.14432436
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Fri Apr  5 00:09:17 2024]  Iteration number: 0 with current cost as 0.30564058383849557 and parameters 
[ 1.45292769  2.23743464 -2.1242793  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552032 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Fri Apr  5 00:11:41 2024]  Iteration number: 0 with current cost as 0.31224371193036243 and parameters 
[ 1.38271467  2.23743448 -2.12427948 -0.11653118  0.55388692 -2.77010929
  3.06858483  2.18960145  1.18551998 -1.06648324  0.60271495  1.14432429
  1.31029883 -1.8735468 ]. 
Working on 0.8 fold... 
[Fri Apr  5 00:14:21 2024]  Iteration number: 0 with current cost as 0.29855781656868585 and parameters 
[ 1.33710453  2.23743464 -2.12427932 -0.11653118  0.55388692 -2.77010897
  3.06858483  2.18960161  1.18552014 -1.06648308  0.60271526  1.14432445
  1.31029914 -1.8735468 ]. 
Working on 1.0 fold... 
[Fri Apr  5 00:17:01 2024]  Iteration number: 0 with current cost as 0.3032996818698616 and parameters 
[ 1.35806314  2.23743464 -2.12427933 -0.11653103  0.55388693 -2.77010912
  3.06858483  2.18960145  1.18552014 -1.06648308  0.6027151   1.1443243
  1.31029899 -1.8735468 ]. 
Training complete taking 896.7350130081177 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 00:19:53 2024]  Iteration number: 0 with current cost as 0.6752914652729203 and parameters 
[-0.07783389  2.23743464 -2.12427945 -0.11653103  0.55388698 -2.77010907
  3.06858479  2.18960145  1.18552008 -1.06648318  0.6027151   1.14432436
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Fri Apr  5 00:24:30 2024]  Iteration number: 0 with current cost as 0.30564058383849557 and parameters 
[ 1.45292769  2.23743464 -2.1242793  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552032 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Fri Apr  5 00:27:03 2024]  Iteration number: 0 with current cost as 0.31224371193036243 and parameters 
[ 1.38271467  2.23743448 -2.12427948 -0.11653118  0.55388692 -2.77010929
  3.06858483  2.18960145  1.18551998 -1.06648324  0.60271495  1.14432429
  1.31029883 -1.8735468 ]. 
Working on 0.8 fold... 
[Fri Apr  5 00:29:55 2024]  Iteration number: 0 with current cost as 0.29855781656868585 and parameters 
[ 1.33710453  2.23743464 -2.12427932 -0.11653118  0.55388692 -2.77010897
  3.06858483  2.18960161  1.18552014 -1.06648308  0.60271526  1.14432445
  1.31029914 -1.8735468 ]. 
Working on 1.0 fold... 
[Fri Apr  5 00:32:36 2024]  Iteration number: 0 with current cost as 0.3032996818698616 and parameters 
[ 1.35806314  2.23743464 -2.12427933 -0.11653103  0.55388693 -2.77010912
  3.06858483  2.18960145  1.18552014 -1.06648308  0.6027151   1.1443243
  1.31029899 -1.8735468 ]. 
Training complete taking 935.6499419212341 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 00:35:27 2024]  Iteration number: 0 with current cost as 0.6752914652729203 and parameters 
[-0.07783389  2.23743464 -2.12427945 -0.11653103  0.55388698 -2.77010907
  3.06858479  2.18960145  1.18552008 -1.06648318  0.6027151   1.14432436
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Fri Apr  5 00:40:05 2024]  Iteration number: 0 with current cost as 0.30564058383849557 and parameters 
[ 1.45292769  2.23743464 -2.1242793  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552032 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Fri Apr  5 00:42:35 2024]  Iteration number: 0 with current cost as 0.31224371193036243 and parameters 
[ 1.38271467  2.23743448 -2.12427948 -0.11653118  0.55388692 -2.77010929
  3.06858483  2.18960145  1.18551998 -1.06648324  0.60271495  1.14432429
  1.31029883 -1.8735468 ]. 
Working on 0.8 fold... 
[Fri Apr  5 00:45:23 2024]  Iteration number: 0 with current cost as 0.29855781656868585 and parameters 
[ 1.33710453  2.23743464 -2.12427932 -0.11653118  0.55388692 -2.77010897
  3.06858483  2.18960161  1.18552014 -1.06648308  0.60271526  1.14432445
  1.31029914 -1.8735468 ]. 
Working on 1.0 fold... 
[Fri Apr  5 00:48:16 2024]  Iteration number: 0 with current cost as 0.3032996818698616 and parameters 
[ 1.35806314  2.23743464 -2.12427933 -0.11653103  0.55388693 -2.77010912
  3.06858483  2.18960145  1.18552014 -1.06648308  0.6027151   1.1443243
  1.31029899 -1.8735468 ]. 
Training complete taking 942.4900045394897 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 00:51:07 2024]  Iteration number: 0 with current cost as 0.6752914652729203 and parameters 
[-0.07783389  2.23743464 -2.12427945 -0.11653103  0.55388698 -2.77010907
  3.06858479  2.18960145  1.18552008 -1.06648318  0.6027151   1.14432436
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Fri Apr  5 00:55:56 2024]  Iteration number: 0 with current cost as 0.30564058383849557 and parameters 
[ 1.45292769  2.23743464 -2.1242793  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552032 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Fri Apr  5 00:58:23 2024]  Iteration number: 0 with current cost as 0.31224371193036243 and parameters 
[ 1.38271467  2.23743448 -2.12427948 -0.11653118  0.55388692 -2.77010929
  3.06858483  2.18960145  1.18551998 -1.06648324  0.60271495  1.14432429
  1.31029883 -1.8735468 ]. 
Working on 0.8 fold... 
[Fri Apr  5 01:01:13 2024]  Iteration number: 0 with current cost as 0.29855781656868585 and parameters 
[ 1.33710453  2.23743464 -2.12427932 -0.11653118  0.55388692 -2.77010897
  3.06858483  2.18960161  1.18552014 -1.06648308  0.60271526  1.14432445
  1.31029914 -1.8735468 ]. 
Working on 1.0 fold... 
[Fri Apr  5 01:04:02 2024]  Iteration number: 0 with current cost as 0.3032996818698616 and parameters 
[ 1.35806314  2.23743464 -2.12427933 -0.11653103  0.55388693 -2.77010912
  3.06858483  2.18960145  1.18552014 -1.06648308  0.6027151   1.1443243
  1.31029899 -1.8735468 ]. 
Training complete taking 946.5407581329346 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 01:06:54 2024]  Iteration number: 0 with current cost as 0.6752914652729203 and parameters 
[-0.07783389  2.23743464 -2.12427945 -0.11653103  0.55388698 -2.77010907
  3.06858479  2.18960145  1.18552008 -1.06648318  0.6027151   1.14432436
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Fri Apr  5 01:11:36 2024]  Iteration number: 0 with current cost as 0.30564058383849557 and parameters 
[ 1.45292769  2.23743464 -2.1242793  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552032 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Fri Apr  5 01:14:06 2024]  Iteration number: 0 with current cost as 0.31224371193036243 and parameters 
[ 1.38271467  2.23743448 -2.12427948 -0.11653118  0.55388692 -2.77010929
  3.06858483  2.18960145  1.18551998 -1.06648324  0.60271495  1.14432429
  1.31029883 -1.8735468 ]. 
Working on 0.8 fold... 
[Fri Apr  5 01:17:03 2024]  Iteration number: 0 with current cost as 0.29855781656868585 and parameters 
[ 1.33710453  2.23743464 -2.12427932 -0.11653118  0.55388692 -2.77010897
  3.06858483  2.18960161  1.18552014 -1.06648308  0.60271526  1.14432445
  1.31029914 -1.8735468 ]. 
Working on 1.0 fold... 
[Fri Apr  5 01:19:55 2024]  Iteration number: 0 with current cost as 0.3032996818698616 and parameters 
[ 1.35806314  2.23743464 -2.12427933 -0.11653103  0.55388693 -2.77010912
  3.06858483  2.18960145  1.18552014 -1.06648308  0.6027151   1.1443243
  1.31029899 -1.8735468 ]. 
Training complete taking 946.1414980888367 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 01:22:35 2024]  Iteration number: 0 with current cost as 0.6752914652729203 and parameters 
[-0.07783389  2.23743464 -2.12427945 -0.11653103  0.55388698 -2.77010907
  3.06858479  2.18960145  1.18552008 -1.06648318  0.6027151   1.14432436
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Fri Apr  5 01:27:04 2024]  Iteration number: 0 with current cost as 0.30564058383849557 and parameters 
[ 1.45292769  2.23743464 -2.1242793  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552032 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Fri Apr  5 01:29:28 2024]  Iteration number: 0 with current cost as 0.31224371193036243 and parameters 
[ 1.38271467  2.23743448 -2.12427948 -0.11653118  0.55388692 -2.77010929
  3.06858483  2.18960145  1.18551998 -1.06648324  0.60271495  1.14432429
  1.31029883 -1.8735468 ]. 
Working on 0.8 fold... 
[Fri Apr  5 01:32:13 2024]  Iteration number: 0 with current cost as 0.29855781656868585 and parameters 
[ 1.33710453  2.23743464 -2.12427932 -0.11653118  0.55388692 -2.77010897
  3.06858483  2.18960161  1.18552014 -1.06648308  0.60271526  1.14432445
  1.31029914 -1.8735468 ]. 
Working on 1.0 fold... 
[Fri Apr  5 01:34:57 2024]  Iteration number: 0 with current cost as 0.3032996818698616 and parameters 
[ 1.35806314  2.23743464 -2.12427933 -0.11653103  0.55388693 -2.77010912
  3.06858483  2.18960145  1.18552014 -1.06648308  0.6027151   1.1443243
  1.31029899 -1.8735468 ]. 
Training complete taking 902.4036872386932 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 01:37:36 2024]  Iteration number: 0 with current cost as 0.6752914652729203 and parameters 
[-0.07783389  2.23743464 -2.12427945 -0.11653103  0.55388698 -2.77010907
  3.06858479  2.18960145  1.18552008 -1.06648318  0.6027151   1.14432436
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Fri Apr  5 01:42:11 2024]  Iteration number: 0 with current cost as 0.30564058383849557 and parameters 
[ 1.45292769  2.23743464 -2.1242793  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552032 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Fri Apr  5 01:44:35 2024]  Iteration number: 0 with current cost as 0.31224371193036243 and parameters 
[ 1.38271467  2.23743448 -2.12427948 -0.11653118  0.55388692 -2.77010929
  3.06858483  2.18960145  1.18551998 -1.06648324  0.60271495  1.14432429
  1.31029883 -1.8735468 ]. 
Working on 0.8 fold... 
[Fri Apr  5 01:47:20 2024]  Iteration number: 0 with current cost as 0.29855781656868585 and parameters 
[ 1.33710453  2.23743464 -2.12427932 -0.11653118  0.55388692 -2.77010897
  3.06858483  2.18960161  1.18552014 -1.06648308  0.60271526  1.14432445
  1.31029914 -1.8735468 ]. 
Working on 1.0 fold... 
[Fri Apr  5 01:50:08 2024]  Iteration number: 0 with current cost as 0.3032996818698616 and parameters 
[ 1.35806314  2.23743464 -2.12427933 -0.11653103  0.55388693 -2.77010912
  3.06858483  2.18960145  1.18552014 -1.06648308  0.6027151   1.1443243
  1.31029899 -1.8735468 ]. 
Training complete taking 910.3475952148438 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 01:52:52 2024]  Iteration number: 0 with current cost as 0.6752914652729203 and parameters 
[-0.07783389  2.23743464 -2.12427945 -0.11653103  0.55388698 -2.77010907
  3.06858479  2.18960145  1.18552008 -1.06648318  0.6027151   1.14432436
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Fri Apr  5 01:57:19 2024]  Iteration number: 0 with current cost as 0.30564058383849557 and parameters 
[ 1.45292769  2.23743464 -2.1242793  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552032 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Fri Apr  5 01:59:46 2024]  Iteration number: 0 with current cost as 0.31224371193036243 and parameters 
[ 1.38271467  2.23743448 -2.12427948 -0.11653118  0.55388692 -2.77010929
  3.06858483  2.18960145  1.18551998 -1.06648324  0.60271495  1.14432429
  1.31029883 -1.8735468 ]. 
Working on 0.8 fold... 
[Fri Apr  5 02:02:26 2024]  Iteration number: 0 with current cost as 0.29855781656868585 and parameters 
[ 1.33710453  2.23743464 -2.12427932 -0.11653118  0.55388692 -2.77010897
  3.06858483  2.18960161  1.18552014 -1.06648308  0.60271526  1.14432445
  1.31029914 -1.8735468 ]. 
Working on 1.0 fold... 
[Fri Apr  5 02:05:16 2024]  Iteration number: 0 with current cost as 0.3032996818698616 and parameters 
[ 1.35806314  2.23743464 -2.12427933 -0.11653103  0.55388693 -2.77010912
  3.06858483  2.18960145  1.18552014 -1.06648308  0.6027151   1.1443243
  1.31029899 -1.8735468 ]. 
Training complete taking 914.1195003986359 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 02:08:03 2024]  Iteration number: 0 with current cost as 0.6752914652729203 and parameters 
[-0.07783389  2.23743464 -2.12427945 -0.11653103  0.55388698 -2.77010907
  3.06858479  2.18960145  1.18552008 -1.06648318  0.6027151   1.14432436
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Fri Apr  5 02:12:38 2024]  Iteration number: 0 with current cost as 0.30564058383849557 and parameters 
[ 1.45292769  2.23743464 -2.1242793  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552032 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Fri Apr  5 02:15:05 2024]  Iteration number: 0 with current cost as 0.31224371193036243 and parameters 
[ 1.38271467  2.23743448 -2.12427948 -0.11653118  0.55388692 -2.77010929
  3.06858483  2.18960145  1.18551998 -1.06648324  0.60271495  1.14432429
  1.31029883 -1.8735468 ]. 
Working on 0.8 fold... 
[Fri Apr  5 02:17:48 2024]  Iteration number: 0 with current cost as 0.29855781656868585 and parameters 
[ 1.33710453  2.23743464 -2.12427932 -0.11653118  0.55388692 -2.77010897
  3.06858483  2.18960161  1.18552014 -1.06648308  0.60271526  1.14432445
  1.31029914 -1.8735468 ]. 
Working on 1.0 fold... 
[Fri Apr  5 02:20:30 2024]  Iteration number: 0 with current cost as 0.3032996818698616 and parameters 
[ 1.35806314  2.23743464 -2.12427933 -0.11653103  0.55388693 -2.77010912
  3.06858483  2.18960145  1.18552014 -1.06648308  0.6027151   1.1443243
  1.31029899 -1.8735468 ]. 
Training complete taking 915.1898534297943 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 02:23:24 2024]  Iteration number: 0 with current cost as 0.6752914652729203 and parameters 
[-0.07783389  2.23743464 -2.12427945 -0.11653103  0.55388698 -2.77010907
  3.06858479  2.18960145  1.18552008 -1.06648318  0.6027151   1.14432436
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Fri Apr  5 02:28:03 2024]  Iteration number: 0 with current cost as 0.30564058383849557 and parameters 
[ 1.45292769  2.23743464 -2.1242793  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552032 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Fri Apr  5 02:30:31 2024]  Iteration number: 0 with current cost as 0.31224371193036243 and parameters 
[ 1.38271467  2.23743448 -2.12427948 -0.11653118  0.55388692 -2.77010929
  3.06858483  2.18960145  1.18551998 -1.06648324  0.60271495  1.14432429
  1.31029883 -1.8735468 ]. 
Working on 0.8 fold... 
[Fri Apr  5 02:33:15 2024]  Iteration number: 0 with current cost as 0.29855781656868585 and parameters 
[ 1.33710453  2.23743464 -2.12427932 -0.11653118  0.55388692 -2.77010897
  3.06858483  2.18960161  1.18552014 -1.06648308  0.60271526  1.14432445
  1.31029914 -1.8735468 ]. 
Working on 1.0 fold... 
[Fri Apr  5 02:35:59 2024]  Iteration number: 0 with current cost as 0.3032996818698616 and parameters 
[ 1.35806314  2.23743464 -2.12427933 -0.11653103  0.55388693 -2.77010912
  3.06858483  2.18960145  1.18552014 -1.06648308  0.6027151   1.1443243
  1.31029899 -1.8735468 ]. 
Training complete taking 925.7225601673126 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 02:38:49 2024]  Iteration number: 0 with current cost as 0.6752914652729203 and parameters 
[-0.07783389  2.23743464 -2.12427945 -0.11653103  0.55388698 -2.77010907
  3.06858479  2.18960145  1.18552008 -1.06648318  0.6027151   1.14432436
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Fri Apr  5 02:43:17 2024]  Iteration number: 0 with current cost as 0.30564058383849557 and parameters 
[ 1.45292769  2.23743464 -2.1242793  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552032 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Fri Apr  5 02:45:38 2024]  Iteration number: 0 with current cost as 0.31224371193036243 and parameters 
[ 1.38271467  2.23743448 -2.12427948 -0.11653118  0.55388692 -2.77010929
  3.06858483  2.18960145  1.18551998 -1.06648324  0.60271495  1.14432429
  1.31029883 -1.8735468 ]. 
Working on 0.8 fold... 
[Fri Apr  5 02:48:17 2024]  Iteration number: 0 with current cost as 0.29855781656868585 and parameters 
[ 1.33710453  2.23743464 -2.12427932 -0.11653118  0.55388692 -2.77010897
  3.06858483  2.18960161  1.18552014 -1.06648308  0.60271526  1.14432445
  1.31029914 -1.8735468 ]. 
Working on 1.0 fold... 
[Fri Apr  5 02:51:04 2024]  Iteration number: 0 with current cost as 0.3032996818698616 and parameters 
[ 1.35806314  2.23743464 -2.12427933 -0.11653103  0.55388693 -2.77010912
  3.06858483  2.18960145  1.18552014 -1.06648308  0.6027151   1.1443243
  1.31029899 -1.8735468 ]. 
Training complete taking 903.8431406021118 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 02:53:49 2024]  Iteration number: 0 with current cost as 0.6752914652729203 and parameters 
[-0.07783389  2.23743464 -2.12427945 -0.11653103  0.55388698 -2.77010907
  3.06858479  2.18960145  1.18552008 -1.06648318  0.6027151   1.14432436
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Fri Apr  5 02:58:18 2024]  Iteration number: 0 with current cost as 0.30564058383849557 and parameters 
[ 1.45292769  2.23743464 -2.1242793  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552032 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Fri Apr  5 03:00:46 2024]  Iteration number: 0 with current cost as 0.31224371193036243 and parameters 
[ 1.38271467  2.23743448 -2.12427948 -0.11653118  0.55388692 -2.77010929
  3.06858483  2.18960145  1.18551998 -1.06648324  0.60271495  1.14432429
  1.31029883 -1.8735468 ]. 
Working on 0.8 fold... 
[Fri Apr  5 03:03:26 2024]  Iteration number: 0 with current cost as 0.29855781656868585 and parameters 
[ 1.33710453  2.23743464 -2.12427932 -0.11653118  0.55388692 -2.77010897
  3.06858483  2.18960161  1.18552014 -1.06648308  0.60271526  1.14432445
  1.31029914 -1.8735468 ]. 
Working on 1.0 fold... 
[Fri Apr  5 03:06:04 2024]  Iteration number: 0 with current cost as 0.3032996818698616 and parameters 
[ 1.35806314  2.23743464 -2.12427933 -0.11653103  0.55388693 -2.77010912
  3.06858483  2.18960145  1.18552014 -1.06648308  0.6027151   1.1443243
  1.31029899 -1.8735468 ]. 
Training complete taking 898.2883808612823 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 03:08:46 2024]  Iteration number: 0 with current cost as 0.6752914652729203 and parameters 
[-0.07783389  2.23743464 -2.12427945 -0.11653103  0.55388698 -2.77010907
  3.06858479  2.18960145  1.18552008 -1.06648318  0.6027151   1.14432436
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Fri Apr  5 03:13:18 2024]  Iteration number: 0 with current cost as 0.30564058383849557 and parameters 
[ 1.45292769  2.23743464 -2.1242793  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552032 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Fri Apr  5 03:15:45 2024]  Iteration number: 0 with current cost as 0.31224371193036243 and parameters 
[ 1.38271467  2.23743448 -2.12427948 -0.11653118  0.55388692 -2.77010929
  3.06858483  2.18960145  1.18551998 -1.06648324  0.60271495  1.14432429
  1.31029883 -1.8735468 ]. 
Working on 0.8 fold... 
[Fri Apr  5 03:18:34 2024]  Iteration number: 0 with current cost as 0.29855781656868585 and parameters 
[ 1.33710453  2.23743464 -2.12427932 -0.11653118  0.55388692 -2.77010897
  3.06858483  2.18960161  1.18552014 -1.06648308  0.60271526  1.14432445
  1.31029914 -1.8735468 ]. 
Working on 1.0 fold... 
[Fri Apr  5 03:21:26 2024]  Iteration number: 0 with current cost as 0.3032996818698616 and parameters 
[ 1.35806314  2.23743464 -2.12427933 -0.11653103  0.55388693 -2.77010912
  3.06858483  2.18960145  1.18552014 -1.06648308  0.6027151   1.1443243
  1.31029899 -1.8735468 ]. 
Training complete taking 922.9509258270264 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 03:24:11 2024]  Iteration number: 0 with current cost as 0.6752914652729203 and parameters 
[-0.07783389  2.23743464 -2.12427945 -0.11653103  0.55388698 -2.77010907
  3.06858479  2.18960145  1.18552008 -1.06648318  0.6027151   1.14432436
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Fri Apr  5 03:28:38 2024]  Iteration number: 0 with current cost as 0.30564058383849557 and parameters 
[ 1.45292769  2.23743464 -2.1242793  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552032 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Fri Apr  5 03:31:04 2024]  Iteration number: 0 with current cost as 0.31224371193036243 and parameters 
[ 1.38271467  2.23743448 -2.12427948 -0.11653118  0.55388692 -2.77010929
  3.06858483  2.18960145  1.18551998 -1.06648324  0.60271495  1.14432429
  1.31029883 -1.8735468 ]. 
Working on 0.8 fold... 
[Fri Apr  5 03:33:47 2024]  Iteration number: 0 with current cost as 0.29855781656868585 and parameters 
[ 1.33710453  2.23743464 -2.12427932 -0.11653118  0.55388692 -2.77010897
  3.06858483  2.18960161  1.18552014 -1.06648308  0.60271526  1.14432445
  1.31029914 -1.8735468 ]. 
Working on 1.0 fold... 
[Fri Apr  5 03:36:30 2024]  Iteration number: 0 with current cost as 0.3032996818698616 and parameters 
[ 1.35806314  2.23743464 -2.12427933 -0.11653103  0.55388693 -2.77010912
  3.06858483  2.18960145  1.18552014 -1.06648308  0.6027151   1.1443243
  1.31029899 -1.8735468 ]. 
Training complete taking 904.5868217945099 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 03:39:14 2024]  Iteration number: 0 with current cost as 0.6752914652729203 and parameters 
[-0.07783389  2.23743464 -2.12427945 -0.11653103  0.55388698 -2.77010907
  3.06858479  2.18960145  1.18552008 -1.06648318  0.6027151   1.14432436
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Fri Apr  5 03:43:49 2024]  Iteration number: 0 with current cost as 0.30564058383849557 and parameters 
[ 1.45292769  2.23743464 -2.1242793  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552032 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Fri Apr  5 03:46:12 2024]  Iteration number: 0 with current cost as 0.31224371193036243 and parameters 
[ 1.38271467  2.23743448 -2.12427948 -0.11653118  0.55388692 -2.77010929
  3.06858483  2.18960145  1.18551998 -1.06648324  0.60271495  1.14432429
  1.31029883 -1.8735468 ]. 
Working on 0.8 fold... 
[Fri Apr  5 03:48:59 2024]  Iteration number: 0 with current cost as 0.29855781656868585 and parameters 
[ 1.33710453  2.23743464 -2.12427932 -0.11653118  0.55388692 -2.77010897
  3.06858483  2.18960161  1.18552014 -1.06648308  0.60271526  1.14432445
  1.31029914 -1.8735468 ]. 
Working on 1.0 fold... 
[Fri Apr  5 03:51:44 2024]  Iteration number: 0 with current cost as 0.3032996818698616 and parameters 
[ 1.35806314  2.23743464 -2.12427933 -0.11653103  0.55388693 -2.77010912
  3.06858483  2.18960145  1.18552014 -1.06648308  0.6027151   1.1443243
  1.31029899 -1.8735468 ]. 
Training complete taking 913.2942531108856 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 03:54:26 2024]  Iteration number: 0 with current cost as 0.6752914652729203 and parameters 
[-0.07783389  2.23743464 -2.12427945 -0.11653103  0.55388698 -2.77010907
  3.06858479  2.18960145  1.18552008 -1.06648318  0.6027151   1.14432436
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Fri Apr  5 03:58:55 2024]  Iteration number: 0 with current cost as 0.30564058383849557 and parameters 
[ 1.45292769  2.23743464 -2.1242793  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552032 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Fri Apr  5 04:01:22 2024]  Iteration number: 0 with current cost as 0.31224371193036243 and parameters 
[ 1.38271467  2.23743448 -2.12427948 -0.11653118  0.55388692 -2.77010929
  3.06858483  2.18960145  1.18551998 -1.06648324  0.60271495  1.14432429
  1.31029883 -1.8735468 ]. 
Working on 0.8 fold... 
[Fri Apr  5 04:04:06 2024]  Iteration number: 0 with current cost as 0.29855781656868585 and parameters 
[ 1.33710453  2.23743464 -2.12427932 -0.11653118  0.55388692 -2.77010897
  3.06858483  2.18960161  1.18552014 -1.06648308  0.60271526  1.14432445
  1.31029914 -1.8735468 ]. 
Working on 1.0 fold... 
[Fri Apr  5 04:06:56 2024]  Iteration number: 0 with current cost as 0.3032996818698616 and parameters 
[ 1.35806314  2.23743464 -2.12427933 -0.11653103  0.55388693 -2.77010912
  3.06858483  2.18960145  1.18552014 -1.06648308  0.6027151   1.1443243
  1.31029899 -1.8735468 ]. 
Training complete taking 925.3728895187378 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 04:10:06 2024]  Iteration number: 0 with current cost as 0.6752914652729203 and parameters 
[-0.07783389  2.23743464 -2.12427945 -0.11653103  0.55388698 -2.77010907
  3.06858479  2.18960145  1.18552008 -1.06648318  0.6027151   1.14432436
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Fri Apr  5 04:15:03 2024]  Iteration number: 0 with current cost as 0.30564058383849557 and parameters 
[ 1.45292769  2.23743464 -2.1242793  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552032 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Fri Apr  5 04:17:37 2024]  Iteration number: 0 with current cost as 0.31224371193036243 and parameters 
[ 1.38271467  2.23743448 -2.12427948 -0.11653118  0.55388692 -2.77010929
  3.06858483  2.18960145  1.18551998 -1.06648324  0.60271495  1.14432429
  1.31029883 -1.8735468 ]. 
Working on 0.8 fold... 
[Fri Apr  5 04:20:28 2024]  Iteration number: 0 with current cost as 0.29855781656868585 and parameters 
[ 1.33710453  2.23743464 -2.12427932 -0.11653118  0.55388692 -2.77010897
  3.06858483  2.18960161  1.18552014 -1.06648308  0.60271526  1.14432445
  1.31029914 -1.8735468 ]. 
Working on 1.0 fold... 
[Fri Apr  5 04:23:20 2024]  Iteration number: 0 with current cost as 0.3032996818698616 and parameters 
[ 1.35806314  2.23743464 -2.12427933 -0.11653103  0.55388693 -2.77010912
  3.06858483  2.18960145  1.18552014 -1.06648308  0.6027151   1.1443243
  1.31029899 -1.8735468 ]. 
Training complete taking 969.4815158843994 seconds. 
Discarding model... 

Training complete taking 22880.54686689377 total seconds. 
Now scoring model... 
Scoring complete taking 2.5951907634735107 seconds. 
Saved predicted values as A1-A1-CNOT_Efficient-CRZ_predicted_values.csv
Model scores: {'MSE_train': (0.5000432171886947,), 'R2_train': 0.004296235492343481, 'MAE_train': 0.6162022730895036, 'MSE_test': 0.5406052054884682, 'R2_test': -0.00820976022755815, 'MAE_test': 0.6648572308883135}. 
Saved model results as A1-A1-CNOT_Efficient-CRZ_results.json. 
