/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /scratch/j/jacobsen/gjones/sine5qubits/sine_train.bin... 
Successfully loaded /scratch/j/jacobsen/gjones/sine5qubits/sine_train.bin into X and y data. 
Loading dataset from /scratch/j/jacobsen/gjones/sine5qubits/sine_test.bin... 
Successfully loaded /scratch/j/jacobsen/gjones/sine5qubits/sine_test.bin into X and y data. 
Training model with dataset /scratch/j/jacobsen/gjones/sine5qubits/sine_train.bin 
 at time Thu Apr  4 21:37:35 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 21:38:49 2024]  Iteration number: 0 with current cost as 0.6752914652729238 and parameters 
[-0.07783389  2.23743473 -2.12427954 -0.11653093  0.55388708 -2.77010897
  3.06858489  2.18960155  1.18552008 -1.06648299  0.6027152   1.14432455
  1.31029908 -1.87354661]. 
Working on 0.4 fold... 
[Thu Apr  4 21:43:15 2024]  Iteration number: 0 with current cost as 0.3056405850133766 and parameters 
[ 1.45292767  2.23743464 -2.12427947 -0.11653103  0.55388708 -2.77010914
  3.06858498  2.18960145  1.18552015 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354647]. 
Working on 0.6 fold... 
[Thu Apr  4 21:45:36 2024]  Iteration number: 0 with current cost as 0.31224367415399956 and parameters 
[ 1.38271498  2.23743479 -2.12427932 -0.11653087  0.55388708 -2.77010882
  3.06858483  2.18960161  1.18552014 -1.06648308  0.60271526  1.14432461
  1.31029914 -1.87354633]. 
Working on 0.8 fold... 
[Thu Apr  4 21:48:17 2024]  Iteration number: 0 with current cost as 0.2985577951821744 and parameters 
[ 1.33710469  2.23743479 -2.12427948 -0.11653087  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.06648324  0.60271526  1.14432461
  1.31029899 -1.87354664]. 
Working on 1.0 fold... 
[Thu Apr  4 21:50:55 2024]  Iteration number: 0 with current cost as 0.30329993839961483 and parameters 
[ 1.358061    2.23743494 -2.12427933 -0.11653072  0.55388723 -2.77010897
  3.06858498  2.18960175  1.18552029 -1.06648308  0.60271541  1.14432475
  1.31029929 -1.8735462 ]. 
Training complete taking 887.6625781059265 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 21:53:34 2024]  Iteration number: 0 with current cost as 0.6752914652729238 and parameters 
[-0.07783389  2.23743473 -2.12427954 -0.11653093  0.55388708 -2.77010897
  3.06858489  2.18960155  1.18552008 -1.06648299  0.6027152   1.14432455
  1.31029908 -1.87354661]. 
Working on 0.4 fold... 
[Thu Apr  4 21:57:55 2024]  Iteration number: 0 with current cost as 0.3056405850133766 and parameters 
[ 1.45292767  2.23743464 -2.12427947 -0.11653103  0.55388708 -2.77010914
  3.06858498  2.18960145  1.18552015 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354647]. 
Working on 0.6 fold... 
[Thu Apr  4 22:00:16 2024]  Iteration number: 0 with current cost as 0.31224367415399956 and parameters 
[ 1.38271498  2.23743479 -2.12427932 -0.11653087  0.55388708 -2.77010882
  3.06858483  2.18960161  1.18552014 -1.06648308  0.60271526  1.14432461
  1.31029914 -1.87354633]. 
Working on 0.8 fold... 
[Thu Apr  4 22:02:52 2024]  Iteration number: 0 with current cost as 0.2985577951821744 and parameters 
[ 1.33710469  2.23743479 -2.12427948 -0.11653087  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.06648324  0.60271526  1.14432461
  1.31029899 -1.87354664]. 
Working on 1.0 fold... 
[Thu Apr  4 22:05:37 2024]  Iteration number: 0 with current cost as 0.30329993839961483 and parameters 
[ 1.358061    2.23743494 -2.12427933 -0.11653072  0.55388723 -2.77010897
  3.06858498  2.18960175  1.18552029 -1.06648308  0.60271541  1.14432475
  1.31029929 -1.8735462 ]. 
Training complete taking 885.9208006858826 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 22:08:18 2024]  Iteration number: 0 with current cost as 0.6752914652729238 and parameters 
[-0.07783389  2.23743473 -2.12427954 -0.11653093  0.55388708 -2.77010897
  3.06858489  2.18960155  1.18552008 -1.06648299  0.6027152   1.14432455
  1.31029908 -1.87354661]. 
Working on 0.4 fold... 
[Thu Apr  4 22:12:49 2024]  Iteration number: 0 with current cost as 0.3056405850133766 and parameters 
[ 1.45292767  2.23743464 -2.12427947 -0.11653103  0.55388708 -2.77010914
  3.06858498  2.18960145  1.18552015 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354647]. 
Working on 0.6 fold... 
[Thu Apr  4 22:15:16 2024]  Iteration number: 0 with current cost as 0.31224367415399956 and parameters 
[ 1.38271498  2.23743479 -2.12427932 -0.11653087  0.55388708 -2.77010882
  3.06858483  2.18960161  1.18552014 -1.06648308  0.60271526  1.14432461
  1.31029914 -1.87354633]. 
Working on 0.8 fold... 
[Thu Apr  4 22:17:55 2024]  Iteration number: 0 with current cost as 0.2985577951821744 and parameters 
[ 1.33710469  2.23743479 -2.12427948 -0.11653087  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.06648324  0.60271526  1.14432461
  1.31029899 -1.87354664]. 
Working on 1.0 fold... 
[Thu Apr  4 22:20:29 2024]  Iteration number: 0 with current cost as 0.30329993839961483 and parameters 
[ 1.358061    2.23743494 -2.12427933 -0.11653072  0.55388723 -2.77010897
  3.06858498  2.18960175  1.18552029 -1.06648308  0.60271541  1.14432475
  1.31029929 -1.8735462 ]. 
Training complete taking 887.7449796199799 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 22:23:10 2024]  Iteration number: 0 with current cost as 0.6752914652729238 and parameters 
[-0.07783389  2.23743473 -2.12427954 -0.11653093  0.55388708 -2.77010897
  3.06858489  2.18960155  1.18552008 -1.06648299  0.6027152   1.14432455
  1.31029908 -1.87354661]. 
Working on 0.4 fold... 
[Thu Apr  4 22:27:39 2024]  Iteration number: 0 with current cost as 0.3056405850133766 and parameters 
[ 1.45292767  2.23743464 -2.12427947 -0.11653103  0.55388708 -2.77010914
  3.06858498  2.18960145  1.18552015 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354647]. 
Working on 0.6 fold... 
[Thu Apr  4 22:30:05 2024]  Iteration number: 0 with current cost as 0.31224367415399956 and parameters 
[ 1.38271498  2.23743479 -2.12427932 -0.11653087  0.55388708 -2.77010882
  3.06858483  2.18960161  1.18552014 -1.06648308  0.60271526  1.14432461
  1.31029914 -1.87354633]. 
Working on 0.8 fold... 
[Thu Apr  4 22:32:50 2024]  Iteration number: 0 with current cost as 0.2985577951821744 and parameters 
[ 1.33710469  2.23743479 -2.12427948 -0.11653087  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.06648324  0.60271526  1.14432461
  1.31029899 -1.87354664]. 
Working on 1.0 fold... 
[Thu Apr  4 22:35:32 2024]  Iteration number: 0 with current cost as 0.30329993839961483 and parameters 
[ 1.358061    2.23743494 -2.12427933 -0.11653072  0.55388723 -2.77010897
  3.06858498  2.18960175  1.18552029 -1.06648308  0.60271541  1.14432475
  1.31029929 -1.8735462 ]. 
Training complete taking 901.8703856468201 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 22:38:12 2024]  Iteration number: 0 with current cost as 0.6752914652729238 and parameters 
[-0.07783389  2.23743473 -2.12427954 -0.11653093  0.55388708 -2.77010897
  3.06858489  2.18960155  1.18552008 -1.06648299  0.6027152   1.14432455
  1.31029908 -1.87354661]. 
Working on 0.4 fold... 
[Thu Apr  4 22:42:41 2024]  Iteration number: 0 with current cost as 0.3056405850133766 and parameters 
[ 1.45292767  2.23743464 -2.12427947 -0.11653103  0.55388708 -2.77010914
  3.06858498  2.18960145  1.18552015 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354647]. 
Working on 0.6 fold... 
[Thu Apr  4 22:45:03 2024]  Iteration number: 0 with current cost as 0.31224367415399956 and parameters 
[ 1.38271498  2.23743479 -2.12427932 -0.11653087  0.55388708 -2.77010882
  3.06858483  2.18960161  1.18552014 -1.06648308  0.60271526  1.14432461
  1.31029914 -1.87354633]. 
Working on 0.8 fold... 
[Thu Apr  4 22:47:43 2024]  Iteration number: 0 with current cost as 0.2985577951821744 and parameters 
[ 1.33710469  2.23743479 -2.12427948 -0.11653087  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.06648324  0.60271526  1.14432461
  1.31029899 -1.87354664]. 
Working on 1.0 fold... 
[Thu Apr  4 22:50:22 2024]  Iteration number: 0 with current cost as 0.30329993839961483 and parameters 
[ 1.358061    2.23743494 -2.12427933 -0.11653072  0.55388723 -2.77010897
  3.06858498  2.18960175  1.18552029 -1.06648308  0.60271541  1.14432475
  1.31029929 -1.8735462 ]. 
Training complete taking 894.7389011383057 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 22:53:05 2024]  Iteration number: 0 with current cost as 0.6752914652729238 and parameters 
[-0.07783389  2.23743473 -2.12427954 -0.11653093  0.55388708 -2.77010897
  3.06858489  2.18960155  1.18552008 -1.06648299  0.6027152   1.14432455
  1.31029908 -1.87354661]. 
Working on 0.4 fold... 
[Thu Apr  4 22:57:32 2024]  Iteration number: 0 with current cost as 0.3056405850133766 and parameters 
[ 1.45292767  2.23743464 -2.12427947 -0.11653103  0.55388708 -2.77010914
  3.06858498  2.18960145  1.18552015 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354647]. 
Working on 0.6 fold... 
[Thu Apr  4 22:59:56 2024]  Iteration number: 0 with current cost as 0.31224367415399956 and parameters 
[ 1.38271498  2.23743479 -2.12427932 -0.11653087  0.55388708 -2.77010882
  3.06858483  2.18960161  1.18552014 -1.06648308  0.60271526  1.14432461
  1.31029914 -1.87354633]. 
Working on 0.8 fold... 
[Thu Apr  4 23:02:40 2024]  Iteration number: 0 with current cost as 0.2985577951821744 and parameters 
[ 1.33710469  2.23743479 -2.12427948 -0.11653087  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.06648324  0.60271526  1.14432461
  1.31029899 -1.87354664]. 
Working on 1.0 fold... 
[Thu Apr  4 23:05:29 2024]  Iteration number: 0 with current cost as 0.30329993839961483 and parameters 
[ 1.358061    2.23743494 -2.12427933 -0.11653072  0.55388723 -2.77010897
  3.06858498  2.18960175  1.18552029 -1.06648308  0.60271541  1.14432475
  1.31029929 -1.8735462 ]. 
Training complete taking 903.9409053325653 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 23:08:09 2024]  Iteration number: 0 with current cost as 0.6752914652729238 and parameters 
[-0.07783389  2.23743473 -2.12427954 -0.11653093  0.55388708 -2.77010897
  3.06858489  2.18960155  1.18552008 -1.06648299  0.6027152   1.14432455
  1.31029908 -1.87354661]. 
Working on 0.4 fold... 
[Thu Apr  4 23:12:40 2024]  Iteration number: 0 with current cost as 0.3056405850133766 and parameters 
[ 1.45292767  2.23743464 -2.12427947 -0.11653103  0.55388708 -2.77010914
  3.06858498  2.18960145  1.18552015 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354647]. 
Working on 0.6 fold... 
[Thu Apr  4 23:15:04 2024]  Iteration number: 0 with current cost as 0.31224367415399956 and parameters 
[ 1.38271498  2.23743479 -2.12427932 -0.11653087  0.55388708 -2.77010882
  3.06858483  2.18960161  1.18552014 -1.06648308  0.60271526  1.14432461
  1.31029914 -1.87354633]. 
Working on 0.8 fold... 
[Thu Apr  4 23:17:48 2024]  Iteration number: 0 with current cost as 0.2985577951821744 and parameters 
[ 1.33710469  2.23743479 -2.12427948 -0.11653087  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.06648324  0.60271526  1.14432461
  1.31029899 -1.87354664]. 
Working on 1.0 fold... 
[Thu Apr  4 23:20:32 2024]  Iteration number: 0 with current cost as 0.30329993839961483 and parameters 
[ 1.358061    2.23743494 -2.12427933 -0.11653072  0.55388723 -2.77010897
  3.06858498  2.18960175  1.18552029 -1.06648308  0.60271541  1.14432475
  1.31029929 -1.8735462 ]. 
Training complete taking 907.194718837738 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 23:23:18 2024]  Iteration number: 0 with current cost as 0.6752914652729238 and parameters 
[-0.07783389  2.23743473 -2.12427954 -0.11653093  0.55388708 -2.77010897
  3.06858489  2.18960155  1.18552008 -1.06648299  0.6027152   1.14432455
  1.31029908 -1.87354661]. 
Working on 0.4 fold... 
[Thu Apr  4 23:27:49 2024]  Iteration number: 0 with current cost as 0.3056405850133766 and parameters 
[ 1.45292767  2.23743464 -2.12427947 -0.11653103  0.55388708 -2.77010914
  3.06858498  2.18960145  1.18552015 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354647]. 
Working on 0.6 fold... 
[Thu Apr  4 23:30:16 2024]  Iteration number: 0 with current cost as 0.31224367415399956 and parameters 
[ 1.38271498  2.23743479 -2.12427932 -0.11653087  0.55388708 -2.77010882
  3.06858483  2.18960161  1.18552014 -1.06648308  0.60271526  1.14432461
  1.31029914 -1.87354633]. 
Working on 0.8 fold... 
[Thu Apr  4 23:33:01 2024]  Iteration number: 0 with current cost as 0.2985577951821744 and parameters 
[ 1.33710469  2.23743479 -2.12427948 -0.11653087  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.06648324  0.60271526  1.14432461
  1.31029899 -1.87354664]. 
Working on 1.0 fold... 
[Thu Apr  4 23:35:50 2024]  Iteration number: 0 with current cost as 0.30329993839961483 and parameters 
[ 1.358061    2.23743494 -2.12427933 -0.11653072  0.55388723 -2.77010897
  3.06858498  2.18960175  1.18552029 -1.06648308  0.60271541  1.14432475
  1.31029929 -1.8735462 ]. 
Training complete taking 915.1944591999054 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 23:38:32 2024]  Iteration number: 0 with current cost as 0.6752914652729238 and parameters 
[-0.07783389  2.23743473 -2.12427954 -0.11653093  0.55388708 -2.77010897
  3.06858489  2.18960155  1.18552008 -1.06648299  0.6027152   1.14432455
  1.31029908 -1.87354661]. 
Working on 0.4 fold... 
[Thu Apr  4 23:43:05 2024]  Iteration number: 0 with current cost as 0.3056405850133766 and parameters 
[ 1.45292767  2.23743464 -2.12427947 -0.11653103  0.55388708 -2.77010914
  3.06858498  2.18960145  1.18552015 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354647]. 
Working on 0.6 fold... 
[Thu Apr  4 23:45:33 2024]  Iteration number: 0 with current cost as 0.31224367415399956 and parameters 
[ 1.38271498  2.23743479 -2.12427932 -0.11653087  0.55388708 -2.77010882
  3.06858483  2.18960161  1.18552014 -1.06648308  0.60271526  1.14432461
  1.31029914 -1.87354633]. 
Working on 0.8 fold... 
[Thu Apr  4 23:48:17 2024]  Iteration number: 0 with current cost as 0.2985577951821744 and parameters 
[ 1.33710469  2.23743479 -2.12427948 -0.11653087  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.06648324  0.60271526  1.14432461
  1.31029899 -1.87354664]. 
Working on 1.0 fold... 
[Thu Apr  4 23:50:56 2024]  Iteration number: 0 with current cost as 0.30329993839961483 and parameters 
[ 1.358061    2.23743494 -2.12427933 -0.11653072  0.55388723 -2.77010897
  3.06858498  2.18960175  1.18552029 -1.06648308  0.60271541  1.14432475
  1.31029929 -1.8735462 ]. 
Training complete taking 907.7087414264679 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 23:53:36 2024]  Iteration number: 0 with current cost as 0.6752914652729238 and parameters 
[-0.07783389  2.23743473 -2.12427954 -0.11653093  0.55388708 -2.77010897
  3.06858489  2.18960155  1.18552008 -1.06648299  0.6027152   1.14432455
  1.31029908 -1.87354661]. 
Working on 0.4 fold... 
[Thu Apr  4 23:58:00 2024]  Iteration number: 0 with current cost as 0.3056405850133766 and parameters 
[ 1.45292767  2.23743464 -2.12427947 -0.11653103  0.55388708 -2.77010914
  3.06858498  2.18960145  1.18552015 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354647]. 
Working on 0.6 fold... 
[Fri Apr  5 00:00:22 2024]  Iteration number: 0 with current cost as 0.31224367415399956 and parameters 
[ 1.38271498  2.23743479 -2.12427932 -0.11653087  0.55388708 -2.77010882
  3.06858483  2.18960161  1.18552014 -1.06648308  0.60271526  1.14432461
  1.31029914 -1.87354633]. 
Working on 0.8 fold... 
[Fri Apr  5 00:03:02 2024]  Iteration number: 0 with current cost as 0.2985577951821744 and parameters 
[ 1.33710469  2.23743479 -2.12427948 -0.11653087  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.06648324  0.60271526  1.14432461
  1.31029899 -1.87354664]. 
Working on 1.0 fold... 
[Fri Apr  5 00:05:35 2024]  Iteration number: 0 with current cost as 0.30329993839961483 and parameters 
[ 1.358061    2.23743494 -2.12427933 -0.11653072  0.55388723 -2.77010897
  3.06858498  2.18960175  1.18552029 -1.06648308  0.60271541  1.14432475
  1.31029929 -1.8735462 ]. 
Training complete taking 879.3340969085693 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 00:08:15 2024]  Iteration number: 0 with current cost as 0.6752914652729238 and parameters 
[-0.07783389  2.23743473 -2.12427954 -0.11653093  0.55388708 -2.77010897
  3.06858489  2.18960155  1.18552008 -1.06648299  0.6027152   1.14432455
  1.31029908 -1.87354661]. 
Working on 0.4 fold... 
[Fri Apr  5 00:12:36 2024]  Iteration number: 0 with current cost as 0.3056405850133766 and parameters 
[ 1.45292767  2.23743464 -2.12427947 -0.11653103  0.55388708 -2.77010914
  3.06858498  2.18960145  1.18552015 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354647]. 
Working on 0.6 fold... 
[Fri Apr  5 00:14:55 2024]  Iteration number: 0 with current cost as 0.31224367415399956 and parameters 
[ 1.38271498  2.23743479 -2.12427932 -0.11653087  0.55388708 -2.77010882
  3.06858483  2.18960161  1.18552014 -1.06648308  0.60271526  1.14432461
  1.31029914 -1.87354633]. 
Working on 0.8 fold... 
[Fri Apr  5 00:17:36 2024]  Iteration number: 0 with current cost as 0.2985577951821744 and parameters 
[ 1.33710469  2.23743479 -2.12427948 -0.11653087  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.06648324  0.60271526  1.14432461
  1.31029899 -1.87354664]. 
Working on 1.0 fold... 
[Fri Apr  5 00:20:13 2024]  Iteration number: 0 with current cost as 0.30329993839961483 and parameters 
[ 1.358061    2.23743494 -2.12427933 -0.11653072  0.55388723 -2.77010897
  3.06858498  2.18960175  1.18552029 -1.06648308  0.60271541  1.14432475
  1.31029929 -1.8735462 ]. 
Training complete taking 876.8615479469299 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 00:22:55 2024]  Iteration number: 0 with current cost as 0.6752914652729238 and parameters 
[-0.07783389  2.23743473 -2.12427954 -0.11653093  0.55388708 -2.77010897
  3.06858489  2.18960155  1.18552008 -1.06648299  0.6027152   1.14432455
  1.31029908 -1.87354661]. 
Working on 0.4 fold... 
[Fri Apr  5 00:27:13 2024]  Iteration number: 0 with current cost as 0.3056405850133766 and parameters 
[ 1.45292767  2.23743464 -2.12427947 -0.11653103  0.55388708 -2.77010914
  3.06858498  2.18960145  1.18552015 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354647]. 
Working on 0.6 fold... 
[Fri Apr  5 00:29:32 2024]  Iteration number: 0 with current cost as 0.31224367415399956 and parameters 
[ 1.38271498  2.23743479 -2.12427932 -0.11653087  0.55388708 -2.77010882
  3.06858483  2.18960161  1.18552014 -1.06648308  0.60271526  1.14432461
  1.31029914 -1.87354633]. 
Working on 0.8 fold... 
[Fri Apr  5 00:32:09 2024]  Iteration number: 0 with current cost as 0.2985577951821744 and parameters 
[ 1.33710469  2.23743479 -2.12427948 -0.11653087  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.06648324  0.60271526  1.14432461
  1.31029899 -1.87354664]. 
Working on 1.0 fold... 
[Fri Apr  5 00:34:51 2024]  Iteration number: 0 with current cost as 0.30329993839961483 and parameters 
[ 1.358061    2.23743494 -2.12427933 -0.11653072  0.55388723 -2.77010897
  3.06858498  2.18960175  1.18552029 -1.06648308  0.60271541  1.14432475
  1.31029929 -1.8735462 ]. 
Training complete taking 876.4003314971924 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 00:37:30 2024]  Iteration number: 0 with current cost as 0.6752914652729238 and parameters 
[-0.07783389  2.23743473 -2.12427954 -0.11653093  0.55388708 -2.77010897
  3.06858489  2.18960155  1.18552008 -1.06648299  0.6027152   1.14432455
  1.31029908 -1.87354661]. 
Working on 0.4 fold... 
[Fri Apr  5 00:41:56 2024]  Iteration number: 0 with current cost as 0.3056405850133766 and parameters 
[ 1.45292767  2.23743464 -2.12427947 -0.11653103  0.55388708 -2.77010914
  3.06858498  2.18960145  1.18552015 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354647]. 
Working on 0.6 fold... 
[Fri Apr  5 00:44:20 2024]  Iteration number: 0 with current cost as 0.31224367415399956 and parameters 
[ 1.38271498  2.23743479 -2.12427932 -0.11653087  0.55388708 -2.77010882
  3.06858483  2.18960161  1.18552014 -1.06648308  0.60271526  1.14432461
  1.31029914 -1.87354633]. 
Working on 0.8 fold... 
[Fri Apr  5 00:47:00 2024]  Iteration number: 0 with current cost as 0.2985577951821744 and parameters 
[ 1.33710469  2.23743479 -2.12427948 -0.11653087  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.06648324  0.60271526  1.14432461
  1.31029899 -1.87354664]. 
Working on 1.0 fold... 
[Fri Apr  5 00:49:38 2024]  Iteration number: 0 with current cost as 0.30329993839961483 and parameters 
[ 1.358061    2.23743494 -2.12427933 -0.11653072  0.55388723 -2.77010897
  3.06858498  2.18960175  1.18552029 -1.06648308  0.60271541  1.14432475
  1.31029929 -1.8735462 ]. 
Training complete taking 888.5985779762268 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 00:52:18 2024]  Iteration number: 0 with current cost as 0.6752914652729238 and parameters 
[-0.07783389  2.23743473 -2.12427954 -0.11653093  0.55388708 -2.77010897
  3.06858489  2.18960155  1.18552008 -1.06648299  0.6027152   1.14432455
  1.31029908 -1.87354661]. 
Working on 0.4 fold... 
[Fri Apr  5 00:56:34 2024]  Iteration number: 0 with current cost as 0.3056405850133766 and parameters 
[ 1.45292767  2.23743464 -2.12427947 -0.11653103  0.55388708 -2.77010914
  3.06858498  2.18960145  1.18552015 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354647]. 
Working on 0.6 fold... 
[Fri Apr  5 00:58:56 2024]  Iteration number: 0 with current cost as 0.31224367415399956 and parameters 
[ 1.38271498  2.23743479 -2.12427932 -0.11653087  0.55388708 -2.77010882
  3.06858483  2.18960161  1.18552014 -1.06648308  0.60271526  1.14432461
  1.31029914 -1.87354633]. 
Working on 0.8 fold... 
[Fri Apr  5 01:01:35 2024]  Iteration number: 0 with current cost as 0.2985577951821744 and parameters 
[ 1.33710469  2.23743479 -2.12427948 -0.11653087  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.06648324  0.60271526  1.14432461
  1.31029899 -1.87354664]. 
Working on 1.0 fold... 
[Fri Apr  5 01:04:07 2024]  Iteration number: 0 with current cost as 0.30329993839961483 and parameters 
[ 1.358061    2.23743494 -2.12427933 -0.11653072  0.55388723 -2.77010897
  3.06858498  2.18960175  1.18552029 -1.06648308  0.60271541  1.14432475
  1.31029929 -1.8735462 ]. 
Training complete taking 871.2367990016937 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 01:06:49 2024]  Iteration number: 0 with current cost as 0.6752914652729238 and parameters 
[-0.07783389  2.23743473 -2.12427954 -0.11653093  0.55388708 -2.77010897
  3.06858489  2.18960155  1.18552008 -1.06648299  0.6027152   1.14432455
  1.31029908 -1.87354661]. 
Working on 0.4 fold... 
[Fri Apr  5 01:11:10 2024]  Iteration number: 0 with current cost as 0.3056405850133766 and parameters 
[ 1.45292767  2.23743464 -2.12427947 -0.11653103  0.55388708 -2.77010914
  3.06858498  2.18960145  1.18552015 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354647]. 
Working on 0.6 fold... 
[Fri Apr  5 01:13:35 2024]  Iteration number: 0 with current cost as 0.31224367415399956 and parameters 
[ 1.38271498  2.23743479 -2.12427932 -0.11653087  0.55388708 -2.77010882
  3.06858483  2.18960161  1.18552014 -1.06648308  0.60271526  1.14432461
  1.31029914 -1.87354633]. 
Working on 0.8 fold... 
[Fri Apr  5 01:16:11 2024]  Iteration number: 0 with current cost as 0.2985577951821744 and parameters 
[ 1.33710469  2.23743479 -2.12427948 -0.11653087  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.06648324  0.60271526  1.14432461
  1.31029899 -1.87354664]. 
Working on 1.0 fold... 
[Fri Apr  5 01:18:50 2024]  Iteration number: 0 with current cost as 0.30329993839961483 and parameters 
[ 1.358061    2.23743494 -2.12427933 -0.11653072  0.55388723 -2.77010897
  3.06858498  2.18960175  1.18552029 -1.06648308  0.60271541  1.14432475
  1.31029929 -1.8735462 ]. 
Training complete taking 878.9233739376068 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 01:21:28 2024]  Iteration number: 0 with current cost as 0.6752914652729238 and parameters 
[-0.07783389  2.23743473 -2.12427954 -0.11653093  0.55388708 -2.77010897
  3.06858489  2.18960155  1.18552008 -1.06648299  0.6027152   1.14432455
  1.31029908 -1.87354661]. 
Working on 0.4 fold... 
[Fri Apr  5 01:25:49 2024]  Iteration number: 0 with current cost as 0.3056405850133766 and parameters 
[ 1.45292767  2.23743464 -2.12427947 -0.11653103  0.55388708 -2.77010914
  3.06858498  2.18960145  1.18552015 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354647]. 
Working on 0.6 fold... 
[Fri Apr  5 01:28:06 2024]  Iteration number: 0 with current cost as 0.31224367415399956 and parameters 
[ 1.38271498  2.23743479 -2.12427932 -0.11653087  0.55388708 -2.77010882
  3.06858483  2.18960161  1.18552014 -1.06648308  0.60271526  1.14432461
  1.31029914 -1.87354633]. 
Working on 0.8 fold... 
[Fri Apr  5 01:30:44 2024]  Iteration number: 0 with current cost as 0.2985577951821744 and parameters 
[ 1.33710469  2.23743479 -2.12427948 -0.11653087  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.06648324  0.60271526  1.14432461
  1.31029899 -1.87354664]. 
Working on 1.0 fold... 
[Fri Apr  5 01:33:18 2024]  Iteration number: 0 with current cost as 0.30329993839961483 and parameters 
[ 1.358061    2.23743494 -2.12427933 -0.11653072  0.55388723 -2.77010897
  3.06858498  2.18960175  1.18552029 -1.06648308  0.60271541  1.14432475
  1.31029929 -1.8735462 ]. 
Training complete taking 864.7101957798004 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 01:35:50 2024]  Iteration number: 0 with current cost as 0.6752914652729238 and parameters 
[-0.07783389  2.23743473 -2.12427954 -0.11653093  0.55388708 -2.77010897
  3.06858489  2.18960155  1.18552008 -1.06648299  0.6027152   1.14432455
  1.31029908 -1.87354661]. 
Working on 0.4 fold... 
[Fri Apr  5 01:40:10 2024]  Iteration number: 0 with current cost as 0.3056405850133766 and parameters 
[ 1.45292767  2.23743464 -2.12427947 -0.11653103  0.55388708 -2.77010914
  3.06858498  2.18960145  1.18552015 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354647]. 
Working on 0.6 fold... 
[Fri Apr  5 01:42:30 2024]  Iteration number: 0 with current cost as 0.31224367415399956 and parameters 
[ 1.38271498  2.23743479 -2.12427932 -0.11653087  0.55388708 -2.77010882
  3.06858483  2.18960161  1.18552014 -1.06648308  0.60271526  1.14432461
  1.31029914 -1.87354633]. 
Working on 0.8 fold... 
[Fri Apr  5 01:45:02 2024]  Iteration number: 0 with current cost as 0.2985577951821744 and parameters 
[ 1.33710469  2.23743479 -2.12427948 -0.11653087  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.06648324  0.60271526  1.14432461
  1.31029899 -1.87354664]. 
Working on 1.0 fold... 
[Fri Apr  5 01:47:37 2024]  Iteration number: 0 with current cost as 0.30329993839961483 and parameters 
[ 1.358061    2.23743494 -2.12427933 -0.11653072  0.55388723 -2.77010897
  3.06858498  2.18960175  1.18552029 -1.06648308  0.60271541  1.14432475
  1.31029929 -1.8735462 ]. 
Training complete taking 862.3163940906525 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 01:50:16 2024]  Iteration number: 0 with current cost as 0.6752914652729238 and parameters 
[-0.07783389  2.23743473 -2.12427954 -0.11653093  0.55388708 -2.77010897
  3.06858489  2.18960155  1.18552008 -1.06648299  0.6027152   1.14432455
  1.31029908 -1.87354661]. 
Working on 0.4 fold... 
[Fri Apr  5 01:54:35 2024]  Iteration number: 0 with current cost as 0.3056405850133766 and parameters 
[ 1.45292767  2.23743464 -2.12427947 -0.11653103  0.55388708 -2.77010914
  3.06858498  2.18960145  1.18552015 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354647]. 
Working on 0.6 fold... 
[Fri Apr  5 01:56:53 2024]  Iteration number: 0 with current cost as 0.31224367415399956 and parameters 
[ 1.38271498  2.23743479 -2.12427932 -0.11653087  0.55388708 -2.77010882
  3.06858483  2.18960161  1.18552014 -1.06648308  0.60271526  1.14432461
  1.31029914 -1.87354633]. 
Working on 0.8 fold... 
[Fri Apr  5 01:59:32 2024]  Iteration number: 0 with current cost as 0.2985577951821744 and parameters 
[ 1.33710469  2.23743479 -2.12427948 -0.11653087  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.06648324  0.60271526  1.14432461
  1.31029899 -1.87354664]. 
Working on 1.0 fold... 
[Fri Apr  5 02:02:25 2024]  Iteration number: 0 with current cost as 0.30329993839961483 and parameters 
[ 1.358061    2.23743494 -2.12427933 -0.11653072  0.55388723 -2.77010897
  3.06858498  2.18960175  1.18552029 -1.06648308  0.60271541  1.14432475
  1.31029929 -1.8735462 ]. 
Training complete taking 892.904776096344 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 02:05:11 2024]  Iteration number: 0 with current cost as 0.6752914652729238 and parameters 
[-0.07783389  2.23743473 -2.12427954 -0.11653093  0.55388708 -2.77010897
  3.06858489  2.18960155  1.18552008 -1.06648299  0.6027152   1.14432455
  1.31029908 -1.87354661]. 
Working on 0.4 fold... 
[Fri Apr  5 02:09:48 2024]  Iteration number: 0 with current cost as 0.3056405850133766 and parameters 
[ 1.45292767  2.23743464 -2.12427947 -0.11653103  0.55388708 -2.77010914
  3.06858498  2.18960145  1.18552015 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354647]. 
Working on 0.6 fold... 
[Fri Apr  5 02:12:13 2024]  Iteration number: 0 with current cost as 0.31224367415399956 and parameters 
[ 1.38271498  2.23743479 -2.12427932 -0.11653087  0.55388708 -2.77010882
  3.06858483  2.18960161  1.18552014 -1.06648308  0.60271526  1.14432461
  1.31029914 -1.87354633]. 
Working on 0.8 fold... 
[Fri Apr  5 02:14:57 2024]  Iteration number: 0 with current cost as 0.2985577951821744 and parameters 
[ 1.33710469  2.23743479 -2.12427948 -0.11653087  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.06648324  0.60271526  1.14432461
  1.31029899 -1.87354664]. 
Working on 1.0 fold... 
[Fri Apr  5 02:17:37 2024]  Iteration number: 0 with current cost as 0.30329993839961483 and parameters 
[ 1.358061    2.23743494 -2.12427933 -0.11653072  0.55388723 -2.77010897
  3.06858498  2.18960175  1.18552029 -1.06648308  0.60271541  1.14432475
  1.31029929 -1.8735462 ]. 
Training complete taking 906.6314399242401 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 02:20:15 2024]  Iteration number: 0 with current cost as 0.6752914652729238 and parameters 
[-0.07783389  2.23743473 -2.12427954 -0.11653093  0.55388708 -2.77010897
  3.06858489  2.18960155  1.18552008 -1.06648299  0.6027152   1.14432455
  1.31029908 -1.87354661]. 
Working on 0.4 fold... 
[Fri Apr  5 02:24:48 2024]  Iteration number: 0 with current cost as 0.3056405850133766 and parameters 
[ 1.45292767  2.23743464 -2.12427947 -0.11653103  0.55388708 -2.77010914
  3.06858498  2.18960145  1.18552015 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354647]. 
Working on 0.6 fold... 
[Fri Apr  5 02:27:12 2024]  Iteration number: 0 with current cost as 0.31224367415399956 and parameters 
[ 1.38271498  2.23743479 -2.12427932 -0.11653087  0.55388708 -2.77010882
  3.06858483  2.18960161  1.18552014 -1.06648308  0.60271526  1.14432461
  1.31029914 -1.87354633]. 
Working on 0.8 fold... 
[Fri Apr  5 02:29:55 2024]  Iteration number: 0 with current cost as 0.2985577951821744 and parameters 
[ 1.33710469  2.23743479 -2.12427948 -0.11653087  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.06648324  0.60271526  1.14432461
  1.31029899 -1.87354664]. 
Working on 1.0 fold... 
[Fri Apr  5 02:32:38 2024]  Iteration number: 0 with current cost as 0.30329993839961483 and parameters 
[ 1.358061    2.23743494 -2.12427933 -0.11653072  0.55388723 -2.77010897
  3.06858498  2.18960175  1.18552029 -1.06648308  0.60271541  1.14432475
  1.31029929 -1.8735462 ]. 
Training complete taking 904.5340888500214 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 02:35:21 2024]  Iteration number: 0 with current cost as 0.6752914652729238 and parameters 
[-0.07783389  2.23743473 -2.12427954 -0.11653093  0.55388708 -2.77010897
  3.06858489  2.18960155  1.18552008 -1.06648299  0.6027152   1.14432455
  1.31029908 -1.87354661]. 
Working on 0.4 fold... 
[Fri Apr  5 02:39:48 2024]  Iteration number: 0 with current cost as 0.3056405850133766 and parameters 
[ 1.45292767  2.23743464 -2.12427947 -0.11653103  0.55388708 -2.77010914
  3.06858498  2.18960145  1.18552015 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354647]. 
Working on 0.6 fold... 
[Fri Apr  5 02:42:16 2024]  Iteration number: 0 with current cost as 0.31224367415399956 and parameters 
[ 1.38271498  2.23743479 -2.12427932 -0.11653087  0.55388708 -2.77010882
  3.06858483  2.18960161  1.18552014 -1.06648308  0.60271526  1.14432461
  1.31029914 -1.87354633]. 
Working on 0.8 fold... 
[Fri Apr  5 02:44:55 2024]  Iteration number: 0 with current cost as 0.2985577951821744 and parameters 
[ 1.33710469  2.23743479 -2.12427948 -0.11653087  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.06648324  0.60271526  1.14432461
  1.31029899 -1.87354664]. 
Working on 1.0 fold... 
[Fri Apr  5 02:47:40 2024]  Iteration number: 0 with current cost as 0.30329993839961483 and parameters 
[ 1.358061    2.23743494 -2.12427933 -0.11653072  0.55388723 -2.77010897
  3.06858498  2.18960175  1.18552029 -1.06648308  0.60271541  1.14432475
  1.31029929 -1.8735462 ]. 
Training complete taking 903.1296734809875 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 02:50:26 2024]  Iteration number: 0 with current cost as 0.6752914652729238 and parameters 
[-0.07783389  2.23743473 -2.12427954 -0.11653093  0.55388708 -2.77010897
  3.06858489  2.18960155  1.18552008 -1.06648299  0.6027152   1.14432455
  1.31029908 -1.87354661]. 
Working on 0.4 fold... 
[Fri Apr  5 02:55:02 2024]  Iteration number: 0 with current cost as 0.3056405850133766 and parameters 
[ 1.45292767  2.23743464 -2.12427947 -0.11653103  0.55388708 -2.77010914
  3.06858498  2.18960145  1.18552015 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354647]. 
Working on 0.6 fold... 
[Fri Apr  5 02:57:29 2024]  Iteration number: 0 with current cost as 0.31224367415399956 and parameters 
[ 1.38271498  2.23743479 -2.12427932 -0.11653087  0.55388708 -2.77010882
  3.06858483  2.18960161  1.18552014 -1.06648308  0.60271526  1.14432461
  1.31029914 -1.87354633]. 
Working on 0.8 fold... 
[Fri Apr  5 03:00:08 2024]  Iteration number: 0 with current cost as 0.2985577951821744 and parameters 
[ 1.33710469  2.23743479 -2.12427948 -0.11653087  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.06648324  0.60271526  1.14432461
  1.31029899 -1.87354664]. 
Working on 1.0 fold... 
[Fri Apr  5 03:02:59 2024]  Iteration number: 0 with current cost as 0.30329993839961483 and parameters 
[ 1.358061    2.23743494 -2.12427933 -0.11653072  0.55388723 -2.77010897
  3.06858498  2.18960175  1.18552029 -1.06648308  0.60271541  1.14432475
  1.31029929 -1.8735462 ]. 
Training complete taking 919.0728194713593 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 03:05:44 2024]  Iteration number: 0 with current cost as 0.6752914652729238 and parameters 
[-0.07783389  2.23743473 -2.12427954 -0.11653093  0.55388708 -2.77010897
  3.06858489  2.18960155  1.18552008 -1.06648299  0.6027152   1.14432455
  1.31029908 -1.87354661]. 
Working on 0.4 fold... 
[Fri Apr  5 03:10:20 2024]  Iteration number: 0 with current cost as 0.3056405850133766 and parameters 
[ 1.45292767  2.23743464 -2.12427947 -0.11653103  0.55388708 -2.77010914
  3.06858498  2.18960145  1.18552015 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354647]. 
Working on 0.6 fold... 
[Fri Apr  5 03:12:45 2024]  Iteration number: 0 with current cost as 0.31224367415399956 and parameters 
[ 1.38271498  2.23743479 -2.12427932 -0.11653087  0.55388708 -2.77010882
  3.06858483  2.18960161  1.18552014 -1.06648308  0.60271526  1.14432461
  1.31029914 -1.87354633]. 
Working on 0.8 fold... 
[Fri Apr  5 03:15:33 2024]  Iteration number: 0 with current cost as 0.2985577951821744 and parameters 
[ 1.33710469  2.23743479 -2.12427948 -0.11653087  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.06648324  0.60271526  1.14432461
  1.31029899 -1.87354664]. 
Working on 1.0 fold... 
[Fri Apr  5 03:18:12 2024]  Iteration number: 0 with current cost as 0.30329993839961483 and parameters 
[ 1.358061    2.23743494 -2.12427933 -0.11653072  0.55388723 -2.77010897
  3.06858498  2.18960175  1.18552029 -1.06648308  0.60271541  1.14432475
  1.31029929 -1.8735462 ]. 
Training complete taking 911.1084446907043 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 03:20:57 2024]  Iteration number: 0 with current cost as 0.6752914652729238 and parameters 
[-0.07783389  2.23743473 -2.12427954 -0.11653093  0.55388708 -2.77010897
  3.06858489  2.18960155  1.18552008 -1.06648299  0.6027152   1.14432455
  1.31029908 -1.87354661]. 
Working on 0.4 fold... 
[Fri Apr  5 03:25:30 2024]  Iteration number: 0 with current cost as 0.3056405850133766 and parameters 
[ 1.45292767  2.23743464 -2.12427947 -0.11653103  0.55388708 -2.77010914
  3.06858498  2.18960145  1.18552015 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354647]. 
Working on 0.6 fold... 
[Fri Apr  5 03:27:56 2024]  Iteration number: 0 with current cost as 0.31224367415399956 and parameters 
[ 1.38271498  2.23743479 -2.12427932 -0.11653087  0.55388708 -2.77010882
  3.06858483  2.18960161  1.18552014 -1.06648308  0.60271526  1.14432461
  1.31029914 -1.87354633]. 
Working on 0.8 fold... 
[Fri Apr  5 03:30:37 2024]  Iteration number: 0 with current cost as 0.2985577951821744 and parameters 
[ 1.33710469  2.23743479 -2.12427948 -0.11653087  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.06648324  0.60271526  1.14432461
  1.31029899 -1.87354664]. 
Working on 1.0 fold... 
[Fri Apr  5 03:33:26 2024]  Iteration number: 0 with current cost as 0.30329993839961483 and parameters 
[ 1.358061    2.23743494 -2.12427933 -0.11653072  0.55388723 -2.77010897
  3.06858498  2.18960175  1.18552029 -1.06648308  0.60271541  1.14432475
  1.31029929 -1.8735462 ]. 
Training complete taking 915.9448873996735 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 03:36:13 2024]  Iteration number: 0 with current cost as 0.6752914652729238 and parameters 
[-0.07783389  2.23743473 -2.12427954 -0.11653093  0.55388708 -2.77010897
  3.06858489  2.18960155  1.18552008 -1.06648299  0.6027152   1.14432455
  1.31029908 -1.87354661]. 
Working on 0.4 fold... 
[Fri Apr  5 03:40:51 2024]  Iteration number: 0 with current cost as 0.3056405850133766 and parameters 
[ 1.45292767  2.23743464 -2.12427947 -0.11653103  0.55388708 -2.77010914
  3.06858498  2.18960145  1.18552015 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354647]. 
Working on 0.6 fold... 
[Fri Apr  5 03:43:22 2024]  Iteration number: 0 with current cost as 0.31224367415399956 and parameters 
[ 1.38271498  2.23743479 -2.12427932 -0.11653087  0.55388708 -2.77010882
  3.06858483  2.18960161  1.18552014 -1.06648308  0.60271526  1.14432461
  1.31029914 -1.87354633]. 
Working on 0.8 fold... 
[Fri Apr  5 03:46:10 2024]  Iteration number: 0 with current cost as 0.2985577951821744 and parameters 
[ 1.33710469  2.23743479 -2.12427948 -0.11653087  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.06648324  0.60271526  1.14432461
  1.31029899 -1.87354664]. 
Working on 1.0 fold... 
[Fri Apr  5 03:48:57 2024]  Iteration number: 0 with current cost as 0.30329993839961483 and parameters 
[ 1.358061    2.23743494 -2.12427933 -0.11653072  0.55388723 -2.77010897
  3.06858498  2.18960175  1.18552029 -1.06648308  0.60271541  1.14432475
  1.31029929 -1.8735462 ]. 
Training complete taking 930.4359955787659 seconds. 
Discarding model... 

Training complete taking 22374.121017932892 total seconds. 
Now scoring model... 
Scoring complete taking 2.0990188121795654 seconds. 
Saved predicted values as A1-A1-CNOT_Efficient-CRX_predicted_values.csv
Model scores: {'MSE_train': (0.500043217193754,), 'R2_train': 0.004296235482269317, 'MAE_train': 0.6162022730768323, 'MSE_test': 0.5406052053646524, 'R2_test': -0.008209759996645971, 'MAE_test': 0.6648572308249922}. 
Saved model results as A1-A1-CNOT_Efficient-CRX_results.json. 
