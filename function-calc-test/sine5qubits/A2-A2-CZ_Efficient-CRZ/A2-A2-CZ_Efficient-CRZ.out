/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /scratch/j/jacobsen/gjones/sine5qubits/sine_train.bin... 
Successfully loaded /scratch/j/jacobsen/gjones/sine5qubits/sine_train.bin into X and y data. 
Loading dataset from /scratch/j/jacobsen/gjones/sine5qubits/sine_test.bin... 
Successfully loaded /scratch/j/jacobsen/gjones/sine5qubits/sine_test.bin into X and y data. 
Training model with dataset /scratch/j/jacobsen/gjones/sine5qubits/sine_train.bin 
 at time Thu Apr  4 21:45:43 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 21:47:47 2024]  Iteration number: 0 with current cost as 0.3085586393992329 and parameters 
[-4.44578289  2.23743438 -2.12427913 -0.11653078  0.55388708 -2.77010897
  3.06858498  2.1896017   1.18551998 -1.06648334  0.60271535  1.1443247
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Apr  4 21:50:32 2024]  Iteration number: 0 with current cost as 0.320551867467593 and parameters 
[-4.32247438  2.23743481 -2.12427946 -0.11653103  0.55388708 -2.77010915
  3.06858481  2.18960145  1.18552016 -1.06648326  0.60271528  1.14432462
  1.31029881 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 21:53:58 2024]  Iteration number: 0 with current cost as 0.3425005497145417 and parameters 
[-4.33926415  2.23743491 -2.12427881 -0.11653048  0.55388708 -2.77010897
  3.06858526  2.189602    1.18552026 -1.06648308  0.60271565  1.144325
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 21:57:07 2024]  Iteration number: 0 with current cost as 0.27645594478593116 and parameters 
[-4.45008133  2.23743475 -2.12427941 -0.1165308   0.55388708 -2.77010897
  3.06858498  2.18960156  1.18551998 -1.0664832   0.60271533  1.14432456
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 22:00:14 2024]  Iteration number: 0 with current cost as 0.34884890222912845 and parameters 
[-7.58077526  2.23743464 -2.12427839 -0.1165304   0.55388708 -2.77010959
  3.06858498  2.18960145  1.18551998 -1.06648433  0.60271572  1.14432445
  1.31029836 -1.8735468 ]. 
Training complete taking 928.9677793979645 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 22:03:24 2024]  Iteration number: 0 with current cost as 0.3085586393992329 and parameters 
[-4.44578289  2.23743438 -2.12427913 -0.11653078  0.55388708 -2.77010897
  3.06858498  2.1896017   1.18551998 -1.06648334  0.60271535  1.1443247
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Apr  4 22:06:12 2024]  Iteration number: 0 with current cost as 0.320551867467593 and parameters 
[-4.32247438  2.23743481 -2.12427946 -0.11653103  0.55388708 -2.77010915
  3.06858481  2.18960145  1.18552016 -1.06648326  0.60271528  1.14432462
  1.31029881 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 22:09:39 2024]  Iteration number: 0 with current cost as 0.3425005497145417 and parameters 
[-4.33926415  2.23743491 -2.12427881 -0.11653048  0.55388708 -2.77010897
  3.06858526  2.189602    1.18552026 -1.06648308  0.60271565  1.144325
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 22:12:49 2024]  Iteration number: 0 with current cost as 0.27645594478593116 and parameters 
[-4.45008133  2.23743475 -2.12427941 -0.1165308   0.55388708 -2.77010897
  3.06858498  2.18960156  1.18551998 -1.0664832   0.60271533  1.14432456
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 22:15:56 2024]  Iteration number: 0 with current cost as 0.34884890222912845 and parameters 
[-7.58077526  2.23743464 -2.12427839 -0.1165304   0.55388708 -2.77010959
  3.06858498  2.18960145  1.18551998 -1.06648433  0.60271572  1.14432445
  1.31029836 -1.8735468 ]. 
Training complete taking 938.4005975723267 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 22:19:04 2024]  Iteration number: 0 with current cost as 0.3085586393992329 and parameters 
[-4.44578289  2.23743438 -2.12427913 -0.11653078  0.55388708 -2.77010897
  3.06858498  2.1896017   1.18551998 -1.06648334  0.60271535  1.1443247
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Apr  4 22:22:02 2024]  Iteration number: 0 with current cost as 0.320551867467593 and parameters 
[-4.32247438  2.23743481 -2.12427946 -0.11653103  0.55388708 -2.77010915
  3.06858481  2.18960145  1.18552016 -1.06648326  0.60271528  1.14432462
  1.31029881 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 22:25:25 2024]  Iteration number: 0 with current cost as 0.3425005497145417 and parameters 
[-4.33926415  2.23743491 -2.12427881 -0.11653048  0.55388708 -2.77010897
  3.06858526  2.189602    1.18552026 -1.06648308  0.60271565  1.144325
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 22:28:43 2024]  Iteration number: 0 with current cost as 0.27645594478593116 and parameters 
[-4.45008133  2.23743475 -2.12427941 -0.1165308   0.55388708 -2.77010897
  3.06858498  2.18960156  1.18551998 -1.0664832   0.60271533  1.14432456
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 22:31:52 2024]  Iteration number: 0 with current cost as 0.34884890222912845 and parameters 
[-7.58077526  2.23743464 -2.12427839 -0.1165304   0.55388708 -2.77010959
  3.06858498  2.18960145  1.18551998 -1.06648433  0.60271572  1.14432445
  1.31029836 -1.8735468 ]. 
Training complete taking 956.458315372467 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 22:34:58 2024]  Iteration number: 0 with current cost as 0.3085586393992329 and parameters 
[-4.44578289  2.23743438 -2.12427913 -0.11653078  0.55388708 -2.77010897
  3.06858498  2.1896017   1.18551998 -1.06648334  0.60271535  1.1443247
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Apr  4 22:37:47 2024]  Iteration number: 0 with current cost as 0.320551867467593 and parameters 
[-4.32247438  2.23743481 -2.12427946 -0.11653103  0.55388708 -2.77010915
  3.06858481  2.18960145  1.18552016 -1.06648326  0.60271528  1.14432462
  1.31029881 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 22:41:24 2024]  Iteration number: 0 with current cost as 0.3425005497145417 and parameters 
[-4.33926415  2.23743491 -2.12427881 -0.11653048  0.55388708 -2.77010897
  3.06858526  2.189602    1.18552026 -1.06648308  0.60271565  1.144325
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 22:44:48 2024]  Iteration number: 0 with current cost as 0.27645594478593116 and parameters 
[-4.45008133  2.23743475 -2.12427941 -0.1165308   0.55388708 -2.77010897
  3.06858498  2.18960156  1.18551998 -1.0664832   0.60271533  1.14432456
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 22:47:58 2024]  Iteration number: 0 with current cost as 0.34884890222912845 and parameters 
[-7.58077526  2.23743464 -2.12427839 -0.1165304   0.55388708 -2.77010959
  3.06858498  2.18960145  1.18551998 -1.06648433  0.60271572  1.14432445
  1.31029836 -1.8735468 ]. 
Training complete taking 968.6979570388794 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 22:51:06 2024]  Iteration number: 0 with current cost as 0.3085586393992329 and parameters 
[-4.44578289  2.23743438 -2.12427913 -0.11653078  0.55388708 -2.77010897
  3.06858498  2.1896017   1.18551998 -1.06648334  0.60271535  1.1443247
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Apr  4 22:53:50 2024]  Iteration number: 0 with current cost as 0.320551867467593 and parameters 
[-4.32247438  2.23743481 -2.12427946 -0.11653103  0.55388708 -2.77010915
  3.06858481  2.18960145  1.18552016 -1.06648326  0.60271528  1.14432462
  1.31029881 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 22:57:14 2024]  Iteration number: 0 with current cost as 0.3425005497145417 and parameters 
[-4.33926415  2.23743491 -2.12427881 -0.11653048  0.55388708 -2.77010897
  3.06858526  2.189602    1.18552026 -1.06648308  0.60271565  1.144325
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 23:00:29 2024]  Iteration number: 0 with current cost as 0.27645594478593116 and parameters 
[-4.45008133  2.23743475 -2.12427941 -0.1165308   0.55388708 -2.77010897
  3.06858498  2.18960156  1.18551998 -1.0664832   0.60271533  1.14432456
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 23:03:38 2024]  Iteration number: 0 with current cost as 0.34884890222912845 and parameters 
[-7.58077526  2.23743464 -2.12427839 -0.1165304   0.55388708 -2.77010959
  3.06858498  2.18960145  1.18551998 -1.06648433  0.60271572  1.14432445
  1.31029836 -1.8735468 ]. 
Training complete taking 937.2228441238403 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 23:06:42 2024]  Iteration number: 0 with current cost as 0.3085586393992329 and parameters 
[-4.44578289  2.23743438 -2.12427913 -0.11653078  0.55388708 -2.77010897
  3.06858498  2.1896017   1.18551998 -1.06648334  0.60271535  1.1443247
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Apr  4 23:09:26 2024]  Iteration number: 0 with current cost as 0.320551867467593 and parameters 
[-4.32247438  2.23743481 -2.12427946 -0.11653103  0.55388708 -2.77010915
  3.06858481  2.18960145  1.18552016 -1.06648326  0.60271528  1.14432462
  1.31029881 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 23:12:51 2024]  Iteration number: 0 with current cost as 0.3425005497145417 and parameters 
[-4.33926415  2.23743491 -2.12427881 -0.11653048  0.55388708 -2.77010897
  3.06858526  2.189602    1.18552026 -1.06648308  0.60271565  1.144325
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 23:16:10 2024]  Iteration number: 0 with current cost as 0.27645594478593116 and parameters 
[-4.45008133  2.23743475 -2.12427941 -0.1165308   0.55388708 -2.77010897
  3.06858498  2.18960156  1.18551998 -1.0664832   0.60271533  1.14432456
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 23:19:30 2024]  Iteration number: 0 with current cost as 0.34884890222912845 and parameters 
[-7.58077526  2.23743464 -2.12427839 -0.1165304   0.55388708 -2.77010959
  3.06858498  2.18960145  1.18551998 -1.06648433  0.60271572  1.14432445
  1.31029836 -1.8735468 ]. 
Training complete taking 952.1937725543976 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 23:22:32 2024]  Iteration number: 0 with current cost as 0.3085586393992329 and parameters 
[-4.44578289  2.23743438 -2.12427913 -0.11653078  0.55388708 -2.77010897
  3.06858498  2.1896017   1.18551998 -1.06648334  0.60271535  1.1443247
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Apr  4 23:25:26 2024]  Iteration number: 0 with current cost as 0.320551867467593 and parameters 
[-4.32247438  2.23743481 -2.12427946 -0.11653103  0.55388708 -2.77010915
  3.06858481  2.18960145  1.18552016 -1.06648326  0.60271528  1.14432462
  1.31029881 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 23:28:56 2024]  Iteration number: 0 with current cost as 0.3425005497145417 and parameters 
[-4.33926415  2.23743491 -2.12427881 -0.11653048  0.55388708 -2.77010897
  3.06858526  2.189602    1.18552026 -1.06648308  0.60271565  1.144325
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 23:32:03 2024]  Iteration number: 0 with current cost as 0.27645594478593116 and parameters 
[-4.45008133  2.23743475 -2.12427941 -0.1165308   0.55388708 -2.77010897
  3.06858498  2.18960156  1.18551998 -1.0664832   0.60271533  1.14432456
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 23:35:26 2024]  Iteration number: 0 with current cost as 0.34884890222912845 and parameters 
[-7.58077526  2.23743464 -2.12427839 -0.1165304   0.55388708 -2.77010959
  3.06858498  2.18960145  1.18551998 -1.06648433  0.60271572  1.14432445
  1.31029836 -1.8735468 ]. 
Training complete taking 958.8720037937164 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 23:38:35 2024]  Iteration number: 0 with current cost as 0.3085586393992329 and parameters 
[-4.44578289  2.23743438 -2.12427913 -0.11653078  0.55388708 -2.77010897
  3.06858498  2.1896017   1.18551998 -1.06648334  0.60271535  1.1443247
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Apr  4 23:41:30 2024]  Iteration number: 0 with current cost as 0.320551867467593 and parameters 
[-4.32247438  2.23743481 -2.12427946 -0.11653103  0.55388708 -2.77010915
  3.06858481  2.18960145  1.18552016 -1.06648326  0.60271528  1.14432462
  1.31029881 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 23:45:10 2024]  Iteration number: 0 with current cost as 0.3425005497145417 and parameters 
[-4.33926415  2.23743491 -2.12427881 -0.11653048  0.55388708 -2.77010897
  3.06858526  2.189602    1.18552026 -1.06648308  0.60271565  1.144325
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 23:48:27 2024]  Iteration number: 0 with current cost as 0.27645594478593116 and parameters 
[-4.45008133  2.23743475 -2.12427941 -0.1165308   0.55388708 -2.77010897
  3.06858498  2.18960156  1.18551998 -1.0664832   0.60271533  1.14432456
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 23:51:37 2024]  Iteration number: 0 with current cost as 0.34884890222912845 and parameters 
[-7.58077526  2.23743464 -2.12427839 -0.1165304   0.55388708 -2.77010959
  3.06858498  2.18960145  1.18551998 -1.06648433  0.60271572  1.14432445
  1.31029836 -1.8735468 ]. 
Training complete taking 972.1738617420197 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 23:54:52 2024]  Iteration number: 0 with current cost as 0.3085586393992329 and parameters 
[-4.44578289  2.23743438 -2.12427913 -0.11653078  0.55388708 -2.77010897
  3.06858498  2.1896017   1.18551998 -1.06648334  0.60271535  1.1443247
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Apr  4 23:57:44 2024]  Iteration number: 0 with current cost as 0.320551867467593 and parameters 
[-4.32247438  2.23743481 -2.12427946 -0.11653103  0.55388708 -2.77010915
  3.06858481  2.18960145  1.18552016 -1.06648326  0.60271528  1.14432462
  1.31029881 -1.8735468 ]. 
Working on 0.6 fold... 
[Fri Apr  5 00:01:15 2024]  Iteration number: 0 with current cost as 0.3425005497145417 and parameters 
[-4.33926415  2.23743491 -2.12427881 -0.11653048  0.55388708 -2.77010897
  3.06858526  2.189602    1.18552026 -1.06648308  0.60271565  1.144325
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Fri Apr  5 00:04:33 2024]  Iteration number: 0 with current cost as 0.27645594478593116 and parameters 
[-4.45008133  2.23743475 -2.12427941 -0.1165308   0.55388708 -2.77010897
  3.06858498  2.18960156  1.18551998 -1.0664832   0.60271533  1.14432456
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Fri Apr  5 00:07:43 2024]  Iteration number: 0 with current cost as 0.34884890222912845 and parameters 
[-7.58077526  2.23743464 -2.12427839 -0.1165304   0.55388708 -2.77010959
  3.06858498  2.18960145  1.18551998 -1.06648433  0.60271572  1.14432445
  1.31029836 -1.8735468 ]. 
Training complete taking 963.87242436409 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 00:10:50 2024]  Iteration number: 0 with current cost as 0.3085586393992329 and parameters 
[-4.44578289  2.23743438 -2.12427913 -0.11653078  0.55388708 -2.77010897
  3.06858498  2.1896017   1.18551998 -1.06648334  0.60271535  1.1443247
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Fri Apr  5 00:13:45 2024]  Iteration number: 0 with current cost as 0.320551867467593 and parameters 
[-4.32247438  2.23743481 -2.12427946 -0.11653103  0.55388708 -2.77010915
  3.06858481  2.18960145  1.18552016 -1.06648326  0.60271528  1.14432462
  1.31029881 -1.8735468 ]. 
Working on 0.6 fold... 
[Fri Apr  5 00:17:22 2024]  Iteration number: 0 with current cost as 0.3425005497145417 and parameters 
[-4.33926415  2.23743491 -2.12427881 -0.11653048  0.55388708 -2.77010897
  3.06858526  2.189602    1.18552026 -1.06648308  0.60271565  1.144325
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Fri Apr  5 00:20:30 2024]  Iteration number: 0 with current cost as 0.27645594478593116 and parameters 
[-4.45008133  2.23743475 -2.12427941 -0.1165308   0.55388708 -2.77010897
  3.06858498  2.18960156  1.18551998 -1.0664832   0.60271533  1.14432456
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Fri Apr  5 00:23:43 2024]  Iteration number: 0 with current cost as 0.34884890222912845 and parameters 
[-7.58077526  2.23743464 -2.12427839 -0.1165304   0.55388708 -2.77010959
  3.06858498  2.18960145  1.18551998 -1.06648433  0.60271572  1.14432445
  1.31029836 -1.8735468 ]. 
Training complete taking 957.9146473407745 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 00:26:53 2024]  Iteration number: 0 with current cost as 0.3085586393992329 and parameters 
[-4.44578289  2.23743438 -2.12427913 -0.11653078  0.55388708 -2.77010897
  3.06858498  2.1896017   1.18551998 -1.06648334  0.60271535  1.1443247
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Fri Apr  5 00:29:48 2024]  Iteration number: 0 with current cost as 0.320551867467593 and parameters 
[-4.32247438  2.23743481 -2.12427946 -0.11653103  0.55388708 -2.77010915
  3.06858481  2.18960145  1.18552016 -1.06648326  0.60271528  1.14432462
  1.31029881 -1.8735468 ]. 
Working on 0.6 fold... 
[Fri Apr  5 00:33:15 2024]  Iteration number: 0 with current cost as 0.3425005497145417 and parameters 
[-4.33926415  2.23743491 -2.12427881 -0.11653048  0.55388708 -2.77010897
  3.06858526  2.189602    1.18552026 -1.06648308  0.60271565  1.144325
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Fri Apr  5 00:36:16 2024]  Iteration number: 0 with current cost as 0.27645594478593116 and parameters 
[-4.45008133  2.23743475 -2.12427941 -0.1165308   0.55388708 -2.77010897
  3.06858498  2.18960156  1.18551998 -1.0664832   0.60271533  1.14432456
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Fri Apr  5 00:39:20 2024]  Iteration number: 0 with current cost as 0.34884890222912845 and parameters 
[-7.58077526  2.23743464 -2.12427839 -0.1165304   0.55388708 -2.77010959
  3.06858498  2.18960145  1.18551998 -1.06648433  0.60271572  1.14432445
  1.31029836 -1.8735468 ]. 
Training complete taking 937.5001931190491 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 00:42:25 2024]  Iteration number: 0 with current cost as 0.3085586393992329 and parameters 
[-4.44578289  2.23743438 -2.12427913 -0.11653078  0.55388708 -2.77010897
  3.06858498  2.1896017   1.18551998 -1.06648334  0.60271535  1.1443247
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Fri Apr  5 00:45:17 2024]  Iteration number: 0 with current cost as 0.320551867467593 and parameters 
[-4.32247438  2.23743481 -2.12427946 -0.11653103  0.55388708 -2.77010915
  3.06858481  2.18960145  1.18552016 -1.06648326  0.60271528  1.14432462
  1.31029881 -1.8735468 ]. 
Working on 0.6 fold... 
[Fri Apr  5 00:48:39 2024]  Iteration number: 0 with current cost as 0.3425005497145417 and parameters 
[-4.33926415  2.23743491 -2.12427881 -0.11653048  0.55388708 -2.77010897
  3.06858526  2.189602    1.18552026 -1.06648308  0.60271565  1.144325
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Fri Apr  5 00:52:00 2024]  Iteration number: 0 with current cost as 0.27645594478593116 and parameters 
[-4.45008133  2.23743475 -2.12427941 -0.1165308   0.55388708 -2.77010897
  3.06858498  2.18960156  1.18551998 -1.0664832   0.60271533  1.14432456
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Fri Apr  5 00:55:15 2024]  Iteration number: 0 with current cost as 0.34884890222912845 and parameters 
[-7.58077526  2.23743464 -2.12427839 -0.1165304   0.55388708 -2.77010959
  3.06858498  2.18960145  1.18551998 -1.06648433  0.60271572  1.14432445
  1.31029836 -1.8735468 ]. 
Training complete taking 961.9966287612915 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 00:58:35 2024]  Iteration number: 0 with current cost as 0.3085586393992329 and parameters 
[-4.44578289  2.23743438 -2.12427913 -0.11653078  0.55388708 -2.77010897
  3.06858498  2.1896017   1.18551998 -1.06648334  0.60271535  1.1443247
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Fri Apr  5 01:01:34 2024]  Iteration number: 0 with current cost as 0.320551867467593 and parameters 
[-4.32247438  2.23743481 -2.12427946 -0.11653103  0.55388708 -2.77010915
  3.06858481  2.18960145  1.18552016 -1.06648326  0.60271528  1.14432462
  1.31029881 -1.8735468 ]. 
Working on 0.6 fold... 
[Fri Apr  5 01:05:14 2024]  Iteration number: 0 with current cost as 0.3425005497145417 and parameters 
[-4.33926415  2.23743491 -2.12427881 -0.11653048  0.55388708 -2.77010897
  3.06858526  2.189602    1.18552026 -1.06648308  0.60271565  1.144325
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Fri Apr  5 01:08:37 2024]  Iteration number: 0 with current cost as 0.27645594478593116 and parameters 
[-4.45008133  2.23743475 -2.12427941 -0.1165308   0.55388708 -2.77010897
  3.06858498  2.18960156  1.18551998 -1.0664832   0.60271533  1.14432456
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Fri Apr  5 01:11:57 2024]  Iteration number: 0 with current cost as 0.34884890222912845 and parameters 
[-7.58077526  2.23743464 -2.12427839 -0.1165304   0.55388708 -2.77010959
  3.06858498  2.18960145  1.18551998 -1.06648433  0.60271572  1.14432445
  1.31029836 -1.8735468 ]. 
Training complete taking 1006.2645769119263 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 01:15:22 2024]  Iteration number: 0 with current cost as 0.3085586393992329 and parameters 
[-4.44578289  2.23743438 -2.12427913 -0.11653078  0.55388708 -2.77010897
  3.06858498  2.1896017   1.18551998 -1.06648334  0.60271535  1.1443247
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Fri Apr  5 01:18:15 2024]  Iteration number: 0 with current cost as 0.320551867467593 and parameters 
[-4.32247438  2.23743481 -2.12427946 -0.11653103  0.55388708 -2.77010915
  3.06858481  2.18960145  1.18552016 -1.06648326  0.60271528  1.14432462
  1.31029881 -1.8735468 ]. 
Working on 0.6 fold... 
[Fri Apr  5 01:21:47 2024]  Iteration number: 0 with current cost as 0.3425005497145417 and parameters 
[-4.33926415  2.23743491 -2.12427881 -0.11653048  0.55388708 -2.77010897
  3.06858526  2.189602    1.18552026 -1.06648308  0.60271565  1.144325
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Fri Apr  5 01:25:13 2024]  Iteration number: 0 with current cost as 0.27645594478593116 and parameters 
[-4.45008133  2.23743475 -2.12427941 -0.1165308   0.55388708 -2.77010897
  3.06858498  2.18960156  1.18551998 -1.0664832   0.60271533  1.14432456
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Fri Apr  5 01:28:18 2024]  Iteration number: 0 with current cost as 0.34884890222912845 and parameters 
[-7.58077526  2.23743464 -2.12427839 -0.1165304   0.55388708 -2.77010959
  3.06858498  2.18960145  1.18551998 -1.06648433  0.60271572  1.14432445
  1.31029836 -1.8735468 ]. 
Training complete taking 969.3457522392273 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 01:31:30 2024]  Iteration number: 0 with current cost as 0.3085586393992329 and parameters 
[-4.44578289  2.23743438 -2.12427913 -0.11653078  0.55388708 -2.77010897
  3.06858498  2.1896017   1.18551998 -1.06648334  0.60271535  1.1443247
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Fri Apr  5 01:34:24 2024]  Iteration number: 0 with current cost as 0.320551867467593 and parameters 
[-4.32247438  2.23743481 -2.12427946 -0.11653103  0.55388708 -2.77010915
  3.06858481  2.18960145  1.18552016 -1.06648326  0.60271528  1.14432462
  1.31029881 -1.8735468 ]. 
Working on 0.6 fold... 
[Fri Apr  5 01:37:52 2024]  Iteration number: 0 with current cost as 0.3425005497145417 and parameters 
[-4.33926415  2.23743491 -2.12427881 -0.11653048  0.55388708 -2.77010897
  3.06858526  2.189602    1.18552026 -1.06648308  0.60271565  1.144325
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Fri Apr  5 01:40:56 2024]  Iteration number: 0 with current cost as 0.27645594478593116 and parameters 
[-4.45008133  2.23743475 -2.12427941 -0.1165308   0.55388708 -2.77010897
  3.06858498  2.18960156  1.18551998 -1.0664832   0.60271533  1.14432456
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Fri Apr  5 01:44:05 2024]  Iteration number: 0 with current cost as 0.34884890222912845 and parameters 
[-7.58077526  2.23743464 -2.12427839 -0.1165304   0.55388708 -2.77010959
  3.06858498  2.18960145  1.18551998 -1.06648433  0.60271572  1.14432445
  1.31029836 -1.8735468 ]. 
Training complete taking 948.2418899536133 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 01:47:11 2024]  Iteration number: 0 with current cost as 0.3085586393992329 and parameters 
[-4.44578289  2.23743438 -2.12427913 -0.11653078  0.55388708 -2.77010897
  3.06858498  2.1896017   1.18551998 -1.06648334  0.60271535  1.1443247
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Fri Apr  5 01:49:57 2024]  Iteration number: 0 with current cost as 0.320551867467593 and parameters 
[-4.32247438  2.23743481 -2.12427946 -0.11653103  0.55388708 -2.77010915
  3.06858481  2.18960145  1.18552016 -1.06648326  0.60271528  1.14432462
  1.31029881 -1.8735468 ]. 
Working on 0.6 fold... 
[Fri Apr  5 01:53:24 2024]  Iteration number: 0 with current cost as 0.3425005497145417 and parameters 
[-4.33926415  2.23743491 -2.12427881 -0.11653048  0.55388708 -2.77010897
  3.06858526  2.189602    1.18552026 -1.06648308  0.60271565  1.144325
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Fri Apr  5 01:56:43 2024]  Iteration number: 0 with current cost as 0.27645594478593116 and parameters 
[-4.45008133  2.23743475 -2.12427941 -0.1165308   0.55388708 -2.77010897
  3.06858498  2.18960156  1.18551998 -1.0664832   0.60271533  1.14432456
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Fri Apr  5 01:59:48 2024]  Iteration number: 0 with current cost as 0.34884890222912845 and parameters 
[-7.58077526  2.23743464 -2.12427839 -0.1165304   0.55388708 -2.77010959
  3.06858498  2.18960145  1.18551998 -1.06648433  0.60271572  1.14432445
  1.31029836 -1.8735468 ]. 
Training complete taking 942.8858885765076 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 02:02:57 2024]  Iteration number: 0 with current cost as 0.3085586393992329 and parameters 
[-4.44578289  2.23743438 -2.12427913 -0.11653078  0.55388708 -2.77010897
  3.06858498  2.1896017   1.18551998 -1.06648334  0.60271535  1.1443247
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Fri Apr  5 02:05:52 2024]  Iteration number: 0 with current cost as 0.320551867467593 and parameters 
[-4.32247438  2.23743481 -2.12427946 -0.11653103  0.55388708 -2.77010915
  3.06858481  2.18960145  1.18552016 -1.06648326  0.60271528  1.14432462
  1.31029881 -1.8735468 ]. 
Working on 0.6 fold... 
[Fri Apr  5 02:09:15 2024]  Iteration number: 0 with current cost as 0.3425005497145417 and parameters 
[-4.33926415  2.23743491 -2.12427881 -0.11653048  0.55388708 -2.77010897
  3.06858526  2.189602    1.18552026 -1.06648308  0.60271565  1.144325
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Fri Apr  5 02:12:18 2024]  Iteration number: 0 with current cost as 0.27645594478593116 and parameters 
[-4.45008133  2.23743475 -2.12427941 -0.1165308   0.55388708 -2.77010897
  3.06858498  2.18960156  1.18551998 -1.0664832   0.60271533  1.14432456
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Fri Apr  5 02:15:32 2024]  Iteration number: 0 with current cost as 0.34884890222912845 and parameters 
[-7.58077526  2.23743464 -2.12427839 -0.1165304   0.55388708 -2.77010959
  3.06858498  2.18960145  1.18551998 -1.06648433  0.60271572  1.14432445
  1.31029836 -1.8735468 ]. 
Training complete taking 944.3726263046265 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 02:18:38 2024]  Iteration number: 0 with current cost as 0.3085586393992329 and parameters 
[-4.44578289  2.23743438 -2.12427913 -0.11653078  0.55388708 -2.77010897
  3.06858498  2.1896017   1.18551998 -1.06648334  0.60271535  1.1443247
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Fri Apr  5 02:21:26 2024]  Iteration number: 0 with current cost as 0.320551867467593 and parameters 
[-4.32247438  2.23743481 -2.12427946 -0.11653103  0.55388708 -2.77010915
  3.06858481  2.18960145  1.18552016 -1.06648326  0.60271528  1.14432462
  1.31029881 -1.8735468 ]. 
Working on 0.6 fold... 
[Fri Apr  5 02:24:52 2024]  Iteration number: 0 with current cost as 0.3425005497145417 and parameters 
[-4.33926415  2.23743491 -2.12427881 -0.11653048  0.55388708 -2.77010897
  3.06858526  2.189602    1.18552026 -1.06648308  0.60271565  1.144325
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Fri Apr  5 02:27:55 2024]  Iteration number: 0 with current cost as 0.27645594478593116 and parameters 
[-4.45008133  2.23743475 -2.12427941 -0.1165308   0.55388708 -2.77010897
  3.06858498  2.18960156  1.18551998 -1.0664832   0.60271533  1.14432456
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Fri Apr  5 02:30:57 2024]  Iteration number: 0 with current cost as 0.34884890222912845 and parameters 
[-7.58077526  2.23743464 -2.12427839 -0.1165304   0.55388708 -2.77010959
  3.06858498  2.18960145  1.18551998 -1.06648433  0.60271572  1.14432445
  1.31029836 -1.8735468 ]. 
Training complete taking 921.1445171833038 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 02:33:57 2024]  Iteration number: 0 with current cost as 0.3085586393992329 and parameters 
[-4.44578289  2.23743438 -2.12427913 -0.11653078  0.55388708 -2.77010897
  3.06858498  2.1896017   1.18551998 -1.06648334  0.60271535  1.1443247
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Fri Apr  5 02:36:38 2024]  Iteration number: 0 with current cost as 0.320551867467593 and parameters 
[-4.32247438  2.23743481 -2.12427946 -0.11653103  0.55388708 -2.77010915
  3.06858481  2.18960145  1.18552016 -1.06648326  0.60271528  1.14432462
  1.31029881 -1.8735468 ]. 
Working on 0.6 fold... 
[Fri Apr  5 02:40:00 2024]  Iteration number: 0 with current cost as 0.3425005497145417 and parameters 
[-4.33926415  2.23743491 -2.12427881 -0.11653048  0.55388708 -2.77010897
  3.06858526  2.189602    1.18552026 -1.06648308  0.60271565  1.144325
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Fri Apr  5 02:43:02 2024]  Iteration number: 0 with current cost as 0.27645594478593116 and parameters 
[-4.45008133  2.23743475 -2.12427941 -0.1165308   0.55388708 -2.77010897
  3.06858498  2.18960156  1.18551998 -1.0664832   0.60271533  1.14432456
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Fri Apr  5 02:46:00 2024]  Iteration number: 0 with current cost as 0.34884890222912845 and parameters 
[-7.58077526  2.23743464 -2.12427839 -0.1165304   0.55388708 -2.77010959
  3.06858498  2.18960145  1.18551998 -1.06648433  0.60271572  1.14432445
  1.31029836 -1.8735468 ]. 
Training complete taking 906.18528008461 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 02:49:22 2024]  Iteration number: 0 with current cost as 0.3085586393992329 and parameters 
[-4.44578289  2.23743438 -2.12427913 -0.11653078  0.55388708 -2.77010897
  3.06858498  2.1896017   1.18551998 -1.06648334  0.60271535  1.1443247
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Fri Apr  5 02:52:09 2024]  Iteration number: 0 with current cost as 0.320551867467593 and parameters 
[-4.32247438  2.23743481 -2.12427946 -0.11653103  0.55388708 -2.77010915
  3.06858481  2.18960145  1.18552016 -1.06648326  0.60271528  1.14432462
  1.31029881 -1.8735468 ]. 
Working on 0.6 fold... 
[Fri Apr  5 02:55:35 2024]  Iteration number: 0 with current cost as 0.3425005497145417 and parameters 
[-4.33926415  2.23743491 -2.12427881 -0.11653048  0.55388708 -2.77010897
  3.06858526  2.189602    1.18552026 -1.06648308  0.60271565  1.144325
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Fri Apr  5 02:58:42 2024]  Iteration number: 0 with current cost as 0.27645594478593116 and parameters 
[-4.45008133  2.23743475 -2.12427941 -0.1165308   0.55388708 -2.77010897
  3.06858498  2.18960156  1.18551998 -1.0664832   0.60271533  1.14432456
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Fri Apr  5 03:01:45 2024]  Iteration number: 0 with current cost as 0.34884890222912845 and parameters 
[-7.58077526  2.23743464 -2.12427839 -0.1165304   0.55388708 -2.77010959
  3.06858498  2.18960145  1.18551998 -1.06648433  0.60271572  1.14432445
  1.31029836 -1.8735468 ]. 
Training complete taking 945.0030388832092 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 03:04:49 2024]  Iteration number: 0 with current cost as 0.3085586393992329 and parameters 
[-4.44578289  2.23743438 -2.12427913 -0.11653078  0.55388708 -2.77010897
  3.06858498  2.1896017   1.18551998 -1.06648334  0.60271535  1.1443247
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Fri Apr  5 03:07:36 2024]  Iteration number: 0 with current cost as 0.320551867467593 and parameters 
[-4.32247438  2.23743481 -2.12427946 -0.11653103  0.55388708 -2.77010915
  3.06858481  2.18960145  1.18552016 -1.06648326  0.60271528  1.14432462
  1.31029881 -1.8735468 ]. 
Working on 0.6 fold... 
[Fri Apr  5 03:10:54 2024]  Iteration number: 0 with current cost as 0.3425005497145417 and parameters 
[-4.33926415  2.23743491 -2.12427881 -0.11653048  0.55388708 -2.77010897
  3.06858526  2.189602    1.18552026 -1.06648308  0.60271565  1.144325
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Fri Apr  5 03:14:01 2024]  Iteration number: 0 with current cost as 0.27645594478593116 and parameters 
[-4.45008133  2.23743475 -2.12427941 -0.1165308   0.55388708 -2.77010897
  3.06858498  2.18960156  1.18551998 -1.0664832   0.60271533  1.14432456
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Fri Apr  5 03:17:14 2024]  Iteration number: 0 with current cost as 0.34884890222912845 and parameters 
[-7.58077526  2.23743464 -2.12427839 -0.1165304   0.55388708 -2.77010959
  3.06858498  2.18960145  1.18551998 -1.06648433  0.60271572  1.14432445
  1.31029836 -1.8735468 ]. 
Training complete taking 926.9928665161133 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 03:20:19 2024]  Iteration number: 0 with current cost as 0.3085586393992329 and parameters 
[-4.44578289  2.23743438 -2.12427913 -0.11653078  0.55388708 -2.77010897
  3.06858498  2.1896017   1.18551998 -1.06648334  0.60271535  1.1443247
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Fri Apr  5 03:23:05 2024]  Iteration number: 0 with current cost as 0.320551867467593 and parameters 
[-4.32247438  2.23743481 -2.12427946 -0.11653103  0.55388708 -2.77010915
  3.06858481  2.18960145  1.18552016 -1.06648326  0.60271528  1.14432462
  1.31029881 -1.8735468 ]. 
Working on 0.6 fold... 
[Fri Apr  5 03:26:26 2024]  Iteration number: 0 with current cost as 0.3425005497145417 and parameters 
[-4.33926415  2.23743491 -2.12427881 -0.11653048  0.55388708 -2.77010897
  3.06858526  2.189602    1.18552026 -1.06648308  0.60271565  1.144325
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Fri Apr  5 03:29:35 2024]  Iteration number: 0 with current cost as 0.27645594478593116 and parameters 
[-4.45008133  2.23743475 -2.12427941 -0.1165308   0.55388708 -2.77010897
  3.06858498  2.18960156  1.18551998 -1.0664832   0.60271533  1.14432456
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Fri Apr  5 03:32:40 2024]  Iteration number: 0 with current cost as 0.34884890222912845 and parameters 
[-7.58077526  2.23743464 -2.12427839 -0.1165304   0.55388708 -2.77010959
  3.06858498  2.18960145  1.18551998 -1.06648433  0.60271572  1.14432445
  1.31029836 -1.8735468 ]. 
Training complete taking 927.2974424362183 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 03:35:46 2024]  Iteration number: 0 with current cost as 0.3085586393992329 and parameters 
[-4.44578289  2.23743438 -2.12427913 -0.11653078  0.55388708 -2.77010897
  3.06858498  2.1896017   1.18551998 -1.06648334  0.60271535  1.1443247
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Fri Apr  5 03:38:35 2024]  Iteration number: 0 with current cost as 0.320551867467593 and parameters 
[-4.32247438  2.23743481 -2.12427946 -0.11653103  0.55388708 -2.77010915
  3.06858481  2.18960145  1.18552016 -1.06648326  0.60271528  1.14432462
  1.31029881 -1.8735468 ]. 
Working on 0.6 fold... 
[Fri Apr  5 03:42:02 2024]  Iteration number: 0 with current cost as 0.3425005497145417 and parameters 
[-4.33926415  2.23743491 -2.12427881 -0.11653048  0.55388708 -2.77010897
  3.06858526  2.189602    1.18552026 -1.06648308  0.60271565  1.144325
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Fri Apr  5 03:45:12 2024]  Iteration number: 0 with current cost as 0.27645594478593116 and parameters 
[-4.45008133  2.23743475 -2.12427941 -0.1165308   0.55388708 -2.77010897
  3.06858498  2.18960156  1.18551998 -1.0664832   0.60271533  1.14432456
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Fri Apr  5 03:48:19 2024]  Iteration number: 0 with current cost as 0.34884890222912845 and parameters 
[-7.58077526  2.23743464 -2.12427839 -0.1165304   0.55388708 -2.77010959
  3.06858498  2.18960145  1.18551998 -1.06648433  0.60271572  1.14432445
  1.31029836 -1.8735468 ]. 
Training complete taking 936.5140981674194 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 03:51:15 2024]  Iteration number: 0 with current cost as 0.3085586393992329 and parameters 
[-4.44578289  2.23743438 -2.12427913 -0.11653078  0.55388708 -2.77010897
  3.06858498  2.1896017   1.18551998 -1.06648334  0.60271535  1.1443247
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Fri Apr  5 03:54:07 2024]  Iteration number: 0 with current cost as 0.320551867467593 and parameters 
[-4.32247438  2.23743481 -2.12427946 -0.11653103  0.55388708 -2.77010915
  3.06858481  2.18960145  1.18552016 -1.06648326  0.60271528  1.14432462
  1.31029881 -1.8735468 ]. 
Working on 0.6 fold... 
[Fri Apr  5 03:57:34 2024]  Iteration number: 0 with current cost as 0.3425005497145417 and parameters 
[-4.33926415  2.23743491 -2.12427881 -0.11653048  0.55388708 -2.77010897
  3.06858526  2.189602    1.18552026 -1.06648308  0.60271565  1.144325
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Fri Apr  5 04:00:42 2024]  Iteration number: 0 with current cost as 0.27645594478593116 and parameters 
[-4.45008133  2.23743475 -2.12427941 -0.1165308   0.55388708 -2.77010897
  3.06858498  2.18960156  1.18551998 -1.0664832   0.60271533  1.14432456
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Fri Apr  5 04:03:53 2024]  Iteration number: 0 with current cost as 0.34884890222912845 and parameters 
[-7.58077526  2.23743464 -2.12427839 -0.1165304   0.55388708 -2.77010959
  3.06858498  2.18960145  1.18551998 -1.06648433  0.60271572  1.14432445
  1.31029836 -1.8735468 ]. 
Training complete taking 938.3874955177307 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 04:07:05 2024]  Iteration number: 0 with current cost as 0.3085586393992329 and parameters 
[-4.44578289  2.23743438 -2.12427913 -0.11653078  0.55388708 -2.77010897
  3.06858498  2.1896017   1.18551998 -1.06648334  0.60271535  1.1443247
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Fri Apr  5 04:09:47 2024]  Iteration number: 0 with current cost as 0.320551867467593 and parameters 
[-4.32247438  2.23743481 -2.12427946 -0.11653103  0.55388708 -2.77010915
  3.06858481  2.18960145  1.18552016 -1.06648326  0.60271528  1.14432462
  1.31029881 -1.8735468 ]. 
Working on 0.6 fold... 
[Fri Apr  5 04:13:18 2024]  Iteration number: 0 with current cost as 0.3425005497145417 and parameters 
[-4.33926415  2.23743491 -2.12427881 -0.11653048  0.55388708 -2.77010897
  3.06858526  2.189602    1.18552026 -1.06648308  0.60271565  1.144325
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Fri Apr  5 04:16:34 2024]  Iteration number: 0 with current cost as 0.27645594478593116 and parameters 
[-4.45008133  2.23743475 -2.12427941 -0.1165308   0.55388708 -2.77010897
  3.06858498  2.18960156  1.18551998 -1.0664832   0.60271533  1.14432456
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Fri Apr  5 04:19:46 2024]  Iteration number: 0 with current cost as 0.34884890222912845 and parameters 
[-7.58077526  2.23743464 -2.12427839 -0.1165304   0.55388708 -2.77010959
  3.06858498  2.18960145  1.18551998 -1.06648433  0.60271572  1.14432445
  1.31029836 -1.8735468 ]. 
Training complete taking 955.147971868515 seconds. 
Discarding model... 

Training complete taking 23702.05513358116 total seconds. 
Now scoring model... 
Scoring complete taking 2.9758522510528564 seconds. 
Saved predicted values as A2-A2-CZ_Efficient-CRZ_predicted_values.csv
Model scores: {'MSE_train': (0.5897807153524447,), 'R2_train': -0.17439224915802853, 'MAE_train': 0.669477044425071, 'MSE_test': 0.6324817531799377, 'R2_test': -0.17955630143382528, 'MAE_test': 0.6679460577881953}. 
Saved model results as A2-A2-CZ_Efficient-CRZ_results.json. 
