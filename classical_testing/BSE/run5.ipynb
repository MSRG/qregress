{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9b35ce-fe5e-49e4-a575-291253f9d54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install openpyxl\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import joblib\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "\n",
    "# SHAP\n",
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002b56e0-e8bb-4d07-9e8c-a18ce499ecb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07c1a2d-7194-42be-b4cc-48df36511533",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "gaussian_kernel = RBF()\n",
    "\n",
    "# remove basic linear models (ridge, lasso, elastic) and expensive models (gpr)\n",
    "\n",
    "models = {\n",
    "    'ridge': Ridge(),\n",
    "    'lasso': Lasso(),\n",
    "    'elastic': ElasticNet(),\n",
    "    'knn': KNeighborsRegressor(),\n",
    "    'rfr': RandomForestRegressor(),\n",
    "    'grad': GradientBoostingRegressor(),\n",
    "    'svr': SVR(),\n",
    "    'krr': KernelRidge(),\n",
    "    'gpr': GaussianProcessRegressor()\n",
    "}\n",
    "\n",
    "param_grid = {\n",
    "    'ridge': {\n",
    "        'alpha': [0.001, 0.01, 0.1, 1, 10, 50, 100, 1000]\n",
    "    },\n",
    "    'knn': {\n",
    "        'n_neighbors': [1,5,10],\n",
    "        'weights': ['uniform', 'distance']\n",
    "    },\n",
    "    'rfr': {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [None, 5, 10],\n",
    "        'min_samples_split': [2, 5, 10]\n",
    "    },\n",
    "    'grad': {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'learning_rate': [0.1, 0.01, 0.001],\n",
    "        'max_depth': [3, 5, 7]\n",
    "    },\n",
    "    'svr': {\n",
    "        'kernel': ['linear', 'rbf'],\n",
    "        'C': [0.1, 1, 10],\n",
    "        'epsilon': [0.1, 0.01, 0.001]\n",
    "    },\n",
    "    'krr': {\n",
    "        'kernel': ['linear', 'laplacian', 'rbf'],\n",
    "        'alpha': [0.001, 0.01, 0.1, 1],\n",
    "        'gamma': [1, 0.1, 0.01, 0.001]\n",
    "    },\n",
    "    'gpr': {\n",
    "        'alpha': [0.0001, 0.001, 0.01, 0.1, 1],\n",
    "        'kernel': [gaussian_kernel]\n",
    "    },\n",
    "    'lasso': {\n",
    "        'alpha': [0.001, 0.01, 0.1, 1],\n",
    "        'max_iter': [1000, 2000, 3000]\n",
    "    },\n",
    "    'elastic': {\n",
    "        'alpha': [0.001, 0.01, 0.1, 1],\n",
    "        'l1_ratio': [0.2, 0.5, 0.8],\n",
    "        'max_iter': [1000, 2000, 3000]\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80860445-3d43-4035-b6f6-79b5145f8edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gridsearch(model,X_tr, y_tr,X_tst,y_tst):\n",
    "    \"\"\"\n",
    "    Perform GridSearchCV for a given model\n",
    "\n",
    "    parameters\n",
    "    ----------\n",
    "    model: str\n",
    "        Name of model\n",
    "\n",
    "    X_tr: np.array\n",
    "        X training\n",
    "\n",
    "    y_tr: np.array\n",
    "        Y Training\n",
    "\n",
    "    X_tst: np.array\n",
    "        X test\n",
    "\n",
    "    y_tst: np.array\n",
    "        y test\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    scores: dict\n",
    "        scored models\n",
    "    \"\"\"\n",
    "    st = time.time()\n",
    "    # Grab model and model parameters to perform gridsearchcv\n",
    "    current_model = models[model]\n",
    "    current_param_grid = param_grid[model]\n",
    "\n",
    "    grid_search = GridSearchCV(current_model, current_param_grid, cv=5,n_jobs=-1)\n",
    "\n",
    "    print(f'Now fitting {model}... ')\n",
    "\n",
    "    grid_search.fit(X_tr, y_tr)\n",
    "\n",
    "    best_model = grid_search.best_estimator_\n",
    "\n",
    "    print(f'Completed fitting {model} in {time.time() - st:.4f} seconds. ')\n",
    "\n",
    "    # Take the best model and evaluate using known metrics\n",
    "    model=best_model\n",
    "    scores = {}\n",
    "    st = time.time()\n",
    "    print('Now scoring model... ')\n",
    "    y_tr_pred = model.predict(X_tr)\n",
    "    y_tst_pred = model.predict(X_tst)\n",
    "\n",
    "    # plt.scatter(y_tr,y_tr_pred)\n",
    "    # plt.scatter(y_tst,y_tst_pred)\n",
    "    # plt.scatter(y_tr, y_tr)\n",
    "    # plt.scatter(y_tst,y_tst)\n",
    "    # plt.show()\n",
    "            \n",
    "    scores['MSE_train'] = mean_squared_error(y_tr, y_tr_pred),\n",
    "    scores['R2_train'] = r2_score(y_tr, y_tr_pred)\n",
    "    scores['MAE_train'] = mean_absolute_error(y_tr, y_tr_pred)\n",
    "    \n",
    "    scores['MSE_test'] = mean_squared_error(y_tst, y_tst_pred)\n",
    "    scores['R2_test'] = r2_score(y_tst, y_tst_pred)\n",
    "    scores['MAE_test'] = mean_absolute_error(y_tst, y_tst_pred)\n",
    "    print(f\"Train R2 {scores['R2_train']:.4f}\")\n",
    "    print(f\"Test R2 {scores['R2_test']:.4f}\")\n",
    "    print(f\"Train MAE {scores['MAE_train']:.4f}\")\n",
    "    print(f\"Test MAE {scores['MAE_test']:.4f}\")\n",
    "    print()\n",
    "    return scores, best_model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebec3b40-6a45-418f-9a67-1d9675bc2b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = pd.read_csv(\"../../database/Morgan_sub.csv.gz\", compression='gzip',index_col=0)\n",
    "scaler = MinMaxScaler\n",
    "x_scaler = scaler((-1, 1))\n",
    "y_scaler = scaler((-1, 1))\n",
    "Y=pd.read_csv(os.path.join(os.path.dirname(os.path.dirname(os.getcwd())),'database','y.csv.gz'), compression='gzip',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74312fc6-f12b-479f-ae01-9652690b03b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fde37e-5a70-434b-b355-15af5ca4c0b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_split = {}\n",
    "\n",
    "# Scalers\n",
    "scaler = MinMaxScaler\n",
    "x_scaler = scaler((-1, 1))\n",
    "y_scaler = scaler((-1, 1))\n",
    "\n",
    "# PCA\n",
    "c=16\n",
    "pca = PCA(n_components=c)\n",
    "for ts in [0.1,0.3,0.5,0.7,0.8]:\n",
    "    # Split data\n",
    "    train,test=train_test_split(list(X.index), train_size=ts,test_size=0.2,random_state=42)\n",
    "        \n",
    "    X_train, X_test, y_train, y_test = x_scaler.fit_transform(pca.fit_transform(X.loc[train])), x_scaler.transform(pca.transform(X.loc[test])), y_scaler.fit_transform(Y.loc[train].to_numpy()).flatten(), y_scaler.transform(Y.loc[test].to_numpy()).flatten()\n",
    "    \n",
    "    print(\"Scaled X:\",X_train.shape,X_test.shape)\n",
    "    print(np.min(X_train),np.max(X_train),np.mean(X_train))\n",
    "    print(\"Scaled y:\",y_train.shape,y_test.shape)\n",
    "    print(np.min(y_train),np.max(y_train),np.mean(y_train))  \n",
    "       \n",
    "    \n",
    "    scoring={}\n",
    "    t0_init=time.perf_counter()\n",
    "    for m in models.keys():\n",
    "        t0=time.perf_counter()\n",
    "        scores,model=gridsearch(m,X_train, y_train,X_test,y_test)\n",
    "        tf=time.perf_counter()-t0\n",
    "        scores['timing']=tf\n",
    "        scoring[m]=scores\n",
    "        print(m,tf,scoring)\n",
    "    print(f\"Overall {time.perf_counter()-t0_init:.2f}\")\n",
    "    stat_df=pd.concat([pd.DataFrame.from_dict(v).rename(index={0:k}) for k,v in scoring.items()])    \n",
    "    data_split[ts]=scoring\n",
    "\n",
    "stackedf = []\n",
    "for k,v in data_split.items():\n",
    "    df = pd.DataFrame.from_dict(v)\n",
    "    df.loc['size'] = len(df.columns)*[k]\n",
    "    stackedf.append(df)\n",
    "\n",
    "stackedf = pd.concat(stackedf,axis=1).T.drop(columns=['MSE_train','MSE_test']).reset_index()\n",
    "stackedf.to_excel(f\"PCA{c}_classical.xlsx\")\n",
    "stackedf=stackedf.melt(id_vars=['index','size'],value_vars=['R2_train','R2_test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aae1ff9-d710-4b13-bad2-bbb7c4454765",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550c7d31-56fb-4df5-a5f5-94d69295c7c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661688da-b376-4fa8-b99d-e95ffd4f1787",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(data=stackedf,x='size',y='value',hue='index',style='variable')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70430227-bb82-4ee0-a265-0d09cab15d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Scalers\n",
    "# scaler = MinMaxScaler\n",
    "# x_scaler = scaler((-1, 1))\n",
    "# y_scaler = scaler((-1, 1))\n",
    "\n",
    "# # PCA\n",
    "# components = [5 , 16]\n",
    "# for c in components:\n",
    "#     pca = PCA(n_components=c)\n",
    "#     for ts in [0.1,0.3,0.5,0.7,0.8]:\n",
    "#         # Split data\n",
    "#         train,test=train_test_split(list(X.index), train_size=ts,test_size=0.2,random_state=42)\n",
    "#         X_train, X_test, y_train, y_test = x_scaler.fit_transform(pca.fit_transform(X.loc[train])), x_scaler.transform(pca.transform(X.loc[test])), y_scaler.fit_transform(Y.loc[train].to_numpy()).flatten(), y_scaler.transform(Y.loc[test].to_numpy()).flatten()\n",
    "        \n",
    "\n",
    "        \n",
    "#         print(\"Scaled X:\",X_train.shape,X_test.shape)\n",
    "#         print(np.min(X_train),np.max(X_train),np.mean(X_train))\n",
    "#         print(\"Scaled y:\",y_train.shape,y_test.shape)\n",
    "#         print(np.min(y_train),np.max(y_train),np.mean(y_train))        \n",
    "#         with open(f'PCA{c}_{ts:.1f}_Morgan_train.bin','wb') as f:\n",
    "#             joblib.dump({'X':X_train,'y':y_train},f)\n",
    "#         with open(f'PCA{c}_{ts:.1f}_Morgan_test.bin','wb') as f:\n",
    "#             joblib.dump({'X':X_test,'y':y_test},f)\n",
    "#         with open(f'PCA{c}_{ts:.1f}_Morgan_scaler.bin','wb') as f:\n",
    "#             joblib.dump(y_scaler,f)    \n",
    "#         print()\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c88e35c-09db-40e4-943c-75d0e15b943d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
