{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09b9906-d9c3-420a-b55b-8043f751a80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# !{sys.executable} -m pip install matplotlib scikit-learn\n",
    "import click\n",
    "import joblib\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "from quantum.Evaluate import evaluate\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db58ad2e-214a-4d9b-8de7-3d167f549640",
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian_kernel = RBF()\n",
    "\n",
    "models = {\n",
    "    'ridge': Ridge(),\n",
    "    'lasso': Lasso(),\n",
    "    'elastic': ElasticNet(),\n",
    "    'knn': KNeighborsRegressor(),\n",
    "    'rfr': RandomForestRegressor(),\n",
    "    'grad': GradientBoostingRegressor(),\n",
    "    'svr': SVR(),\n",
    "    'krr': KernelRidge(),\n",
    "    'gpr': GaussianProcessRegressor()\n",
    "}\n",
    "\n",
    "param_grid = {\n",
    "    'ridge': {\n",
    "        'alpha': [0.001, 0.01, 0.1, 1, 10, 50, 100, 1000]\n",
    "    },\n",
    "    'knn': {\n",
    "        'n_neighbors': range(1, 10),\n",
    "        'weights': ['uniform', 'distance']\n",
    "    },\n",
    "    'rfr': {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [None, 5, 10],\n",
    "        'min_samples_split': [2, 5, 10]\n",
    "    },\n",
    "    'grad': {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'learning_rate': [0.1, 0.01, 0.001],\n",
    "        'max_depth': [3, 5, 7]\n",
    "    },\n",
    "    'svr': {\n",
    "        'kernel': ['linear', 'rbf'],\n",
    "        'C': [0.1, 1, 10],\n",
    "        'epsilon': [0.1, 0.01, 0.001]\n",
    "    },\n",
    "    'krr': {\n",
    "        'kernel': ['linear', 'poly', 'rbf'],\n",
    "        'alpha': [0.001, 0.01, 0.1, 1],\n",
    "        'gamma': [1, 0.1, 0.01, 0.001]\n",
    "    },\n",
    "    'gpr': {\n",
    "        'alpha': [0.0001, 0.001, 0.01, 0.1, 1],\n",
    "        'kernel': [gaussian_kernel]\n",
    "    },\n",
    "    'lasso': {\n",
    "        'alpha': [0.001, 0.01, 0.1, 1],\n",
    "        'max_iter': [1000, 2000, 3000]\n",
    "    },\n",
    "    'elastic': {\n",
    "        'alpha': [0.001, 0.01, 0.1, 1],\n",
    "        'l1_ratio': [0.2, 0.5, 0.8],\n",
    "        'max_iter': [1000, 2000, 3000]\n",
    "    }\n",
    "}\n",
    "\n",
    "'''\n",
    "def score_model(model, sacler, X_tr, y_tr, X_te, y_te):\n",
    "    \"\"\"\n",
    "    :param X_tr:\n",
    "    :param model:\n",
    "    :param X_te:\n",
    "    :param y_tr:  y training data\n",
    "    :param y_te:  y test data\n",
    "    :return:  scores: dict ['r2_tr', 'r2_te', 'mse_tr', 'mse_te', 'rmse_tr', rmse_te']\n",
    "    \"\"\"\n",
    "    y_tr_pred = model.predict(X_tr)\n",
    "    y_te_pred = model.predict(X_te)\n",
    "    train_r2 = r2_score(y_tr, y_tr_pred)\n",
    "    train_mse = mean_squared_error(y_tr, y_tr_pred)\n",
    "    train_mae = mean_absolute_error(y_tr, y_tr_pred)\n",
    "    test_r2 = r2_score(y_te, y_te_pred)\n",
    "    test_mse = mean_squared_error(y_te, y_te_pred)\n",
    "    test_mae = mean_absolute_error(y_te, y_te_pred)\n",
    "    scores = {\n",
    "        'r2_tr': train_r2,\n",
    "        'r2_te': test_r2,\n",
    "        'mse_tr': train_mse,\n",
    "        'mse_te': test_mse,\n",
    "        'mae_tr': train_mae,\n",
    "        'mae_te': test_mae\n",
    "    }\n",
    "    print(scores['mae_tr'], scores['mae_te'])\n",
    "    scores = list(scores.values())\n",
    "    return scores\n",
    "'''\n",
    "\n",
    "\n",
    "def classical_regressor(model: str, scaler, X_tr, y_tr, X_te, y_te, plot=True, save=False):\n",
    "    \"\"\"\n",
    "    :param scaler:\n",
    "    :param model: defines the model to run, raises a ValueError if unexpected type\n",
    "    :param X_tr: X training data\n",
    "    :param y_tr: y training data\n",
    "    :param X_te: X testing data\n",
    "    :param y_te: y testing data\n",
    "    :param plot: whether to produce a plot, plot will be predicted vs actual\n",
    "    :param save: whether to save the plot saves to /plots/model.png\n",
    "    :return: returns current_scores dict of test scores from evaluate function\n",
    "    \"\"\"\n",
    "    if model not in models.keys():\n",
    "        raise ValueError('Model must be one of', models.keys())\n",
    "\n",
    "    st = time.time()\n",
    "    current_model = models[model]\n",
    "    current_param_grid = param_grid[model]\n",
    "\n",
    "    grid_search = GridSearchCV(current_model, current_param_grid, cv=5)\n",
    "\n",
    "    print(f'Now fitting {model}... ')\n",
    "\n",
    "    grid_search.fit(X_tr, y_tr)\n",
    "\n",
    "    best_model = grid_search.best_estimator_\n",
    "\n",
    "    print(f'Completed fitting {model} in {time.time() - st} seconds. ')\n",
    "\n",
    "    current_scores, y_te_pred, y_tr_pred = evaluate(model=best_model, X_train=X_tr, X_test=X_te, y_train=y_tr,\n",
    "                                                    y_test=y_te, y_scaler=scaler, plot=True, title=model)\n",
    "    if plot:\n",
    "        plt.scatter(y_te, y_te_pred, color='r', label='Test data')\n",
    "        plt.scatter(y_tr, y_tr_pred, color='b', label='Train data')\n",
    "        plt.xlabel('Actual')\n",
    "        plt.ylabel('Predicted')\n",
    "        plt.legend()\n",
    "        if save:\n",
    "            plt.savefig('plots/' + model + '.png')\n",
    "        plt.show()\n",
    "    return current_scores, y_te_pred, y_tr_pred\n",
    "\n",
    "\n",
    "def run_models(scaler, X_tr, y_tr, X_te, y_te, save_plots):\n",
    "    scores = dict()\n",
    "    keys = [\"Train\", \"Test\"]\n",
    "    y_te_pred = {}\n",
    "    y_tr_pred = {}\n",
    "    for model in models.keys():\n",
    "        scores[model], y_te_pred[model], y_tr_pred[model] = classical_regressor(model, scaler, X_tr, y_tr, X_te, y_te,\n",
    "                                                                                plot=save_plots)\n",
    "\n",
    "    \"\"\"\n",
    "    for i in range(3):  # 3 because we have 3 different scoring types, this will loop over each of them and create a\n",
    "        # bar plot comparing the models for each scoring metric\n",
    "        heights = list(scores.values())\n",
    "        heights = np.array(heights).transpose()\n",
    "        heights = dict(zip(keys, heights[i*2:2*i+2]))  # because keys only has two elements this zips the first two\n",
    "        # lists in ith set of lists which corresponds to a score type.\n",
    "\n",
    "        width = 0.25\n",
    "        multiplier = 0\n",
    "        bar_clusters = np.arange(len(scores.keys()))\n",
    "\n",
    "        fig, ax = plt.subplots(layout='constrained')\n",
    "\n",
    "        for key, height in heights.items():\n",
    "            offset = width * multiplier\n",
    "            bar_plt = plt.bar(bar_clusters + offset, height, width, label=key)\n",
    "            plt.bar_label(bar_plt)\n",
    "            multiplier += 1\n",
    "        ax.set_xticks(bar_clusters + width, scores.keys())\n",
    "        ax.legend(loc='upper left', ncols=2)\n",
    "        titles = ['R2 Scores', 'MSE', 'MAE']\n",
    "        ax.set_title(titles[i])\n",
    "        plt.show()\n",
    "        if save_plots:\n",
    "            plt.savefig('plots/model_comparison.svg')\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    return scores, y_tr_pred, y_te_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353f90dd-0c12-4d5d-85f0-1d685fccabc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('classical_testing/BSE/BSE49_full_train.bin','rb') as f:\n",
    "#     print(f.readlines()[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c29119-10e6-4d7d-ba42-ea266c3592ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "@click.command()\n",
    "@click.option('--scaler', type=click.Path(exists=True), required=True, help='File for y scaler to unscale after '\n",
    "                                                                            'prediction')\n",
    "@click.option('--train_set', type=click.Path(exists=True), required=True, help='File for train set')\n",
    "@click.option('--test_set', type=click.Path(exists=True), required=True, help='File for test set')\n",
    "@click.option('--save_plots', default=False, help=\"Don't use: depreceating soon... \")\n",
    "def main(scaler, train_set, test_set, save_plots):\n",
    "    train = joblib.load(train_set)\n",
    "    test = joblib.load(test_set)\n",
    "    X_tr = train['X']\n",
    "    X_te = test['X']\n",
    "    y_tr = np.array(train['y']).reshape(-1)\n",
    "    y_te = np.array(test['y']).reshape(-1)\n",
    "    scaler = joblib.load(scaler)\n",
    "\n",
    "    scores, y_tr_pred, y_te_pred = run_models(scaler, X_tr, y_tr, X_te, y_te, save_plots)\n",
    "\n",
    "    y_tr = scaler.inverse_transform(y_tr.reshape(-1, 1))\n",
    "    y_te = scaler.inverse_transform(y_te.reshape(-1, 1))\n",
    "\n",
    "    for (model_name, train_pred), (test_pred) in zip(y_tr_pred.items(), y_te_pred.values()):\n",
    "        name = model_name + '_predicted_values.csv'\n",
    "        train_pred = np.array(train_pred).reshape(-1).tolist()\n",
    "        y_tr = np.array(y_tr).reshape(-1).tolist()\n",
    "        test_pred = np.array(test_pred).reshape(-1).tolist()\n",
    "        y_te = np.array(y_te).reshape(-1).tolist()\n",
    "\n",
    "        df_train = pd.DataFrame({'Predicted': train_pred, 'Reference': y_tr})\n",
    "        df_train['Data'] = 'Train'\n",
    "        df_test = pd.DataFrame({'Predicted': test_pred, 'Reference': y_te})\n",
    "        df_test['Data'] = 'Test'\n",
    "        df = pd.concat([df_train, df_test], ignore_index=True)\n",
    "        df = df[['Data', 'Predicted', 'Reference']]\n",
    "\n",
    "        df.to_csv(name, index=False)\n",
    "        print(f'Saved predicted values as {name}')\n",
    "\n",
    "    with open('scores.json', 'w') as outfile:\n",
    "        json.dump(scores, outfile)\n",
    "        print(f'Scores saved as {outfile.name}. ')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
