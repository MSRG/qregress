{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c641266f-7fb6-4160-a56c-48a0eb460152",
   "metadata": {},
   "outputs": [
    {
     "ename": "QuantumFunctionError",
     "evalue": "A quantum function must return either a single measurement, or a nonempty sequence of measurements.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mQuantumFunctionError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 57\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m xs, ys \u001b[38;5;129;01min\u001b[39;00m data_loader:\n\u001b[1;32m     55\u001b[0m     opt\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 57\u001b[0m     loss_evaluated \u001b[38;5;241m=\u001b[39m loss(model(xs), ys)\n\u001b[1;32m     58\u001b[0m     loss_evaluated\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     60\u001b[0m     opt\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/miniconda3/envs/simple_qml/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/simple_qml/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/simple_qml/lib/python3.12/site-packages/torch/nn/modules/container.py:219\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 219\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/simple_qml/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/simple_qml/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/simple_qml/lib/python3.12/site-packages/pennylane/qnn/torch.py:404\u001b[0m, in \u001b[0;36mTorchLayer.forward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    401\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mreshape(inputs, (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, inputs\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]))\n\u001b[1;32m    403\u001b[0m \u001b[38;5;66;03m# calculate the forward pass as usual\u001b[39;00m\n\u001b[0;32m--> 404\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_evaluate_qnode(inputs)\n\u001b[1;32m    406\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(results, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    407\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m has_batch_dim:\n",
      "File \u001b[0;32m~/miniconda3/envs/simple_qml/lib/python3.12/site-packages/pennylane/qnn/torch.py:430\u001b[0m, in \u001b[0;36mTorchLayer._evaluate_qnode\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Evaluates the QNode for a single input datapoint.\u001b[39;00m\n\u001b[1;32m    419\u001b[0m \n\u001b[1;32m    420\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[38;5;124;03m    tensor: output datapoint\u001b[39;00m\n\u001b[1;32m    425\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    426\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    427\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_arg: x},\n\u001b[1;32m    428\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{arg: weight\u001b[38;5;241m.\u001b[39mto(x) \u001b[38;5;28;01mfor\u001b[39;00m arg, weight \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqnode_weights\u001b[38;5;241m.\u001b[39mitems()},\n\u001b[1;32m    429\u001b[0m }\n\u001b[0;32m--> 430\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqnode(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(res, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m    433\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\u001b[38;5;241m.\u001b[39mtype(x\u001b[38;5;241m.\u001b[39mdtype)\n",
      "File \u001b[0;32m~/miniconda3/envs/simple_qml/lib/python3.12/site-packages/pennylane/workflow/qnode.py:1164\u001b[0m, in \u001b[0;36mQNode.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1162\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m qml\u001b[38;5;241m.\u001b[39mcapture\u001b[38;5;241m.\u001b[39menabled():\n\u001b[1;32m   1163\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m qml\u001b[38;5;241m.\u001b[39mcapture\u001b[38;5;241m.\u001b[39mqnode_call(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m-> 1164\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_impl_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/simple_qml/lib/python3.12/site-packages/pennylane/workflow/qnode.py:1144\u001b[0m, in \u001b[0;36mQNode._impl_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1141\u001b[0m     override_shots \u001b[38;5;241m=\u001b[39m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshots\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1143\u001b[0m \u001b[38;5;66;03m# construct the tape\u001b[39;00m\n\u001b[0;32m-> 1144\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconstruct(args, kwargs)\n\u001b[1;32m   1146\u001b[0m original_grad_fn \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_fn, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_kwargs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice]\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_gradient_fn(shots\u001b[38;5;241m=\u001b[39moverride_shots, tape\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tape)\n",
      "File \u001b[0;32m~/miniconda3/envs/simple_qml/lib/python3.12/site-packages/pennylane/logging/decorators.py:61\u001b[0m, in \u001b[0;36mlog_string_debug_func.<locals>.wrapper_entry\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     54\u001b[0m     s_caller \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m::L\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[1;32m     55\u001b[0m         [\u001b[38;5;28mstr\u001b[39m(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39mgetouterframes(inspect\u001b[38;5;241m.\u001b[39mcurrentframe(), \u001b[38;5;241m2\u001b[39m)[\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m3\u001b[39m]]\n\u001b[1;32m     56\u001b[0m     )\n\u001b[1;32m     57\u001b[0m     lgr\u001b[38;5;241m.\u001b[39mdebug(\n\u001b[1;32m     58\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCalling \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf_string\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00ms_caller\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     59\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_debug_log_kwargs,\n\u001b[1;32m     60\u001b[0m     )\n\u001b[0;32m---> 61\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/simple_qml/lib/python3.12/site-packages/pennylane/workflow/qnode.py:988\u001b[0m, in \u001b[0;36mQNode.construct\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m    983\u001b[0m     measurement_processes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_qfunc_output\n\u001b[1;32m    985\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m measurement_processes \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\n\u001b[1;32m    986\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(m, qml\u001b[38;5;241m.\u001b[39mmeasurements\u001b[38;5;241m.\u001b[39mMeasurementProcess) \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m measurement_processes\n\u001b[1;32m    987\u001b[0m ):\n\u001b[0;32m--> 988\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m qml\u001b[38;5;241m.\u001b[39mQuantumFunctionError(\n\u001b[1;32m    989\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA quantum function must return either a single measurement, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    990\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor a nonempty sequence of measurements.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    991\u001b[0m     )\n\u001b[1;32m    993\u001b[0m terminal_measurements \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    994\u001b[0m     m \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtape\u001b[38;5;241m.\u001b[39mmeasurements \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(m, MidMeasureMP)\n\u001b[1;32m    995\u001b[0m ]\n\u001b[1;32m    997\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m m \u001b[38;5;28;01mfor\u001b[39;00m ret, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(measurement_processes, terminal_measurements)):\n",
      "\u001b[0;31mQuantumFunctionError\u001b[0m: A quantum function must return either a single measurement, or a nonempty sequence of measurements."
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import pennylane as qml\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "n_qubits = 5\n",
    "dev = qml.device(\"default.qubit\", wires=n_qubits)\n",
    "\n",
    "@qml.qnode(dev)\n",
    "def qnode(inputs, weights):\n",
    "    qml.AngleEmbedding(inputs, wires=range(n_qubits))\n",
    "    qml.BasicEntanglerLayers(weights, wires=range(n_qubits))\n",
    "    return qml.PauliZ(0)\n",
    "\n",
    "\n",
    "n_layers = 6\n",
    "weight_shapes = {\"weights\": (n_layers, n_qubits)}\n",
    "\n",
    "\n",
    "qlayer = qml.qnn.TorchLayer(qnode, weight_shapes)\n",
    "\n",
    "\n",
    "clayer_1 = torch.nn.Linear(2, 2)\n",
    "clayer_2 = torch.nn.Linear(2, 2)\n",
    "softmax = torch.nn.Softmax(dim=1)\n",
    "# layers = [clayer_1, qlayer, clayer_2, softmax]\n",
    "layers = [qlayer]\n",
    "model = torch.nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "opt = torch.optim.SGD(model.parameters(), lr=0.2)\n",
    "loss = torch.nn.L1Loss()\n",
    "\n",
    "with open(\"PCA5_0.8_Morgan_train.bin\",'rb') as f:\n",
    "    data = joblib.load(f)\n",
    "\n",
    "X = torch.tensor(data['X'], requires_grad=True).float()\n",
    "y_hot = torch.tensor(data['y'], requires_grad=True).float()\n",
    "\n",
    "\n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    list(zip(X, y_hot)), batch_size=32, shuffle=True, drop_last=True\n",
    ")\n",
    "\n",
    "epochs = 6\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    running_loss = 0\n",
    "\n",
    "    for xs, ys in data_loader:\n",
    "        opt.zero_grad()\n",
    "\n",
    "        loss_evaluated = loss(model(xs), ys)\n",
    "        loss_evaluated.backward()\n",
    "\n",
    "        opt.step()\n",
    "\n",
    "        running_loss += loss_evaluated\n",
    "\n",
    "    avg_loss = running_loss / batches\n",
    "    print(\"Average loss over epoch {}: {:.4f}\".format(epoch + 1, avg_loss))\n",
    "\n",
    "y_pred = model(X)\n",
    "predictions = torch.argmax(y_pred, axis=1).detach().numpy()\n",
    "\n",
    "correct = [1 if p == p_true else 0 for p, p_true in zip(predictions, y)]\n",
    "accuracy = sum(correct) / len(correct)\n",
    "print(f\"Accuracy: {accuracy * 100}%\")\n",
    "\n",
    "###############################################################################\n",
    "# How did we do? The model looks to have successfully trained and the accuracy is reasonably\n",
    "# high. In practice, we would aim to push the accuracy higher by thinking carefully about the\n",
    "# model design and the choice of hyperparameters such as the learning rate.\n",
    "#\n",
    "# Creating non-sequential models\n",
    "# ------------------------------\n",
    "#\n",
    "# The model we created above was composed of a sequence of classical and quantum layers. This\n",
    "# type of model is very common and is suitable in a lot of situations. However, in some cases we\n",
    "# may want a greater degree of control over how the model is constructed, for example when we\n",
    "# have multiple inputs and outputs or when we want to distribute the output of one layer into\n",
    "# multiple subsequent layers.\n",
    "#\n",
    "# Suppose we want to make a hybrid model consisting of:\n",
    "#\n",
    "# 1. a 4-neuron fully connected classical layer\n",
    "# 2. a 2-qubit quantum layer connected to the first two neurons of the previous classical layer\n",
    "# 3. a 2-qubit quantum layer connected to the second two neurons of the previous classical layer\n",
    "# 4. a 2-neuron fully connected classical layer which takes a 4-dimensional input from the\n",
    "#    combination of the previous quantum layers\n",
    "# 5. a softmax activation to convert to a probability vector\n",
    "#\n",
    "# A diagram of the model can be seen in the figure below.\n",
    "#\n",
    "# .. figure:: /_static/demonstration_assets/qnn_module/qnn2_torch.png\n",
    "#    :width: 100%\n",
    "#    :align: center\n",
    "#\n",
    "# This model can also be constructed by creating a new class that inherits from the\n",
    "# ``torch.nn`` `Module <https://pytorch.org/docs/stable/nn.html#torch.nn.Module>`__ and\n",
    "# overriding the ``forward()`` method:\n",
    "\n",
    "class HybridModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.clayer_1 = torch.nn.Linear(2, 4)\n",
    "        self.qlayer_1 = qml.qnn.TorchLayer(qnode, weight_shapes)\n",
    "        self.qlayer_2 = qml.qnn.TorchLayer(qnode, weight_shapes)\n",
    "        self.clayer_2 = torch.nn.Linear(4, 2)\n",
    "        self.softmax = torch.nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.clayer_1(x)\n",
    "        x_1, x_2 = torch.split(x, 2, dim=1)\n",
    "        x_1 = self.qlayer_1(x_1)\n",
    "        x_2 = self.qlayer_2(x_2)\n",
    "        x = torch.cat([x_1, x_2], axis=1)\n",
    "        x = self.clayer_2(x)\n",
    "        return self.softmax(x)\n",
    "\n",
    "model = HybridModel()\n",
    "\n",
    "###############################################################################\n",
    "# As a final step, let's train the model to check if it's working:\n",
    "\n",
    "opt = torch.optim.SGD(model.parameters(), lr=0.2)\n",
    "epochs = 6\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    running_loss = 0\n",
    "\n",
    "    for xs, ys in data_loader:\n",
    "        opt.zero_grad()\n",
    "        \n",
    "        loss_evaluated = loss(model(xs), ys)\n",
    "        loss_evaluated.backward()\n",
    "\n",
    "        opt.step()\n",
    "\n",
    "        running_loss += loss_evaluated\n",
    "\n",
    "    avg_loss = running_loss / batches\n",
    "    print(\"Average loss over epoch {}: {:.4f}\".format(epoch + 1, avg_loss))\n",
    "\n",
    "y_pred = model(X)\n",
    "predictions = torch.argmax(y_pred, axis=1).detach().numpy()\n",
    "\n",
    "correct = [1 if p == p_true else 0 for p, p_true in zip(predictions, y)]\n",
    "accuracy = sum(correct) / len(correct)\n",
    "print(f\"Accuracy: {accuracy * 100}%\")\n",
    "\n",
    "###############################################################################\n",
    "# Great! We've mastered the basics of constructing hybrid classical-quantum models using\n",
    "# PennyLane and Torch. Can you think of any interesting hybrid models to construct? How do they\n",
    "# perform on realistic datasets?\n",
    "\n",
    "##############################################################################\n",
    "# About the author\n",
    "# ----------------\n",
    "# .. include:: ../_static/authors/thomas_bromley.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e47e42-6c30-4f3a-981c-1debcca88b76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
