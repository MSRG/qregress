{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fecc455-f3a5-4ad0-b6fd-b6fe5743f45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qiskit_ibm_runtime import QiskitRuntimeService\n",
    "import os\n",
    "n_jobs = 16\n",
    "os.environ[\"OMP_NUM_THREADS\"] = str(n_jobs)\n",
    "import joblib\n",
    "import click\n",
    "import json\n",
    "import time\n",
    "from glob import glob\n",
    "import itertools\n",
    "import collections.abc\n",
    "import sys\n",
    "from tqdm.notebook import tqdm\n",
    "# !{sys.executable} -m pip install qcircuit\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pennylane as qml\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import re\n",
    "from joblib import Parallel, delayed, dump, load\n",
    "# !pip install tzlocal\n",
    "from datetime import datetime\n",
    "from tzlocal import get_localzone\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c66ab3b-f071-44a7-92a9-e004a6ca3d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(path):\n",
    "    loss = []\n",
    "    for i in sorted(glob(os.path.join(path, '*log.csv'))):\n",
    "        print(f\"Processing file: {i}\")\n",
    "        loss.append(split(i)[:, 1])  # Get only the loss values (assumed to be in the second column)\n",
    "    print(500-np.hstack(loss).shape[0],\"left\")\n",
    "#   plt.plot(np.hstack(loss))\n",
    "#   plt.xlabel(\"Iteration\")\n",
    "#   plt.ylabel(\"Loss\")\n",
    "#   plt.title(\"Loss Curve\")\n",
    "#   plt.show()\n",
    "\n",
    "def split(path):\n",
    "    \"\"\"Helper function to read CSV file and extract required columns.\"\"\"\n",
    "    save = []\n",
    "    with open(path, 'r') as f:\n",
    "        for line in f:\n",
    "            if ':' in line:\n",
    "                split_line = line.strip().split(',')\n",
    "                # Assuming the structure of each line is: [timestamp, loss_value, ...]\n",
    "                save.append((float(split_line[1]), float(split_line[2])))\n",
    "    return np.array(save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118a3a01-b071-4ce2-a9ec-91f3cb224036",
   "metadata": {},
   "outputs": [],
   "source": [
    "main('./')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590beb6d-3eeb-4342-8298-cca35b7a4867",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model = pd.concat([pd.read_csv(i,index_col=0).dropna() for i in sorted(glob('*csv'))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1785330d-dc46-4924-a4ef-2475bbf7eda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model.sort_index().index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8945873-61dd-4ddf-8e5b-db6c44df2d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_model[\"timestamp\"] = pd.to_datetime(df_model.index)\n",
    "df_model[\"delta_time\"] = df_model[\"timestamp\"].diff()\n",
    "df_model[\"delta_seconds\"] = df_model[\"delta_time\"].dt.total_seconds()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889a3045-5adf-4d99-8617-cd6e2c9dbaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "SMALL_SIZE = 8\n",
    "MEDIUM_SIZE = 12\n",
    "BIGGER_SIZE = 12\n",
    " \n",
    "plt.rc('font', size=MEDIUM_SIZE)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=MEDIUM_SIZE)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=MEDIUM_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=MEDIUM_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=MEDIUM_SIZE)    # legend fontsize\n",
    "plt.rc('figure', titlesize=MEDIUM_SIZE)  # fontsize of the figure title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5bbf6d-dc56-49e4-a334-10fc8e7b180a",
   "metadata": {},
   "outputs": [],
   "source": [
    "top=os.getcwd()\n",
    "with open(os.path.join(top,'0.1_5_DDCC_train.bin'),'rb') as f:\n",
    "    ddcc_train = joblib.load(f)\n",
    "\n",
    "with open(os.path.join(top,'0.1_5_DDCC_test.bin'),'rb') as f:\n",
    "    ddcc_test = joblib.load(f)\n",
    "\n",
    "with open(os.path.join(top,'0.1_5_DDCC_scaler.bin'),'rb') as f:\n",
    "    ddcc_scaler = joblib.load(f)\n",
    "\n",
    "X_ddcc_train, y_ddcc_train = ddcc_train['X'],ddcc_train['y']\n",
    "X_ddcc_test, y_ddcc_test = ddcc_test['X'],ddcc_test['y']\n",
    "\n",
    "X_ddcc_train = X_ddcc_train.reshape(-1,64,5)\n",
    "X_ddcc_test = X_ddcc_test.reshape(-1,64,5)\n",
    "y_ddcc_train = y_ddcc_train.reshape(-1,64)\n",
    "y_ddcc_test = y_ddcc_test.reshape(-1,64)\n",
    "\n",
    "\n",
    "# X_train, y_train = X_ddcc_train, y_ddcc_train\n",
    "# X_test, y_test = X_ddcc_test, y_ddcc_test\n",
    "X_train, y_train = X_ddcc_train, y_ddcc_train\n",
    "X_test, y_test = X_ddcc_test, y_ddcc_test\n",
    "\n",
    "# X_train = [X_train[i:i+4] for i in range(0,len(X_train),4)]\n",
    "# X_test = [X_test[i:i+4] for i in range(0,len(X_test),4)]\n",
    "scaler = ddcc_scaler\n",
    "\n",
    "# print(len(X_train),X_train[0].shape,X_train[-1].shape)\n",
    "print(y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33080687-acb6-4977-b3cd-8d43350f607d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grab_data(path):\n",
    "    '''\n",
    "    Given a globbed path, return the job_n.txt, *_train.txt, and *_test.txt files\n",
    "    '''\n",
    "    iterations = []\n",
    "    train_metric = []\n",
    "    test_metric = []\n",
    "    for i in path:\n",
    "        try:\n",
    "            numbers = int(re.search(r'jobs_(\\d+)\\.txt', i).group(1))\n",
    "            iterations.append(i)\n",
    "        except:\n",
    "            if 'test' in i:\n",
    "                test_metric.append(i)\n",
    "            else:\n",
    "                train_metric.append(i)\n",
    "            \n",
    "    return sorted(iterations,key=lambda x: int(re.search(r'jobs_(\\d+)\\.txt', x).group(1))), train_metric, test_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08d44b0-0b11-46de-907d-1b3417758584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def fetch_jobs_from_file(service, filename):\n",
    "#     \"\"\"Fetch all job results from a given file.\"\"\"\n",
    "#     with open(filename, 'r') as f:\n",
    "#         job_ids = f.readlines()\n",
    "#     return [service.job(job_id.strip()).result() for job_id in job_ids]\n",
    "\n",
    "# def grab_jobs(lst):\n",
    "#     \"\"\"Parallelize over job files, fetching results concurrently.\"\"\"\n",
    "\n",
    "\n",
    "#     service = QiskitRuntimeService(\n",
    "#         channel='ibm_quantum',\n",
    "#         instance='pinq-quebec-hub/univ-toronto/pr-hans-arno-jac'\n",
    "#     )\n",
    "#     # Parallel processing of files\n",
    "#     jobs = Parallel(n_jobs=-1,backend='threading')(\n",
    "#         delayed(fetch_jobs_from_file)(service, filename) for filename in tqdm(lst)\n",
    "#     )\n",
    "    \n",
    "#     return np.array(jobs)\n",
    "\n",
    "\n",
    "def fetch_jobs_from_file(filename):\n",
    "    \"\"\"Fetch all job results from a given file.\"\"\"\n",
    "    service = QiskitRuntimeService(\n",
    "        channel='ibm_quantum',\n",
    "        instance='pinq-quebec-hub/univ-toronto/pr-hans-arno-jac'\n",
    "    )    \n",
    "    with open(filename, 'r') as f:\n",
    "        job_ids = f.readlines()\n",
    "    return [service.job(job_id.strip()) for job_id in job_ids]\n",
    "\n",
    "def grab_jobs(lst):\n",
    "    \"\"\"Parallelize over job files, fetching results concurrently.\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "    # Parallel processing of files\n",
    "    jobs = Parallel(n_jobs=-1)(\n",
    "        delayed(fetch_jobs_from_file)(filename) for filename in tqdm(lst)\n",
    "    )\n",
    "    \n",
    "    return np.array(jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94590046-60d2-40bd-9b14-92b88477b687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set of jobs to exclude\n",
    "EXCLUDED_JOBS = {\n",
    "    'd0gyptvfbx30008w5j80',\n",
    "    # Add more if needed\n",
    "}\n",
    "\n",
    "def get_service():\n",
    "    try:\n",
    "        return QiskitRuntimeService(\n",
    "            channel='ibm_quantum',\n",
    "            instance='pinq-quebec-hub/univ-toronto/pr-hans-arno-jac'\n",
    "        )\n",
    "    except Exception:\n",
    "        return QiskitRuntimeService(\n",
    "            channel='ibm_quantum',\n",
    "            instance='pinq-quebec-hub/univ-toronto/default'\n",
    "        )\n",
    "\n",
    "def openfiles(filename):\n",
    "    from qiskit_ibm_runtime.exceptions import RuntimeJobNotFound  # âœ… local import\n",
    "\n",
    "    service = get_service()\n",
    "\n",
    "    with open(filename, 'r') as f:\n",
    "        job_ids = [line.strip() for line in f if line.strip() not in EXCLUDED_JOBS]\n",
    "\n",
    "    jobs = []\n",
    "    runtimefailures = []\n",
    "    otherexception = []\n",
    "    for job_id in job_ids:\n",
    "        try:\n",
    "            job = service.job(job_id)\n",
    "            jobs.append(job.usage_estimation)\n",
    "        except RuntimeJobNotFound:\n",
    "            print(f\"[Skipped] Job not found: {job_id}\")\n",
    "            runtimefailures.append(job_id)\n",
    "        except Exception as e:\n",
    "            print(f\"[Error] Problem with job {job_id}: {e}\")\n",
    "            otherexception.append(job_id)\n",
    "    return jobs, runtimefailures, otherexception\n",
    "\n",
    "filenames = glob('run*/*.txt')\n",
    "\n",
    "job = Parallel(n_jobs=-1)(\n",
    "    delayed(openfiles)(filename) for filename in tqdm(filenames)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6401df-b4a4-462e-b5db-42bd64ebf9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean([sum(j['quantum_seconds'] for j in i[0] if j['quantum_seconds'] is not None) for i in job]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6622a1f1-68cb-45d9-827f-6a29d31995b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "(1448.1314576194297 * 500 ) / 60 / 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9b8a67-eeaf-45fe-b7fa-b2a722ee6c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "201.1293691138097 / 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c71d0f0-4a41-4972-9dcc-8d1a77613301",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "\n",
    "# Input in hours\n",
    "hours = 201.1293691138097\n",
    "\n",
    "# Convert to timedelta\n",
    "delta = timedelta(hours=hours)\n",
    "\n",
    "# Extract days, hours, minutes\n",
    "days = delta.days\n",
    "hours = delta.seconds // 3600\n",
    "minutes = (delta.seconds % 3600) // 60\n",
    "\n",
    "print(f\"{days} days, {hours} hours, {minutes} minutes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6639a4aa-e593-4abb-81cf-2de0cd8b242b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len([i[1] for i in job if len(i[1])!=0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae26348-4e98-42a7-9c71-a9e7791c6556",
   "metadata": {},
   "outputs": [],
   "source": [
    "len([i[0] for i in job if len(i[0])!=0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6702338-c301-47f6-ad5e-de2ff9f6149d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model[\"timestamp\"].sort_values().iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd88bb4-7c28-4096-a49d-2bd531c57afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# date = df_model[\"timestamp\"].sort_values().iloc[0].to_pydatetime()\n",
    "from datetime import datetime\n",
    "date = datetime(2025,5,1) \n",
    "service = QiskitRuntimeService(\n",
    "    channel='ibm_quantum',\n",
    "    instance='pinq-quebec-hub/univ-toronto/default'\n",
    ")\n",
    "jobs_default = service.jobs(created_after=date,descending=True,instance='pinq-quebec-hub/univ-toronto/default',limit=None)\n",
    "\n",
    "service = QiskitRuntimeService(\n",
    "    channel='ibm_quantum',\n",
    "    instance='pinq-quebec-hub/univ-toronto/pr-hans-arno-jac'\n",
    ")\n",
    "jobs_group = service.jobs(created_after=date,descending=True,instance='pinq-quebec-hub/univ-toronto/pr-hans-arno-jac',limit=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f712534e-78bc-492d-bc79-980e06a3256b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ad20c1-a5a2-4603-b697-17ee163ee8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_times = [i.to_pydatetime().astimezone() for i in df_model[\"timestamp\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8a60dd-5efc-4c3f-a90b-755313434ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "found = []\n",
    "for idx in range(1, len(log_times)):\n",
    "    start = log_times[idx - 1]\n",
    "    end = log_times[idx]\n",
    "    for j in jobs_default + jobs_group:\n",
    "        creation_time = j.creation_date.astimezone()\n",
    "        if start <= creation_time <= end:\n",
    "            found.append((idx, j))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450fe253-9573-4225-92a3-5582962057af",
   "metadata": {},
   "outputs": [],
   "source": [
    "min(j.creation_date.astimezone() for j in jobs_default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffe3c4a-12e8-4362-af0c-04f6df795688",
   "metadata": {},
   "outputs": [],
   "source": [
    "checklast = sorted([(j.creation_date.astimezone(),j.job_id(),j.usage_estimation) for j in tqdm(jobs_default + jobs_group)])[-500*19:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af938412-b006-48db-8968-e725d53715f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(found),len(jobs_default + jobs_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b14d35-bd94-4de4-b4bf-b46496b710ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "8615 / 19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7000a1-3018-4544-b2d8-014f4345a2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model[\"timestamp\"].sort_values().iloc[0].to_pydatetime()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478c4403-ef21-471f-a50e-35398d869725",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_default[0].creation_date.astimezone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbdfd6a6-662b-42f3-973b-342e1431d003",
   "metadata": {},
   "outputs": [],
   "source": [
    "500 * 19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1768b2de-46a1-40b4-8f07-1abff9a788f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch(jobid):\n",
    "    job = service.job(jobid).usage_estimation\n",
    "    return job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ba556f-a2a3-422c-8133-623fcd4ec450",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_recent =  Parallel(n_jobs=-1)(\n",
    "        delayed(fetch)(i.job_id()) for i in tqdm(jobs_default+jobs_group) if i.primitive_id == 'estimator'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df14d964-1ccb-4a80-8e9c-7454dbeef240",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{np.mean([i['quantum_seconds'] for i in data_recent if type(i['quantum_seconds']) is float]):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3d3636-471b-4b30-a45d-8583600951d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "len([i['quantum_seconds'] for i in data_recent if type(i['quantum_seconds']) is float])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b08217-49e2-4ae6-bc61-3f3d88b269ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([i['quantum_seconds'] for i in data_recent if type(i['quantum_seconds']) is float]) / 60 / 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7912714a-c2bd-4351-8782-9d5b2b698b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{(79.8972 * 500) / 60 / 60:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8fcc81-78c8-4725-8b79-d6325ead4e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max([i['quantum_seconds'] for i in data_recent if type(i['quantum_seconds']) is float])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbad37d6-2f07-4989-ae45-2ccdeef10456",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min([i['quantum_seconds'] for i in data_recent if type(i['quantum_seconds']) is float])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e713c59-b6c4-4208-b618-880a4619c663",
   "metadata": {},
   "outputs": [],
   "source": [
    "len([list(i.values())[0] for i in data_recent])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b097d7-3a9b-4b98-80c6-fd32e49b1145",
   "metadata": {},
   "outputs": [],
   "source": [
    "8708 / 19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e694ea3-c342-4f68-8cc2-e5201a78185f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3af2810-df51-43b4-be56-d4f1721c6fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "8708"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddba7f5-bcf4-45b6-b4d9-7e7a7b33dbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "datadelta = datetime.datetime.now() - datetime.datetime(2025,5,6)\n",
    " \n",
    "service = QiskitRuntimeService(\n",
    "    channel='ibm_quantum',\n",
    "    instance='pinq-quebec-hub/univ-toronto/pr-hans-arno-jac'\n",
    ")\n",
    "jobs_in_last_months = service.jobs(created_after=datadelta,descending=True,instance='pinq-quebec-hub/univ-toronto/pr-hans-arno-jac',limit=None)\n",
    "\n",
    "data_recent =  Parallel(n_jobs=-1)(\n",
    "        delayed(fetch)(i.job_id()) for i in tqdm(jobs_in_last_months) if i.primitive_id == 'estimator'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b139da6c-b4d9-4dce-af48-ba902cc39092",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(jobs_in_last_months)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f2b23a-1e95-4604-95fd-7fc82c5b77f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_recent =  Parallel(n_jobs=-1)(\n",
    "        delayed(fetch)(i.job_id()) for i in tqdm(jobs_in_last_months) if i.primitive_id == 'estimator'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe0d442-6ed9-4ff9-8d5d-a98baaf4042a",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_in_last_months[0].primitive_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6ae920-0429-4abe-b8cd-6b84d516b7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_in_last_months[0].job_id()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5920aa73-604e-4a5e-abf6-c6ce025c3de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(service.jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8331bd2c-17d4-477a-bf9c-96f9c5728a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "service = QiskitRuntimeService(\n",
    "    channel='ibm_quantum',\n",
    "    instance='pinq-quebec-hub/univ-toronto/default'\n",
    ")\n",
    "service.job('d0crshyd8drg008zaby0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c597cc7b-1754-4eed-9377-cb2b7396d14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(service.jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b05e14-4ec2-4457-9a30-1b7a4f5dbdd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "three_months_ago"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2edc09-36f4-446b-a22e-4e8aadfd1c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_in_last_months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426b690c-d1db-41b6-9799-9cac4839d2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobids = []\n",
    "for i in sorted(glob(\"run*/*txt\"),key=lambda x: (x.split('/')[0],int(x.split('/')[1].replace('jobs_','').replace('.txt','')))):\n",
    "    with open(i,'r') as f:\n",
    "        jobids.append([i.strip() for i in f.readlines()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95354dd6-19ef-4ae2-964d-5b546b76e684",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "[i.job_id() for i in jobs_in_last_months if i.job_id() in sum(jobids,[])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcac59f1-54e7-46a8-bc21-79d4a1ef868e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for i in sorted(glob('run*')):\n",
    "    if os.path.isdir(i):\n",
    "        print(i)\n",
    "        try:\n",
    "            data.append(grab_jobs(sorted(glob(os.path.join(i,'job*txt')),key=lambda x: int(x.split('/')[1].replace('jobs_','').replace('.txt','')))))\n",
    "        except:\n",
    "            print(f\"This one is cooked {i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb7e36d-1267-41a7-9a5f-f48dc39e1681",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = grab_jobs(sorted(glob(\"run*/*txt\"),key=lambda x: (x.split('/')[0],int(x.split('/')[1].replace('jobs_','').replace('.txt',''))))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a822dc-5b1d-40c1-89b2-507cc7feed55",
   "metadata": {},
   "outputs": [],
   "source": [
    "statevectordf = pd.read_csv(os.path.join(os.path.expanduser('~'),'qregress/qml_DDCC/RUD_AL/5AL/A2_HWE-CNOT/A2_HWE-CNOT_predicted_values.csv'))\n",
    "statevectordf['Predicted'] = [float(i.strip('[]')) for i in statevectordf['Predicted']]\n",
    "statevectordf['Reference'] = [float(i.strip('[]')) for i in statevectordf['Reference']]\n",
    "statevectordf['Device'] = len(statevectordf)*['State Vector']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5601fa68-f8d8-45f7-adc6-a90f78a07ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "statevectordf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0d0b77-d0ed-4e46-9288-7acaa657d2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "spread = 10e-2\n",
    "# Create the figure with a 2D grid (scatter + KDE for Predicted + KDE for Reference)\n",
    "fig, axes = plt.subplots(\n",
    "    2, 2, \n",
    "    figsize=(10, 10),\n",
    "    gridspec_kw={'width_ratios': [4, 1.7], 'height_ratios': [1.7, 4]},\n",
    "    constrained_layout=True,\n",
    "    # sharey='row',  # Keep y-axis sharing, but remove sharex to control ticks manually,\n",
    "    # sharex='col'\n",
    ")\n",
    "axes[0, -1].axis(\"off\")\n",
    "# sns.scatterplot(data=finaldf,x='Reference',y='Predicted',hue='Data',style='Data',markers=['o', 's'], edgecolors='black',ax=axes[1,0],palette=)\n",
    "axes[1,0].scatter(y_ddcc_train.flatten(),y_1000_train.flatten(),marker='s',label=\"ibm_quebec Train R$^{2}=$\"+f\"{r2_score(y_ddcc_train.flatten(),y_1000_train.flatten()):.4f}\",color=cmap[0], edgecolors='black')\n",
    "axes[1,0].scatter(y_ddcc_test.flatten(),y_1000_test.flatten(),label=\"ibm_quebec Test R$^{2}=$\"+f\"{r2_score(y_ddcc_train.flatten(),y_1000_test.flatten()):.4f}\",color=cmap[1], edgecolors='black')\n",
    "# sns.scatterplot(data=statevectordf,hue='Data',x='Reference',y='Predicted',style='Data',markers=['d', 'D'], edgecolors='black',ax=axes[1,0],palette=cmap[2:4])\n",
    "axes[1,0].plot(range(-1,2),range(-1,2),'k--')\n",
    "axes[1,0].set_ylim(-spread,spread)\n",
    "axes[1,0].set_xlim(-spread,spread)\n",
    "axes[1,0].set_ylabel(\"Predicted t$_{2}$-amplitudes\")\n",
    "axes[1,0].set_xlabel(\"Calculated t$_{2}$-amplitudes\")\n",
    "axes[1,0].legend()\n",
    "\n",
    "\n",
    "# sns.histplot(data=pd.concat([finaldf,statevectordf]),hue='Data',x='Reference',ax=axes[0,0],fill=True,palette=cmap[0:4])\n",
    "# sns.histplot(data=finaldf,hue='Data',x='Reference',ax=axes[0,0],fill=True,palette=cmap[0:2],stat='probability',kde=True)\n",
    "# axes[0,0].set_yscale('log')\n",
    "sns.kdeplot(data=finaldf,hue='Data',x='Reference',ax=axes[0,0],fill=True, bw_adjust=2,palette=cmap[0:2])\n",
    "# sns.kdeplot(data=statevectordf,hue='Data',x='Reference',ax=axes[0,0],fill=True, bw_adjust=2,palette=cmap[3:5])\n",
    "axes[0,0].set_xticklabels([])  # Hide labels but keep ticks\n",
    "axes[0,0].set_xlabel(\"\")  # Remove x-labels\n",
    "axes[0,0].set_xlim(-spread,spread)\n",
    "axes[0,0].set_ylim(1,60)\n",
    "# axes[0,0].legend(loc=3)\n",
    "\n",
    "# sns.histplot(data=pd.concat([finaldf,statevectordf]),hue='Data',y='Predicted',ax=axes[1,1],fill=True,palette=cmap[0:4])\n",
    "# sns.histplot(data=finaldf,hue='Data',y='Predicted',ax=axes[1,1],fill=True,palette=cmap[0:2],stat='probability',kde=True)\n",
    "# axes[1,1].set_xscale('log')\n",
    "sns.kdeplot(data=finaldf,hue='Data',y='Predicted',ax=axes[1,1],fill=True, bw_adjust=2,palette=cmap[0:2])\n",
    "# sns.kdeplot(data=statevectordf,hue='Data',y='Predicted',ax=axes[1,1],fill=True, bw_adjust=2,palette=cmap[3:5])\n",
    "# axes[1,1].set_xticks(np.hstack([0,np.logspace(0,4,3)]))\n",
    "# axes[1,1].set_xticklabels(['0']+[\"10$^{\"+f\"{np.log10(i):n}\"+\"}$\" for i in np.logspace(0,4,3)])  # Hide labels but keep ticks\n",
    "axes[1,1].set_yticklabels([])  # Hide labels but keep ticks\n",
    "axes[1,1].set_ylabel(\"\")  # Remove x-labels\n",
    "axes[1,1].set_ylim(-spread,spread)\n",
    "axes[1,1].set_xlim(1,60)\n",
    "plt.subplots_adjust(wspace=0.1, hspace=0.1)  # Adjust width and height spacing\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(os.path.expanduser('~'),'qregress/images/DDCC/finalibm_vs_statevector.png'),dpi=300,bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4294a3a0-68e4-46a6-9a8d-d90280f8b7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "len([len(y_ddcc_train.flatten())*['Train']]+[len(y_ddcc_test.flatten())*['Test']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c75269f-de94-489b-be88-6450bfae9fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# import numpy as np\n",
    "# from sklearn.metrics import r2_score\n",
    "\n",
    "# fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "# # Scatter plots\n",
    "# ax.scatter(y_ddcc_train.flatten(), y_1000_train.flatten(), \n",
    "#            label=\"ibm_quebec Train R$^{2}=$\"+f\"{r2_score(y_ddcc_train.flatten(), y_1000_train.flatten()):.4f}\", \n",
    "#            color='b', edgecolors='black')\n",
    "# ax.scatter(y_ddcc_test.flatten(), y_1000_test.flatten(), \n",
    "#            label=\"ibm_quebec Test R$^{2}=$\"+f\"{r2_score(y_ddcc_test.flatten(), y_1000_test.flatten()):.4f}\", \n",
    "#            color='g', edgecolors='black')\n",
    "\n",
    "# sns.scatterplot(data=statevectordf[statevectordf['Data'] == 'Train'], x='Reference', y='Predicted', \n",
    "#                 label=\"State Vector Train: R$^{2}=$\"+f\"{r2_score(statevectordf[statevectordf['Data'] == 'Train']['Reference'], statevectordf[statevectordf['Data'] == 'Train']['Predicted']):.4f}\", \n",
    "#                 ax=ax, edgecolor='black')\n",
    "\n",
    "# sns.scatterplot(data=statevectordf[statevectordf['Data'] == 'Test'], x='Reference', y='Predicted', \n",
    "#                 label=\"State Vector Test: R$^{2}=$\"+f\"{r2_score(statevectordf[statevectordf['Data'] == 'Test']['Reference'], statevectordf[statevectordf['Data'] == 'Test']['Predicted']):.4f}\", \n",
    "#                 ax=ax, edgecolor='black')\n",
    "\n",
    "# # Identity line\n",
    "# ax.plot(np.linspace(-1, 2, 100), np.linspace(-1, 2, 100), 'k--')\n",
    "\n",
    "# ax.set_ylim(-3e-2, 3e-2)\n",
    "# ax.set_xlim(-3e-2, 3e-2)\n",
    "# ax.set_ylabel(\"Predicted t$_{2}$-amplitudes\")\n",
    "# ax.set_xlabel(\"Calculated t$_{2}$-amplitudes\")\n",
    "# ax.legend()\n",
    "# plt.tight_layout()\n",
    "\n",
    "# # Create twin axes for KDE plots\n",
    "# top_ax = ax.twiny()\n",
    "# right_ax = ax.twinx()\n",
    "\n",
    "# # KDE distributions\n",
    "# sns.kdeplot(statevectordf['Reference'], ax=top_ax, color='gray', lw=2, clip=(-3e-2, 3e-2))\n",
    "# sns.kdeplot(statevectordf['Predicted'], ax=right_ax, color='gray', lw=2, clip=(-3e-2, 3e-2))\n",
    "\n",
    "# # Hide tick labels for KDE axes\n",
    "# top_ax.set_xticks([])\n",
    "# right_ax.set_yticks([])\n",
    "\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35577954-5f08-4932-b7d7-1444ed3e2852",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(MSE_10+MSE_100+MSE_500+MSE_1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8b4195-032e-439b-821d-c7c111ebb96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(min_MSE_idx,combined_MSE[min_MSE_idx],color='r',label=f'MSE:{combined_MSE[min_MSE_idx]:.4e}')\n",
    "plt.plot(range(1,len(MSE_10+MSE_100+MSE_500+MSE_1000)+1),MSE_10+MSE_100+MSE_500+MSE_1000)\n",
    "plt.ylabel('Training Loss (MSE)')\n",
    "plt.xlabel('Iterations')\n",
    "plt.xlim(0,900)\n",
    "plt.ylim(0,8e-4)\n",
    "# plt.hlines(statevector['MSE_train'][0],-100,1e4,color='r',linestyle='--',label=f'State Vector MSE:{statevector['MSE_train'][0]:.4e}')\n",
    "# plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(os.path.expanduser('~'),'qregress/images/DDCC/ibmq_loss.png'),dpi=300,bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a289863-685f-4f04-9bb9-9fe96aeb4181",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
