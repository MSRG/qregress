{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5112808-c742-4cd2-b023-37076e3b7dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import click\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "import itertools\n",
    "import collections.abc\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pennylane as qml\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold\n",
    "from qiskit_ibm_provider import IBMProvider\n",
    "\n",
    "from quantum.Evaluate import evaluate\n",
    "from settings import ANSATZ_LIST, ENCODER_LIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e0726a-d45d-46cb-a108-00f7ce428f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables\n",
    "OPTIMIZER = None\n",
    "SHOTS = None\n",
    "X_DIM = None\n",
    "BACKEND = None\n",
    "DEVICE = None\n",
    "SCALE_FACTORS = None\n",
    "ANSATZ = None\n",
    "ENCODER = None\n",
    "POSTPROCESS = None\n",
    "ERROR_MITIGATION = None\n",
    "LAYERS = None\n",
    "PROVIDER = None\n",
    "TOKEN = None\n",
    "HYPERPARAMETERS = None\n",
    "RE_UPLOAD_DEPTH = None\n",
    "MAX_ITER = None\n",
    "TOLERANCE = None\n",
    "NUM_QUBITS = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa00ad10-c945-4325-97e2-9ae9241be350",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_settings(settings_file):\n",
    "    with open(settings_file, 'r') as fp:\n",
    "        settings = json.load(fp)\n",
    "\n",
    "    global OPTIMIZER\n",
    "    OPTIMIZER = settings['OPTIMIZER']\n",
    "\n",
    "    global SHOTS\n",
    "    SHOTS = settings['SHOTS']\n",
    "\n",
    "    global BACKEND\n",
    "    BACKEND = settings['BACKEND']\n",
    "\n",
    "    global DEVICE\n",
    "    DEVICE = settings['DEVICE']\n",
    "\n",
    "    global SCALE_FACTORS\n",
    "    SCALE_FACTORS = settings['SCALE_FACTORS']\n",
    "\n",
    "    global POSTPROCESS\n",
    "    POSTPROCESS = settings['POSTPROCESS']\n",
    "\n",
    "    global ERROR_MITIGATION\n",
    "    ERROR_MITIGATION = settings['ERROR_MITIGATION']\n",
    "\n",
    "    global LAYERS\n",
    "    LAYERS = settings['LAYERS']\n",
    "\n",
    "    global HYPERPARAMETERS\n",
    "    HYPERPARAMETERS = settings['HYPERPARAMETERS']\n",
    "    # f was removed from HYPERPARAMETERS, this ensures old settings files can still run.\n",
    "    if 'f' in HYPERPARAMETERS.keys():\n",
    "        _ = HYPERPARAMETERS.pop('f', None)\n",
    "\n",
    "    global RE_UPLOAD_DEPTH\n",
    "    RE_UPLOAD_DEPTH = settings['RE-UPLOAD_DEPTH']\n",
    "\n",
    "    global MAX_ITER\n",
    "    MAX_ITER = settings['MAX_ITER']\n",
    "\n",
    "    global TOLERANCE\n",
    "    try:\n",
    "        TOLERANCE = settings['TOLERANCE']\n",
    "    except KeyError:\n",
    "        TOLERANCE = None\n",
    "\n",
    "    global NUM_QUBITS\n",
    "    try:\n",
    "        NUM_QUBITS = settings['NUM_QUBITS']\n",
    "    except KeyError:\n",
    "        NUM_QUBITS = None\n",
    "\n",
    "    # classes aren't JSON serializable, so we store the key in the settings file and access it here.\n",
    "    global ANSATZ\n",
    "    ANSATZ = ANSATZ_LIST[settings['ANSATZ']]\n",
    "\n",
    "    global ENCODER\n",
    "    ENCODER = ENCODER_LIST[settings['ENCODER']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab98b271-af9f-46f1-b84b-bf2785334aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(file):\n",
    "    print(f'Loading dataset from {file}... ')\n",
    "    data = joblib.load(file)\n",
    "    X = data['X']\n",
    "    y = data['y']\n",
    "\n",
    "    global X_DIM\n",
    "    _, X_DIM = X.shape\n",
    "    print(f'Successfully loaded {file} into X and y data. ')\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a06c366-5945-4009-bb08-fc7c647f89a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_kwargs():\n",
    "    #  First have to apply specific ansatz settings: setting number of layers and the number of wires based on features\n",
    "    ANSATZ.layers = LAYERS\n",
    "    ANSATZ.set_wires(range(X_DIM))\n",
    "\n",
    "    kwargs = {\n",
    "        'encoder': ENCODER,\n",
    "        'variational': ANSATZ,\n",
    "        'num_qubits': X_DIM,\n",
    "        # 'optimizer': OPTIMIZER,\n",
    "        'optimizer': 'L-BFGS-B',\n",
    "        'max_iterations': MAX_ITER,\n",
    "        'tol': TOLERANCE,\n",
    "        'device': DEVICE,\n",
    "        'backend': BACKEND,\n",
    "        'postprocess': POSTPROCESS,\n",
    "        'error_mitigation': ERROR_MITIGATION,\n",
    "        'provider': PROVIDER,\n",
    "        'token': TOKEN,\n",
    "        're_upload_depth': RE_UPLOAD_DEPTH,\n",
    "    }\n",
    "    return kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d131df3-5497-4af5-aaf3-a34d094dbf87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4448111f-96e3-4870-8af4-d040fd2cf54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "settings=\"./function-calc-test/5qubits/A2-A2-CNOT_ESU2/A2-A2-CNOT_ESU2.json\"\n",
    "train_set=\"./function-calc-test/linear/linear_train.bin\"\n",
    "test_set=\"./function-calc-test/linear/linear_test.bin\"\n",
    "scaler=\"./function-calc-test/linear/linear_scaler.bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed38a76-6541-4a89-bf8e-cf29642bbf5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = load_dataset(train_set)\n",
    "parse_settings(settings)\n",
    "if DEVICE == 'qiskit.ibmq':\n",
    "    save_token(instance, token)\n",
    "\n",
    "global NUM_QUBITS\n",
    "global X_DIM\n",
    "if NUM_QUBITS is not None:\n",
    "    X_DIM = NUM_QUBITS\n",
    "elif X_DIM == 1:  # if X_DIM is None and num_qubits wasn't specified anywhere use a default value of 2.\n",
    "    NUM_QUBITS = 2\n",
    "    X_DIM = NUM_QUBITS\n",
    "\n",
    "kwargs = create_kwargs()\n",
    "title=False\n",
    "if title is None:\n",
    "    title = os.path.basename(settings)\n",
    "    title, _ = os.path.splitext(title)\n",
    "\n",
    "\n",
    "\n",
    "if test_set is not None:\n",
    "    X_test, y_test = load_dataset(test_set)\n",
    "else:\n",
    "    X_test, y_test = None, None\n",
    "\n",
    "scaler = joblib.load(scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809fc218-9d2f-4056-8679-d2914b57c1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.dtype,y_train.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a27a571-606a-4889-99be-d737960dd253",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.dtype,y_test.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b02951-6bba-41da-b079-8f3c904f2383",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPTIMIZER='L-BFGS-B'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6d989d-7c41-4d9f-833d-2d03e0767574",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pennylane as qml\n",
    "import numpy as np\n",
    "from pennylane import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.optimize import minimize, basinhopping\n",
    "from qiskit_ibm_runtime import QiskitRuntimeService\n",
    "from qiskit_ibm_provider import IBMProvider\n",
    "from mitiq.zne.scaling import fold_global\n",
    "from mitiq.zne.inference import RichardsonFactory, LinearFactory\n",
    "import joblib\n",
    "import mthree\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "\n",
    "\n",
    "class BasinBounds:\n",
    "    def __init__(self, xmax=np.pi, xmin=-np.pi):\n",
    "        self.xmax = xmax\n",
    "        self.xmin = xmin\n",
    "\n",
    "    def __call__(self, **kwargs):\n",
    "        x = kwargs[\"x_new\"]\n",
    "        tmax = bool(np.all(x <= self.xmax))\n",
    "        tmin = bool(np.all(x >= self.xmin))\n",
    "        return tmax and tmin\n",
    "\n",
    "\n",
    "class QuantumRegressor:\n",
    "    \"\"\"\n",
    "    Machine learning model based on quantum circuit learning.\n",
    "\n",
    "    Methods\n",
    "    ------\n",
    "    fit(x, y, initial_parameters=None, detailed_results=False, load_state=None, callback_interval=None)\n",
    "        Fits the model instance to the given x and y data.\n",
    "    predict(x)\n",
    "        Predicts y values for a given array of input data based on previous training.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            encoder,\n",
    "            variational,\n",
    "            num_qubits,\n",
    "            optimizer: str = 'COBYLA',\n",
    "            max_iterations: int = None,\n",
    "            tol: float = 1e-8,\n",
    "            device: str = 'default.qubit',\n",
    "            backend: str = None,\n",
    "            postprocess: str = None,\n",
    "            error_mitigation=None,\n",
    "            scale_factors: list = None,\n",
    "            folding=fold_global,\n",
    "            shots: int = None,\n",
    "            re_upload_depth: int = 1,\n",
    "            f: float = 1.,\n",
    "            alpha: float = 0.,\n",
    "            beta: float = 0,\n",
    "            provider=None,\n",
    "            token: str = None):\n",
    "        self.hyperparameters = {'f': f, 'alpha': alpha, 'beta': beta}\n",
    "        if scale_factors is None:\n",
    "            scale_factors = [1, 3, 5]\n",
    "        self.callback_interval = None\n",
    "        self.x = None\n",
    "        self.y = None\n",
    "        self.params = None\n",
    "        self._re_upload_depth = re_upload_depth\n",
    "        self.error_mitigation = error_mitigation\n",
    "        self.num_qubits = num_qubits\n",
    "        self.max_iterations = max_iterations\n",
    "        if postprocess == 'None':\n",
    "            postprocess = None\n",
    "        self.postprocess = postprocess\n",
    "        self.encoder = encoder\n",
    "        self.variational = variational\n",
    "        self._set_device(device, backend, shots, provider, token)\n",
    "        self._set_optimizer(optimizer)\n",
    "        self._tol = tol\n",
    "        self._build_qnode(scale_factors, folding)\n",
    "        self.fit_count = 0\n",
    "        self.cached_results = {}\n",
    "\n",
    "    def _set_device(self, device, backend, shots, provider=None, token=None):\n",
    "        #  sets the models quantum device. If using IBMQ asks for proper credentials\n",
    "        if device == 'qiskit.ibmq':\n",
    "            print('Running on IBMQ Runtime')\n",
    "            if provider is None:\n",
    "                instance = input('Enter runtime setting: instance')\n",
    "                provider = IBMProvider(instance)\n",
    "            if token is None:\n",
    "                token = input('Enter IBMQ token')\n",
    "            # QiskitRuntimeService.save_account(channel='ibm_quantum', instance=instance, token=token, overwrite=True)\n",
    "            self.device = qml.device(device, wires=self.num_qubits, backend=backend, shots=shots, provider=provider,\n",
    "                                     token=token)\n",
    "            service = QiskitRuntimeService()\n",
    "            self._backend = service.backend(backend)\n",
    "            if self.error_mitigation == 'TREX':\n",
    "                self.device.set_transpile_args(**{'resilience_level': 1})\n",
    "        else:\n",
    "            self.device = qml.device(device, wires=self.num_qubits, shots=shots)\n",
    "\n",
    "    def _set_optimizer(self, optimizer):\n",
    "        #  sets the desired optimizer. SPSA is not available in scipy and has to be handled separately in fitting\n",
    "        if optimizer == 'SPSA':\n",
    "            self.use_scipy = False\n",
    "            self.optimizer = optimizer\n",
    "        elif optimizer == 'BasinHopping':\n",
    "            self.use_scipy = False\n",
    "            self.optimizer = optimizer\n",
    "        else:\n",
    "            self.use_scipy = True\n",
    "            self.optimizer = optimizer\n",
    "\n",
    "    def _circuit(self, features, parameters):\n",
    "        #  builds the circuit with the given encoder and variational circuits.\n",
    "        #  encoder and variational circuits must have only two required parameters, params/feats and wires\n",
    "        for i in range(self._re_upload_depth):\n",
    "            params = parameters[self._num_params() * i:self._num_params() * (i + 1)]\n",
    "            self.encoder(features, wires=range(self.num_qubits))\n",
    "            self.variational(params, wires=range(self.num_qubits))\n",
    "\n",
    "        if self.postprocess is None and self.error_mitigation != 'M3':\n",
    "            return qml.expval(qml.PauliZ(0))\n",
    "        elif self.postprocess is None and self.error_mitigation == 'M3':\n",
    "            return [qml.counts(qml.PauliZ(0))]\n",
    "        elif self.postprocess is not None and self.error_mitigation != 'M3':\n",
    "            return [qml.expval(qml.PauliZ(i)) for i in range(self.num_qubits)]\n",
    "        elif self.postprocess is not None and self.error_mitigation == 'M3':\n",
    "            return [qml.counts(qml.PauliZ(i)) for i in range(self.num_qubits)]\n",
    "\n",
    "    def _build_qnode(self, scale_factors, folding):\n",
    "        #  builds QNode from device and circuit using mitiq error mitigation if specified.\n",
    "        self.qnode = qml.QNode(self._circuit, self.device)\n",
    "        if self.error_mitigation == 'MITIQ_Linear':\n",
    "            factory = LinearFactory.extrapolate\n",
    "            scale_factors = scale_factors\n",
    "            noise_scale_method = folding\n",
    "            self.qnode = qml.transforms.mitigate_with_zne(self.qnode, scale_factors, noise_scale_method, factory)\n",
    "        elif self.error_mitigation == 'MITIQ_Richardson':\n",
    "            factory = RichardsonFactory.extrapolate\n",
    "            scale_factors = scale_factors\n",
    "            noise_scale_method = folding\n",
    "            self.qnode = qml.transforms.mitigate_with_zne(self.qnode, scale_factors, noise_scale_method, factory)\n",
    "        elif self.error_mitigation == 'M3':\n",
    "            mit = mthree.M3Mitigation(self._backend)\n",
    "            mit.cals_from_system()\n",
    "            old_qnode = self.qnode\n",
    "\n",
    "            def new_qnode(features, params):\n",
    "                raw_counts = old_qnode(features, params)\n",
    "                m3_counts = [mit.apply_correction(raw_counts[i], [i], return_mitigation_overhead=False)\n",
    "                             for i in range(len(raw_counts))]\n",
    "                expval = [counts.expval() for counts in m3_counts]\n",
    "                if len(expval) == 1:\n",
    "                    expval = expval[0]\n",
    "                return expval\n",
    "\n",
    "            self.qnode = new_qnode\n",
    "\n",
    "    def _cost(self, parameters):\n",
    "\n",
    "        pred = self.predict(self.x, params=parameters)\n",
    "        base_cost = mean_squared_error(self.y, pred)\n",
    "        if self.postprocess is None or self.postprocess == 'None' or self.postprocess == 'simple':\n",
    "            return base_cost\n",
    "        elif self.postprocess == 'ridge':\n",
    "            extra_params = parameters[-self.num_qubits:]\n",
    "            alpha = self.hyperparameters['alpha']\n",
    "\n",
    "            return base_cost + alpha * np.linalg.norm(extra_params)\n",
    "        elif self.postprocess == 'lasso':\n",
    "            extra_params = parameters[-self.num_qubits:]\n",
    "            alpha = self.hyperparameters['alpha']\n",
    "\n",
    "            l1_norm = 0\n",
    "            for param in extra_params:\n",
    "                l1_norm += np.abs(param)\n",
    "\n",
    "            return base_cost + alpha * l1_norm\n",
    "        elif self.postprocess == 'elastic':\n",
    "            extra_params = parameters[-self.num_qubits:]\n",
    "            alpha = self.hyperparameters['alpha']\n",
    "            beta = self.hyperparameters['beta']\n",
    "\n",
    "            l1_norm = 0\n",
    "            for param in extra_params:\n",
    "                l1_norm += np.abs(param)\n",
    "\n",
    "            return base_cost + beta * (alpha * l1_norm + (1 - alpha) * np.linalg.norm(extra_params))\n",
    "        else:\n",
    "            raise NotImplementedError(f'The given postprocess type {self.postprocess} is not implemented. ')\n",
    "\n",
    "    def _cost_wrapper(self, parameters):\n",
    "        # caches the results from the cost function, so they don't have to be recalculated if they get called again i.e.\n",
    "        # during the callback function for logging.\n",
    "        param_hash = hash(parameters.data.tobytes())\n",
    "        if param_hash in self.cached_results:\n",
    "            cost = self.cached_results[param_hash]\n",
    "        else:\n",
    "            cost = self._cost(parameters)\n",
    "            self.cached_results[param_hash] = cost\n",
    "        return cost\n",
    "\n",
    "    def _num_params(self):\n",
    "        #  computes the number of parameters required for the implemented variational circuit\n",
    "        num_params = self.variational.num_params\n",
    "        return num_params\n",
    "\n",
    "    def _callback(self, xk):\n",
    "        cost_at_step = self._cost_wrapper(xk)\n",
    "        if self.fit_count % 50 == 0:\n",
    "            print(f'[{time.asctime()}]  Iteration number: {self.fit_count} with current cost as {cost_at_step} and '\n",
    "                  f'parameters \\n{xk}. ')\n",
    "        filename = 'model_log.csv'\n",
    "        log = f'{time.asctime()},{self.fit_count},{cost_at_step},{xk}'\n",
    "        with open(filename, 'a') as outfile:\n",
    "            outfile.write(log)\n",
    "            outfile.write('\\n')\n",
    "        self.fit_count += 1\n",
    "        self._save_partial_state(xk)\n",
    "\n",
    "    def _save_partial_state(self, param_vector, force=False):\n",
    "        # saves every call to a bin file able to be loaded later by calling fit with load_state set to filename\n",
    "        interval = self.callback_interval\n",
    "        if interval is None:\n",
    "            interval = 5\n",
    "        if self.fit_count % interval == 0 or force:\n",
    "            partial_results = {\n",
    "                'parameters': param_vector,\n",
    "                'iterations': self.fit_count\n",
    "            }\n",
    "            print(f\"Fucking {force}\")\n",
    "            if force is True:\n",
    "                outfile = 'final_state_model.bin'\n",
    "                os.remove('partial_state_model.bin')\n",
    "            else:\n",
    "                outfile = 'partial_state_model.bin'\n",
    "            joblib.dump(partial_results, outfile)\n",
    "\n",
    "    def _load_partial_state(self, infile):\n",
    "        print('Loading partial state from file ' + infile)\n",
    "        partial_state = joblib.load(infile)\n",
    "        if type(partial_state) == dict:\n",
    "            param_vector = partial_state['parameters']\n",
    "            iteration = partial_state['iterations']\n",
    "            print('Loaded parameter_vector as', param_vector)\n",
    "            return param_vector, iteration\n",
    "        else:\n",
    "            print('Outdated partial file detected! Unexpected behaviour may occur.')\n",
    "            param_vector = partial_state\n",
    "            print('Loaded parameter_vector as', param_vector)\n",
    "        return param_vector, 0\n",
    "\n",
    "    def fit(self, x, y, initial_parameters=None, detailed_results=False, load_state=None, callback_interval=None):\n",
    "        \"\"\"\n",
    "        Fits the current model to the given x and y data. If no initial parameters are given then random ones will be\n",
    "        chosen. Optimal parameters are stored in the model for use in predict and returned in this function.\n",
    "\n",
    "        :param x: np.array\n",
    "            x data to fit\n",
    "        :param y: np.array\n",
    "            y data to fit\n",
    "        :param initial_parameters: list, optional\n",
    "            initial parameters to start optimizer\n",
    "        :param detailed_results: bool, optional\n",
    "            whether to return detailed results of optimization or just parameters\n",
    "        :param load_state: str, optional\n",
    "            file to load partial fit data from\n",
    "        :param callback_interval: int, optional\n",
    "            how often to save the optimization steps to file\n",
    "        :return:\n",
    "            returns the optimal parameters found by optimizer. If detailed_results=True and optimizer is scipy, then\n",
    "            will be of type scipy optimizer results stored in dictionary.\n",
    "        \"\"\"\n",
    "        self.fit_count = 0\n",
    "        with open('model_log.csv', 'w') as outfile:\n",
    "            outfile.write('Time,Iteration,Cost,Parameters')\n",
    "            outfile.write('\\n')\n",
    "        self.callback_interval = callback_interval\n",
    "        if load_state is not None:\n",
    "            param_vector, self.fit_count = self._load_partial_state(load_state)\n",
    "            initial_parameters = param_vector\n",
    "        elif initial_parameters is None:\n",
    "            num_params = self._num_params() * self._re_upload_depth\n",
    "            generator = np.random.default_rng(12958234)\n",
    "            initial_parameters = generator.uniform(-np.pi, np.pi, num_params)\n",
    "            if self.postprocess is not None:\n",
    "                additional_num_params = self.num_qubits\n",
    "                additional_params = generator.uniform(-1, 1, additional_num_params)\n",
    "                initial_parameters = np.concatenate((initial_parameters, additional_params))\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        params = initial_parameters\n",
    "\n",
    "        if self.use_scipy:\n",
    "            options = {\n",
    "                'maxiter': self.max_iterations - self.fit_count,\n",
    "                'tol': self._tol\n",
    "            }\n",
    "            opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,\n",
    "                                  options=options)\n",
    "            self.params = opt_result['x']\n",
    "        elif self.optimizer == 'BasinHopping':\n",
    "            minimizer_kwargs = {\"method\": \"BFGS\"}\n",
    "            opt_result = basinhopping(self._cost_wrapper, x0=params, minimizer_kwargs=minimizer_kwargs,\n",
    "                                      accept_test=BasinBounds(xmax=np.pi, xmin=-np.pi), niter=self.max_iterations,\n",
    "                                      callback=self._callback)\n",
    "            self.params = opt_result['x']\n",
    "        else:\n",
    "            opt = qml.SPSAOptimizer(maxiter=self.max_iterations)\n",
    "            cost = []\n",
    "            for _ in range(self.max_iterations):\n",
    "                params, temp_cost = opt.step_and_cost(self._cost_wrapper, params)\n",
    "                cost.append(temp_cost)\n",
    "                self._callback(params)\n",
    "            opt_result = [params, cost]\n",
    "            self.params = params\n",
    "        self._save_partial_state(params, force=False)\n",
    "        # self._save_partial_state(params, force=True)\n",
    "        if detailed_results:\n",
    "            for key, value in opt_result.items():\n",
    "                if type(value) is np.ndarray:\n",
    "                    value = value.tolist()\n",
    "                    for i, x in enumerate(value):\n",
    "                        if type(x) is np.bool_:\n",
    "                            value[i] = bool(x)\n",
    "                    opt_result[key] = value\n",
    "                elif type(value) is np.bool_:\n",
    "                    value = bool(value)\n",
    "                    opt_result[key] = value\n",
    "            with open('detailed_results.json', 'w') as outfile:\n",
    "                try:\n",
    "                    json.dump(opt_result, outfile)\n",
    "                except:\n",
    "                    print('Could not dump detailed results. Not json serializable. ')\n",
    "        return self.params\n",
    "\n",
    "    def predict(self, x, params=None):\n",
    "        \"\"\"\n",
    "        Predicts a set of output data given a set of input data x using the trained parameters found with fit\n",
    "\n",
    "        :param x: np.array\n",
    "            x data to predict outputs of in the model\n",
    "        :param params: list\n",
    "            optional parameters to use in prediction, used for internal cost functions.\n",
    "        :raises ValueError:\n",
    "            if fit is not first called then raises error explaining that the model must first be trained\n",
    "        :return: np.ndarray\n",
    "            predicted values corresponding to each datapoint in x\n",
    "        \"\"\"\n",
    "        f = self.hyperparameters['f']\n",
    "        if params is None:\n",
    "            # if no parameters are passed then we are predicting the fitted model, so we use the stored parameters.\n",
    "            params = self.params\n",
    "\n",
    "        if self.postprocess is None:\n",
    "            return [f * self.qnode(features=features, parameters=params) for features in x]\n",
    "        else:\n",
    "            return [np.dot(f * np.array(self.qnode(features=features, parameters=params[:-self.num_qubits])),\n",
    "                           params[-self.num_qubits:]) for features in x]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dafed2b-aa53-4836-b73d-dd8722208cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = QuantumRegressor(**kwargs)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867da149-12fa-487d-87ac-d24d6885fdce",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(model.predict(X_train),y_train)\n",
    "plt.scatter(model.predict(X_test),y_test)\n",
    "plt.scatter(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ec2bb7-5c6a-42a3-aa0d-48927007a375",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qchem",
   "language": "python",
   "name": "qchem"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
